[
    {
        "api_name": "jittor_core.Var.abs",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor_core.Var.abs",
        "api_signature": "jittor_core.Var.abs()",
        "api_description": "函数C++定义格式:\njt.Var abs(jt.Var x)\n创建一个张量，其将输入变量 x 中的每一个数值转换为其绝对值。\n\\[out_i = |input_i|\\]",
        "return_value": "返回一个新的张量, 其数值是 x 中对应位置的绝对值",
        "parameters": "x (Var): Var类型的张量",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> jt.float32([-1, 0, 1]).abs()\njt.Var([1. 0. 1.], dtype=float32)"
    },
    {
        "api_name": "jittor.abs_",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.abs_",
        "api_signature": "jittor.abs_(x)",
        "api_description": "计算给定输入的张量 x 的绝对值，并直接在原地进行修改。数学公式如下:\n\\[y = |x|\\]\n其中 \\(y\\) 是输出张量，\\(x\\) 是输入张量。",
        "return_value": "计算后的原地修改的张量(Var)，维度和输入相同。",
        "parameters": "x (Var): 要进行绝对值计算的输入张量。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> x = jt.array([-1, -2, 3])\n>>> jt.abs_(x)\njt.Var([1 2 3], dtype=int32)"
    },
    {
        "api_name": "jittor_core.Var.acos",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor_core.Var.acos",
        "api_signature": "jittor_core.Var.acos()",
        "api_description": "函数C++定义格式:\njt.Var acos(jt.Var x)\n创建一个张量, 其将输入变量 x 中的每一个数值进行反余弦运算。\n\\[y_i = \\cos^{-1}(x_i)\\]",
        "return_value": "返回一个新的张量, 其数值是 x 中对应位置进行反余弦运算的结果",
        "parameters": "x (Var): Var类型的张量",
        "input_shape": "",
        "notes": "反余弦函数的定义域是 [-1, 1] ，值域是[0,pi]， 如果定义域不在规定范围内， 返回 nan\n支持使用 jt.arccos() 进行调用\n同 jt.acos() 函数",
        "code_example": ">>> x = jt.randn(5)\n>>> x\njt.Var([-0.8017247   2.4553642  -0.57173574 -0.06912863  1.5478854 ], dtype=float32)\n>>> x.arccos()\njt.Var([2.5009716       nan 2.1794162 1.6399801       nan], dtype=float32)"
    },
    {
        "api_name": "jittor_core.Var.acosh",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor_core.Var.acosh",
        "api_signature": "jittor_core.Var.acosh()",
        "api_description": "函数C++定义格式:\njt.Var acosh(jt.Var x)\n创建一个张量, 其将输入变量 x 中的每一个元素进行反双曲余弦运算。\n\\[y_i = \\cosh^{-1}(x_i) \\]",
        "return_value": "返回一个新的张量, 其数值是 x 中对应位置进行反双曲余弦运算的结果",
        "parameters": "x (Var): Var类型的张量",
        "input_shape": "",
        "notes": "该函数的定义域是[1, ∞)，值域是实数集， 如果定义域不在规定范围内， 返回 nan\n支持使用 x.arccosh() 进行调用\n同 jt.arccosh()",
        "code_example": ">>> x = jt.randn(5) + 1\n>>> x\njt.Var([-0.46972263 -0.6531948   1.7161269   1.007987    1.3451817 ], dtype=float32)\n>>> x.acosh()\njt.Var([      nan       nan 1.1348774 0.1263045 0.8086661], dtype=float32)"
    },
    {
        "api_name": "jittor.optim.AdamW",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.optim.html#jittor.optim.AdamW",
        "api_signature": "jittor.optim.AdamW(params, lr, eps=1e-08, betas=(0.9, 0.999)",
        "api_description": "AdamW 优化器，是 Adam 优化器的一个变体，其中对权重衰减进行了修正。\nstep 更新操作数学描述如下:\n\\[\\begin{split}\\begin{aligned}\nt &\\leftarrow t + 1 \\\\\n\\widetilde{\\eta_t} &\\leftarrow \\eta_t \\times a^{t} \\\\\ng_t &\\leftarrow abla_{\\theta} L_t(\\theta) + \\lambda \\theta \\\\\nm_t &\\leftarrow \\beta_{1_t} m_{t-1} + (1 - \\beta_{1_t}) g_t \\\\\n\\widetilde{m_t} &\\leftarrow \\frac{m_t}{1 - \\beta_{1_t}^t} \\\\\nv_t &\\leftarrow \\beta_{2_t} v_{t-1} + (1 - \\beta_{2_t}) g_t^2 \\\\\n\\widetilde{v_t} &\\leftarrow \\frac{v_t}{1 - \\beta_{2_t}^t} \\\\\n\\theta &\\leftarrow \\theta - \\widetilde{\\eta_t} abla_{\\theta} L_t(\\theta) \\\\\n\\end{aligned}\\end{split}\\]\n其中， \\(\\theta\\) 是参数， \\(g\\) 是梯度， \\(v\\) 是梯度的平方，\n\\(m\\) 是梯度的指数移动平均值， \\({m_t}\\) 是偏差修正后的梯度的移动平均值,\n\\({v_t}\\) 是偏差修正后的梯度的平方的移动平均值， \\(\\eta\\) 是学习率，\n\\(\\beta_{1_t}\\) 和 \\(\\beta_{2_t}\\) 是梯度和梯度平方动量项的系数，\n\\(\\lambda\\) 是权重衰减系数。",
        "return_value": "",
        "parameters": "params（iterable）：待优化参数的迭代器，或是定义了参数组的字典。\nlr (float)：学习率。\neps (float)：为了增加数值计算的稳定性而加到分母里的项。默认值: 1e-8\nbetas (Tuple[float, float])：计算一阶动量和二阶动量的指数衰减率元组。默认值: (0.9, 0.999)",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> model = jt.nn.Linear(10, 2)\n>>> loss_fn = jt.nn.CrossEntropyLoss()\n>>> optimizer = jt.optim.AdamW(params=model.parameters(), lr=0.1, eps=1e-8, betas=(0.9, 0.999))\n>>> x = jt.randn([5, 10])\n>>> y_true = jt.array([0, 1, 0, 1, 1])\n>>> y_pred = model(x)\n>>> loss = loss_fn(y_pred, y_true)\n>>> optimizer.step(loss)"
    },
    {
        "api_name": "jittor.optim.Adam",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.optim.html#jittor.optim.Adam",
        "api_signature": "jittor.optim.Adam(params, lr, eps=1e-08, betas=(0.9, 0.999)",
        "api_description": "Adam优化器，它结合了Momentum和RMSprop两种优化方法的主要观点，优化公式如下：\n\\[\\begin{split}m_t &= b1 \\times m_{t-1} + (1 - b1) \\times g_t \\\\\nv_t &= b2 \\times v_{t-1} + (1 - b2) \\times g_t^2 \\\\\np_t &= p_{t-1} - \\frac{lr \\times m_t} {\\sqrt{v_t} + eps}\\end{split}\\]\n其中，\\(m_t\\) 和 \\(v_t\\) 分别是梯度的一阶矩估计和二阶矩估计，\\(b1\\), \\(b2\\) 是估计的系数，\\(lr\\) 是学习率，\\(eps\\) 是用于数值稳定性的项，\\(g_t\\) 是梯度，\\(p_t\\) 是参数。",
        "return_value": "",
        "parameters": "loss (Var, 可选): 已经计算好的神经网络loss。默认值: None\nretain_graph (bool, 可选): 是否保留计算图。默认值: False",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> optimizer = jt.optim.Adam(model.parameters(), lr, eps=1e-8, betas=(0.9, 0.999))\n>>> optimizer.step(loss)"
    },
    {
        "api_name": "jittor.optim.Adan",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.optim.html#jittor.optim.Adan",
        "api_signature": "jittor.optim.Adan(params, lr=0.001, betas=(0.98, 0.92, 0.99)",
        "api_description": "实现Adan优化器。它在 Adan: Adaptive Nesterov Momentum Algorithm for Faster Optimizing Deep Models[J].arXiv preprint arXiv:2208.06677, 2022 中提出。\nAdan是大多数DNN框架的高效优化器，计算负载比其他最新方法小约2倍，对训练设置和批处理大小具有稳健性，易于即插即用。",
        "return_value": "",
        "parameters": "loss (Var, 可选): 已经计算好的神经网络loss。默认值: None\nretain_graph (bool, 可选): 是否保留计算图。默认值: False",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> optimizer = jt.optim.Adan(model.parameters(), lr=0.001)\n>>> optimizer.step(loss)"
    },
    {
        "api_name": "jittor.nn.AdaptiveAvgPool2d",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.nn.html#jittor.nn.AdaptiveAvgPool2d",
        "api_signature": "jittor.nn.AdaptiveAvgPool2d(output_size)",
        "api_description": "对输入进行二维自适应平均池化处理的类。\n参数:\noutput_size  (int, tuple, list) : 期望的输出形状。\n形状:\n输入: \\([N, C, H, W]\\)。\n输出: \\([N, C, S_0, S_1]\\), 此处  (S_0, S_1) = output_size 。\n属性:\noutput_size (int, tuple, list) : 期望的输出形状。\n代码示例:>>> m = nn.AdaptiveAvgPool2d((5, 7))  # target output size of 5x7\n>>> input = jt.randn(1, 64, 8, 9)\n>>> output = m(input)\n>>> m = nn.AdaptiveAvgPool2d(7) # target output size of 7x7 (square)\n>>> input = jt.randn(1, 64, 10, 9)\n>>> output = m(input)\n>>> m = nn.AdaptiveAvgPool2d((None, 7)) # target output size of 10x7\n>>> input = jt.randn(1, 64, 10, 9)\n>>> output = m(input)",
        "return_value": "",
        "parameters": "output_size  (int, tuple, list) : 期望的输出形状。",
        "input_shape": "输入: \\([N, C, H, W]\\)。\n输出: \\([N, C, S_0, S_1]\\), 此处  (S_0, S_1) = output_size 。",
        "notes": "",
        "code_example": ">>> m = nn.AdaptiveAvgPool2d((5, 7))  # target output size of 5x7\n>>> input = jt.randn(1, 64, 8, 9)\n>>> output = m(input)\n>>> m = nn.AdaptiveAvgPool2d(7) # target output size of 7x7 (square)\n>>> input = jt.randn(1, 64, 10, 9)\n>>> output = m(input)\n>>> m = nn.AdaptiveAvgPool2d((None, 7)) # target output size of 10x7\n>>> input = jt.randn(1, 64, 10, 9)\n>>> output = m(input)"
    },
    {
        "api_name": "jittor.nn.AdaptiveAvgPool3d",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.nn.html#jittor.nn.AdaptiveAvgPool3d",
        "api_signature": "jittor.nn.AdaptiveAvgPool3d(output_size)",
        "api_description": "对输入进行三维自适应平均池化处理的类。\n参数:\noutput_size  (int, tuple, list) : 期望的输出形状。\n形状:\n输入: \\([N, C, D, H, W]\\)\n输出: \\([N, C, S_0, S_1, S_2]\\), 此处  (S_0, S_1, S_2) = output_size 。\n属性:\noutput_size (int, tuple, list) : 期望的输出形状。\n代码示例:>>> # target output size of 5x7x9\n>>> m = nn.AdaptiveAvgPool3d((5, 7, 9))\n>>> input = jt.randn(1, 64, 8, 9, 10)\n>>> output = m(input)\n>>> # target output size of 7x7x7 (cube)\n>>> m = nn.AdaptiveAvgPool3d(7)\n>>> input = jt.randn(1, 64, 10, 9, 8)\n>>> output = m(input)",
        "return_value": "",
        "parameters": "output_size  (int, tuple, list) : 期望的输出形状。",
        "input_shape": "输入: \\([N, C, D, H, W]\\)\n输出: \\([N, C, S_0, S_1, S_2]\\), 此处  (S_0, S_1, S_2) = output_size 。",
        "notes": "",
        "code_example": ">>> # target output size of 5x7x9\n>>> m = nn.AdaptiveAvgPool3d((5, 7, 9))\n>>> input = jt.randn(1, 64, 8, 9, 10)\n>>> output = m(input)\n>>> # target output size of 7x7x7 (cube)\n>>> m = nn.AdaptiveAvgPool3d(7)\n>>> input = jt.randn(1, 64, 10, 9, 8)\n>>> output = m(input)"
    },
    {
        "api_name": "id0",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.nn.html#id0",
        "api_signature": "jittor.nn.AdaptiveMaxPool2d(output_size, return_indices=False)",
        "api_description": "对输入进行二维自适应最大池化处理的类。\n参数:\noutput_size (int, tuple, list) : 期望的输出形状。\nreturn_indices(bool, optional): 是否返回最大值的索引。默认值: False。\n形状:\n输入 : \\([N, C, H, W]\\)\n输出 : \\([N, C, S_0, S_1]\\), 此处  (S_0, S_1) = output_size 。\n属性:\noutput_size (int, tuple, list) : 期望的输出形状。\nreturn_indices (bool) : 是否返回最大值的索引。\n代码示例:>>> # target output size of 5x7\n>>> m = nn.AdaptiveMaxPool2d((5, 7))\n>>> input = jt.randn(1, 64, 8, 9)\n>>> output = m(input)\n>>> # target output size of 7x7 (square)\n>>> m = nn.AdaptiveMaxPool2d(7)\n>>> input = jt.randn(1, 64, 10, 9)\n>>> output = m(input)\n>>> # target output size of 10x7\n>>> m = nn.AdaptiveMaxPool2d((None, 7))\n>>> input = jt.randn(1, 64, 10, 9)\n>>> output = m(input)",
        "return_value": "",
        "parameters": "output_size (int, tuple, list) : 期望的输出形状。\nreturn_indices(bool, optional): 是否返回最大值的索引。默认值: False。",
        "input_shape": "输入 : \\([N, C, H, W]\\)\n输出 : \\([N, C, S_0, S_1]\\), 此处  (S_0, S_1) = output_size 。",
        "notes": "",
        "code_example": ">>> # target output size of 5x7\n>>> m = nn.AdaptiveMaxPool2d((5, 7))\n>>> input = jt.randn(1, 64, 8, 9)\n>>> output = m(input)\n>>> # target output size of 7x7 (square)\n>>> m = nn.AdaptiveMaxPool2d(7)\n>>> input = jt.randn(1, 64, 10, 9)\n>>> output = m(input)\n>>> # target output size of 10x7\n>>> m = nn.AdaptiveMaxPool2d((None, 7))\n>>> input = jt.randn(1, 64, 10, 9)\n>>> output = m(input)"
    },
    {
        "api_name": "jittor.nn.AdaptiveMaxPool2d",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.nn.html#jittor.nn.AdaptiveMaxPool2d",
        "api_signature": "jittor.nn.AdaptiveMaxPool2d(output_size, return_indices=False)",
        "api_description": "对输入进行二维自适应最大池化处理的类。\n参数:\noutput_size (int, tuple, list) : 期望的输出形状。\nreturn_indices(bool, optional): 是否返回最大值的索引。默认值: False。\n形状:\n输入 : \\([N, C, H, W]\\)\n输出 : \\([N, C, S_0, S_1]\\), 此处  (S_0, S_1) = output_size 。\n属性:\noutput_size (int, tuple, list) : 期望的输出形状。\nreturn_indices (bool) : 是否返回最大值的索引。\n代码示例:>>> # target output size of 5x7\n>>> m = nn.AdaptiveMaxPool2d((5, 7))\n>>> input = jt.randn(1, 64, 8, 9)\n>>> output = m(input)\n>>> # target output size of 7x7 (square)\n>>> m = nn.AdaptiveMaxPool2d(7)\n>>> input = jt.randn(1, 64, 10, 9)\n>>> output = m(input)\n>>> # target output size of 10x7\n>>> m = nn.AdaptiveMaxPool2d((None, 7))\n>>> input = jt.randn(1, 64, 10, 9)\n>>> output = m(input)",
        "return_value": "",
        "parameters": "output_size (int, tuple, list) : 期望的输出形状。\nreturn_indices(bool, optional): 是否返回最大值的索引。默认值: False。",
        "input_shape": "输入 : \\([N, C, H, W]\\)\n输出 : \\([N, C, S_0, S_1]\\), 此处  (S_0, S_1) = output_size 。",
        "notes": "",
        "code_example": ">>> # target output size of 5x7\n>>> m = nn.AdaptiveMaxPool2d((5, 7))\n>>> input = jt.randn(1, 64, 8, 9)\n>>> output = m(input)\n>>> # target output size of 7x7 (square)\n>>> m = nn.AdaptiveMaxPool2d(7)\n>>> input = jt.randn(1, 64, 10, 9)\n>>> output = m(input)\n>>> # target output size of 10x7\n>>> m = nn.AdaptiveMaxPool2d((None, 7))\n>>> input = jt.randn(1, 64, 10, 9)\n>>> output = m(input)"
    },
    {
        "api_name": "jittor_core.Var.add",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor_core.Var.add",
        "api_signature": "jittor_core.Var.add()",
        "api_description": "函数C++定义格式:\njt.Var add(jt.Var x, jt.Var y)\n对两个张量中对应元素计算求和，也可以使用 + 运算符调用",
        "return_value": "x 和 y 每对对应元素相加结果，形状与 x 和 y 相同",
        "parameters": "x (Var): 输入数据\ny (Var): 输入数据，形状与 x 相同",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> x = jt.array([1, 2, 4])\n>>> y = jt.array([3, 4, 5])\n>>> x.add(y)\njt.Var([4 6 9], dtype=int32)\n>>> x + y\njt.Var([4 6 9], dtype=int32)"
    },
    {
        "api_name": "jittor.add_",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.add_",
        "api_signature": "jittor.add_(x, y)",
        "api_description": "函数是 \\(add()\\) 的 原地替换版本,即结果会在原始变量x上就地进行修改，不会产生新的变量。",
        "return_value": "返回的是经过添加操作后的x(Var)。",
        "parameters": "x(Jittor.Var)：第一个输入变量，其值会被函数的结果替代。\ny(Jittor.Var)：第二个输入变量。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> x = jt.array([1.0, 2.0, 3.0])\n>>> y = jt.array([4.0, 5.0, 6.0])\n>>> jt.add_(x, y)\njt.Var([5.0, 7.0, 9.0], dtype=float32)"
    },
    {
        "api_name": "jittor.transform.adjust_brightness",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.transform.html#jittor.transform.adjust_brightness",
        "api_signature": "jittor.transform.adjust_brightness(img, brightness_factor)",
        "api_description": "调整RGB图像的亮度。",
        "return_value": "PIL.Image.Image: 亮度调整后的图像",
        "parameters": "img (PIL.Image.Image): 输入图像\nbrightness_factor (float): 调整亮度的程度。可以为任何非负数。0会得到一个黑色的图像，1会得到原始图像，2则会将亮度提高一倍",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> import numpy as np\n>>> from PIL import Image\n>>> data = np.random.rand(200,200, 3)\n>>> img = Image.fromarray(data, 'RGB')   \n>>> img.show() # RGB噪声图\n>>> img_black = jt.transform.adjust_brightness(img, 0) \n>>> img_black.show() # 亮度调整为0，即全黑图"
    },
    {
        "api_name": "jittor.transform.adjust_contrast",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.transform.html#jittor.transform.adjust_contrast",
        "api_signature": "jittor.transform.adjust_contrast(img, contrast_factor)",
        "api_description": "调整 RGB 图像的对比度。",
        "return_value": "返回调整对比度后的图像。",
        "parameters": "img (Image.Image): 需要调整的图像。\ncontrast_factor (float): 调整对比度的程度。可以是任意非负数。0 会将图像转换为纯灰色，1 保持原图，2 增加两倍对比度。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import numpy as np\n>>> from PIL import Image\n>>> from jittor import transform\n>>> img = np.ones((100, 100, 3)).astype(np.uint8)\n>>> img = Image.fromarray(img)\n>>> img_adjusted = transform.adjust_contrast(img, 0.5)"
    },
    {
        "api_name": "jittor.transform.adjust_gamma",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.transform.html#jittor.transform.adjust_gamma",
        "api_signature": "jittor.transform.adjust_gamma(img, gamma, gain=1)",
        "api_description": "图像的伽玛校正，也被称为幂律转换。更多详细信息，请参见 WIKI。基于以下公式对RGB模式的强度进行调整：\n\\[I_{\\text{out}} = 255 \\times \\text{gain} \\times (\\frac{I_{\\text{in}}}{255})^{\\gamma}\\]",
        "return_value": "PIL.Image.Image: 伽马校正后的图像",
        "parameters": "img (PIL.Image.Image): 输入图像\ngamma (float): 非负实数，即上述公式中的 \\(\\gamma\\) 。 gamma大于1使阴影变暗，gamma小于1使暗区变亮。\ngain (float, optional): 常数乘数，默认值: 1",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> import numpy as np\n>>> from PIL import Image\n>>> data = np.random.rand(200,200, 3)\n>>> img = Image.fromarray(data, 'RGB')   \n>>> img.show() # RGB噪声图\n>>> jt.transform.adjust_gamma(img, 0.2).show() # 暗区变亮"
    },
    {
        "api_name": "jittor.transform.adjust_hue",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.transform.html#jittor.transform.adjust_hue",
        "api_signature": "jittor.transform.adjust_hue(img, hue_factor)",
        "api_description": "调整图像的色调。先将图像转换为HSV空间，并在色调通道（H）中循环移动强度来调整的；然后，再转换回原始图像模式。 hue_factor 是H通道中的移动量，必须在`[-0.5，0.5]`内。更多详细信息，请参见 Hue",
        "return_value": "PIL.Image.Image: 色调调整后的图像",
        "parameters": "img (PIL.Image.Image): 输入图像\nhue_factor (float): 色调通道移动量。取值应在[-0.5, 0.5]之间。0.5和-0.5分别在HSV空间中以正和负方向给出色调通道的完全反转。0表示无移动。因此，-0.5和0.5都会给出具有互补色的图像，而0则给出原始图像。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> import numpy as np\n>>> from PIL import Image\n>>> data = np.random.rand(200,200, 3)\n>>> img = Image.fromarray(data, 'RGB')   \n>>> img.show() # RGB噪声图\n>>> img_complementary = jt.transform.adjust_hue(img, 0.5) \n>>> img_complementary.show() # 色调反转，即互补色图"
    },
    {
        "api_name": "jittor.transform.adjust_saturation",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.transform.html#jittor.transform.adjust_saturation",
        "api_signature": "jittor.transform.adjust_saturation(img, saturation_factor)",
        "api_description": "调整图像饱和度。",
        "return_value": "PIL.Image.Image: 饱和度调整后的图像",
        "parameters": "img (PIL.Image.Image): 输入图像\nsaturation_factor (float): 调整饱和度的程度。0将产生黑白图像，1将给出原始图像，2将增强2倍的饱和度",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> import numpy as np\n>>> from PIL import Image\n>>> data = np.random.rand(200,200, 3)\n>>> img = Image.fromarray(data, 'RGB')   \n>>> img.show() # RGB噪声图\n>>> img_blackwhite = jt.transform.adjust_saturation(img, 0) \n>>> img_blackwhite.show() # 饱和度调整为0，即黑白图"
    },
    {
        "api_name": "jittor.nn.affine_grid",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.nn.html#jittor.nn.affine_grid",
        "api_signature": "jittor.nn.affine_grid(theta, size, align_corners=False)",
        "api_description": "根据给定的尺寸生成一个4D或5D的仿射网格。",
        "return_value": "Var: 生成的仿射网格，形状为 (N, C, H, W, 2) 或 (N, C, D, H, W, 3)，具体取决于 size 的长度",
        "parameters": "theta (Var): 仿射变换矩阵，对于4D网格，形状为 (N, 2, 3)；对于5D网格，形状为 (N, 3, 4)\nsize (list of int): 定义输出网格的尺寸，长度为4时表示4D网格，长度为5时表示5D网格\nalign_corners (bool): 是否对齐角点，默认为 False",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> # 4D网格示例\n>>> theta_4d = jt.array([[[1., 0, 0], [0, 1., 0]]])\n>>> size_4d = [1, 3, 5, 5]\n>>> grid_4d = affine_grid(theta_4d, size_4d)\n>>> print(grid_4d.shape)  # Output: (1, 5, 5, 2)\n>>> # 5D网格示例\n>>> theta_5d = jt.array([[[1., 0, 0, 0], [0, 1., 0, 0], [0, 0, 1., 0]]])\n>>> size_5d = [1, 3, 4, 5, 5]\n>>> grid_5d = affine_grid(theta_5d, size_5d)\n>>> print(grid_5d.shape)  # Output: (1, 4, 5, 5, 3)"
    },
    {
        "api_name": "jittor.nn.affine_grid_generator_4D",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.nn.html#jittor.nn.affine_grid_generator_4D",
        "api_signature": "jittor.nn.affine_grid_generator_4D(theta, N, C, H, W, align_corners)",
        "api_description": "生成一个四维的仿射网格，并使用给定的仿射矩阵在特征维度上对其进行变换。",
        "return_value": "生成的四维仿射网格 (Var)，形状为 (N, H, W, 2)",
        "parameters": "theta (Var): 仿射变换矩阵，形状为 (N, 2, 3)\nN (int): 批次大小\nC (int): 通道数\nH (int): 输出高度\nW (int): 输出宽度\nalign_corners (bool): 控制网格对齐方式的布尔值",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> theta = jt.array([[[1., 0, 0], [0, 1., 0]]])\n>>> N = 1\n>>> C = 3\n>>> H = 5\n>>> W = 5\n>>> align_corners = True\n>>> grid = affine_grid_generator_4D(theta, N, C, H, W, align_corners)\n>>> grid.shape\n [1,5,5,2,]"
    },
    {
        "api_name": "jittor.nn.affine_grid_generator_5D",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.nn.html#jittor.nn.affine_grid_generator_5D",
        "api_signature": "jittor.nn.affine_grid_generator_5D(theta, N, C, D, H, W, align_corners)",
        "api_description": "用于生成一个五维的仿射网格。该函数首先通过对给定的仿射矩阵和基础网格进行矩阵乘法，然后通过变换生成新的仿射网格。",
        "return_value": "生成的五维仿射网格 (Var)，大小为 (N, D, H, W, 3)",
        "parameters": "theta (Var): 仿射变换矩阵，形状为 (N, 3, 4)\nN (int): 输出网格的批次大小\nC (int): 输出网格的通道数\nD (int): 输出网格的深度\nH (int): 输出网格的高度\nW (int): 输出网格的宽度\nalign_corners (bool): 是否对齐角点",
        "input_shape": "",
        "notes": "",
        "code_example": "theta = jt.array([[[1., 0, 0, 0], [0, 1., 0, 0], [0, 0, 1., 0]]])\nN = 1\nC = 3\nD = 4\nH = 5\nW = 5\nalign_corners = True\ngrid = affine_grid_generator_5D(theta, N, C, D, H, W, align_corners)\nprint(grid.shape)  # Output: (1, 4, 5, 5, 3)"
    },
    {
        "api_name": "jittor.models.alexnet",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.models.html#jittor.models.alexnet",
        "api_signature": "jittor.models.alexnet(pretrained=False, **kwargs)",
        "api_description": "构建一个AlexNet模型\nAlexNet模型源于 ImageNet Classification with Deep Convolutional Neural Networks 。\n参数:\npretrained (bool, optional): 表示是否预加载预训练模型。默认为 False。\n**kwargs (dict, optional): AlexNet模型的其他参数。默认为 `None。\n返回值:\n返回构建好的AlexNet模型实例。如果 pretrained 为 True, 则返回在ImageNet上预训练的模型。\n代码示例:>>> import jittor as jt\n>>> from jittor.models.alexnet import *\n>>> net = alexnet(pretrained=False)\n>>> x = jt.rand(1, 3, 224, 224)\n>>> y = net(x)\n>>> y.shape\n[1, 1000]",
        "return_value": "返回构建好的AlexNet模型实例。如果 pretrained 为 True, 则返回在ImageNet上预训练的模型。",
        "parameters": "pretrained (bool, optional): 表示是否预加载预训练模型。默认为 False。\n**kwargs (dict, optional): AlexNet模型的其他参数。默认为 `None。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> from jittor.models.alexnet import *\n>>> net = alexnet(pretrained=False)\n>>> x = jt.rand(1, 3, 224, 224)\n>>> y = net(x)\n>>> y.shape\n[1, 1000]"
    },
    {
        "api_name": "jittor.models.AlexNet",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.models.html#jittor.models.AlexNet",
        "api_signature": "jittor.models.AlexNet(num_classes=1000)",
        "api_description": "AlexNet模型源于 ImageNet Classification with Deep Convolutional Neural Networks, 其架构来自于论文 One weird trick for parallelizing convolutional neural networks。\n参数:\nnum_classes (int, optional): 分类任务中类别的数量, 默认值: 1000。\n属性:\nfeatures (nn.Sequential): AlexNet模型的特征提取部分。\navgpool (nn.AdaptiveAvgPool2d): 用于将特征提取部分的输出转换为固定大小的特征图。\nclassifier (nn.Sequential): AlexNet模型的分类部分。\n代码示例:>>> import jittor as jt\n>>> from jittor import nn\n>>> from jittor.models import AlexNet\n>>> model = AlexNet(500) # 创建一个有500个类别的AlexNet模型\n>>> x = jt.random([10, 3, 224, 224]) # 创建一个随机的图像数据批次\n>>> y = model(x) # 得到模型的输出\n>>> # y的形状将会是 [10, 500], 表示10幅图像在500个类别上的分类结果",
        "return_value": "",
        "parameters": "num_classes (int, optional): 分类任务中类别的数量, 默认值: 1000。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> from jittor import nn\n>>> from jittor.models import AlexNet  \n>>> model = AlexNet(500) # 创建一个有500个类别的AlexNet模型\n>>> x = jt.random([10, 3, 224, 224]) # 创建一个随机的图像数据批次\n>>> y = model(x) # 得到模型的输出\n>>> # y的形状将会是 [10, 500], 表示10幅图像在500个类别上的分类结果"
    },
    {
        "api_name": "jittor.misc.all",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.misc.all",
        "api_signature": "jittor.misc.all(x, dim=()",
        "api_description": "在指定维度上计算逻辑与",
        "return_value": "计算逻辑与的结果，形状为 x 去掉 dim 所选维度，元素类型为 bool",
        "parameters": "x (Var): 被计算逻辑与的张量\ndim (int | tuple[int, …]): 计算逻辑与的维度，默认为全部维度，即全部元素逻辑与得到一个元素",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> x = jt.randn(2, 3) > 0\n>>> x\njt.Var([[ True  True False]\n        [ True False  True]], dtype=bool)\n>>> jt.all(x)\njt.Var([False], dtype=bool)\n>>> jt.all(x, 0)\njt.Var([ True False False], dtype=bool)\n>>> jt.all(x, 1)\njt.Var([False False], dtype=bool)"
    },
    {
        "api_name": "jittor_core.Var.all_",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor_core.Var.all_",
        "api_signature": "jittor_core.Var.all_()",
        "api_description": "函数C++定义格式:\njt.Var reduce_logical_and(jt.Var x,  int dim,  bool keepdims=false)\n对输入的数组计算指定维度上所有元素的逻辑与。\n如果输入 Var 中沿此维度的元素中全为 True，结果对应位置才是 True，否则为 False。 x 无论是什么数据类型，都将被转换为 bool 类型来计算。",
        "return_value": "Var: 返回一个新的数组，进行过一次或多次降维求逻辑与后的结果。",
        "parameters": "x(Var): 输入的张量\ndims(int, or Tuple of int): 要计算逻辑与的轴，如果是 Tuple 则在这些维度上依次进行降维求逻辑与计算，如果是 int 则在这一维度上进行降维求逻辑与计算。如果为空则对张量整体求逻辑与运算\nkeepdims(bool or int): 如果为 True，则进行求逻辑与的维度将被保留，否则消除这一维度。如果输入的是 int, 0 被认为是 False，非 0 被认为是 True。默认值：False",
        "input_shape": "",
        "notes": "jt.all_ 等价于 jt.reduce_logical_and",
        "code_example": ">>> a = jt.array([[1,1,1],[1,0,1],[0,1,1]])\n>>> jt.all_(a, 1)\njt.Var([1 0 0], dtype=int32)\n>>> jt.reduce_logical_and(a, 1, True)\njt.Var([[1]\n        [0]\n        [0]], dtype=int32)\n>>> jt.all_(a, (1, 0))\njt.Var([0], dtype=int32)\n>>> jt.reduce_logical_and(a, 0)\njt.Var([0 0 1], dtype=int32)"
    },
    {
        "api_name": "jittor.misc.all_equal",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.misc.all_equal",
        "api_signature": "jittor.misc.all_equal(a: Var, b: Var)",
        "api_description": "检查两个Jittor变量（jt.Var）是否完全相等。如果每一个元素都一致，函数则返回True，否则返回False。\n这个函数通过逐元素的比较来判断两个变量是否相等。首先，使用 a == b 操作进行元素级的比较。然后，通过 .all() 操作来验证所有元素是否与 b 一致。最后， .item() 操作将结果转化为Python的bool值。\n传入变量应该是Jittor的Var类型，a和b的尺寸应该是一样的。如果不一样，函数将抛出ValueError`。",
        "return_value": "bool类型，如果a和b所有元素相等，返回True，否则返回False",
        "parameters": "a (jt.Var) : 第一个Jittor变量，用于比较的输入\nb (jt.Var):  第二个Jittor变量，用于比较的输入",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> a = jt.array([1, 2, 3])\n>>> b = jt.array([1, 2, 3])\n>>> print(all_equal(a, b))\nTrue\n>>> a = jt.array([1, 2, 3])\n>>> b = jt.array([1, 2, 4])\n>>> print(all_equal(a, b))\nFalse"
    },
    {
        "api_name": "jittor.misc.any",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.misc.any",
        "api_signature": "jittor.misc.any(x, dim=()",
        "api_description": "在指定维度上计算逻辑或",
        "return_value": "计算逻辑或的结果，形状为 x 去掉 dim 所选维度，元素类型为 bool",
        "parameters": "x (Var): 被计算逻辑或的张量\ndim (int | tuple[int, …]): 计算逻辑或的维度，默认为全部维度，即全部元素逻辑或得到一个元素",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> x = jt.randn(2, 3) > 0\n>>> x\njt.Var([[ True False  True]\n        [False False  True]], dtype=bool)\n>>> jt.any(x)\njt.Var([ True], dtype=bool)\n>>> jt.any(x, 0)\njt.Var([ True False  True], dtype=bool)\n>>> jt.any(x, 1)\njt.Var([ True  True], dtype=bool)"
    },
    {
        "api_name": "jittor_core.Var.any_",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor_core.Var.any_",
        "api_signature": "jittor_core.Var.any_()",
        "api_description": "函数C++定义格式:\njt.Var reduce_logical_or(jt.Var x,  int dim,  bool keepdims=false)\n对输入的数组计算指定维度上所有元素的逻辑或。\n如果输入 Var 中沿此维度的元素中全为 False，结果对应位置才是 False，否则为 True。 x 无论是什么数据类型，都将被转换为 bool 类型来计算。",
        "return_value": "Var: 返回一个新的张量，进行过一次或多次降维求逻辑或后的结果。",
        "parameters": "x(Var): 输入的张量\ndims(int, or Tuple of int，可选): 要计算逻辑或的轴，如果是 Tuple 则在这些维度上依次进行降维求逻辑或计算，如果是 int 则在这一个维度上进行降维求逻辑或计算。如果为空则对张量整体求逻辑与运算\nkeepdims(bool or int，可选): 如果为 True，则进行求逻辑或的维度将被保留，否则消除这一维度。如果输入的是 int, 0 被认为是 False，非 0 被认为是 True。默认值：False",
        "input_shape": "",
        "notes": "jt.any_ 等价于 jt.reduce_logical_or",
        "code_example": ">>> a = jt.array([[0,0,0],[0,1,0],[1,0,0]])\n>>> jt.reduce_logical_or(a, 1)\njt.Var([0 1 1], dtype=int32)\n>>> jt.any_(a, 1, True)\njt.Var([[0]\n        [1]\n        [1]], dtype=int32)\n>>> jt.reduce_logical_or(a, (1, 0))\njt.Var([1], dtype=int32)\n>>> jt.any_(a, 0)\njt.Var([1 1 0], dtype=int32)"
    },
    {
        "api_name": "jittor.misc.arange",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.misc.arange",
        "api_signature": "jittor.misc.arange(start=0, end=None, step=1, dtype=None)",
        "api_description": "生成一个包含在区间[ start , end )内等间隔分布的数字的序列。",
        "return_value": "一个 1-D 的 Var 对象，包含从  start (包含)到 end (不包含)以 step 为步长的等差序列。",
        "parameters": "start(float | Var, 可选): 默认值=0, 序列的开始值。也可以是一个仅包含一个元素的 Var\nend(float | Var, 可选): , 默认值=None, 序列的结束值，也可以是一个仅包含一个元素的 Var\nstep(float | Var, 可选):, 默认值=1, 生成的数字序列之间的固定间隔，也可以是一个仅包含一个元素的 Var\ndtype(type, 可选): 把返回的 Var 转换为指定的数据类型。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> jt.arange(1, 3.5, 0.5, jt.float32)\njt.Var([1.  1.5 2.  2.5 3. ], dtype=float32)"
    },
    {
        "api_name": "jittor_core.Var.arccos",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor_core.Var.arccos",
        "api_signature": "jittor_core.Var.arccos()",
        "api_description": "函数C++定义格式:\njt.Var acos(jt.Var x)\n创建一个张量, 其将输入变量 x 中的每一个数值进行反余弦运算。\n\\[y_i = \\cos^{-1}(x_i)\\]",
        "return_value": "返回一个新的张量, 其数值是 x 中对应位置进行反余弦运算的结果",
        "parameters": "x (Var): Var类型的张量",
        "input_shape": "",
        "notes": "反余弦函数的定义域是 [-1, 1] ，值域是[0,pi]， 如果定义域不在规定范围内， 返回 nan\n支持使用 jt.arccos() 进行调用\n同 jt.acos() 函数",
        "code_example": ">>> x = jt.randn(5)\n>>> x\njt.Var([-0.8017247   2.4553642  -0.57173574 -0.06912863  1.5478854 ], dtype=float32)\n>>> x.arccos()\njt.Var([2.5009716       nan 2.1794162 1.6399801       nan], dtype=float32)"
    },
    {
        "api_name": "jittor_core.Var.arcsin",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor_core.Var.arcsin",
        "api_signature": "jittor_core.Var.arcsin()",
        "api_description": "函数C++定义格式:\njt.Var asin(jt.Var x)\n创建一个张量, 其将 x 中的每一个数值进行反正弦运算。\n\\[y_i = \\sin^{-1}(x_i)\\]",
        "return_value": "返回一个新的张量, 其数值是 x 中对应位置进行反正弦运算的结果",
        "parameters": "x (Var): Var类型的张量",
        "input_shape": "",
        "notes": "反正弦函数的定义域是 [-1, 1] ，值域是实数集， 如果定义域不在规定范围内， 返回 nan\n支持使用 jt.arcsin() 进行调用\n同 jt.asin() 函数",
        "code_example": ">>> x = jt.randn(5)\n>>> x\njt.Var([-1.6485653  -0.10077649 -0.7546536  -0.02543143  1.0830703 ], dtype=float32)\n>>> x.arcsin()\njt.Var([        nan -0.10094785 -0.85512596 -0.02543417         nan], dtype=float32)"
    },
    {
        "api_name": "jittor_core.Var.arcsinh",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor_core.Var.arcsinh",
        "api_signature": "jittor_core.Var.arcsinh()",
        "api_description": "函数C++定义格式:\njt.Var asinh(jt.Var x)\n创建一个张量, 其将 x 中的每一个数值进行反双曲正弦运算。\n\\[y_i = \\sinh^{-1}(x_i) = \\ln\\left(x_i + \\sqrt{x_i^2 + 1}\\right)\\]",
        "return_value": "返回一个新的张量, 其数值是 x 中对应位置进行反双曲正弦运算的结果",
        "parameters": "x (Var): Var类型的张量",
        "input_shape": "",
        "notes": "反正弦函数的定义域和值域是实数集\n支持使用 x.asinh() 进行调用\n同 jt.asinh()",
        "code_example": ">>> x = jt.randn(5)\n>>> x\njt.Var([ 0.50530916 -1.0814334   0.0039318   1.1737247  -1.4199417 ], dtype=float32)\n>>> x.arcsinh()\njt.Var([ 0.48595542 -0.9377998   0.00393179  0.99904275 -1.1495185 ], dtype=float32)"
    },
    {
        "api_name": "jittor_core.Var.arctan",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor_core.Var.arctan",
        "api_signature": "jittor_core.Var.arctan()",
        "api_description": "函数C++定义格式:\njt.Var atan(jt.Var x)\n创建一个张量, 其将 x 中的每一个数值进行反正切运算。\n\\[y_i = \\tan^{-1}(x_i)\\]",
        "return_value": "返回一个新的张量, 其数值是 x 中对应位置进行反正切运算的结果",
        "parameters": "x (Var): Var类型的张量",
        "input_shape": "",
        "notes": "该函数的定义域为实数集，值域为\n\n\n\\[\\left(-\\frac{\\pi}{2}, \\frac{\\pi}{2}\\right)\\]\n\n支持使用 jt.arctan() 进行调用\n同 jt.atan() 函数",
        "code_example": ">>> x = jt.randn(5)\n>>> x\njt.Var([-0.1146331  -0.22897322 -1.1813502   0.7146521  -0.02658333], dtype=float32)\n>>> x.arctan()\njt.Var([-0.1141349  -0.22509298 -0.86834407  0.6204921  -0.02657707], dtype=float32)"
    },
    {
        "api_name": "jittor.misc.arctan2",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.misc.arctan2",
        "api_signature": "jittor.misc.arctan2(y, x)",
        "api_description": "按元素计算y/x的反正切值，以弧度计算。注意第一个参数 y 中的元素是 y 坐标，第二个参数 x 中的元素是 x 坐标。\n\\[\\begin{split}\\text{arctan2}(y, x) =\n\\begin{cases}\n\\arctan(y / x) - \\pi, & \\text{ if } x \\lt 0, y \\lt 0 \\\\\n\\arctan(y / x) + \\pi, & \\text{ if } x \\lt 0, y \\geq 0 \\\\\n\\arctan(y / x), & \\text{ otherwise }\n\\end{cases}\\end{split}\\]\n参数：\ny (Var): Jittor数组，任何shape和dtype。\nx (Var): Jittor数组，与y相同的shape和dtype。+\n返回：\nJittor数组，具有和输入相同的shape和dtype。\n代码示例：\n>>> x = jt.randn(3)\njt.Var([0.8616086  0.81152385 0.0437891], dtype=float32)\n>>> y = jt.randn(3)\njt.Var([-0.0904924   1.4761028  -1.367789], dtype=float32)\n>>> c = jt.arctan2(y, x)\njt.Var([-0.10464364  1.0681262  -1.5387927], dtype=float32)",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "jittor_core.Var.arctanh",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor_core.Var.arctanh",
        "api_signature": "jittor_core.Var.arctanh()",
        "api_description": "函数C++定义格式:\njt.Var atanh(jt.Var x)\n创建一个张量, 其将 x 中的每一个数值进行反双曲正切运算。\n\\[atanh(x) = \\frac{1}{2} * log(\\frac{1 + x}{1 - x})\\]",
        "return_value": "返回一个新的张量, 其数值是 x 中对应位置进行反双曲正切运算的结果",
        "parameters": "x (Var): Var类型的张量",
        "input_shape": "",
        "notes": "该函数的定义域是(-1, 1)，值域是实数集，如果定义域不在规定范围内， 返回 nan\n支持使用 jt.arctanh() 进行调用\n同 jt.atanh()",
        "code_example": ">>> x = jt.randn(5) \n>>> x\njt.Var([ 0.561267    1.7155168  -0.53718305  1.6382825   0.5129911 ], dtype=float32)\n>>> x.arctanh()\njt.Var([ 0.634681         nan -0.6001876        nan  0.5667807], dtype=float32)"
    },
    {
        "api_name": "jittor_core.Var.arg_reduce",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor_core.Var.arg_reduce",
        "api_signature": "jittor_core.Var.arg_reduce()",
        "api_description": "函数C++定义格式:\nvector_to_tuple<VarHolder*> arg_reduce(jt.Var x,  String op,  int dim,  bool keepdims)\n计算张量在 dim 维度最大或最小值的索引和实际值",
        "return_value": "tuple[Var, Var] 类型，表示 x 在 dim 维度的最大和最小值的索引和实际值。如果 keepdims 为 True 则形状为 x 将 dim 维的长度变成 1，否则是 x 的形状去掉 dim 维",
        "parameters": "x (Var): 输入数据\nop (Literal[‘max’, ‘min’]): 计算的操作， 有2个选择： 'max' 或 'min'\ndim (int): 在哪个维度计算\nkeepdims (bool): 是否保留 dim 维度",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> x = jt.randint(0, 10, shape=(2, 3))\n>>> x\njt.Var([[4 2 5]\n        [6 7 1]], dtype=int32)\n>>> jt.arg_reduce(x, 'max', dim=1, keepdims=False)\n(jt.Var([2 1], dtype=int32), jt.Var([5 7], dtype=int32))\n>>> jt.arg_reduce(x, 'min', dim=1, keepdims=False)\n(jt.Var([1 2], dtype=int32), jt.Var([2 1], dtype=int32))\n>>> jt.arg_reduce(x, 'max', dim=0, keepdims=False)\n(jt.Var([1 1 0], dtype=int32), jt.Var([6 7 5], dtype=int32))"
    },
    {
        "api_name": "jittor.argmax",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.argmax",
        "api_signature": "jittor.argmax(x: Var, dim: int, keepdims: bool = False)",
        "api_description": "用于找出张量 x 中某个维度上元素的最大值的索引。",
        "return_value": "返回一个新的张量，其中包含指定维度上每个“切片”中最大元素的索引。",
        "parameters": "x (Var): Var类型的张量\ndim (int， optional): 指定在哪个维度上进行求最大值, 如果 dim 指定为 None, 则张量自动被展开\nkeepdims (bool, optional): 是否保持原Var的维度，默认为 False",
        "input_shape": "",
        "notes": "如果在指定的维度上有多个最大值，argmax 函数通常会返回这些最大值中第一个遇到的索引",
        "code_example": ">>> x = jt.randn(3,2)\njt.Var([[-0.1429974  -1.1169171 ]\n        [-0.35682714 -1.5031573 ]\n        [ 0.66668254  1.1606413 ]], dtype=float32)\n>>> jt.argmax(x, 0)\n(jt.Var([2 2], dtype=int32), jt.Var([0.66668254 1.1606413 ], dtype=float32))"
    },
    {
        "api_name": "jittor.contrib.argmax_pool",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.contrib.html#jittor.contrib.argmax_pool",
        "api_signature": "jittor.contrib.argmax_pool(x, size, stride, padding=0)",
        "api_description": "对输入张量进行最大值池化操作。即在池化窗口内找到最大值作为该窗口的值。该操作按照给定的步长值来移动池化窗口。\n\\[Y = \\max_{(i, j) \\in \\text{{window}}(size)} X_{i, j, k}\\]\n其中，\\(\\text{{window}}(size)\\) 是池化窗口，\\(X_{i,j,k}\\) 是输入张量在 \\((i, j, k)\\) 位置的元素，\\(Y\\) 是进行最大池化操作后的输出张量值。",
        "return_value": "经过最大池化操作后的结果(Var)",
        "parameters": "x(Var): 输入的张量。\nsize(int): 池化窗口尺寸。\nstride(int): 池化窗口的移动步长，步长值确定了池化窗口的移动速度。\npadding(int, 可选): 输入的每一条边补充 0 的层数，默认值： 0",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> from jittor.contrib import argmax_pool\n>>> input_array = jt.random([1, 1, 4, 4])\njt.Var([[[[0.49449673 0.00643021 0.07254869 0.3258533 ]\n  [0.61617774 0.09950083 0.3104945  0.48131013]\n  [0.37913334 0.09407917 0.18861724 0.09006661]\n  [0.7495838  0.25495356 0.00436674 0.3918325 ]]]], dtype=float32)\n>>> output = argmax_pool(input_array, 2, 2)\njt.Var([[[[0.61617774 0.48131013]\n  [0.7495838  0.3918325 ]]]], dtype=float32)"
    },
    {
        "api_name": "jittor.argmin",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.argmin",
        "api_signature": "jittor.argmin(x, dim: int, keepdims: bool = False)",
        "api_description": "返回输入张量中沿给定维度的最小值的索引。",
        "return_value": "包含两个张量(Var)的元组(tuple)，第一位为返回输入张量 x 沿给定维度 dim 的最小值的索引的张量，第二位为索引对应的最小值的张量。类型为Var的元组。",
        "parameters": "x (jt.Var) : 输入的张量。\ndim(int) : 计算最小值的索引的维度。\nkeepdims (bool) : 是否要求输出张量的维度是否与输入张量保持一致。如果这个值为True,结果张量的维度将与输入张量 x 的维度一致。默认值: False 。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> x = jt.random((4,4))\njt.Var([[0.4455578  0.4899658  0.854926   0.7409189 ]\n        [0.62386227 0.2531969  0.48017916 0.37137902]\n        [0.76723385 0.8991033  0.22901663 0.08265838]\n        [0.23934306 0.63881433 0.55240405 0.2548534 ]], dtype=float32)\n>>> jt.argmin(x, dim=0)\n(jt.Var([3 1 2 2], dtype=int32), jt.Var([0.23934306 0.2531969  0.22901663 0.08265838], dtype=float32))\n>>> jt.argmin(x, dim = 1)\n(jt.Var([0 1 3 0], dtype=int32), jt.Var([0.4455578  0.2531969  0.08265838 0.23934306], dtype=float32))\n>>> jt.argmin(x, dim = 1, keepdims = True)\n(jt.Var([[0]\n        [1]\n        [3]\n        [0]], dtype=int32), \njt.Var([[0.4455578 ]\n        [0.2531969 ]\n        [0.08265838]\n        [0.23934306]], dtype=float32))"
    },
    {
        "api_name": "jittor_core.Var.argsort",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor_core.Var.argsort",
        "api_signature": "jittor_core.Var.argsort()",
        "api_description": "函数C++定义格式:\nvector_to_tuple<VarHolder*> argsort(jt.Var x,  int dim=-1,  bool descending=false,  String dtype=ns_int32)\n返回张量在 dim 维排序的所用的下标排列和排序后的值",
        "return_value": "tuple[Var, Var] ，其中第一个返回张量中每个第 dim 维的切片是一个 [0, x.shape[dim]] 的排列，表示将 x 中对应的切片中元素排序的下标顺序，第二个返回张量是排序结果",
        "parameters": "x (Var): 被排序的数据\ndim (int): 在哪个维度排序，负数表示从后往前数。默认值：-1\ndescending (bool): 是否降序排列，False表示升序排列。默认值：False\ndtype (str): 返回的下标的数据类型",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> x = jt.array([3, 2, 10, 8])\n>>> jt.argsort(x)\n(jt.Var([1 0 3 2], dtype=int32), jt.Var([ 2  3  8 10], dtype=int32))"
    },
    {
        "api_name": "jittor.array",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.array",
        "api_signature": "jittor.array(data, dtype=None)",
        "api_description": "将输入的对象转化为 Jittor 的Var对象。 即函数主要用于将给定的对象转化为 Jittor 框架可以处理的 Var 对象。如果输入的是数值，则创建的 Var 的 dtype 为输入的数值类型，若是``numpy.ndarray`` ，则 Var 的 dtype 以 ndarray 的 dtype 为准；若输入为 Var 对象，其 dtype 保持不变。",
        "return_value": "对应的 Var 对象",
        "parameters": "obj(float | int | numpy.ndarray | Var): 需要转化的对象。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> import numpy as np\n>>> jt.array(1) \nVar(1, dtype=int32)\n>>> jt.array(1.0) \nVar(1.0, dtype=float32)\n>>> x=jt.array(np.array([1,2,3]))\n>>> print(x)\nVar([1 2 3], dtype=int32)\n>>> jt.array(x)\nVar([1 2 3], dtype=int32)"
    },
    {
        "api_name": "jittor.array64",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.array64",
        "api_signature": "jittor.array64(data, dtype=None)",
        "api_description": "本函数将输入转换为 int64 类型的张量。",
        "return_value": "jittor.Var: 一个 int64 类型的张量",
        "parameters": "data(array_like): 任何可以被转换为 jittor.Var 的数据形式，可以是list，list的list，元组等。\ndtype(Any | None, 可选): 输出的数据的数据类型，当缺省时，生成的数组将具有 numpy.float64 的数据类型。默认值: None",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> jt.array64([1,2,3])\njt.Var([1 2 3], dtype=int32)"
    },
    {
        "api_name": "jittor.einops.asnumpy",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.einops.html#jittor.einops.asnumpy",
        "api_signature": "jittor.einops.asnumpy(tensor)",
        "api_description": "将一个张量转换为numpy.ndarray",
        "return_value": "输入张量转换后得到的numpy.ndarray",
        "parameters": "tensor (Var): 输入张量",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> from jittor import einops\n>>> einops.asnumpy(jt.ones(3,3)) \narray([[1., 1., 1.],\n    [1., 1., 1.],\n    [1., 1., 1.]], dtype=float32)"
    },
    {
        "api_name": "jittor_core.Var.assign",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor_core.Var.assign",
        "api_signature": "jittor_core.Var.assign()",
        "api_description": "函数C++定义格式:\njt.Var assign(jt.Var v)\n将另一个Var的数据复制到这个Var中。",
        "return_value": "Var: 返回自身，但实际上已经原地复制完毕",
        "parameters": "v (Var): 另一个Var",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> x, y = jt.randn(5), jt.randn(5)\n>>> x\njt.Var([-1.3378737   0.8863208   1.9383168   0.35481802  1.4916613 ], dtype=float32)\n>>> y\njt.Var([ 0.59842813  1.281807    0.05155467 -0.74704844 -1.0341489 ], dtype=float32)\n>>> x.assign(y)\njt.Var([ 0.59842813  1.281807    0.05155467 -0.74704844 -1.0341489 ], dtype=float32)\n>>> x\njt.Var([ 0.59842813  1.281807    0.05155467 -0.74704844 -1.0341489 ], dtype=float32)"
    },
    {
        "api_name": "jittor.attrs",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.attrs",
        "api_signature": "jittor.attrs(var)",
        "api_description": "获取变量 var 的属性。",
        "return_value": "返回一个字典，包含变量的所有属性。",
        "parameters": "var (jt.Var) : 输入的变量。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> a = jt.array([1, 2, 3])\n>>> print(jt.attrs(a))\n{'is_stop_fuse': False, 'is_stop_grad': True, 'shape': [3,], 'dtype': int32}"
    },
    {
        "api_name": "jittor.nn.avg_pool2d",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.nn.html#jittor.nn.avg_pool2d",
        "api_signature": "jittor.nn.avg_pool2d(x, kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True)",
        "api_description": "对输入张量进行2D平均池化操作。\n参数:\nx(Var): 输入张量, 形状为 \\((N, C, H_{in}, W_{in})\\)。\nkernel_size (int or tuple): 池化核的大小。\nstride(int or tuple, optional): 步长。\npadding(int, optional): 在输入张量的所有边界上隐式零填充。默认值: 0。\nceil_mode(bool, optional): 当设置为True时, 会使用 \\(ceil\\) 函数计算输出形状。默认值: False。\ncount_include_pad(bool, optional): 当设置为True时, 将在计算平均值时包含零填充。默认值: True。\n返回值:\n进行2D平均池化后的张量。\n代码示例:>>> import jittor as jt\n>>> x = jt.random((1, 1, 4, 4))\n>>> jt.nn.avg_pool2d(x, kernel_size=2, stride=2)",
        "return_value": "进行2D平均池化后的张量。",
        "parameters": "x(Var): 输入张量, 形状为 \\((N, C, H_{in}, W_{in})\\)。\nkernel_size (int or tuple): 池化核的大小。\nstride(int or tuple, optional): 步长。\npadding(int, optional): 在输入张量的所有边界上隐式零填充。默认值: 0。\nceil_mode(bool, optional): 当设置为True时, 会使用 \\(ceil\\) 函数计算输出形状。默认值: False。\ncount_include_pad(bool, optional): 当设置为True时, 将在计算平均值时包含零填充。默认值: True。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> x = jt.random((1, 1, 4, 4))\n>>> jt.nn.avg_pool2d(x, kernel_size=2, stride=2)"
    },
    {
        "api_name": "jittor.nn.AvgPool2d",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.nn.html#jittor.nn.AvgPool2d",
        "api_signature": "jittor.nn.AvgPool2d(kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True)",
        "api_description": "二维平均池化 (pooling) 类。对二维输入的高度和宽度进行平均池化计算。\n参数:\nkernel_size (int or tuple): 池化核的大小。\nstride (int or tuple, optional): 池化操作的步长。\npadding (int or tuple, optional): 在输入数据的高度和宽度上各边缘处添加的零填充的大小。\nceil_mode (bool, optional): 是否使用 ceil 函数计算输出的高度和宽度。默认值: False。\ncount_include_pad (bool, optional): 是否在计算平均池化时包含零填充的格子。默认值: True。\n形状:\n输入: \\((N, C, H_{in}, W_{in})\\)。\n输出: \\((N, C, H_{out}, W_{out})\\), 其中\n\\[\\begin{split}& \\qquad H_{\\text {out }}=\\left\\lfloor\\frac{H_{\\text {in }}+2 \\times \\text { padding }[0]-\\text { kernel_size }[0]}{\\text { stride }[0]}+1\\right\\rfloor \\\\\n& \\qquad W_{\\text {out }}=\\left\\lfloor\\frac{W_{\\text {in }}+2 \\times \\text { padding }[1]-\\text { kernel_size }[1]}{\\text { stride }[1]}+1\\right\\rfloor\\end{split}\\]\n属性:\nlayer (jt.Module): 用于执行池化操作的模块。\n代码示例:>>> # pool of square window of size=3, stride=2\n>>> m = nn.AvgPool2d(3, stride=2)\n>>> # pool of non-square window\n>>> m = nn.AvgPool2d((3, 2), stride=(2, 1))\n>>> input = jt.randn(20, 16, 50, 32)\n>>> output = m(input)",
        "return_value": "",
        "parameters": "kernel_size (int or tuple): 池化核的大小。\nstride (int or tuple, optional): 池化操作的步长。\npadding (int or tuple, optional): 在输入数据的高度和宽度上各边缘处添加的零填充的大小。\nceil_mode (bool, optional): 是否使用 ceil 函数计算输出的高度和宽度。默认值: False。\ncount_include_pad (bool, optional): 是否在计算平均池化时包含零填充的格子。默认值: True。",
        "input_shape": "输入: \\((N, C, H_{in}, W_{in})\\)。\n输出: \\((N, C, H_{out}, W_{out})\\), 其中\n\n\n\\[\\begin{split}& \\qquad H_{\\text {out }}=\\left\\lfloor\\frac{H_{\\text {in }}+2 \\times \\text { padding }[0]-\\text { kernel_size }[0]}{\\text { stride }[0]}+1\\right\\rfloor \\\\\n& \\qquad W_{\\text {out }}=\\left\\lfloor\\frac{W_{\\text {in }}+2 \\times \\text { padding }[1]-\\text { kernel_size }[1]}{\\text { stride }[1]}+1\\right\\rfloor\\end{split}\\]",
        "notes": "",
        "code_example": ">>> # pool of square window of size=3, stride=2\n>>> m = nn.AvgPool2d(3, stride=2)\n>>> # pool of non-square window\n>>> m = nn.AvgPool2d((3, 2), stride=(2, 1))\n>>> input = jt.randn(20, 16, 50, 32)\n>>> output = m(input)"
    },
    {
        "api_name": "jittor.nn.AvgPool3d",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.nn.html#jittor.nn.AvgPool3d",
        "api_signature": "jittor.nn.AvgPool3d(kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True)",
        "api_description": "三维平均池化 (pooling) 类。对三维输入的深度, 高度和宽度进行平均池化计算。\n参数:\nkernel_size (int or tuple): 池化核的大小。\nstride (int or tuple, optional): 池化操作的步长。\npadding (int or tuple, optional): 在输入数据的高度和宽度上各边缘处添加的零填充的大小。\nceil_mode (bool, optional): 是否使用 ceil 函数计算输出的高度和宽度。默认值: False。\ncount_include_pad (bool, optional): 是否在计算平均池化时包含零填充的格子。默认值: True。\n形状:\n输入: \\((N, C, D_{in}, H_{in}, W_{in})\\)。\n输出: \\((N, C, D_{out}, H_{out}, W_{out})\\), 其中\n\\[\\begin{split}& \\qquad D_{\\text {out }}=\\left\\lfloor\\frac{D_{\\text {in }}+2 \\times \\text { padding }[0]-\\text { kernel_size }[0]}{\\text { stride }[0]}+1\\right\\rfloor \\\\\n& \\qquad H_{\\text {out }}=\\left\\lfloor\\frac{H_{\\text {in }}+2 \\times \\text { padding }[1]-\\text { kernel_size }[1]}{\\text { stride }[1]}+1\\right\\rfloor \\\\\n& \\qquad W_{\\text {out }}=\\left\\lfloor\\frac{W_{\\text {in }}+2 \\times \\text { padding }[2]-\\text { kernel_size }[2]}{\\text { stride }[2]}+1\\right\\rfloor\\end{split}\\]\n属性:\nlayer (jt.Module): 用于执行池化操作的模块。\n代码示例: >>> import jittor as jt\n>>> from jittor import nn\n>>> m = nn.AvgPool3d(2, stride=1, padding=1, count_include_pad=False)\n>>> input = jt.random([1, 6, 2, 2, 2])\n>>> output = m(input)\n>>> print(output.shape)\n[1,6,3,3,3,]",
        "return_value": "",
        "parameters": "kernel_size (int or tuple): 池化核的大小。\nstride (int or tuple, optional): 池化操作的步长。\npadding (int or tuple, optional): 在输入数据的高度和宽度上各边缘处添加的零填充的大小。\nceil_mode (bool, optional): 是否使用 ceil 函数计算输出的高度和宽度。默认值: False。\ncount_include_pad (bool, optional): 是否在计算平均池化时包含零填充的格子。默认值: True。",
        "input_shape": "输入: \\((N, C, D_{in}, H_{in}, W_{in})\\)。\n输出: \\((N, C, D_{out}, H_{out}, W_{out})\\), 其中\n\n\n\\[\\begin{split}& \\qquad D_{\\text {out }}=\\left\\lfloor\\frac{D_{\\text {in }}+2 \\times \\text { padding }[0]-\\text { kernel_size }[0]}{\\text { stride }[0]}+1\\right\\rfloor \\\\\n& \\qquad H_{\\text {out }}=\\left\\lfloor\\frac{H_{\\text {in }}+2 \\times \\text { padding }[1]-\\text { kernel_size }[1]}{\\text { stride }[1]}+1\\right\\rfloor \\\\\n& \\qquad W_{\\text {out }}=\\left\\lfloor\\frac{W_{\\text {in }}+2 \\times \\text { padding }[2]-\\text { kernel_size }[2]}{\\text { stride }[2]}+1\\right\\rfloor\\end{split}\\]",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> from jittor import nn\n>>> m = nn.AvgPool3d(2, stride=1, padding=1, count_include_pad=False)\n>>> input = jt.random([1, 6, 2, 2, 2])\n>>> output = m(input)\n>>> print(output.shape)\n[1,6,3,3,3,]"
    },
    {
        "api_name": "jittor.optim.Optimizer.backward",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.optim.html#jittor.optim.Optimizer.backward",
        "api_signature": "backward(loss, retain_graph=False)",
        "api_description": "optimize.backward(loss) 用于累积多个step的梯度，可以如下使用：",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "jittor.nn.backward",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.nn.html#jittor.nn.backward",
        "api_signature": "jittor.nn.backward(v, *args, **kw)",
        "api_description": "反向传播函数。在Jittor中不存在 backward 变量接口。请改用 optimizer.backward(loss) 或 optimizer.step(loss)。\n例如，如果您的代码如下所示:\noptimizer.zero_grad()\nloss.backward()\noptimizer.step()\n可以修改为:\noptimizer.zero_grad()\noptimizer.backward(loss)\noptimizer.step()\n或者更简洁的:\noptimizer.step(loss)",
        "return_value": "无返回值",
        "parameters": "v(Var): 无用张量",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> optimizer.step(loss)"
    },
    {
        "api_name": "jittor.nn.baddbmm",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.nn.html#jittor.nn.baddbmm",
        "api_signature": "jittor.nn.baddbmm(input, batch1, batch2, beta=1, alpha=1)",
        "api_description": "执行 batch1 和 batch2 矩阵的批量矩阵-矩阵乘积。并将 input 加到最终结果中。\nbatch1 和 batch2 必须是 3-D 张量，每个都包含相同数量的矩阵。\n\\[out = beta * input + alpha * (batch1 @ batch2)\\]\n假设 batch1 的形状是 [batch, n, m]， batch2 的形状是 [batch, m, k], 则 input 将是一个形状为 [batch, n, k] 的矩阵。",
        "return_value": "output(Var): 结果对应的张量",
        "parameters": "input (Var): 一个形状为 [batch, n, k] 的张量\nbatch1 (Var): 一个形状为 [batch, n, m] 的张量\nbatch2 (Var): 一个形状为 [batch, m, k] 的张量\nalpha (float): 乘积的权重, 默认为 1\nbeta (float): input 的权重, 默认为 1",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> x = jt.randn(10, 3, 5)\n>>> batch1 = jt.randn(10, 3, 4)\n>>> batch2 = jt.randn(10, 4, 5)\n>>> jt.baddmm(x, batch1, batch2).shape\n[10, 3, 5]"
    },
    {
        "api_name": "jittor.nn.batch_norm",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.nn.html#jittor.nn.batch_norm",
        "api_signature": "jittor.nn.batch_norm(x, running_mean, running_var, weight=1, bias=0, training=False, momentum=0.1, eps=1e-05)",
        "api_description": "对输入的张量进行批量归一化。批量归一化是一种用于提高神经网络性能和稳定性的技术。该操作可以使网络处理的内部协变量变化变得更小。 在正向传播时，该层会将输入中心化（均值为0）和规范化（方差为1），然后将结果乘以比例因子 gamma 并加上偏移量 beta。在反向传播时，本层将计算当前批次的均值和方差的梯度。计算方式如下:\n\\[norm_x = \\frac{x - running_mean} {\\sqrt{running_var + eps}} * weight + bias\\]\n参数:\nx (Var) : 输入张量。维度 (batch_size, num_channels, ..)。\nrunning_mean (Var) : 运行期间的平均值，用于将输入中心化。维度和 num_channels 相同。\nrunning_var (Var) :  运行期间的方差，用于将输入规范化。维度和 num_channels 相同。\nweight (float，optional) : 批量归一化的缩放系数。默认值: 1\nbias (float，optional) : 批量归一化的平移系数。默认值: 0\ntraining (bool，optional) : 为 True 时， 化函数使用在线定义的计数方法对batch数据进行归一化。只有当你使用的模型在每次训练时修改输入的统计数据,并保存更新后的统计数据以备后续使用时，才需要使用该参数。如果训练数据在每一次训练过程中都保持不变，就无需使用该参数。默认值: False\nmomentum (float, optional) : 动量值，有效值为 [0,1] 。默认值: 0.1\neps (float，optional) : 用作分母以增加数值稳定性的项。默认值: 1e-5\n返回值:Var: 线性变换后的结果，大小可以是(batch_size, output_dim)\n代码示例：>>> import jittor as jt\n>>> from jittor import nn\n>>> x = jt.array([[0, 0.5, 1.0], [-0.3, 0.5, 0.8]])\n>>> running_mean = jt.mean(x, dim=0) # 实际应用时，自行计算\n>>> running_var = jt.var(x, dim=0) # 实际应用时，自行计算\n>>> nn.batch_norm(x, running_mean, running_var)\njt.Var([[ 0.99977785  0.          0.9995003 ]\n[-0.99977785  0.         -0.99950075]], dtype=float32",
        "return_value": "Var: 线性变换后的结果，大小可以是(batch_size, output_dim)",
        "parameters": "x (Var) : 输入张量。维度 (batch_size, num_channels, ..)。\nrunning_mean (Var) : 运行期间的平均值，用于将输入中心化。维度和 num_channels 相同。\nrunning_var (Var) :  运行期间的方差，用于将输入规范化。维度和 num_channels 相同。\nweight (float，optional) : 批量归一化的缩放系数。默认值: 1\nbias (float，optional) : 批量归一化的平移系数。默认值: 0\ntraining (bool，optional) : 为 True 时， 化函数使用在线定义的计数方法对batch数据进行归一化。只有当你使用的模型在每次训练时修改输入的统计数据,并保存更新后的统计数据以备后续使用时，才需要使用该参数。如果训练数据在每一次训练过程中都保持不变，就无需使用该参数。默认值: False\nmomentum (float, optional) : 动量值，有效值为 [0,1] 。默认值: 0.1\neps (float，optional) : 用作分母以增加数值稳定性的项。默认值: 1e-5",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> from jittor import nn\n>>> x = jt.array([[0, 0.5, 1.0], [-0.3, 0.5, 0.8]])\n>>> running_mean = jt.mean(x, dim=0) # 实际应用时，自行计算\n>>> running_var = jt.var(x, dim=0) # 实际应用时，自行计算\n>>> nn.batch_norm(x, running_mean, running_var) \njt.Var([[ 0.99977785  0.          0.9995003 ]\n        [-0.99977785  0.         -0.99950075]], dtype=float32"
    },
    {
        "api_name": "jittor.nn.BatchNorm1d",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.nn.html#jittor.nn.BatchNorm1d",
        "alias": "BatchNorm"
    },
    {
        "api_name": "jittor.nn.BatchNorm2d",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.nn.html#jittor.nn.BatchNorm2d",
        "alias": "BatchNorm"
    },
    {
        "api_name": "jittor.nn.BatchNorm3d",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.nn.html#jittor.nn.BatchNorm3d",
        "alias": "BatchNorm"
    },
    {
        "api_name": "jittor.nn.BatchNorm",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.nn.html#jittor.nn.BatchNorm",
        "api_signature": "jittor.nn.BatchNorm(num_features, eps=1e-05, momentum=0.1, affine=True, is_train=True, sync=True)",
        "api_description": "对输入进行批次归一化\n将一个批次中输入的特性值根据均值 \\(\\mu\\) 和方差 \\(\\sigma^2\\) 归一化，然后进行一个线性变换：\n\\[x_i^{\\prime} = \\frac{x_i - \\mu}{\\sqrt{\\sigma^2 + \\varepsilon}} \\cdot w_i + b_i\\]\n训练时，每个批次计算均值和方差并归一化，而且在内部记录并更新见到的数据的均值和方差；测试时使用记录的均值和方差进行归一化",
        "return_value": "",
        "parameters": "num_features (int): 输入的特性个数\neps (float): 给方差加上的小量，避免除以 0。默认值：1e-5\nmomentum (float): 更新保存的均值和方差的惯性量。默认值：0.1\naffine (bool): 是否对输入进行线性变换。默认值：True\nis_train (bool): 是否更新保存的均值和方差。默认值：True\nsync (bool): 使用 MPI 训练时，是否在各结点间同步均值和方差。默认值：True",
        "input_shape": "Input: (N, num_features, ...) 其中 N 是批次中的数据个数\nOutput: (N, num_features, ...)",
        "notes": "",
        "code_example": ">>> x = jt.array([1, 2, 3])\n>>> y = jt.array([4, 5, 6])\n>>> bn = nn.BatchNorm(3)\n>>> bn(x), bn(y)\n(jt.Var([[-1.2247354  0.         1.2247355]], dtype=float32),\n jt.Var([[-1.2247343  0.         1.2247348]], dtype=float32))\n>>> bn.is_train = False\n>>> bn(x), bn(y)\n(jt.Var([[0.33063978 1.363889   2.397138  ]], dtype=float32),\n jt.Var([[3.4303875 4.463637  5.496886 ]], dtype=float32))"
    },
    {
        "api_name": "jittor.dataset.BatchSampler",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.dataset.html#jittor.dataset.BatchSampler",
        "api_signature": "jittor.dataset.BatchSampler(sampler, batch_size, drop_last)",
        "api_description": "将数据集的索引分成多个批次, 每个批次包含一个索引列表。\n参数:\nsampler (jittor.dataset.Sampler): 用于提供索引的Sampler对象。\nbatch_size (int): 每批样本的大小。\ndrop_last (bool): 若设置为True, 则可能会丢弃最后一批, 如果它的大小小于 batch_size 。\n代码示例:  >>> import jittor as jt\n>>> from jittor.dataset import Dataset\n>>> from jittor.dataset import BatchSampler, SequentialSampler\n>>>\n>>> class MyDataset(Dataset):\n>>>      def __len__(self):\n>>>          return 10\n>>>\n>>>      def __getitem__(self, index):\n>>>          return index\n>>>\n>>> dataset = MyDataset()\n>>> sampler = BatchSampler(SequentialSampler(dataset), 3, True)\n>>> list(iter(sampler))\n>>> [[0, 1, 2], [3, 4, 5], [6, 7, 8]]\n>>> sampler = BatchSampler(SequentialSampler(dataset), 3, False)\n>>> list(iter(sampler))\n>>> [[0, 1, 2], [3, 4, 5], [6, 7, 8], [9]]",
        "return_value": "",
        "parameters": "sampler (jittor.dataset.Sampler): 用于提供索引的Sampler对象。\nbatch_size (int): 每批样本的大小。\ndrop_last (bool): 若设置为True, 则可能会丢弃最后一批, 如果它的大小小于 batch_size 。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> from jittor.dataset import Dataset\n>>> from jittor.dataset import BatchSampler, SequentialSampler\n>>>\n>>> class MyDataset(Dataset):\n>>>      def __len__(self):\n>>>          return 10\n>>>\n>>>      def __getitem__(self, index):\n>>>          return index\n>>>      \n>>> dataset = MyDataset()\n>>> sampler = BatchSampler(SequentialSampler(dataset), 3, True)\n>>> list(iter(sampler))\n>>> [[0, 1, 2], [3, 4, 5], [6, 7, 8]]\n>>> sampler = BatchSampler(SequentialSampler(dataset), 3, False) \n>>> list(iter(sampler))\n>>> [[0, 1, 2], [3, 4, 5], [6, 7, 8], [9]]"
    },
    {
        "api_name": "jittor.nn.bce_loss",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.nn.html#jittor.nn.bce_loss",
        "api_signature": "jittor.nn.bce_loss(output, target, weight=None, size_average=True)",
        "api_description": "计算二分类交叉熵(Binary Cross Entropy)损失。对于给定的类别标签  target 和网络输出结果 output ，计算二分类交叉熵损失，如果指定了权重，则每个样本的损失会乘以权重：\n\\[L = -\\frac{1}{n} \\sum_i^n (target[i] * \\log(output[i]) + (1 - target[i]) * \\log(1 - output[i]))\\]",
        "return_value": "Var: 二分类交叉熵(Binary Cross Entropy)损失。",
        "parameters": "output (Var): 预测值，模型输出。形状为(batch_size, num_classes)，元素数据类型为float32\ntarget (Var): 目标值，实际值或者是标签。形状应与output保持一致，元素数据类型为float32\nweight (float，optional): 每个样本的权重，如果指定，应该是一个1-D张量，长度等于batch_size。默认值: None\nsize_average (bool): 是否对损失求平均。如果是True, 返回损失的平均值；否则，返回损失之和。默认值: True",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> from jittor import nn\n>>> output = jt.array([1.0, 1.0, 1.0])\n>>> target = jt.array([0.5, 0.6, -2.0])\n>>> nn.bce_loss(output, target)\njt.Var([59.867214], dtype=float32)"
    },
    {
        "api_name": "jittor.nn.BCELoss",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.nn.html#jittor.nn.BCELoss",
        "api_signature": "jittor.nn.BCELoss(weight=None, size_average=True)",
        "api_description": "用于计算输出值和目标值的二进制交叉熵。创建一个衡量 x 和目标 y 之间二进制交叉熵标准：\n\\[\\ell(x, y) = L = \\{l_1,\\dots,l_N\\}^\\top, \\quad\nl_n = - w_n \\left[ y_n \\cdot \\log x_n + (1 - y_n) \\cdot \\log (1 - x_n) \\right]\\]\n其中 \\(N\\) 是批次大小。",
        "return_value": "",
        "parameters": "weight (Var, optional): 每个类的权重，如果你的训练样本很不均衡的话，是非常有用的。默认值：None，表示所有类权重相等。\nsize_average (bool, optional): 如果为 True，损失会在每个小批量中平均。如果为 False，损失会在小批量中求和。默认值：True",
        "input_shape": "Input:\noutput: \\((*)\\)，模型输出的预测值，其中 * 表示任意数量的附加维数。\ntarget: \\((*)\\)，表示目标值，和输入形状相同。\n\n\n\n\nOutput: 一个标量，表示计算得到的二进制交叉熵。",
        "notes": "",
        "code_example": ">>> m = nn.Sigmoid()\n>>> loss = nn.BCELoss()\n>>> output = jt.randn((3,2))\n>>> target = jt.rand((3,2))\n>>> loss_var = loss(m(output), target)\n>>> loss_var\njt.Var([0.7875105], dtype=float32)"
    },
    {
        "api_name": "jittor.nn.BCEWithLogitsLoss",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.nn.html#jittor.nn.BCEWithLogitsLoss",
        "api_signature": "jittor.nn.BCEWithLogitsLoss(weight=None, pos_weight=None, size_average=True)",
        "api_description": "实现了带有逻辑值（logits）的二元交叉熵（Binary Cross Entropy, BCE）损失。 它结合了 Sigmoid 层和 BCELoss 于一个单一的类中，相比单独使用  Sigmoid 后接 BCELoss ，在数值上更为稳定。\n该损失可以使用如下公式表示：\n\\[\\ell(x, y) = L = \\{l_1,\\dots,l_N\\}^\\top, \\quad\nl_n = - w_n[y_n \\cdot \\log(\\sigma(x_n)) + (1 - y_n) \\cdot \\log(1 - \\sigma(x_n))]\\]\n其中 \\(\\sigma\\) 是 Sigmoid 函数，\\(x\\) 是输入的逻辑值，\\(y\\) 是目标值，\\(w_n\\) 是第 \\(n\\) 类的权重，\\(N\\) 是批次大小。",
        "return_value": "",
        "parameters": "weight (Var, optional): 各类的权重。默认值：None，表示各类权重相等\npos_weight (Var, optional): 一个正类别的权重的张量。如果给定，损失输出中正类别的部分会乘以pos_weight，默认值：None\nsize_average (bool, optional): 如果为 True ，会将损失 \\(L\\) 在每个小批量中平均。如果为 False ，损失会在小批量中求和。默认值：True",
        "input_shape": "output: \\(( *)\\)，其中 * 表示任意数量的附加维数。\ntarget: \\((*)\\), 和输入形状相同",
        "notes": "",
        "code_example": ">>> target = jt.ones((10, 64))  # 64 classes, batch size = 10\n>>> output = jt.full([10, 64], 1.5)\n>>> pos_weight = jt.ones([64])  # All weights are equal to 1\n>>> loss = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n>>> loss_var = loss(output, target)\n>>> loss_var\njt.Var([0.20141378], dtype=float32)"
    },
    {
        "api_name": "jittor.misc.bernoulli",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.misc.bernoulli",
        "api_signature": "jittor.misc.bernoulli(input)",
        "api_description": "对于每个元素 p ，返回概率为 p 的伯努利分布的随机值：\n\\[\\begin{split}\\begin{cases}\n1 & \\text{ with probability } p \\\\\n0 & \\text{ with probability } (1 - p)\n\\end{cases}\\end{split}\\]",
        "return_value": "形状与数据类型和 input 一样的张量，原张量中每个元素 p 对应返回值里的元素，以概率 p 为 1，概率 1 - p 为 0",
        "parameters": "input (Var): 每个采样所用的概率 p",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> x = jt.array([0.0, 0.5, 1.0])\n>>> jt.bernoulli(x)\njt.Var([0. 0. 1.], dtype=float32)\n>>> jt.bernoulli(x)\njt.Var([0. 0. 1.], dtype=float32)\n>>> jt.bernoulli(x)\njt.Var([0. 1. 1.], dtype=float32)"
    },
    {
        "api_name": "jittor.Module.bfloat16",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.Module.bfloat16",
        "api_signature": "bfloat16()",
        "api_description": "将所有参数转换为bfloat16。",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "jittor_core.Var.bfloat16",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor_core.Var.bfloat16",
        "api_signature": "jittor_core.Var.bfloat16()",
        "api_description": "函数C++定义格式:\njt.Var bfloat16_(jt.Var x)\n创建并返回一个 x 的张量副本，并将其类型转换为 bfloat16 (brain half-precision float)",
        "return_value": "返回一个新的张量, 数据类型为 bfloat16",
        "parameters": "x (Var): Var类型的张量",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> x = jt.rand(3) * 10 \n>>> x\njt.Var([4.093273  2.0086648 8.474352 ], dtype=float32)\n>>> x.bfloat16()\njt.Var([4.094 2.008 8.48 ], dtype=bfloat16)\n>>> jt.bfloat16(x)\njt.Var([4.094 2.008 8.48 ], dtype=bfloat16)"
    },
    {
        "api_name": "jittor.nn.bilinear",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.nn.html#jittor.nn.bilinear",
        "api_signature": "jittor.nn.bilinear(in1, in2, weight, bias)",
        "api_description": "该函数对输入的in1, in2, weight和bias参数进行双线性（bilinear）运算。在数学上，双线性运算可以表示为：\n\\[z = x * y + bias\\]\n其中：\n\\(x\\) 是第一个输入矩阵和权重矩阵的乘积，形状为 [weight.shape[0], weight.shape[2]]\n\\(y\\) 是扩展到与 \\(x\\) 相同形状的第二个输入矩阵\n\\(*\\) 表示矩阵乘法\n\\(bias\\) 是偏置值\n参数:\nin1（Var）：第一个输入张量\nin2（Var）：第二个输入张量\nweight（Var）：用于完成双线性运算的权重张量\nbias（Var，optional）：用于完成双线性运算的偏置值。如果该参数为None，则不使用偏置值。默认值: None\n返回值:Var: 完成双线性运算后的结果\n代码示例：>>> import jittor as jt\n>>> batch_size = 10\n>>> feature_size = 5\n>>> out_features = 7\n>>> in1 = jt.randn(batch_size, feature_size)\n>>> in2 = jt.randn(batch_size, feature_size)\n>>> weight = jt.randn(out_features,  feature_size, feature_size)\n>>> bias = jt.randn(out_features)\n>>> jt.nn.bilinear(in1, in2, weight, bias)",
        "return_value": "Var: 完成双线性运算后的结果",
        "parameters": "in1（Var）：第一个输入张量\nin2（Var）：第二个输入张量\nweight（Var）：用于完成双线性运算的权重张量\nbias（Var，optional）：用于完成双线性运算的偏置值。如果该参数为None，则不使用偏置值。默认值: None",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> batch_size = 10\n>>> feature_size = 5\n>>> out_features = 7\n>>> in1 = jt.randn(batch_size, feature_size)\n>>> in2 = jt.randn(batch_size, feature_size)  \n>>> weight = jt.randn(out_features,  feature_size, feature_size)\n>>> bias = jt.randn(out_features)\n>>> jt.nn.bilinear(in1, in2, weight, bias)"
    },
    {
        "api_name": "jittor.nn.Bilinear",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.nn.html#jittor.nn.Bilinear",
        "api_signature": "jittor.nn.Bilinear(in1_features, in2_features, out_features, bias=True, dtype='float32')",
        "api_description": "对输入数据应用双线性变换：\n\\[y = x_1^T A x_2 + b\\]\n其中，\\(x_1\\) 和 \\(x_2\\) 是输入数据，\\(A\\) 是权重矩阵，\\(b\\) 是偏置项。\n参数：\nin1_features(int)： 第一个输入的大小\nin2_features(int) ：第二个输入的大小\nout_features(int)： 输出的大小\nbias(bool)： 如果为 False ，则层不会使用偏置项。默认值: True\n代码示例：>>> m = nn.Bilinear(20, 30, 40)\n>>> input1 = jt.randn(128, 20)\n>>> input2 = jt.randn(128, 30)\n>>> output = m(input1, input2)\n>>> print(output.size())\n[128,40,]",
        "return_value": "",
        "parameters": "in1_features(int)： 第一个输入的大小\nin2_features(int) ：第二个输入的大小\nout_features(int)： 输出的大小\nbias(bool)： 如果为 False ，则层不会使用偏置项。默认值: True",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> m = nn.Bilinear(20, 30, 40)\n>>> input1 = jt.randn(128, 20)\n>>> input2 = jt.randn(128, 30)\n>>> output = m(input1, input2)\n>>> print(output.size())\n[128,40,]"
    },
    {
        "api_name": "jittor_core.Var.binary",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor_core.Var.binary",
        "api_signature": "jittor_core.Var.binary()",
        "api_description": "函数C++定义格式:\njt.Var binary(jt.Var x, jt.Var y,  String p)\n对两个张量元素计算二元运算",
        "return_value": "二元运算的结果，形状与 x 和 y 相同",
        "parameters": "x (Var): 二元运算的第一个操作数\ny (Var): 二元运算的第二个操作数，形状与 x 相同\np (str): 二元运算的名字对应字符串，例如 'add', 'divide' 等",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> x = jt.array([1, 2, 3])\n>>> y = jt.array([4, 7, 5])\n>>> jt.binary(x, y, 'add')\njt.Var([5 9 8], dtype=int32)\n>>> jt.binary(x, y, 'divide')\njt.Var([0.24999999 0.28571427 0.6       ], dtype=float32)"
    },
    {
        "api_name": "jittor.nn.binary_cross_entropy_with_logits",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.nn.html#jittor.nn.binary_cross_entropy_with_logits",
        "api_signature": "jittor.nn.binary_cross_entropy_with_logits(output, target, weight=None, pos_weight=None, size_average=True)",
        "api_description": "该函数用于计算具有logits的二元交叉熵损失，基于Sigmoid激活函数实现。该函数对于解决数据不平衡问题是非常有效的。\n参数:\noutput (Var) : 网络的输出张量，元素float32/float64，形状用 [batch_size,*] 表示。其中 * 代表任意的其他尺寸。\ntarget (Var) : 目标张量，类型同output，形状与output相同。\nweight (Var, optional) : 一个手动指定每个类别的权重的张量。默认值：None\npos_weight (Var, optional): 正样本的权重。如果给定，必须为张量，而与output形状相同，且类型与output类型相同。默认值：None\nsize_average (bool, optional): 如果为True，返回交叉熵损失的平均值。否则，返回交叉熵损失的总和。默认值：True\n返回值:Var: 与output形状相同的二元交叉熵损失张量，具有同样的数据类型。\n代码示例：>>> import jittor as jt\n>>> from jittor import nn\n>>> output = jt.array([0.2, 1, -0.3])\n>>> target = jt.array([3, 2, 2])\n>>> nn.binary_cross_entropy_with_logits(output, target)\njt.Var([0.22191863], dtype=float32)",
        "return_value": "Var: 与output形状相同的二元交叉熵损失张量，具有同样的数据类型。",
        "parameters": "output (Var) : 网络的输出张量，元素float32/float64，形状用 [batch_size,*] 表示。其中 * 代表任意的其他尺寸。\ntarget (Var) : 目标张量，类型同output，形状与output相同。\nweight (Var, optional) : 一个手动指定每个类别的权重的张量。默认值：None\npos_weight (Var, optional): 正样本的权重。如果给定，必须为张量，而与output形状相同，且类型与output类型相同。默认值：None\nsize_average (bool, optional): 如果为True，返回交叉熵损失的平均值。否则，返回交叉熵损失的总和。默认值：True",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> from jittor import nn\n>>> output = jt.array([0.2, 1, -0.3])\n>>> target = jt.array([3, 2, 2])\n>>> nn.binary_cross_entropy_with_logits(output, target)\njt.Var([0.22191863], dtype=float32)"
    },
    {
        "api_name": "jittor_core.Var.bitwise_and",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor_core.Var.bitwise_and",
        "api_signature": "jittor_core.Var.bitwise_and()",
        "api_description": "函数C++定义格式:\njt.Var bitwise_and(jt.Var x, jt.Var y)\n对两个张量中对应元素计算按位与，也可以使用 & 运算符调用",
        "return_value": "形状与 x 和 y 相同的张量。 其元素为 x 和 y 对应索引位置上元素的按位与结果。",
        "parameters": "x (Var): 输入数据\ny (Var): 输入数据，形状与 x 相同",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> x = jt.array([1, 2, 4])\n>>> y = jt.array([3, 4, 5])\n>>> x.bitwise_and(y)\njt.Var([1 0 4], dtype=int32)\n>>> x & y\njt.Var([1 0 4], dtype=int32)"
    },
    {
        "api_name": "jittor_core.Var.bitwise_not",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor_core.Var.bitwise_not",
        "api_signature": "jittor_core.Var.bitwise_not()",
        "api_description": "函数C++定义格式:\njt.Var bitwise_not(jt.Var x)\n创建一个张量，其将输入变量 x 中的每一个数值进行位非运算。",
        "return_value": "返回一个新的张量, 其数值是 x 中对应位置进行位非运算后的值",
        "parameters": "x (Var): Var类型的张量，数据类型应为布尔型或整型",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> (jt.int32([-2, 3, 1])).bitwise_not()\njt.Var([ 1 -4 -2], dtype=int32)"
    },
    {
        "api_name": "jittor_core.Var.bitwise_or",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor_core.Var.bitwise_or",
        "api_signature": "jittor_core.Var.bitwise_or()",
        "api_description": "函数C++定义格式:\njt.Var bitwise_or(jt.Var x, jt.Var y)\n两个张量的元素级按位或，也可以使用 | 运算符调用",
        "return_value": "形状与 x 和 y 相同的张量。 x 和 y 对应索引位置上元素的按位或结果。",
        "parameters": "x (Var): 输入数据\ny (Var): 输入数据，形状与 x 相同",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> x = jt.array([1, 2, 4])\n>>> y = jt.array([3, 4, 5])\n>>> x.bitwise_or(y)\njt.Var([3 6 5], dtype=int32)\n>>> x | y\njt.Var([3 6 5], dtype=int32)"
    },
    {
        "api_name": "jittor_core.Var.bitwise_xor",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor_core.Var.bitwise_xor",
        "api_signature": "jittor_core.Var.bitwise_xor()",
        "api_description": "函数C++定义格式:\njt.Var bitwise_xor(jt.Var x, jt.Var y)\n两个张量的元素级按位异或，也可以使用 ^ 运算符调用",
        "return_value": "形状与 x 和 y 相同的张量。 x 和 y 对应索引位置上元素的按位异或结果。",
        "parameters": "x (Var): 输入数据\ny (Var): 输入数据，形状与 x 相同",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> x = jt.array([1, 2, 4])\n>>> y = jt.array([3, 4, 5])\n>>> jt.(x.bitwise_xor(y)\njt.Var([2 6 1], dtype=int32)\n>>> x ^ y\njt.Var([2 6 1], dtype=int32)"
    },
    {
        "api_name": "jittor.nn.bmm",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.nn.html#jittor.nn.bmm",
        "api_signature": "jittor.nn.bmm(a, b)",
        "api_description": "执行 a 和 b 矩阵的批量矩阵-矩阵乘积。\na 和 b 必须是3维度张量，每个都包含相同数量的矩阵。\n\\[out = a @ b\\]\n假设 a 的形状是 [batch, n, m]， b 的形状是 [batch, m, k], 批量矩阵乘法的结果将是一个形状为 [batch, n, k] 的新矩阵。",
        "return_value": "output(Var): 乘积对应的张量",
        "parameters": "a(Var): 矩阵A\nb(Var): 矩阵B",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> x = jt.rand(3, 5, 6)\n>>> y = jt.rand(3, 6, 7)\n>>> jt.bmm(x, y).shape\n[3, 5, 7]"
    },
    {
        "api_name": "jittor.nn.bmm_transpose",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.nn.html#jittor.nn.bmm_transpose",
        "api_signature": "jittor.nn.bmm_transpose(a, b)",
        "api_description": "对两个矩阵进行批次矩阵乘法并对第二个矩阵转置。即：\n\\[out = A @ B^T\\]\n第一个矩阵和第二个矩阵转置的乘积。转置操作在最后两个维度上进行。",
        "return_value": "output(Var): 乘积对应的张量",
        "parameters": "a(Var): 矩阵A\nb(Var): 矩阵B",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> x = jt.rand(3, 4, 5, 6)\n>>> y = jt.rand(3, 4, 7, 6)\n>>> jt.bmm_transpose(x, y).shape\n[3, 4, 5, 7]"
    },
    {
        "api_name": "jittor_core.Var.bool",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor_core.Var.bool",
        "api_signature": "jittor_core.Var.bool()",
        "api_description": "函数C++定义格式:\njt.Var bool_(jt.Var x)\n创建并返回一个 x 的张量副本，并将其类型转换为 bool 。",
        "return_value": "返回一个新的张量, 数据类型为 bool",
        "parameters": "x (Var): Var类型的张量",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> x = jt.arange(3) \n>>> x\njt.Var([0 1 2], dtype=int32)\n>>> x.bool()    \njt.Var([False  True  True], dtype=bool)\n>>> jt.bool(x)\njt.Var([False  True  True], dtype=bool)"
    },
    {
        "api_name": "jittor_core.Var.broadcast",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor_core.Var.broadcast",
        "api_signature": "jittor_core.Var.broadcast()",
        "api_description": "函数C++定义格式:\njt.Var broadcast_to(jt.Var x,  NanoVector shape,  NanoVector dims=NanoVector())\n将 x 广播到与 y 相同的形状。",
        "return_value": "广播后的结果(jt.Var)",
        "parameters": "x(Var)：输入的 Var。\ny(jt.Var)：作为参考的 Var。\ndims(Tuple[int])：指定输出形状中的新维度，为一个整数数组。默认值：()",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> jt.randint(0, 10, shape=(2, 2))\njt.Var([[8 1]\n        [7 6]], dtype=int32)\n>>> jt.randint(0, 10, shape=(2, 3, 2))\n>>> jt.broadcast(x, y, dims=[1])\njt.Var([[[8 1]\n        [8 1]\n        [8 1]],\n        [[7 6]\n        [7 6]\n        [7 6]]], dtype=int32)\n>>> jt.broadcast_var(x, y, dims=[1])\njt.Var([[[8 1]\n        [8 1]\n        [8 1]],\n        [[7 6]\n        [7 6]\n        [7 6]]], dtype=int32)"
    },
    {
        "api_name": "jittor.init.calculate_gain",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.init.html#jittor.init.calculate_gain",
        "api_signature": "jittor.init.calculate_gain(nonlinearity, param=None)",
        "api_description": "返回给定非线性函数的推荐增益值，值如下：\nnonlinearity\ngain\nLinear / Identity\n\\(1\\)\nConv{1,2,3}D\n\\(1\\)\nSigmoid\n\\(1\\)\nTanh\n\\(\\displaystyle \\frac{5}{3}\\)\nReLU\n\\(\\sqrt{2}\\)\nLeaky Relu\n\\(\\displaystyle \\sqrt{\\frac{2}{1 + \\text{negative_slope}^2}}\\)\nSELU\n\\(\\displaystyle \\frac{3}{4}\\)",
        "return_value": "",
        "parameters": "nonlinearity: 非线性函数（nn.functional 名称）\nparam: 非线性函数的可选参数",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> gain = nn.init.calculate_gain('leaky_relu', 0.2)  # leaky_relu with negative_slope=0.2"
    },
    {
        "api_name": "jittor.init.calculate_std",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.init.html#jittor.init.calculate_std",
        "api_signature": "jittor.init.calculate_std(var, mode, nonlinearity, param=0.01)",
        "api_description": "计算标准差。",
        "return_value": "标准差（float）",
        "parameters": "var (Var): 输入Var\nmode (str): 模式，可选值为 fan_in 和 fan_out。默认值：fan_in\nnonlinearity (str): 非线性函数，可选值为 linear、conv1d、conv2d、conv3d、conv_transpose1d、conv_transpose2d、conv_transpose3d、sigmoid、tanh、relu 和 leaky_relu。默认值：linear\nparam (float): 非线性函数的参数。默认值：0.01",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> x = jt.random((2, 2))\njt.Var([[ 2.520024   -0.4921519 ]\n                [-1.1624513  -0.62531066]], dtype=float32)\n>>> jt.calculate_std(x)\n0.7071067811865476"
    },
    {
        "api_name": "jittor_core.Var.candidate",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor_core.Var.candidate",
        "api_signature": "jittor_core.Var.candidate()",
        "api_description": "函数C++定义格式:\njt.Var candidate(jt.Var x,  string&& fail_cond,  String dtype=ns_int32)\nCandidate算子根据给定的fail_cond进行过滤操作。\nx是输入。y是输出, 且满足如下条件,这里m是选中的数量。:\nnot fail_cond(y[0], y[1]) and\nnot fail_cond(y[0], y[2]) and not fail_cond(y[1], y[2]) and\n...\n... and not fail_cond(y[m-2], y[m-1])\npython伪代码:\ny = []\nfor i in range(n):\npass = True\nfor j in y:\nif (@fail_cond):\npass = false\nbreak\nif (pass):\ny.append(i)\nreturn y\n参数:\nx: 等待过滤输入\nfail_cond: 失败情况对应代码\ndtype: 返回类型\nindex: 下标\n代码示例:\njt.candidate(jt.random(100,2), '(@x(j,0)>@x(i,0))or(@x(j,1)>@x(i,1))')\n# return y satisfy:\n#    x[y[0], 0] <= x[y[1], 0] and x[y[1], 0] <= x[y[2], 0] and ... and x[y[m-2], 0] <= x[y[m-1], 0] and\n#    x[y[0], 1] <= x[y[1], 1] and x[y[1], 1] <= x[y[2], 1] and ... and x[y[m-2], 1] <= x[y[m-1], 1]",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "jittor.distributions.Categorical",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.distributions.html#jittor.distributions.Categorical",
        "api_signature": "jittor.distributions.Categorical(probs=None, logits=None)",
        "api_description": "处理分类问题的概率分布。可以根据给定的概率或者 logits 初始化类别分布、根据初始化的类别分布进行采样、计算某个类别的对数概率、计算该类别分布的熵。类别分布的熵的计算形式为\n\\[-\\sum p \\log p\\]",
        "return_value": "",
        "parameters": "probs (Var 或 None): 概率分布向量，为 None 时，根据 logits 参数计算得出。默认值为 None 。 probs 的正规化后得到\n\n\n\\[p_i^{\\prime} = \\log\\left(\\frac{p_i}{1-\\sum p_i}\\right)\\]\n\nlogits (Var 或 None): 逻辑值向量，默认值为 None 。logits 先作用 Sigmoid 函数转换为概率分布，然后再使用 probs 的正规化方法。\n\nprobs 和 logits 应有至少有一者不为 None。",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "jittor_core.Var.ceil",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor_core.Var.ceil",
        "api_signature": "jittor_core.Var.ceil()",
        "api_description": "函数C++定义格式:\njt.Var ceil(jt.Var x)\n创建一个张量, 其将 x 中的每一个数值转换为向上取整离其最近的整数。\n如：ceil(3.7) 的结果是4, ceil(-3.7) 的结果是-3。",
        "return_value": "返回一个新的张量, 其数值是 x 中对应位置向上取整离其最近的整数",
        "parameters": "x (Var): Var类型的张量",
        "input_shape": "",
        "notes": "支持使用 jt.ceil() 进行调用",
        "code_example": ">>> x = jt.rand(5) * 5\n>>> x\njt.Var([ 1.1675875  -0.36471805  2.0246403   1.1496693  -0.09650089], dtype=float32)\n>>> x.ceil()\njt.Var([ 2. -0.  3.  2. -0.], dtype=float32)"
    },
    {
        "api_name": "jittor_core.Var.ceil_int",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor_core.Var.ceil_int",
        "api_signature": "jittor_core.Var.ceil_int()",
        "api_description": "函数C++定义格式:\njt.Var ceil_int(jt.Var x)\n创建一个张量, 其将 x 中的每一个数值转换为向上取整离其最近的整数。\n如：ceil_int(3.7) 的结果是4, ceil_int(-3.7) 的结果是-3。",
        "return_value": "返回一个新的张量, 其数值是 x 中对应向上取整离其最近的整数, 数据类型为 int32",
        "parameters": "x (Var): Var类型的张量",
        "input_shape": "",
        "notes": "支持使用 jt.ceil_int() 进行调用",
        "code_example": ">>> x = jt.randn(5)\n>>> x\njt.Var([ 2.00422    -2.081075   -0.7260027   0.46558696 -0.1570169 ], dtype=float32)\n>>> x.ceil_int()\njt.Var([ 3 -2  0  1  0], dtype=int32)"
    },
    {
        "api_name": "jittor.transform.center_crop",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.transform.html#jittor.transform.center_crop",
        "api_signature": "jittor.transform.center_crop(img, output_size)",
        "api_description": "对输入图像在中心处进行裁剪。",
        "return_value": "PIL.Image.Image: 裁剪后的图像",
        "parameters": "img (PIL.Image.Image): 输入图像\noutput_size (int, list) ：裁剪框的（高度、宽度）。如果为整数或只有单个整数的序列，则高度和宽度都使用该整数",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> import numpy as np\n>>> from PIL import Image\n>>> data = np.random.rand(200,200)\n>>> img = Image.fromarray(data, 'L')       \n>>> img.size\n(200, 200)\n>>> jt.transform.center_crop(img, 100).size \n(100, 100)"
    },
    {
        "api_name": "jittor.transform.CenterCrop",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.transform.html#jittor.transform.CenterCrop",
        "api_signature": "jittor.transform.CenterCrop(size)",
        "api_description": "中心裁剪图像的类。本类用于创建一个中心裁剪变换。当使用此类的实例调用一张图像时，它将图像裁剪为指定的大小，并且裁剪的中心与图像的中心对齐。",
        "return_value": "",
        "parameters": "size(int, tuple): 裁剪后的图像尺寸。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> from jittor import transform\n>>> import numpy as np\n>>> from PIL import Image\n>>> center_crop = transform.CenterCrop(16)\n>>> data = np.random.rand(200, 200, 3)\n>>> img = Image.fromarray(data, 'RGB')\n>>> center_crop(img).size\n(16, 16)"
    },
    {
        "api_name": "jittor.loss3d.chamfer_loss",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.loss3d.html#jittor.loss3d.chamfer_loss",
        "api_signature": "jittor.loss3d.chamfer_loss(pc1, pc2, reduction='mean', dims='BNC', bidirectional=False)",
        "api_description": "return the chamfer loss from pc1 to pc2.\nExample:\n>>> import jittor as jt\n>>> from jittor.loss3d import chamfer_loss\n>>> jt.flags.use_cuda = True\n>>> pc1 = jt.rand([10, 100, 3], dtype=jt.float32)\n>>> pc2 = jt.rand([10, 100, 3], dtype=jt.float32)\n>>> cf = chamfer_loss(pc1, pc2, dims='BNC', bidirectional=True)\n>>> print('chamfer loss =', cf.item())",
        "return_value": "",
        "parameters": "pc1 (jittor array) – input point cloud\npc2 (jittor array) – input point cloud\nreduction (str, optional) – reduction method in batches, can be ‘mean’, ‘sum’, or None. Default: ‘mean’.\ndims (str, optional) – a string that represents each dimension, can be\n‘[BNC]’ ([batch, number of points, xyz]), or\n‘[BCN]’ ([batch, xyz, number of points]). Default: ‘BNC’.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "jittor.loss3d.ChamferLoss",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.loss3d.html#jittor.loss3d.ChamferLoss",
        "api_signature": "jittor.loss3d.ChamferLoss(reduction='mean', dims='BNC', bidirectional=False)",
        "api_description": "A loss layer that computes the chamfer loss from pc1 to pc2.\nExample:\n>>> import jittor as jt\n>>> from jittor.loss3d import ChamferLoss\n>>> jt.flags.use_cuda = True\n>>> pc1 = jt.rand([10, 100, 3], dtype=jt.float32)\n>>> pc2 = jt.rand([10, 100, 3], dtype=jt.float32)\n>>> CF = ChamferLoss(dims='BNC', bidirectional=True)\n>>> cf = CF(pc1, pc2)\n>>> print('chamfer loss =', cf.item())",
        "return_value": "",
        "parameters": "pc1 (jittor array) – input point cloud\npc2 (jittor array) – input point cloud\nreduction (str, optional) – reduction method in batches, can be ‘mean’, ‘sum’, or None. Default: ‘mean’.\ndims (str, optional) – a string that represents each dimension, can be\n‘[BNC]’ ([batch, number of points, xyz]), or\n‘[BCN]’ ([batch, xyz, number of points]). Default: ‘BNC’.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "jittor.contrib.check",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.contrib.html#jittor.contrib.check",
        "api_signature": "jittor.contrib.check(bc)",
        "api_description": "检查bc中的每个元素是否等于1或等于bc中维度0的最大值。",
        "return_value": "返回输入数组的按照轴 0 进行最大值操作后的结果( int )。",
        "parameters": "bc(Var): 输入的数组，代表要进行检查的数组。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> bc = jt.Var([[1, 2, 3], [1, 1, 1]])\n>>> print(jt.contrib.check(bc))\n[1 2 3]\n>>> bc = jt.Var([[1, 2, 3], [1, 4, 1]])\n>>> print(jt.contrib.check(bc))\nException: Shape not match."
    },
    {
        "api_name": "jittor.Module.children",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.Module.children",
        "api_signature": "children()",
        "api_description": "返回子模块的列表。",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "jittor.linalg.cholesky",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.linalg.html#jittor.linalg.cholesky",
        "api_signature": "jittor.linalg.cholesky(x)",
        "api_description": "对输入矩阵的Cholesky分解, 形式如下公式：\n\\[x = LL^T\\]\n其中 \\(x\\) 必须是 Hermite 和正定矩阵。 \\(L\\) 是一个下三角矩阵。",
        "return_value": "返回 \\(x\\) 的Cholesky分解的下三角矩阵 \\(L\\) ( Var ), 其维度与 \\(x\\) 相同, 为 (..., M, M) 。",
        "parameters": "x (Var): 输入的二维矩阵, 维度为 (..., M, M) 。 \\(x\\) 应满足如下条件：首先,  \\(x\\) 应是正定的；其次,  \\(x\\) 应是 Hermite 的(即 \\(x\\) 等于其共轭转置)。 \\(x\\) 可以是实矩阵或者复矩阵。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> a = jt.array([[4.0, 12, -16], [12, 37, -43], [-16, -43, 98]])\n>>> print(jt.linalg.cholesky(a))\njt.Var([[ 2.  0.  0.]\n        [ 6.  1.  0.]\n        [-8.  5.  3.]], dtype=float32)"
    },
    {
        "api_name": "jittor.misc.chunk",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.misc.chunk",
        "api_signature": "jittor.misc.chunk(x, chunks, dim=0)",
        "api_description": "将张量在 dim 维度切分为 chunks 块\n切块尽量均匀。如果 x 在 dim 维的长度不是 chunks 的整倍数，最后一个块的长度会小于之前块的长度。如果无法切分出 chunks 个非空的块，则会返回少于 chunks 个块。",
        "return_value": "list[Var] 为切块后按顺序得到的各个块组成的列表。",
        "parameters": "x (Var): 被切块的张量，形状 \\((n_0, \\ldots, n_{\\mathtt{dim}}, \\ldots, n_k)\\)\nchunks (int): 切成的块数\ndim (int, 可选): 在哪个维度切块，负数表示从后往前数。默认值：0",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "jittor.dataset.CIFAR100",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.dataset.html#jittor.dataset.CIFAR100",
        "api_signature": "jittor.dataset.CIFAR100(root='/home/zwy/.cache/jittor/dataset/cifar_data/', train=True, transform=None, target_transform=None, download=True)",
        "api_description": "CIFAR100 数据集, 是 CIFAR10 数据集的子类。",
        "return_value": "",
        "parameters": "split (str, optional) - 指定数据集分割的类型, ‘train’ 表示训练集, ‘test’ 表示测试集, 默认值:  ‘train’ 。\ntransform (callable, optional) - 一个函数或transform对象, 用于对样本进行处理, 默认值:  None 。\ntarget_transform (callable, optional) - 一个函数或transform对象, 用于对标签进行处理, 默认值:  None 。\ndownload (bool, optional) - 是否下载数据集, 如果数据集未下载, 则设为 True, 默认值:  False 。\nbatch_size (int, optional) - 每个batch中的样本数, 默认值: 1。\nshuffle (bool, optional) - 是否在每个epoch开始时打乱数据, 默认值:  False 。\nnum_workers (int, optional) - 加载数据时使用的子进程数量, 默认值: 0。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> from jittor.dataset.cifar import CIFAR100\n>>> dataset = CIFAR100(split='train')"
    },
    {
        "api_name": "jittor.dataset.CIFAR10",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.dataset.html#jittor.dataset.CIFAR10",
        "api_signature": "jittor.dataset.CIFAR10(root='/home/zwy/.cache/jittor/dataset/cifar_data/', train=True, transform=None, target_transform=None, download=True)",
        "api_description": "CIFAR10 数据集。\n参数:\nroot (str, optional): 数据集的根目录, 其中应该包含或将要保存包含 cifar-10-batches-py 的目录, 如果设置了download为True, 则会自动下载并解压数据集, 默认值: dataset_root + '/cifar_data/' 。\ntrain (bool, optional): 如果为True, 则创建训练集的数据集对象；如果为False, 则创建测试集的数据集对象, 默认值: True。\ntransform (callable, optional): 一个用于图像变换的函数或可调用对象, 它接收一个PIL图片, 并返回经过变换的图片。例如,  transforms.RandomCrop , 默认值: None。\ntarget_transform (callable, optional): 一个用于标签变换的函数或可调用对象, 它接收目标标签, 并返回转换后的标签。默认值: None\ndownload (bool, optional): 如果为True, 则从互联网下载数据集并将其保存到root目录。如果数据集已经下载, 不会再次下载, 默认值: True。\n属性:\nroot (str): 数据集的根目录。\ndata (numpy.ndarray): 图像数据, 形状为(50000, 3, 32, 32)或(10000, 3, 32, 32)。\ntargets (numpy.ndarray): 标签数据, 形状为(50000,)或(10000,)。\nclasses (list): 类别名称列表, 形状为(10,)。\nclass_to_idx (dict): 类别名称到索引的映射:({‘airplane’: 0, ‘automobile’: 1, ‘bird’: 2, ‘cat’: 3, ‘deer’: 4, ‘dog’: 5, ‘frog’: 6, ‘horse’: 7,’ship’: 8, ‘truck’: 9})。\n代码示例:  >>> from jittor.dataset.cifar import CIFAR10\n>>> dataset = CIFAR10()",
        "return_value": "",
        "parameters": "root (str, optional): 数据集的根目录, 其中应该包含或将要保存包含 cifar-10-batches-py 的目录, 如果设置了download为True, 则会自动下载并解压数据集, 默认值: dataset_root + '/cifar_data/' 。\ntrain (bool, optional): 如果为True, 则创建训练集的数据集对象；如果为False, 则创建测试集的数据集对象, 默认值: True。\ntransform (callable, optional): 一个用于图像变换的函数或可调用对象, 它接收一个PIL图片, 并返回经过变换的图片。例如,  transforms.RandomCrop , 默认值: None。\ntarget_transform (callable, optional): 一个用于标签变换的函数或可调用对象, 它接收目标标签, 并返回转换后的标签。默认值: None\ndownload (bool, optional): 如果为True, 则从互联网下载数据集并将其保存到root目录。如果数据集已经下载, 不会再次下载, 默认值: True。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> from jittor.dataset.cifar import CIFAR10\n>>> dataset = CIFAR10()"
    },
    {
        "api_name": "jittor.clamp",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.clamp",
        "api_signature": "jittor.clamp(x, min_v=None, max_v=None)",
        "api_description": "",
        "return_value": "返回一个新的张量（Var），其数值的维度被限制在 [min_v, max_v] 之间 。",
        "parameters": "x (Var): Var类型的张量\nmin_v (int, optional): 数值的最小允许值。如果为 None，则不应用最小值限制\nmax_v (int, optional): 数值的最大允许值。如果为 None，则不应用最大值限制",
        "input_shape": "",
        "notes": "如果 min_v 大于 max_v ，会引起异常",
        "code_example": ">>> x = jt.randn(4)\njt.Var([ 0.8474737   0.9300491   2.8043647  -0.32242754], dtype=float32)\n>>> jt.clamp(x, -0.5, 0.5)\njt.Var([ 0.5         0.5         0.5        -0.32242754], dtype=float32)"
    },
    {
        "api_name": "jittor.clamp_",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.clamp_",
        "api_signature": "jittor.clamp_(x, min_v=None, max_v=None)",
        "api_description": "原地修改张量 x ，限制其数值在特定的范围 [min_v, max_v] 内。对于张量 x 中的任何一个数值均满足：\n\\[x_i = \\min(\\max(x_i, \\text{min_value}_i), \\text{max_value}_i)\\]\n参数：\nx (Var): Var类型的张量\nmin_v (int, optional): 数值的最小允许值。如果为 None，则不应用最小值限制\nmax_v (int, optional): 数值的最大允许值。如果为 None，则不应用最大值限制\n返回值：就地修改输入张量 x ，其数值的维度被限制在 [min_v, max_v] 之间 。\n代码示例：>>> x = jt.randn(4)\njt.Var([ 0.77959126  0.8394206  -1.426833    0.7886605 ], dtype=float32)\n>>> jt.clamp_(x, -0.5, 0.5)\n>>> print(x)\njt.Var([ 0.5  0.5 -0.5  0.5], dtype=float32)\n注意事项:\n如果 min_v 大于 max_v ，会引起异常",
        "return_value": "就地修改输入张量 x ，其数值的维度被限制在 [min_v, max_v] 之间 。",
        "parameters": "x (Var): Var类型的张量\nmin_v (int, optional): 数值的最小允许值。如果为 None，则不应用最小值限制\nmax_v (int, optional): 数值的最大允许值。如果为 None，则不应用最大值限制",
        "input_shape": "",
        "notes": "如果 min_v 大于 max_v ，会引起异常",
        "code_example": ">>> x = jt.randn(4)\njt.Var([ 0.77959126  0.8394206  -1.426833    0.7886605 ], dtype=float32)\n>>> jt.clamp_(x, -0.5, 0.5)\n>>> print(x)\njt.Var([ 0.5  0.5 -0.5  0.5], dtype=float32)"
    },
    {
        "api_name": "jittor.clean",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.clean",
        "api_signature": "jittor.clean()",
        "api_description": "清理和重置Jittor的内部状态。",
        "return_value": "无。",
        "parameters": "不需要任何参数。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> jt.clean()"
    },
    {
        "api_name": "jittor.nn.clip_coordinates",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.nn.html#jittor.nn.clip_coordinates",
        "api_signature": "jittor.nn.clip_coordinates(x, clip_limit)",
        "api_description": "将输入坐标 x 裁剪到 [0, clip_limit - 1] 的范围内。",
        "return_value": "裁剪后的坐标(Var):其形状与输入 x 相同。",
        "parameters": "x (Var): 需要裁剪的坐标\nclip_limit (int): 裁剪的上限",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> x = jt.array([-5, 0, 5, 10, 15])\n>>> clip_limit = 10\n>>> clipped_x = clip_coordinates(x, clip_limit)\n>>> print(clipped_x)  \njt.Var([0 0 5 9 9], dtype=int32)"
    },
    {
        "api_name": "jittor.optim.Optimizer.clip_grad_norm",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.optim.html#jittor.optim.Optimizer.clip_grad_norm",
        "api_signature": "clip_grad_norm(max_norm: float, norm_type: int = 2)",
        "api_description": "剪切此优化器的梯度范数，范数是对所有梯度一起计算的。",
        "return_value": "",
        "parameters": "max_norm (float or int): 梯度的最大范数\nnorm_type (int): 1-范数或2-范数",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "jittor_core.Var.clone",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor_core.Var.clone",
        "api_signature": "jittor_core.Var.clone()",
        "api_description": "函数C++定义格式:\njt.Var clone(jt.Var x)\n创建并返回一个新的变量，该变量是张量 x 的深度拷贝",
        "return_value": "返回一个新的张量, 其形状、类型和值与输入张量相同。",
        "parameters": "x(Var): Var类型的张量",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> x = jt.randn((2,3))\n>>> x\njt.Var([[-0.22726686 -0.7055701  -0.40404484]\n        [ 0.25013736 -1.2239726   0.45218712]], dtype=float32)\n>>> x.clone()\n>jt.Var([[-0.22726686 -0.7055701  -0.40404484]\n        [ 0.25013736 -1.2239726   0.45218712]], dtype=float32)"
    },
    {
        "api_name": "jittor.dataset.Dataset.collate_batch",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.dataset.html#jittor.dataset.Dataset.collate_batch",
        "api_signature": "collate_batch(batch)",
        "api_description": "用于将输入变量的列表转换为 Jittor 变量的方法。默认情况下, 它会将输入变量的列表转换为 Jittor 变量的列表。如果输入变量的列表中的任何变量的维度为1, 则会将其从输出变量中去除。\n参数:\nbatch (list): 需要转换的输入变量的列表。\n返回值:\njt.Var: 一个包含输入变量的列表的 Jittor 变量。\n代码示例:>>> batch = [np.array([1,2,3]), np.array([4,5,6])]\n>>> collate_batch(batch)\n>>> jt.Var([[1, 2, 3], [4, 5, 6]])",
        "return_value": "jt.Var: 一个包含输入变量的列表的 Jittor 变量。",
        "parameters": "batch (list): 需要转换的输入变量的列表。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> batch = [np.array([1,2,3]), np.array([4,5,6])]\n>>> collate_batch(batch)\n>>> jt.Var([[1, 2, 3], [4, 5, 6]])"
    },
    {
        "api_name": "jittor.dataset.VarDataset.collate_batch",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.dataset.html#jittor.dataset.VarDataset.collate_batch",
        "api_signature": "collate_batch(batch)",
        "api_description": "用于将输入的Jittor变量的列表转换为特定格式 Jittor 变量的方法。\n参数:\nbatch (list of jt.Var): 需要转换的输入变量的列表。\n返回值:\njt.Var: 一个包含输入变量的列表的 Jittor 变量。\n代码示例:>>> batch = [jt.array([1,2,3]), jt.array([4,5,6])]\n>>> collate_batch(batch)\n>>> jt.Var([[1, 2, 3], [4, 5, 6]])",
        "return_value": "jt.Var: 一个包含输入变量的列表的 Jittor 变量。",
        "parameters": "batch (list of jt.Var): 需要转换的输入变量的列表。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> batch = [jt.array([1,2,3]), jt.array([4,5,6])]\n>>> collate_batch(batch)\n>>> jt.Var([[1, 2, 3], [4, 5, 6]])"
    },
    {
        "api_name": "jittor.transform.ColorJitter",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.transform.html#jittor.transform.ColorJitter",
        "api_signature": "jittor.transform.ColorJitter(brightness=0, contrast=0, saturation=0, hue=0)",
        "api_description": "随机改变图像的亮度、对比度、饱和度和色调。该类用于对图像进行随机的颜色变换，以进行数据增强。这些变换包括亮度、对比度、饱和度和色调的调整。",
        "return_value": "",
        "parameters": "brightness (float, tuple(min, max)): 要改变的亮度范围。亮度因子从 \\([\\max(0, 1 - brightness), 1 + brightness]\\) 或给定的 \\([min, max]\\) 中均匀选择。应为非负数。默认值为 0，表示不改变亮度。\ncontrast (float,  tuple(min, max)): 要改变的对比度范围。对比度因子从 \\([\\max(0, 1 - contrast), 1 + contrast]\\) 或给定的 \\([min, max]\\) 中均匀选择。应为非负数。默认值为 0，表示不改变对比度。\nsaturation (float,  tuple(min, max)): 要改变的饱和度范围。饱和度因子从 \\([\\max(0, 1 - saturation), 1 + saturation]\\) 或给定的 \\([min, max]\\) 中均匀选择。应为非负数。默认值为 0，表示不改变饱和度。\nhue (float,  tuple(min, max)): 要改变的色调范围。色调因子从 \\([-hue, hue]\\) 或给定的 \\([min, max]\\) 中均匀选择。应满足 \\(0\\leq hue\\leq 0.5\\) 或 \\(-0.5 \\leq min \\leq max \\leq 0.5\\)。默认值为 0，表示不改变色调。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> from jittor.transform import ColorJitter\n>>> from PIL import Image\n>>> img = Image.open('''path_to_image.jpg''')\n>>> transform = ColorJitter(brightness=0.2, contrast=0.3, saturation=0.4, hue=0.1)\n>>> img_transformed = transform(img)"
    },
    {
        "api_name": "jittor.nn.ComplexNumber",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.nn.html#jittor.nn.ComplexNumber",
        "api_signature": "jittor.nn.ComplexNumber(real: Var, imag: Var | None = None, is_concat_value=False)",
        "api_description": "复数类。它以 jt.stack(real, imag, dim=-1) 的形式保存。实现了复数与复数，复数与 jt.Var，复数与整数，复数与浮点数之间的加、减、乘和除运算。可以使用 shape ，reshape 等 jt.Var 的方法。\n参数:\nreal (jittor.Var): 实部\nimag (jittor.Var, 可选): 虚部。默认值: None\nis_concat_value (bool, 可选): 是否以 jt.stack 后的值作为输入，默认值: False\n属性:\nvalue: 用jt.stack存储的复数的实部与虚部。其中value[…, 0]为实部，value[…, 1]为虚部。\n代码示例:  >>> import jittor as jt\n>>> real = jt.array([[[1., -2., 3.]]])\n>>> imag = jt.array([[[0., 1., 6.]]])\n>>> a = jt.nn.ComplexNumber(real, imag)\n>>> a + a\n>>> a / a\n>>> a.norm()                # sqrt(real^2+imag^2)\n>>> a.exp()                 # e^real(cos(imag)+isin(imag))\n>>> a.conj()                # ComplexNumber(real, -imag)\n>>> a.fft2()                # cuda only now. len(real.shape) equals 3\n>>> a.ifft2()               # cuda only now. len(real.shape) equals 3\n>>> a = jt.array([[1,1],[1,-1]])\n>>> b = jt.array([[0,-1],[1,0]])\n>>> c = jt.nn.ComplexNumber(a,b) / jt.sqrt(3)\n>>> c @ c.transpose().conj()\nComplexNumber(real=jt.Var([[0.99999994 0.        ]\n[0.         0.99999994]], dtype=float32), imag=jt.Var([[0. 0.]\n[0. 0.]], dtype=float32))",
        "return_value": "",
        "parameters": "real (jittor.Var): 实部\nimag (jittor.Var, 可选): 虚部。默认值: None\nis_concat_value (bool, 可选): 是否以 jt.stack 后的值作为输入，默认值: False",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> real = jt.array([[[1., -2., 3.]]])\n>>> imag = jt.array([[[0., 1., 6.]]])\n>>> a = jt.nn.ComplexNumber(real, imag)\n>>> a + a\n>>> a / a\n>>> a.norm()                # sqrt(real^2+imag^2)\n>>> a.exp()                 # e^real(cos(imag)+isin(imag))\n>>> a.conj()                # ComplexNumber(real, -imag)\n>>> a.fft2()                # cuda only now. len(real.shape) equals 3\n>>> a.ifft2()               # cuda only now. len(real.shape) equals 3\n>>> a = jt.array([[1,1],[1,-1]])\n>>> b = jt.array([[0,-1],[1,0]])\n>>> c = jt.nn.ComplexNumber(a,b) / jt.sqrt(3)\n>>> c @ c.transpose().conj()\nComplexNumber(real=jt.Var([[0.99999994 0.        ]\n        [0.         0.99999994]], dtype=float32), imag=jt.Var([[0. 0.]\n        [0. 0.]], dtype=float32))"
    },
    {
        "api_name": "jittor.transform.Compose",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.transform.html#jittor.transform.Compose",
        "api_signature": "jittor.transform.Compose(transforms)",
        "api_description": "将多个图像变换组合在一起的类。该类将输入的变换列表依次应用到输入数据上。如果输入数据为单个对象，会依次通过每个变换；如果输入为多个对象（例如图像和对应标签），则每个变换都会接收相同的参数列表，并依次处理。",
        "return_value": "",
        "parameters": "transforms (list): 需要组合的变换列表。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> from jittor import transform\n>>> compose = transform.Compose([\n    transform.Resize(224),\n    transform.Gray(),\n    transform.ImageNormalize(mean=[0.5], std=[0.5]),\n])\n>>> img_ = compose(img)"
    },
    {
        "api_name": "jittor.weightnorm.WeightNorm.compute_weight",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.weightnorm.html#jittor.weightnorm.WeightNorm.compute_weight",
        "api_signature": "compute_weight(module: Module)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "jittor.init.constant",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.init.html#jittor.init.constant",
        "api_signature": "jittor.init.constant(shape, dtype='float32', value=0.0)",
        "api_description": "创建一个数值均为 value, 形状由可变参数 shape 确定， 默认填充 float32 类型的零值。\n参数：\nshape (Tuple[int]): 整数序列，定义了输出的形状\ndtype (var.dtype, optional): 数据类型，默认为 float32\nvalue (int or float): 填充的数值",
        "return_value": "返回一个填充值为 value ，形状大小为 shape 的张量(Var)",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> init.constant(2)\njt.Var([0. 0.], dtype=float32)\n>>> init.constant((2,3), \"float32\", 3.8)\n>>> x\njt.Var([[3.8 3.8 3.8]\n        [3.8 3.8 3.8]], dtype=float32)"
    },
    {
        "api_name": "jittor.init.constant_",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.init.html#jittor.init.constant_",
        "api_signature": "jittor.init.constant_(var, value=0.0)",
        "api_description": "原地修改张量 var ，将其修改为数值均为 value 的张量，默认填充零值。\n参数：\nvar (Var): Var类型的张量\nvalue (int or float): 填充的数值",
        "return_value": "就地修改 var ，返回一个填充值为 value ，形状和 var 相同的张量(Var)",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> x = jt.randn((3,2))\n>>> x\njt.Var([[ 0.0335454  -0.18658346]\n        [-0.53283346 -1.2073938 ]\n        [-1.344916   -1.6093305 ]], dtype=float32)\n>>> init.constant_(x, 3.8)\n>>> x\n>jt.Var([[3.8 3.8]\n         [3.8 3.8]\n         [3.8 3.8]], dtype=float32)"
    },
    {
        "api_name": "jittor.nn.ConstantPad2d",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.nn.html#jittor.nn.ConstantPad2d",
        "api_signature": "jittor.nn.ConstantPad2d(padding, value)",
        "api_description": "使用固定值来填充输入张量的边界。\n输入为 \\((N, C, H_{in}, W_{in})\\)，输出为 \\((N, C, H_{out}, W_{out})\\)，其中:\n\\[\\begin{split}H_{out} = H_{in} + \\text{padding_top} + \\text{padding_bottom}\n\\\\W_{out} = W_{in} + \\text{padding_left} + \\text{padding_right}\\end{split}\\]",
        "return_value": "",
        "parameters": "padding(int, tuple): 填充的大小\nvalue(float): 填充的值",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> m = nn.ConstantPad2d((1,1,2,0), 3.5)\n>>> input = jt.randn(1, 1, 3, 3)\n>>> input\njt.Var([[[[ 0.18378055 -0.60490954 -0.68662244]\n        [-0.42572546 -1.4829487  -0.6552902 ]\n        [-0.92770797  0.2502182  -0.10983822]]]], dtype=float32)\n>>> m(input)\njt.Var([[[[ 3.5         3.5         3.5         3.5         3.5       ]\n        [ 3.5         3.5         3.5         3.5         3.5       ]\n        [ 3.5         0.18378055 -0.60490954 -0.68662244  3.5       ]\n        [ 3.5        -0.42572546 -1.4829487  -0.6552902   3.5       ]\n        [ 3.5        -0.92770797  0.2502182  -0.10983822  3.5       ]]]], dtype=float32)"
    },
    {
        "api_name": "jittor.misc.contiguous",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.misc.contiguous",
        "api_signature": "jittor.misc.contiguous(x)",
        "api_description": "在内存中创建给定 Var 的深拷贝。\n该函数确保结果的内存是连续的，且与源张量分开。这在您需要更改张量，但不希望这些更改反映在原始张量中时非常有用。",
        "return_value": "Var, x 的深拷贝，占用连续的内存",
        "parameters": "x (Var): 输入的张量。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> x = jt.array([1,2,3])\n>>> y = contiguous(x)\n>>> y[0] = 5\n>>> print(x) \njt.Var([1 2 3], dtype=int32)\n>>> print(y)\njt.Var([5 2 3], dtype=int32)"
    },
    {
        "api_name": "jittor.nn.conv",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.nn.html#jittor.nn.conv",
        "api_signature": "jittor.nn.conv(x, weight, bias=None, stride=1, padding=0, dilation=1, groups=1)",
        "api_description": "将一个2D卷积应用于由多个输入平面组成的输入信号。\n参数:\nx (Var) : 输入张量\nweight (Var) : 卷积核\nbias (Var，optional) : 卷积后的偏置。默认值：None\nstride (int, tuple，optional) : 卷积的步长。默认值: 1\npadding (int, tuple, optional) : 输入的四周添加的填充长度。默认值: 0\ndilation (int, tuple，optional) : 卷积核元素之间的间距。默认值: 1\ngroups (int，optional) : 输入通道和输出通道之间的阻塞连接数。默认值: 1\n返回值:Var: 执行2D卷积之后的结果张量\n代码示例：>>> x = jt.randn(4, 24, 100, 100)\n>>> w = jt.randn(32, 24, 3, 3)\n>>> y = nn.conv2d(x, w)",
        "return_value": "Var: 执行2D卷积之后的结果张量",
        "parameters": "x (Var) : 输入张量\nweight (Var) : 卷积核\nbias (Var，optional) : 卷积后的偏置。默认值：None\nstride (int, tuple，optional) : 卷积的步长。默认值: 1\npadding (int, tuple, optional) : 输入的四周添加的填充长度。默认值: 0\ndilation (int, tuple，optional) : 卷积核元素之间的间距。默认值: 1\ngroups (int，optional) : 输入通道和输出通道之间的阻塞连接数。默认值: 1",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> x = jt.randn(4, 24, 100, 100)\n>>> w = jt.randn(32, 24, 3, 3)\n>>> y = nn.conv2d(x, w)"
    },
    {
        "api_name": "jittor.nn.Conv1d_sp",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.nn.html#jittor.nn.Conv1d_sp",
        "api_signature": "jittor.nn.Conv1d_sp(inchannels, outchannels, kernel_size=1, bias=True)",
        "api_description": "实现了对一维输入数据的卷积操作，卷积核大小固定为1不支持修改。该类为一维数据的卷积操作，故输入的数据维度需要是3维，即 [batch_size, channel, Length]\n参数:\ninchannels (int): 输入数据的通道数量，必须为一个大于0的整数。\noutchannels (int): 输出数据的通道数量，必须为一个大于0的整数。\nkernel_size (int, optional): 卷积核的大小，由于类的设计，参数值只能为1。默认值: 1\nbias (bool, optional): 是否需要加入偏置项(bias term)。默认值: True\n形状:\n输入形状: [batch_size, inchannels, Length]\n输出形状: [batch_size, outchannels, Length]\n代码示例:  >>> import jittor as jt\n>>> conv1d = jt.nn.Conv1d_sp(3,64)\n>>> conv1d(jt.randn(10,3,8)).shape\n[10,64,8,]",
        "return_value": "",
        "parameters": "inchannels (int): 输入数据的通道数量，必须为一个大于0的整数。\noutchannels (int): 输出数据的通道数量，必须为一个大于0的整数。\nkernel_size (int, optional): 卷积核的大小，由于类的设计，参数值只能为1。默认值: 1\nbias (bool, optional): 是否需要加入偏置项(bias term)。默认值: True",
        "input_shape": "输入形状: [batch_size, inchannels, Length]\n输出形状: [batch_size, outchannels, Length]",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> conv1d = jt.nn.Conv1d_sp(3,64)\n>>> conv1d(jt.randn(10,3,8)).shape \n[10,64,8,]"
    },
    {
        "api_name": "jittor.nn.Conv1d",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.nn.html#jittor.nn.Conv1d",
        "api_signature": "jittor.nn.Conv1d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True)",
        "api_description": "对由多个输入平面组成的输入信号应用1D卷积。\n输入张量为 \\((N, C_{in}, L_{in})\\)，输出张量为 \\((N, C_{out}, L_{out})\\)。\n\\[L_{out} = \\left\\lfloor\\frac{L_{in} + 2 \\times \\text{padding} - \\text{dilation} \\times (\\text{kernel_size} - 1) - 1}{\\text{stride}} + 1\\right\\rfloor\\]\n代码示例:\n>>> m = jt.nn.Conv1d(16, 33, 3, stride=2)\n>>> input = jt.randn(20, 16, 50)\n>>> m(input).shape\n[20, 33, 24]",
        "return_value": "",
        "parameters": "in_channels(int): 输入信号的通道数\nout_channels(int): 卷积产生的通道数\nkernel_size(int): 卷积核的尺寸\nstride(int , optional): 卷积步长, 默认值为1\npadding(int , optional): 输入的每一条边补充0的层数, 默认值为0\ndilation(int , optional): 卷积核元素之间的间距, 默认值为1\ngroups(int, optional): 从输入通道到输出通道的分组连接数, 默认值为1\nbias(bool, optional): 如果bias=True，添加偏置, 默认值为True",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "jittor.nn.Conv2d",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.nn.html#jittor.nn.Conv2d",
        "alias": "Conv"
    },
    {
        "api_name": "jittor.nn.conv2d",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.nn.html#jittor.nn.conv2d",
        "api_signature": "jittor.nn.conv2d(x, weight, bias=None, stride=1, padding=0, dilation=1, groups=1)",
        "api_description": "将一个2D卷积应用于由多个输入平面组成的输入信号。\n参数:\nx (Var) : 输入张量\nweight (Var) : 卷积核\nbias (Var，optional) : 卷积后的偏置。默认值：None\nstride (int, tuple，optional) : 卷积的步长。默认值: 1\npadding (int, tuple, optional) : 输入的四周添加的填充长度。默认值: 0\ndilation (int, tuple，optional) : 卷积核元素之间的间距。默认值: 1\ngroups (int，optional) : 输入通道和输出通道之间的阻塞连接数。默认值: 1\n返回值:Var: 执行2D卷积之后的结果张量\n代码示例：>>> x = jt.randn(4, 24, 100, 100)\n>>> w = jt.randn(32, 24, 3, 3)\n>>> y = nn.conv2d(x, w)",
        "return_value": "Var: 执行2D卷积之后的结果张量",
        "parameters": "x (Var) : 输入张量\nweight (Var) : 卷积核\nbias (Var，optional) : 卷积后的偏置。默认值：None\nstride (int, tuple，optional) : 卷积的步长。默认值: 1\npadding (int, tuple, optional) : 输入的四周添加的填充长度。默认值: 0\ndilation (int, tuple，optional) : 卷积核元素之间的间距。默认值: 1\ngroups (int，optional) : 输入通道和输出通道之间的阻塞连接数。默认值: 1",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> x = jt.randn(4, 24, 100, 100)\n>>> w = jt.randn(32, 24, 3, 3)\n>>> y = nn.conv2d(x, w)"
    },
    {
        "api_name": "jittor.nn.conv3d",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.nn.html#jittor.nn.conv3d",
        "api_signature": "jittor.nn.conv3d(x, weight, bias=None, stride=1, padding=0, dilation=1, groups=1)",
        "api_description": "该函数实现了3D卷积运算。此函数首先将padding、stride、dilation参数进行处理。然后计算输出通道的数量。如果支持cuda并且使用cudnn，则进行cuda加速的卷积运算。如果不支持cuda或者groups参数不为1，则进行常规的卷积运算。最后，如果提供了bias参数，则在卷积后加上偏差。最终返回卷积后的结果。\n参数:\nx (Var) : 输入张量\nweight (Var) : 卷积核\nbias (Var，optional) : 卷积后的偏置，如未输入默认无偏置。默认值：None\nstride (int, tuple，optional) : 卷积的步长。默认值: 1\npadding (int,tuple, optional) : 输入的四周添加的填充长度。默认值: 0\ndilation (int,tuple，optional) : 卷积核元素之间的间距。默认值: 1\ngroups (int，optional) : 输入通道和输出通道之间的阻塞连接数。默认值: 1\n返回值:Var: 执行2D卷积之后的结果张量\n代码示例：>>> x = jt.randn(4, 24, 50, 50, 50)\n>>> w = jt.randn(32, 24, 3, 3, 3)\n>>> y = nn.conv3d(x, w)",
        "return_value": "Var: 执行2D卷积之后的结果张量",
        "parameters": "x (Var) : 输入张量\nweight (Var) : 卷积核\nbias (Var，optional) : 卷积后的偏置，如未输入默认无偏置。默认值：None\nstride (int, tuple，optional) : 卷积的步长。默认值: 1\npadding (int,tuple, optional) : 输入的四周添加的填充长度。默认值: 0\ndilation (int,tuple，optional) : 卷积核元素之间的间距。默认值: 1\ngroups (int，optional) : 输入通道和输出通道之间的阻塞连接数。默认值: 1",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> x = jt.randn(4, 24, 50, 50, 50)\n>>> w = jt.randn(32, 24, 3, 3, 3)\n>>> y = nn.conv3d(x, w)"
    },
    {
        "api_name": "jittor.nn.conv_transpose",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.nn.html#jittor.nn.conv_transpose",
        "api_signature": "jittor.nn.conv_transpose(input, weight, bias=None, stride=1, padding=0, output_padding=0, groups=1, dilation=1)",
        "api_description": "对输入的数据进行转置卷积操作。对输入的每个channel进行转置卷积操作，也就是通过在输入的每个channel上作卷积，得到若干子矩阵，将这些子矩阵线性映射并相加后得到的结果。\n参数:\ninput (Var) : 输入张量,shape为(N, C, H, W), N为batch size(每次处理的大小),C为通道数,H和W为高和宽\nweight (Var) : 卷积核权重,shape为(i, o, h, w), i为输入通道数,o为输出通道数,h和w分别为卷积核的高和宽\nbias (Var,optional) : 卷积后的偏置,除非为None,否则其shape应当为(o,)。默认值：None\nstride (int, tuple,optional) : 卷积的步长,单位为像素。默认值: 1\npadding (int, tuple, optional) : 输入的四周添加的填充长度,单位为像素。默认值: 0\noutput_padding (int, tuple, optional) : 输出的四周添加的填充长度,表示在转置卷积操作结束后添加到输出的零填充的数额,单位为像素。默认值: 0\ngroups (int,optional) : 输入通道和输出通道之间的阻塞连接数。默认值: 1\ndilation (int, tuple,optional) : 卷积核元素之间的间距。默认值: 1\n返回值:Var: 对输入的数据进行转置卷积操作后的结果，shape为(N, o, h_out, w_out)\n代码示例：>>> import jittor as jt\n>>> from jittor import nn\n>>> x = jt.ones(2,3,4,5)\n>>> w = jt.ones(3,6,3,3)\n>>> nn.conv_transpose(x,w).shape\n[2,6,6,7,]",
        "return_value": "Var: 对输入的数据进行转置卷积操作后的结果，shape为(N, o, h_out, w_out)",
        "parameters": "input (Var) : 输入张量,shape为(N, C, H, W), N为batch size(每次处理的大小),C为通道数,H和W为高和宽\nweight (Var) : 卷积核权重,shape为(i, o, h, w), i为输入通道数,o为输出通道数,h和w分别为卷积核的高和宽\nbias (Var,optional) : 卷积后的偏置,除非为None,否则其shape应当为(o,)。默认值：None\nstride (int, tuple,optional) : 卷积的步长,单位为像素。默认值: 1\npadding (int, tuple, optional) : 输入的四周添加的填充长度,单位为像素。默认值: 0\noutput_padding (int, tuple, optional) : 输出的四周添加的填充长度,表示在转置卷积操作结束后添加到输出的零填充的数额,单位为像素。默认值: 0\ngroups (int,optional) : 输入通道和输出通道之间的阻塞连接数。默认值: 1\ndilation (int, tuple,optional) : 卷积核元素之间的间距。默认值: 1",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> from jittor import nn\n>>> x = jt.ones(2,3,4,5)            \n>>> w = jt.ones(3,6,3,3)             \n>>> nn.conv_transpose(x,w).shape\n[2,6,6,7,]"
    },
    {
        "api_name": "jittor.nn.conv_transpose2d",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.nn.html#jittor.nn.conv_transpose2d",
        "api_signature": "jittor.nn.conv_transpose2d(input, weight, bias=None, stride=1, padding=0, output_padding=0, groups=1, dilation=1)",
        "api_description": "对输入的数据进行转置卷积操作。对输入的每个channel进行转置卷积操作，也就是通过在输入的每个channel上作卷积，得到若干子矩阵，将这些子矩阵线性映射并相加后得到的结果。\n参数:\ninput (Var) : 输入张量,shape为(N, C, H, W), N为batch size(每次处理的大小),C为通道数,H和W为高和宽\nweight (Var) : 卷积核权重,shape为(i, o, h, w), i为输入通道数,o为输出通道数,h和w分别为卷积核的高和宽\nbias (Var,optional) : 卷积后的偏置,除非为None,否则其shape应当为(o,)。默认值：None\nstride (int, tuple,optional) : 卷积的步长,单位为像素。默认值: 1\npadding (int, tuple, optional) : 输入的四周添加的填充长度,单位为像素。默认值: 0\noutput_padding (int, tuple, optional) : 输出的四周添加的填充长度,表示在转置卷积操作结束后添加到输出的零填充的数额,单位为像素。默认值: 0\ngroups (int,optional) : 输入通道和输出通道之间的阻塞连接数。默认值: 1\ndilation (int, tuple,optional) : 卷积核元素之间的间距。默认值: 1\n返回值:Var: 对输入的数据进行转置卷积操作后的结果，shape为(N, o, h_out, w_out)\n代码示例：>>> import jittor as jt\n>>> from jittor import nn\n>>> x = jt.ones(2,3,4,5)\n>>> w = jt.ones(3,6,3,3)\n>>> nn.conv_transpose(x,w).shape\n[2,6,6,7,]",
        "return_value": "Var: 对输入的数据进行转置卷积操作后的结果，shape为(N, o, h_out, w_out)",
        "parameters": "input (Var) : 输入张量,shape为(N, C, H, W), N为batch size(每次处理的大小),C为通道数,H和W为高和宽\nweight (Var) : 卷积核权重,shape为(i, o, h, w), i为输入通道数,o为输出通道数,h和w分别为卷积核的高和宽\nbias (Var,optional) : 卷积后的偏置,除非为None,否则其shape应当为(o,)。默认值：None\nstride (int, tuple,optional) : 卷积的步长,单位为像素。默认值: 1\npadding (int, tuple, optional) : 输入的四周添加的填充长度,单位为像素。默认值: 0\noutput_padding (int, tuple, optional) : 输出的四周添加的填充长度,表示在转置卷积操作结束后添加到输出的零填充的数额,单位为像素。默认值: 0\ngroups (int,optional) : 输入通道和输出通道之间的阻塞连接数。默认值: 1\ndilation (int, tuple,optional) : 卷积核元素之间的间距。默认值: 1",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> from jittor import nn\n>>> x = jt.ones(2,3,4,5)            \n>>> w = jt.ones(3,6,3,3)             \n>>> nn.conv_transpose(x,w).shape\n[2,6,6,7,]"
    },
    {
        "api_name": "jittor.nn.conv_transpose3d",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.nn.html#jittor.nn.conv_transpose3d",
        "api_signature": "jittor.nn.conv_transpose3d(input, weight, bias=None, stride=1, padding=0, output_padding=0, groups=1, dilation=1)",
        "api_description": "对输入数据执行三维反卷积操作。此操作也称为转置卷积，它将卷积核应用于输入数据，以创建具有更大尺寸的输出。\n参数:\ninput (Var) : 形状为(N, C, D, H, W)的输入张量，其中N是批量大小，C是输入通道数，D是深度，H是高度，W是宽度。\nweight (Var) : 形状为(i, o, d, h, w)的卷积核权重张量，其中i是输入通道数，o是输出通道数，d，h和w分别是深度，高度和宽度。\nbias (Var，optional) : 形状为(o,)的偏置张量。如果没有指定，将不使用任何偏置。默认值：None\nstride (int, tuple，optional) : 卷积的步长，单位为像素。如果是tuple，则按照(d, h, w)的顺序设定步长。默认值: 1\npadding (int, tuple, optional) : 输入的四周添加的填充长度，单位为像素。如果是tuple，则按照(d, h, w)的顺序设定填充。默认值: 0\noutput_padding (int, tuple, optional) : 输出的四周添加的填充长度，表示在转置卷积操作结束后添加到输出的零填充的数额，单位为像素。如果是tuple，则按照(d, h, w)的顺序设定输出填充。默认值: 0\ngroups (int，optional) : 输入通道和输出通道之间的阻塞连接数。默认值: 1\ndilation (int, tuple，optional) : 卷积核元素之间的间距。如果是元组，则按照(d, h, w)的顺序设定间距。默认值: 1\n返回值:Var: 对输入的数据进行转置卷积操作后的结果\n代码示例：>>> import jittor as jt\n>>> from jittor import nn\n>>> input = jt.ones(1,1,3,3,3)\n>>> weight = jt.ones(1,1,2,2,2)\n>>> nn.conv_transpose3d(input, weight).shape\n[1,1,4,4,4,]",
        "return_value": "Var: 对输入的数据进行转置卷积操作后的结果",
        "parameters": "input (Var) : 形状为(N, C, D, H, W)的输入张量，其中N是批量大小，C是输入通道数，D是深度，H是高度，W是宽度。\nweight (Var) : 形状为(i, o, d, h, w)的卷积核权重张量，其中i是输入通道数，o是输出通道数，d，h和w分别是深度，高度和宽度。\nbias (Var，optional) : 形状为(o,)的偏置张量。如果没有指定，将不使用任何偏置。默认值：None\nstride (int, tuple，optional) : 卷积的步长，单位为像素。如果是tuple，则按照(d, h, w)的顺序设定步长。默认值: 1\npadding (int, tuple, optional) : 输入的四周添加的填充长度，单位为像素。如果是tuple，则按照(d, h, w)的顺序设定填充。默认值: 0\noutput_padding (int, tuple, optional) : 输出的四周添加的填充长度，表示在转置卷积操作结束后添加到输出的零填充的数额，单位为像素。如果是tuple，则按照(d, h, w)的顺序设定输出填充。默认值: 0\ngroups (int，optional) : 输入通道和输出通道之间的阻塞连接数。默认值: 1\ndilation (int, tuple，optional) : 卷积核元素之间的间距。如果是元组，则按照(d, h, w)的顺序设定间距。默认值: 1",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> from jittor import nn\n>>> input = jt.ones(1,1,3,3,3) \n>>> weight = jt.ones(1,1,2,2,2) \n>>> nn.conv_transpose3d(input, weight).shape\n[1,1,4,4,4,]"
    },
    {
        "api_name": "jittor.nn.ConvTranspose2d",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.nn.html#jittor.nn.ConvTranspose2d",
        "alias": "ConvTranspose"
    },
    {
        "api_name": "jittor.nn.ConvTranspose3d",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.nn.html#jittor.nn.ConvTranspose3d",
        "api_signature": "jittor.nn.ConvTranspose3d(in_channels, out_channels, kernel_size, stride=1, padding=0, output_padding=0, groups=1, bias=True, dilation=1)",
        "api_description": "应用于由多个输入平面组成的输入图像的3D转置卷积操作。转置卷积操作通过可学习的卷积核，对每个输入值逐元素相乘，并对所有输入特征平面的输出进行求和。\n这个模块可以视为对其输入的 Conv3d 的梯度。它也被称为分数步长卷积或反卷积（尽管它不是实际的反卷积操作，因为它不计算卷积的真实逆）。输入的形状是 \\((N, C_{in}, D_{in}, H_{in}, W_{in})\\)，输出的形状是 \\((N, C_{out}, D_{out}, H_{out}, W_{out})\\)。\n\\[\\begin{split}D_{out} = (D_{in}-1) \\times \\text{stride[0]} - 2 \\times \\text{padding[0]} + \\text{dilation[0]} \\times (\\text{kernel_size[0]} - 1) + \\text{output_padding[0]} + 1\n\\\\H_{out} = (H_{in}-1) \\times \\text{stride[1]} - 2 \\times \\text{padding[1]} + \\text{dilation[1]} \\times (\\text{kernel_size[1]} - 1) + \\text{output_padding[1]} + 1\n\\\\W_{out} = (W_{in}-1) \\times \\text{stride[2]} - 2 \\times \\text{padding[2]} + \\text{dilation[2]} \\times (\\text{kernel_size[2]} - 1) + \\text{output_padding[2]} + 1\\end{split}\\]",
        "return_value": "",
        "parameters": "in_channels(int): 输入图像通道数\nout_channels(int): 输出图像通道数\nkernel_size(int, tuple): 卷积核的大小\nstride(int, tuple): 卷积步长,默认为1\npadding(int, tuple): 卷积填充,默认为0\noutput_padding(int, tuple): 输出填充,默认为0\ngroups(int): 从输入通道到输出通道的分组连接数,默认为1\nbias(bool): 是否使用偏置,默认为True\ndilation(int, tuple): 卷积核元素之间的间距,默认为1",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> m = nn.ConvTranspose3d(16, 33, (3, 5, 2), stride=(2, 1, 1), padding=(0, 4, 2))\n>>> input = jt.randn(20, 16, 10, 50, 100)\n>>> m(input).shape\n[20,33,21,46,97,]"
    },
    {
        "api_name": "jittor.nn.ConvTranspose",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.nn.html#jittor.nn.ConvTranspose",
        "api_signature": "jittor.nn.ConvTranspose(in_channels, out_channels, kernel_size, stride=1, padding=0, output_padding=0, groups=1, bias=True, dilation=1)",
        "api_description": "应用于由多个输入平面组成的输入图像的2D转置卷积操作符。\n这个模块可以视为对其输入的 Conv2D 的梯度。它也被称为分数步长卷积或反卷积（虽然它不是真正的反卷积操作，因为它不计算卷积的真实逆）。输入的形状是 \\((N, C_{in}, H_{in}, W_{in})\\)，输出的形状是 \\((N, C_{out}, H_{out}, W_{out})\\)。\n\\[\\begin{split}H_{out} = (H_{in}-1) \\times \\text{stride[0]} - 2 \\times \\text{padding[0]} + \\text{dilation[0]} \\times (\\text{kernel_size[0]} - 1) + \\text{output_padding[0]} + 1\n\\\\W_{out} = (W_{in}-1) \\times \\text{stride[1]} - 2 \\times \\text{padding[1]} + \\text{dilation[1]} \\times (\\text{kernel_size[1]} - 1) + \\text{output_padding[1]} + 1\\end{split}\\]",
        "return_value": "",
        "parameters": "in_channels(int): 输入图像通道数\nout_channels(int): 输出图像通道数\nkernel_size(int, tuple): 卷积核的大小\nstride(int, tuple): 卷积步长,默认为1\npadding(int, tuple): 卷积填充,默认为0\noutput_padding(int, tuple): 输出填充,默认为0\ngroups(int): 从输入通道到输出通道的分组连接数,默认为1\nbias(bool): 是否使用偏置,默认为True\ndilation(int, tuple): 卷积核元素之间的间距,默认为1",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> m = nn.ConvTranspose(16, 33, (3, 5), stride=(2, 1), padding=(4, 2))\n>>> input = jt.randn(20, 16, 50, 100)\n>>> m(input).shape\n[20,33,93,100,]"
    },
    {
        "api_name": "jittor.nn.Conv",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.nn.html#jittor.nn.Conv",
        "api_signature": "jittor.nn.Conv(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True)",
        "api_description": "对由多个量化输入平面组成的量化输入信号应用2D卷积。\n输入张量为 \\((N, C_{in}, H_{in}, W_{in})\\)，输出张量为 \\((N, C_{out}, H_{out}, W_{out})\\)。\n\\[\\begin{split}H_{out} = \\left\\lfloor\\frac{H_{in}  + 2 \\times \\text{padding}[0] - \\text{dilation}[0]\n\\times (\\text{kernel_size}[0] - 1) - 1}{\\text{stride}[0]} + 1\\right\\rfloor\n\\\\W_{out} = \\left\\lfloor\\frac{W_{in}  + 2 \\times \\text{padding}[1] - \\text{dilation}[1]\n\\times (\\text{kernel_size}[1] - 1) - 1}{\\text{stride}[1]} + 1\\right\\rfloor\\end{split}\\]",
        "return_value": "",
        "parameters": "in_channels(int): 输入信号的通道数\nout_channels(int): 卷积产生的通道数\nkernel_size(int): 卷积核的尺寸\nstride(int , optional): 卷积步长, 默认值为1\npadding(int , optional): 输入的每一条边补充0的层数, 默认值为0\ndilation(int , optional): 卷积核元素之间的间距, 默认值为1\ngroups(int, optional):从输入通道到输出通道的分组连接数, 默认值为1\nbias(bool, optional): 如果bias=True,添加偏置, 默认值为True",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> m = jt.nn.Conv2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2), dilation=(3, 1))\n>>> input = jt.randn(20, 16, 50, 100)\n>>> m(input).shape\n[20, 33, 26, 100]"
    },
    {
        "api_name": "jittor_core.Var.copy",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor_core.Var.copy",
        "api_signature": "jittor_core.Var.copy()",
        "api_description": "函数C++定义格式:\njt.Var copy(jt.Var x)\n该函数将复制输入变量并返回一个新的变量。它会创建一个与输入变量具有相同形状和类型的新变量，并将新变量的值设置为输入变量的值。函数不会改变输入变量 x 的值。",
        "return_value": "复制的新变量(Var)，其形状、类型和值与输入变量相同。",
        "parameters": "x (Var): 需要复制的输入变量。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> x = jt.array([1, 2, 3])\n>>> y = jt.copy(x)"
    },
    {
        "api_name": "jittor_core.Var.cos",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor_core.Var.cos",
        "api_signature": "jittor_core.Var.cos()",
        "api_description": "函数C++定义格式:\njt.Var cos(jt.Var x)\n创建一个张量, 其将 x 中的每一个数值进行余弦运算。\n\\[y_i = \\cos(x_i) \\]",
        "return_value": "返回一个新的张量, 其数值是 x 中对应位置进行余弦运算的结果",
        "parameters": "x (Var): Var类型的张量",
        "input_shape": "",
        "notes": "余弦函数的定义域是实数集，值域是 [-1, 1]\n支持使用 jt.cos() 进行调用",
        "code_example": ">>> x = jt.randn(5)\n>>> x\njt.Var([-0.8017247   2.4553642  -0.57173574 -0.06912863  1.5478854 ], dtype=float32)\n>>> x.cos()\njt.Var([ 0.6954685  -0.7736413   0.84096307  0.9976116   0.0229089 ], dtype=float32)"
    },
    {
        "api_name": "jittor_core.Var.cosh",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor_core.Var.cosh",
        "api_signature": "jittor_core.Var.cosh()",
        "api_description": "函数C++定义格式:\njt.Var cosh(jt.Var x)\n创建一个张量, 其将 x 中的每一个数值进行双曲余弦运算。\n\\[y_i = \\cosh(x_i) = \\frac{e^x_i + e^{-x_i}}{2}\\]",
        "return_value": "返回一个新的张量, 其数值是 x 中对应位置进行双曲余弦运算的结果",
        "parameters": "x (Var): Var类型的张量",
        "input_shape": "",
        "notes": "双曲余弦运算的定义域和值域为 [1, ∞)\n支持使用 x.cosh() 进行调用",
        "code_example": ">>> x = jt.randn(5) + 1\n>>> x\njt.Var([-0.46972263 -0.6531948   1.7161269   1.007987    1.3451817 ], dtype=float32)\n>>> x.cosh()\njt.Var([1.1123631 1.2210255 2.871351  1.5525162 2.0496883], dtype=float32)"
    },
    {
        "api_name": "jittor.lr_scheduler.CosineAnnealingLR",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.optim.html#jittor.lr_scheduler.CosineAnnealingLR",
        "api_signature": "jittor.lr_scheduler.CosineAnnealingLR(optimizer, T_max, eta_min=0, last_epoch=-1)",
        "api_description": "是用于实现余弦退火学习率调度策略的工具。该类实现了余弦退火学习率调度策略，其中第 \\(t\\) 步的学习率计算过程如下：\n\\[ \\begin{align}\\begin{aligned}\\eta_t = \\eta_{min} + \\dfrac{1}{2}(\\eta_{max} - \\eta_{min})\\left(1+cos\\left(\\dfrac{T_{cur}}{T_{max}}\\pi\\right)\\right), &\\quad T_{cur} = (2k+1)T_{max};\\\\\\eta_{t+1} = \\eta_{t} + \\dfrac{1}{2}(\\eta_{max} - \\eta_{min})\\left(1-cos\\left(\\dfrac{1}{T_{max}}\\pi\\right)\\right), &\\quad T_{cur} = (2k+1)T_{max}.\\end{aligned}\\end{align} \\]\n其中，\\(\\eta_{max}\\) 是初始学习率，\\(\\eta_{min}\\) 是学习率下限，\\(T_{max}\\) 是最大周期数， \\(T_{cur}\\) 是 SGDR 上一次重启后的周期（epoch）数，\n通过 last_epoch 指定。特别地， last_epoch 为 ``-1``（默认）时，将学习率设置为优化器的初始学习率。",
        "return_value": "",
        "parameters": "optimizer (Optimizer): 优化器对象，其学习率将被调整。\nT_max (int): 用来计算学习率的最大周期数。\neta_min (float, 可选): 学习率下限，默认为 0\nlast_epoch (int, 可选): 最后一次迭代的周期数，默认为 -1",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "jittor.misc.cpu",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.misc.cpu",
        "api_signature": "jittor.misc.cpu(x)",
        "api_description": "将指定的 Var 复制到CPU。",
        "return_value": "Var, 与输入数据有相同值的新数据变量，位于 CPU 上。",
        "parameters": "x (Var): 输入的张量。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> a = jt.array([1, 2, 3])\n>>> b = jt.cpu(a)\n>>> print(b)\njt.Var([1 2 3], dtype=int32)"
    },
    {
        "api_name": "jittor.transform.crop",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.transform.html#jittor.transform.crop",
        "api_signature": "jittor.transform.crop(img, top, left, height, width)",
        "api_description": "对图像进行裁剪，根据给定的边界参数从原始图像中裁剪出一个矩形区域。",
        "return_value": "返回裁剪后的图像。",
        "parameters": "img (Image.Image): 输入的 PIL 图像。\ntop (int): 裁剪框的上边界。\nleft (int): 裁剪框的左边界。\nheight (int): 裁剪框的高度。\nwidth (int): 裁剪框的宽度。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> img = np.ones((100, 100, 3)).astype(np.uint8)\n>>> img = Image.fromarray(img)\n>>> img_cropped = crop(img, 10, 10, 100, 100)"
    },
    {
        "api_name": "jittor.transform.crop_and_resize",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.transform.html#jittor.transform.crop_and_resize",
        "api_signature": "jittor.transform.crop_and_resize(img, top, left, height, width, size, interpolation=2)",
        "api_description": "裁剪后调整图像大小。",
        "return_value": "PIL.Image.Image: 裁剪并且调整大小之后的图像",
        "parameters": "img (PIL.Image.Image): 输入图像\ntop (int): 裁剪框的顶部边界\nleft (int): 裁剪框的左边界\nheight (int): 裁剪框的高度\nwidth (int): 裁剪框的宽度\nsize (list)：裁剪之后图像要调整到的（高度, 宽度）\ninterpolation (int, optional): 调整大小所用的插值方法。默认值: PIL.Image.BILINEAR",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> import numpy as np\n>>> from PIL import Image\n>>> data = np.random.rand(200,200)\n>>> img = Image.fromarray(data, 'L')       \n>>> img.size\n(200, 200)\n>>> # 截取坐标为[10,90]x[10,90]的正方形并且缩小0.5倍\n>>> jt.transform.crop_and_resize(img, 10, 10, 80, 80, [40, 40]).size \n(40, 40)"
    },
    {
        "api_name": "jittor.transform.Crop",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.transform.html#jittor.transform.Crop",
        "api_signature": "jittor.transform.Crop(top, left, height, width)",
        "api_description": "根据指定大小裁剪PIL图像。在初始化时需要指定裁剪区域的边界，之后可以作为一个可调用对象直接应用在图像上。",
        "return_value": "",
        "parameters": "top (int): 裁剪区域顶部边界的像素索引\nleft (int): 裁剪区域左侧边界的像素索引\nheight (int): 裁剪区域高度\nwidth (int): 裁剪区域宽度",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> from jittor import transform\n>>> cropper = transform.Crop(top=100, left=100, height=200, width=200)\n>>> cropped_img = cropper(img)"
    },
    {
        "api_name": "jittor.misc.cross",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.misc.cross",
        "api_signature": "jittor.misc.cross(input, other, dim=-1)",
        "api_description": "批量计算向量叉乘：\n\\((a_1, a_2, a_3) \\times (b_1, b_2, b_3) = (a_2 b_3 - a_3 b_2 ,  a_3 b_1 - a_1 b_3 ,  a_1 b_2 - a_2 b_1)\\)",
        "return_value": "input 和 other 对应向量叉乘的结果，形状与 input 和 other 相同",
        "parameters": "input (Var): 叉乘的左参数\nother (Var): 叉乘的右参数，形状与 input 相同\ndim (int): 计算叉乘的向量所在的维度，要求 input 和 other 在 dim 维的长度都是 3。负数表示从后往前数。默认值：-1",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> x = jt.randn(2, 3)\n>>> y = jt.randn(2, 3)\n>>> x, y, x.cross(y)\n(jt.Var([[ 0.6452728  -1.7699949   0.7562031 ]\n         [ 0.32050335  0.7093985  -0.44102713]], dtype=float32),\n jt.Var([[ 0.42947495  0.6956272  -0.35488197]\n         [ 0.44775873 -0.19148853 -0.52726024]], dtype=float32),\n jt.Var([[ 0.10210377  0.553766    1.2090378 ]\n         [-0.45848927 -0.02848507 -0.3790121 ]], dtype=float32))\n>>> x.cross(x)\njt.Var([[0. 0. 0.]\n        [0. 0. 0.]], dtype=float32)"
    },
    {
        "api_name": "jittor.nn.cross_entropy_loss",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.nn.html#jittor.nn.cross_entropy_loss",
        "api_signature": "jittor.nn.cross_entropy_loss(output, target, weight=None, ignore_index=None, reduction='mean')",
        "api_description": "计算交叉熵损失。对于给定的类别标签 target 和网络输出结果 output ，计算交叉熵损失。该函数主要用于分类问题的损失计算。其具体计算公式如下，其中, x 是分类预测的输出，class 是真实的类别标签：\n\\[L = -log( {e^{x[class]}}/{\\sum_{i}^{j}e^{x[i]}})\\]\n参数:\noutput (Var): 网络输出的结果，形状为(N, C)，这里N表示的是batchsize，而C为类别的数量，也可以为(N, H, W, C)形状，H，W分别为高和宽。\ntarget (Var): 真实的类别标签，形状为(N,)或者(N, 1)，也可以为(N, H, W)形状。\nweight (Var, optional): 各个类别的权重，用于权衡不同类别对损失的贡献。默认值：None，如果提供此项，其形状应为(C,)。\nignore_index (int, optional): 需要忽略的类别标签。默认值：None，即不忽略任何标签。\nreduction (str, optional): 指定减小损失的方式，可选值为’none’、’mean’或’sum’。’none’表示不执行任何减小损失，’sum’表示通过所有元素求和将损失减少，并且’mean’表示将损失通过所有元素平均值减小。默认值： ‘mean’\n返回值:Var: 计算得到的交叉熵损失。\n代码示例：>>> import jittor as jt\n>>> from jittor import nn\n>>> output = jt.array([[1,0,0], [0,1,0], [0,0,1], [1,0,0]])\n>>> label = jt.array([1, 3, 2, 2])\n>>> nn.cross_entropy_loss(output, label)\njt.Var([1.2181114], dtype=float32)",
        "return_value": "Var: 计算得到的交叉熵损失。",
        "parameters": "output (Var): 网络输出的结果，形状为(N, C)，这里N表示的是batchsize，而C为类别的数量，也可以为(N, H, W, C)形状，H，W分别为高和宽。\ntarget (Var): 真实的类别标签，形状为(N,)或者(N, 1)，也可以为(N, H, W)形状。\nweight (Var, optional): 各个类别的权重，用于权衡不同类别对损失的贡献。默认值：None，如果提供此项，其形状应为(C,)。\nignore_index (int, optional): 需要忽略的类别标签。默认值：None，即不忽略任何标签。\nreduction (str, optional): 指定减小损失的方式，可选值为’none’、’mean’或’sum’。’none’表示不执行任何减小损失，’sum’表示通过所有元素求和将损失减少，并且’mean’表示将损失通过所有元素平均值减小。默认值： ‘mean’",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> from jittor import nn\n>>> output = jt.array([[1,0,0], [0,1,0], [0,0,1], [1,0,0]])\n>>> label = jt.array([1, 3, 2, 2])\n>>> nn.cross_entropy_loss(output, label)\njt.Var([1.2181114], dtype=float32)"
    },
    {
        "api_name": "jittor.nn.CrossEntropyLoss",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.nn.html#jittor.nn.CrossEntropyLoss",
        "api_signature": "jittor.nn.CrossEntropyLoss(weight=None, ignore_index=None)",
        "api_description": "该类用于计算输出值和目标值的交叉熵损失。交叉熵损失是分类任务中常用的损失函数，特别是在处理多分类问题时。关于交叉熵损失，其数学公式为：\n\\[L = - \\sum_{i=1}^{n} y_i \\log(\\hat{y_i})\\]\n其中，\\(y_i\\) 为真实标签，\\(\\hat{y_i}\\) 为预测的概率。",
        "return_value": "",
        "parameters": "weight (Var, optional): 每个类别的权重。默认值：None，即所有类别权重相同。\nignore_index (int, optional): 指定一个要忽略的目标值，该目标值不会对输入梯度产生贡献。如果此参数未给定或者给定为None，则不会有任何目标值被忽略。默认值：None",
        "input_shape": "output: 模型输出，形状为 \\((N, C)\\)，其中 \\(N\\) 为批次大小，\\(C\\) 为类别的数量。\ntarget: 目标输出，形状为 \\((N,)\\)，其中 \\(N\\) 为批次大小。",
        "notes": "",
        "code_example": ">>> m = nn.CrossEntropyLoss()\n>>> output = jt.array([[1.5, 2.3, 0.7], [1.8, 0.5, 2.2]])\n>>> target = jt.array([1, 2])\n>>> loss_var = m(output, target)\n>>> loss_var\njt.Var([0.5591628], dtype=float32)"
    },
    {
        "api_name": "jittor.misc.ctc_loss",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.misc.ctc_loss",
        "api_signature": "jittor.misc.ctc_loss(log_probs, targets, input_lengths, target_lengths, blank=0, reduction='mean', zero_infinity=False)",
        "api_description": "完成连通时间分类 (CTC, Connectionist Temporal Classification) 损失的计算。\n计算连续（未分段）时间序列与目标序列之间的损失。CTCLoss 对输入到目标的可能对齐的概率求和，生成一个损失值，该值对每个输入节点可微分。假定输入到目标的对齐是“多对一”的，这限制了目标序列的长度，使其必须小于等于输入长度。\n代码示例:\nimport jittor as jt\nT = 50      # Input sequence length\nC = 20      # Number of classes (including blank)\nN = 16      # Batch size\nS = 30      # Target sequence length of longest target in batch (padding length)\nS_min = 10  # Minimum target length, for demonstration purposes\ninput = jt.randn(T, N, C).log_softmax(2)\n# Initialize random batch of targets (0 = blank, 1:C = classes)\ntarget = jt.randint(low=1, high=C, shape=(N, S), dtype=jt.int)\ninput_lengths = jt.full((N,), T, dtype=jt.int)\ntarget_lengths = jt.randint(low=S_min, high=S+1, shape=(N,), dtype=jt.int)\nloss = jt.ctc_loss(input, target, input_lengths, target_lengths)\n# calculation result, loss is: jt.Var([115.42529], dtype=float32)\ndinput = jt.grad(loss, input)",
        "return_value": "",
        "parameters": "log_probs (Var): 形状为[T, N, C]的张量，T为序列长度，N为批次大小，C为类别数量。\ntargets (Var): 形状为[N, S]的张量，N为批次大小，S为目标序列长度，其中的元素应在[0,C)范围内。\ninput_lengths (Var): 形状为[N]的张量，每个元素代表输入序列的长度，元素应在[0,T]范围内。\ntarget_lengths (Var): 形状为[N]的张量，每个元素代表目标序列的长度，元素应在[0,S]范围内。\nblank (int)： 默认为 0，空白标签索引\nreduction (str)：减少批量损失，如果reduction是none，它会返回一个(N,)的数组，如果reduction是mean或sum，它会返回一个标量。默认为 “mean”。\nzero_infinity (bool):  默认为 False, 用于梯度计算",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "jittor.misc.CTCLoss",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.misc.CTCLoss",
        "api_signature": "jittor.misc.CTCLoss(blank=0, reduction='mean', zero_infinity=False)",
        "api_description": "计算链接主义时序分类 (Connectionist Temporal Classification) 损失函数。\n计算一个连续时间序列和目标差别的损失函数，刻画输入与目标的对应。CTCLoss 对输入到目标的各个对齐方式的概率求和，得到一个对每个输入可微分的损失值。输入到目标的对齐应该是“多对一的”，因此目标序列长度被限制为不超过输入序列长度。",
        "return_value": "",
        "parameters": "blank (int): 空标签的编号。默认值：0\nreduction (string): 如何聚合所有批次的计算结果，mean 是均值，sum 是求和，none 是不聚合全部返回。默认值： 'mean'\nzero_infinity (bool): 将损失函数中的无穷大项和对应梯度置零（这种情况一般是在输入短于目标时出现）。默认值：False",
        "input_shape": "Input:\nlog_probs: \\((T, N, C)\\)，其中 \\(T\\) 为序列长度，\\(N\\) 为批次大小，\\(S\\) 为分类个数，表示输出序列的每项被分类到各个类的概率的对数。\ntargets: \\((N, S)\\)，其中 \\(N\\) 为批次大小，\\(S\\) 为目标序列长度，表示目标序列每项的分类序号，范围 \\([0, C)\\)\ninput_lengths: \\((N)\\)，其中 \\(N\\) 为批次大小，表示每个输入的实际长度，范围 \\([0, T]\\)\ntarget_lengths: \\((N)\\)，其中 \\(N\\) 为批次大小，表示每个目标序列的实际长度，范围 \\([0, S]\\)\n\n\n\n\nOutput: 如果 reduction 为 'mean' （默认）或 'sum' 则为一标量；如果 reduction 为 'none' 则为 \\((N,)\\)，其中 \\(N\\) 为批次大小。",
        "notes": "",
        "code_example": ">>> T, C, N, S = 50, 20, 16, 30\n>>> S_min = 10  # 最小目标长度，用于演示目的\n>>> input = jt.randn(T, N, C).log_softmax(2)\n>>> # 初始化随机批次的目标 (0 = 空白, 1:C = 类别)\n>>> target = jt.randint(low=1, high=C, shape=(N, S), dtype=jt.int)\n>>> input_lengths = jt.full((N,), T, dtype=jt.int)\n>>> target_lengths = jt.randint(low=S_min, high=S+1, shape=(N,), dtype=jt.int)\n>>> ctc_loss = jt.CTCLoss()\n>>> ctc_loss(input, target, input_lengths, target_lengths)\njt.Var([114.68321], dtype=float32)"
    },
    {
        "api_name": "jittor.misc.cub_cumsum",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.misc.cub_cumsum",
        "api_signature": "jittor.misc.cub_cumsum(x, dim=None)",
        "api_description": "沿着指定维度计算输入张量在这个维度上的累加求和，使用CUB实现。\n累加求和是指，在指定方向上：\n\\[y_i=x_1+x_2+x_3+\\cdots+x_i\\]\n注意，该函数不应该直接调用。推荐使用jittor.misc.cumsum。该函数不支持 CPU 模式，需要先指定在 GPU 上计算。",
        "return_value": "Var，输出的张量，累加求和后的结果",
        "parameters": "x (Var): 输入张量 Var, 任意形状\ndim (int): 指定进行累加求和的轴。默认为最后一维。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> jt.flags.use_cuda = 1 # 使用 GPU 计算\n>>> a = jt.randn(4, 2)\njt.Var([[ 0.7793252  -2.1805465 ]\n        [-0.46900165  2.0724497 ]\n        [-1.2677554   0.31553593]\n        [ 0.14260457 -1.4511695 ]], dtype=float32)\n>>> jt.cub_cumsum(a, 1)\njt.Var([[ 0.7793252  -1.4012213 ]\n        [-0.46900165  1.603448  ]\n        [-1.2677554  -0.9522195 ]\n        [ 0.14260457 -1.3085649 ]], dtype=float32)\n>>> jt.cub_cumsum(a, 0)\njt.Var([[ 0.7793252  -2.1805465 ]\n        [ 0.31032354 -0.10809684]\n        [-0.95743185  0.2074391 ]\n        [-0.81482726 -1.2437304 ]], dtype=float32)"
    },
    {
        "api_name": "jittor.Module.cuda",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.Module.cuda",
        "api_signature": "cuda(device=None)",
        "api_description": "将模块放置于cuda运行",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "jittor.misc.cuda",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.misc.cuda",
        "api_signature": "jittor.misc.cuda(x)",
        "api_description": "这是一个用于将Jittor的全局标记 use_cuda 设置为1，并返回输入值的函数。当 use_cuda 被设置为1时，这意味着Jittor将使用CUDA进行加速。",
        "return_value": "x : 传入该函数的输入值。",
        "parameters": "x : 输入值, 类型不限。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> jt.flags.use_cuda\n0\n>>> jt.cuda(5)\n5\n>>> jt.flags.use_cuda\n1"
    },
    {
        "api_name": "jittor.misc.cumprod",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.misc.cumprod",
        "api_signature": "jittor.misc.cumprod(x, dim=None)",
        "api_description": "沿着指定维度计算输入张量在这个维度上的累乘。\n累乘是指，在指定方向上：\n\\[y_i=x_1 \\times x_2 \\times x_3 \\times \\cdots \\times x_i\\]\n注意，由于此函数的实现是先取 \\(\\log\\) 再做累加，因此务必保证输入为正数，否则计算结果会有 nan。",
        "return_value": "Var，输出的张量，累乘后的结果，形状和输入一致",
        "parameters": "x (Var): 输入张量 Var, 任意形状\ndim (int): 指定进行乘法的轴。默认为最后一维。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> a = jt.arange(1, 7).reshape(2, 3)\njt.Var([[1 2 3]\n        [4 5 6], dtype=int32)\n>>> jt.cumprod(a, 1)\njt.Var([[  1.        2.        6.     ]\n        [  4.       20.      120.00001]], dtype=float32)\n>>> jt.cumprod(a, 0)\njt.Var([[ 1.  2.  3.]\n        [ 4. 10. 18.]], dtype=float32)"
    },
    {
        "api_name": "jittor.misc.cumsum",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.misc.cumsum",
        "api_signature": "jittor.misc.cumsum(x, dim=None)",
        "api_description": "沿着指定维度计算输入张量在这个维度上的累加求和。\n累加求和是指，在指定方向上：\n\\[y_i=x_1+x_2+x_3+\\cdots+x_i\\]\n推荐直接调用该函数计算 cumsum。取决于是否指定在 GPU 上计算，该函数会调用不同的算子实现。",
        "return_value": "Var，输出的张量，累加求和后的结果",
        "parameters": "x (Var): 输入张量 Var, 任意形状\ndim (int): 指定进行累加求和的轴。默认为最后一维。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> a = jt.randn(4, 2)\njt.Var([[ 0.7793252  -2.1805465 ]\n        [-0.46900165  2.0724497 ]\n        [-1.2677554   0.31553593]\n        [ 0.14260457 -1.4511695 ]], dtype=float32)\n>>> jt.cumsum(a, 1)\njt.Var([[ 0.7793252  -1.4012213 ]\n        [-0.46900165  1.603448  ]\n        [-1.2677554  -0.9522195 ]\n        [ 0.14260457 -1.3085649 ]], dtype=float32)\n>>> jt.cumsum(a, 0)\njt.Var([[ 0.7793252  -2.1805465 ]\n        [ 0.31032354 -0.10809684]\n        [-0.95743185  0.2074391 ]\n        [-0.81482726 -1.2437304 ]], dtype=float32)"
    },
    {
        "api_name": "jittor.dataset.DataLoader",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.dataset.html#jittor.dataset.DataLoader",
        "api_signature": "jittor.dataset.DataLoader(dataset: Dataset, *args, **kargs)",
        "api_description": "DataLoader结合了数据集和采样器, 并在给定的数据集上提供可迭代性。\n参数:\ndataset(Dataset): 需要进行封装的数据集。\nargs: 数据集属性设置的可变位置参数。\nkargs: 数据集属性设置的可变关键字参数, 使用字典的形式传递参数值。\n返回值:封装好的数据加载器( DataLoader )。\n代码示例:>>> from jittor.dataset.cifar import CIFAR10\n>>> from jittor.dataset import DataLoader\n>>> train_dataset = CIFAR10()\n>>> dataloader = DataLoader(train_dataset, batch_size=8)\n>>> for batch_idx, (x_, target) in enumerate(dataloader):\n>>>     # 处理每个批次的数据",
        "return_value": "封装好的数据加载器( DataLoader )。",
        "parameters": "dataset(Dataset): 需要进行封装的数据集。\nargs: 数据集属性设置的可变位置参数。\nkargs: 数据集属性设置的可变关键字参数, 使用字典的形式传递参数值。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> from jittor.dataset.cifar import CIFAR10 \n>>> from jittor.dataset import DataLoader\n>>> train_dataset = CIFAR10()\n>>> dataloader = DataLoader(train_dataset, batch_size=8)\n>>> for batch_idx, (x_, target) in enumerate(dataloader):\n>>>     # 处理每个批次的数据"
    },
    {
        "api_name": "jittor.dataset.Dataset",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.dataset.html#jittor.dataset.Dataset",
        "api_signature": "jittor.dataset.Dataset(batch_size=16, shuffle=False, drop_last=False, num_workers=0, buffer_size=536870912, stop_grad=True, keep_numpy_array=False, endless=False)",
        "api_description": "数据集的抽象类。用户需要继承此类并实现其中的 __getitem__ 方法以便遍历数据。\n参数:\nbatch_size (int, optional): 批大小。默认值: 16\nshuffle (bool, optional): 每个epoch是否进行随机打乱。默认值: False\ndrop_last (bool, optional): 若为True, 则可能丢弃每个epoch中最后不足batch_size的数据。默认值: False\nnum_workers (int, optional): 用于加载数据的工作进程数。默认值: 0\nbuffer_size (int, optional): 每个工作进程的缓存大小(字节)。默认值: 512MB(即512 * 1024 * 1024)\nkeep_numpy_array (bool, optional): 返回numpy数组而非jittor数组。默认值: False\nendless (bool, optional): 此数据集是否无限循环地产生数据。默认值: False\n属性:\ntotal_len (int): 数据集的总长度\nepoch_id (int): 当前的epoch编号\n代码示例:  >>> class YourDataset(Dataset):\n>>>     def __init__(self):\n>>>         super().__init__()\n>>>         self.set_attrs(total_len=1024)\n>>>\n>>>     def __getitem__(self, k):\n>>>         # 返回数据的逻辑\n>>>         return k, k*k\n>>> # 实例化你的数据集, 并设置批大小和是否打乱数据\n>>> dataset = YourDataset().set_attrs(batch_size=256, shuffle=True)\n>>> # 遍历数据集\n>>> for x, y in dataset:\n>>>     # 处理数据的逻辑",
        "return_value": "",
        "parameters": "batch_size (int, optional): 批大小。默认值: 16\nshuffle (bool, optional): 每个epoch是否进行随机打乱。默认值: False\ndrop_last (bool, optional): 若为True, 则可能丢弃每个epoch中最后不足batch_size的数据。默认值: False\nnum_workers (int, optional): 用于加载数据的工作进程数。默认值: 0\nbuffer_size (int, optional): 每个工作进程的缓存大小(字节)。默认值: 512MB(即512 * 1024 * 1024)\nkeep_numpy_array (bool, optional): 返回numpy数组而非jittor数组。默认值: False\nendless (bool, optional): 此数据集是否无限循环地产生数据。默认值: False",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> class YourDataset(Dataset):\n>>>     def __init__(self):\n>>>         super().__init__()\n>>>         self.set_attrs(total_len=1024)\n>>> \n>>>     def __getitem__(self, k):\n>>>         # 返回数据的逻辑\n>>>         return k, k*k\n>>> # 实例化你的数据集, 并设置批大小和是否打乱数据\n>>> dataset = YourDataset().set_attrs(batch_size=256, shuffle=True)\n>>> # 遍历数据集\n>>> for x, y in dataset:\n>>>     # 处理数据的逻辑"
    },
    {
        "api_name": "jittor_core.Var.debug_msg",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor_core.Var.debug_msg",
        "api_signature": "jittor_core.Var.debug_msg()",
        "api_description": "函数C++定义格式:\nstring debug_msg()\n打印对应参数的信息用于debug。",
        "return_value": "str: 返回对应参数的信息。",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> x = jt.randn(5)\n>>> x.debug_msg()\n'Var(17:1:1:1:i1:o0:s0:n0:g1,float32,,0)[5,]'"
    },
    {
        "api_name": "jittor.misc.deg2rad",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.misc.deg2rad",
        "api_signature": "jittor.misc.deg2rad(x)",
        "api_description": "将输入中的每一个元素从角度值（以度为单位）转换为弧度值。转换的公式为:\n\\[rad = deg * \\frac{\\pi}{180}\\]\n代码示例:\n>>> input = jt.float32([[180.0, -180.0], [360.0, -360.0], [90.0, -90.0]])\n>>> jt.deg2rad(a)\njt.Var([[ 3.141593  -3.141593 ]\n[ 6.283186  -6.283186 ]\n[ 1.5707965 -1.5707965]], dtype=float32)",
        "return_value": "",
        "parameters": "x (Var) : 输入的角度值，可以是单一浮点数或任意形状的输入 Var",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "jittor.models.densenet121",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.models.html#jittor.models.densenet121",
        "api_signature": "jittor.models.densenet121(pretrained=False, **kwargs)",
        "api_description": "构建一个Densenet121模型\nDensenet源自论文 Densely Connected Convolutional Networks, DenseNet的特点是每个层都与前面的层连接, 这种连接方式使得网络的特征传递更加高效, 同时也有助于解决梯度消失的问题。DenseNet的网络结构包含了多个DenseBlock和Transition层, 其中DenseBlock用于特征提取, Transition层用于特征维度的缩减。\n参数:\npretrained (bool, optional): 如果为 True, 返回在ImageNet上预训练的模型。默认为 False。\n**kwargs: 可变参数, 用于传递额外的配置选项。\n返回值:\n返回构建好的Densenet121模型实例。如果 pretrained 为 True, 则返回在ImageNet上预训练的模型。\n代码示例:>>> import jittor as jt\n>>> from jittor.models.densenet import *\n>>> net = densenet121(pretrained=False)\n>>> x = jt.rand(1, 3, 224, 224)\n>>> y = net(x)\n>>> y.shape\n[1, 1000]",
        "return_value": "返回构建好的Densenet121模型实例。如果 pretrained 为 True, 则返回在ImageNet上预训练的模型。",
        "parameters": "pretrained (bool, optional): 如果为 True, 返回在ImageNet上预训练的模型。默认为 False。\n**kwargs: 可变参数, 用于传递额外的配置选项。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> from jittor.models.densenet import *\n>>> net = densenet121(pretrained=False)\n>>> x = jt.rand(1, 3, 224, 224)\n>>> y = net(x)\n>>> y.shape\n[1, 1000]"
    },
    {
        "api_name": "jittor.models.densenet161",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.models.html#jittor.models.densenet161",
        "api_signature": "jittor.models.densenet161(pretrained=False, **kwargs)",
        "api_description": "构建一个Densenet161模型\nDensenet源自论文 Densely Connected Convolutional Networks , DenseNet的特点是每个层都与前面的层连接, 这种连接方式使得网络的特征传递更加高效, 同时也有助于解决梯度消失的问题。DenseNet的网络结构包含了多个DenseBlock和Transition层, 其中DenseBlock用于特征提取, Transition层用于特征维度的缩减。\n参数:\npretrained (bool, optional): 如果为 True, 返回在ImageNet上预训练的模型。默认为 False。\n**kwargs: 可变参数, 用于传递额外的配置选项。\n返回值:\n返回构建好的Densenet161模型实例。如果 pretrained 为 True, 则返回在ImageNet上预训练的模型。\n代码示例:>>> import jittor as jt\n>>> from jittor.models.densenet import *\n>>> net = densenet161(pretrained=False)\n>>> x = jt.rand(1, 3, 224, 224)\n>>> y = net(x)\n>>> y.shape\n[1, 1000]",
        "return_value": "返回构建好的Densenet161模型实例。如果 pretrained 为 True, 则返回在ImageNet上预训练的模型。",
        "parameters": "pretrained (bool, optional): 如果为 True, 返回在ImageNet上预训练的模型。默认为 False。\n**kwargs: 可变参数, 用于传递额外的配置选项。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> from jittor.models.densenet import *\n>>> net = densenet161(pretrained=False)\n>>> x = jt.rand(1, 3, 224, 224)\n>>> y = net(x)\n>>> y.shape\n[1, 1000]"
    },
    {
        "api_name": "jittor.models.densenet169",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.models.html#jittor.models.densenet169",
        "api_signature": "jittor.models.densenet169(pretrained=False, **kwargs)",
        "api_description": "构建一个Densenet169模型\nDensenet源自论文 Densely Connected Convolutional Networks , DenseNet的特点是每个层都与前面的层连接, 这种连接方式使得网络的特征传递更加高效, 同时也有助于解决梯度消失的问题。DenseNet的网络结构包含了多个DenseBlock和Transition层, 其中DenseBlock用于特征提取, Transition层用于特征维度的缩减。\n参数:\npretrained (bool, optional): 如果为 True, 返回在ImageNet上预训练的模型。默认为 False。\n**kwargs: 可变参数, 用于传递额外的配置选项。\n返回值:\n返回构建好的Densenet169模型实例。如果 pretrained 为 True, 则返回在ImageNet上预训练的模型。\n代码示例:>>> import jittor as jt\n>>> from jittor.models.densenet import *\n>>> net = densenet169(pretrained=False)\n>>> x = jt.rand(1, 3, 224, 224)\n>>> y = net(x)\n>>> y.shape\n[1, 1000]",
        "return_value": "返回构建好的Densenet169模型实例。如果 pretrained 为 True, 则返回在ImageNet上预训练的模型。",
        "parameters": "pretrained (bool, optional): 如果为 True, 返回在ImageNet上预训练的模型。默认为 False。\n**kwargs: 可变参数, 用于传递额外的配置选项。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> from jittor.models.densenet import *\n>>> net = densenet169(pretrained=False)\n>>> x = jt.rand(1, 3, 224, 224)\n>>> y = net(x)\n>>> y.shape\n[1, 1000]"
    },
    {
        "api_name": "jittor.models.densenet201",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.models.html#jittor.models.densenet201",
        "api_signature": "jittor.models.densenet201(pretrained=False, **kwargs)",
        "api_description": "构建一个Densenet201模型\nDensenet源自论文 Densely Connected Convolutional Networks , DenseNet的特点是每个层都与前面的层连接, 这种连接方式使得网络的特征传递更加高效, 同时也有助于解决梯度消失的问题。DenseNet的网络结构包含了多个DenseBlock和Transition层, 其中DenseBlock用于特征提取, Transition层用于特征维度的缩减。\n参数:\npretrained (bool, optional): 如果为 True, 返回在ImageNet上预训练的模型。默认为 False。\n**kwargs: 可变参数, 用于传递额外的配置选项。\n返回值:\n返回构建好的Densenet201模型实例。如果 pretrained 为 True, 则返回在ImageNet上预训练的模型。\n代码示例:>>> import jittor as jt\n>>> from jittor.models.densenet import *\n>>> net = densenet201(pretrained=False)\n>>> x = jt.rand(1, 3, 224, 224)\n>>> y = net(x)\n>>> y.shape\n[1, 1000]",
        "return_value": "返回构建好的Densenet201模型实例。如果 pretrained 为 True, 则返回在ImageNet上预训练的模型。",
        "parameters": "pretrained (bool, optional): 如果为 True, 返回在ImageNet上预训练的模型。默认为 False。\n**kwargs: 可变参数, 用于传递额外的配置选项。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> from jittor.models.densenet import *\n>>> net = densenet201(pretrained=False)\n>>> x = jt.rand(1, 3, 224, 224)\n>>> y = net(x)\n>>> y.shape\n[1, 1000]"
    },
    {
        "api_name": "jittor.models.DenseNet",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.models.html#jittor.models.DenseNet",
        "api_signature": "jittor.models.DenseNet(growth_rate=32, block_config=(6, 12, 24, 16)",
        "api_description": "DenseNet源自论文 Densely Connected Convolutional Networks, DenseNet的特点是每个层都与前面的层连接, 这种连接方式使得网络的特征传递更加高效, 同时也有助于解决梯度消失的问题。DenseNet的网络结构包含了多个DenseBlock和Transition层, 其中DenseBlock用于特征提取, Transition层用于特征维度的缩减。",
        "return_value": "",
        "parameters": "growth_rate (int): 每层添加的过滤器数量(论文中的 k)。\nblock_config (list of 4 ints): 每个池化块中的层数。\nnum_init_features (int): 第一个卷积层学习的过滤器数量。\nbn_size (int): 瓶颈层的数量的乘法因子(即瓶颈层中的特征数量为 bn_size * k)。\ndrop_rate (float): 每个密集层后的丢弃率。\nnum_classes (int): 分类类别数。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> from jittor.models.densenet import DenseNet\n>>> model = DenseNet(growth_rate=32, block_config=(6, 12, 24, 16),\n...                  num_init_features=64, bn_size=4, drop_rate=0.5,\n...                  num_classes=1000)\n>>> input = jt.randn(1, 3, 224, 224)\n>>> output = model(input)\n>>> output.shape\n[1,1000,]"
    },
    {
        "api_name": "jittor.linalg.det",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.linalg.html#jittor.linalg.det",
        "api_signature": "jittor.linalg.det(x)",
        "api_description": "计算输入矩阵的行列式值。",
        "return_value": "输入矩阵 \\(x\\) 的行列式值, 形状为 (..., 1) 的 Var。",
        "parameters": "x (Var): 输入的张量, 要求为形状 (..., M, M) 的张量, 其中 \\(M\\) 是矩阵的维度。此张量可以是任意维度, 最后两个维度被视为2D矩阵。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> x = jt.array([[1.0,2.0],[3.0,4.0]])\n>>> print(jt.linalg.det(x))\njt.Var([-2.], dtype=float32)"
    },
    {
        "api_name": "jittor.detach",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.detach",
        "api_signature": "jittor.detach(x)",
        "api_description": "创建一个已经存在的张量 x 的副本，这个副本与原始张量共享数据，但不参与梯度计算。",
        "return_value": "返回副本张量（Var）",
        "parameters": "x (Var): Var类型的张量",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> x = jt.randn((3,2))\njt.Var([[-1.7332076 -3.9374337]\n        [ 2.2316337  1.2128713]\n        [-2.067835   0.8186041]], dtype=float32)\n>>> x.detach()\njt.Var([[-1.7332076 -3.9374337]\n        [ 2.2316337  1.2128713]\n        [-2.067835   0.8186041]], dtype=float32)"
    },
    {
        "api_name": "jittor_core.Var.detach",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor_core.Var.detach",
        "api_signature": "jittor_core.Var.detach()",
        "api_description": "函数C++定义格式:\ninline jt.Var detach()\n创建一个已经存在的张量 x 的副本，这个副本与原始张量共享数据，但不参与梯度计算。这在神经网络的训练过程中非常重要，尤其是在需要操作数据但又不想影响梯度计算时。",
        "return_value": "返回副本张量（Var）",
        "parameters": "x (Var): Var类型的张量",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> x = jt.randn((3,2))\njt.Var([[-1.7332076 -3.9374337]\n        [ 2.2316337  1.2128713]\n        [-2.067835   0.8186041]], dtype=float32)\n>>> x.detach()\njt.Var([[-1.7332076 -3.9374337]\n        [ 2.2316337  1.2128713]\n        [-2.067835   0.8186041]], dtype=float32)"
    },
    {
        "api_name": "jittor_core.Var.detach_inplace",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor_core.Var.detach_inplace",
        "api_signature": "jittor_core.Var.detach_inplace()",
        "api_description": "函数C++定义格式:\njt.Var start_grad()\n开启当前变量的梯度记录。",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "jittor.dfs_to_numpy",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.dfs_to_numpy",
        "api_signature": "jittor.dfs_to_numpy(x)",
        "api_description": "将 Jittor 的变量对象转换为 \\(numpy\\) 数组的函数。\n函数使用深度优先搜索 \\(（DFS）\\) 的方式，遍历输入数据中的所有元素，对每个元素进行判断，如果某个元素是 Jittor 的 \\(var\\) 对象，就将其转换为 \\(numpy\\) 数组。否则，保持不变。",
        "return_value": "输入数据的转换结果 (类型和输入变量一致)。所有的 Jittor Var 对象都被转换为 numpy 数组，其余数据不变。",
        "parameters": "x(list, dict, Var 或其它类型)：输入数据，可以是 \\(Python\\) 的内置类型（list，dict）或者 Jittor 的 \\(Var\\) 对象，或者是这些类型的嵌套。如果输入数据包含 \\(Var\\) 对象，它将被转换为 \\(numpy\\) 数组。其它类型的数据将保持不变。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import numpy as np\n>>> a = jt.ones([2, 3]))\n>>> b = [a, a]\n>>> c = {\"x\": a, \"y\": b}\n>>> res_c = dfs_to_numpy(c)\n>>> isinstance(res_c[\"x\"], np.ndarray)\nTrue\n >>> isinstance(res_c[\"y\"][0], np.ndarray)\nTrue"
    },
    {
        "api_name": "jittor.misc.diag",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.misc.diag",
        "api_signature": "jittor.misc.diag(x, diagonal=0)",
        "api_description": "从向量构建对角矩阵或从方阵中提取对角元素。\n如果输入是一个一维向量，返回一个二维方阵，其对角线上的元素由输入向量中的元素构成。\n如果输入是一个二维方阵，返回一个由它的对角元素构成的一维向量。\n对角线的偏移量可以用 diagonal 确定，diagonal = 0 代表主对角线， diagonal > 0 为沿右上方移动， diagonal < 0  为沿左下方移动。",
        "return_value": "向量或方阵 (Var)",
        "parameters": "x (Var): 输入张量 Var, 向量或矩阵。\ndiagonal (int): 对角偏移量，默认为 0。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> a = jt.arange(4)\n>>> jt.diag(a)\njt.Var([[0 0 0 0]\n        [0 1 0 0]\n        [0 0 2 0]\n        [0 0 0 3]], dtype=int32)\n>>> b = jt.randn(2,2)\njt.Var([[-0.36892104  1.5115978 ]\n        [ 0.8735136   0.81891364]], dtype=float32)\n>>> jt.diag(b)\njt.Var([-0.36892104, 0.81891364], dtype=float32)"
    },
    {
        "api_name": "jittor_core.Var.dim",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor_core.Var.dim",
        "api_signature": "jittor_core.Var.dim()",
        "api_description": "函数C++定义格式:\ninline int ndim()\n返回张量有维度数量。",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "jittor.dirty_fix_pytorch_runtime_error",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.dirty_fix_pytorch_runtime_error",
        "api_signature": "jittor.dirty_fix_pytorch_runtime_error()",
        "api_description": "用于处理PyTorch的运行时错误。",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> jt.dirty_fix_pytorch_runtime_error()\n>>> import torch"
    },
    {
        "api_name": "jittor.display_memory_info",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.display_memory_info",
        "api_signature": "jittor.display_memory_info()",
        "api_description": "展示当前Jittor正在使用的内存相关信息。",
        "return_value": "None。",
        "parameters": "不需要任何参数",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> jt.display_memory_info()\n=== display_memory_info ===\ntotal_cpu_ram: 125.7GB total_device_ram: 23.69GB\nhold_vars: 0 lived_vars: 0 lived_ops: 0\nname: sfrl is_device: 1 used:     0 B(-nan%) unused:     0 B(-nan%) total:     0 B\nname: sfrl is_device: 0 used:     0 B(-nan%) unused:     0 B(-nan%) total:     0 B\nname: sfrl is_device: 0 used:     0 B(-nan%) unused:     0 B(-nan%) total:     0 B\nname: sfrl is_device: 0 used:     0 B(-nan%) unused:     0 B(-nan%) total:     0 B\ncpu&gpu:     0 B gpu:     0 B cpu:     0 B\nfree: cpu(51.98GB) gpu(22.28GB)\nswap: total(    0 B) last(    0 B)\n==========================="
    },
    {
        "api_name": "jittor_core.Var.div",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor_core.Var.div",
        "api_signature": "jittor_core.Var.div()",
        "api_description": "函数C++定义格式:\njt.Var divide(jt.Var x, jt.Var y)\n对两个张量中对应元素计算求商，也可以使用 / 运算符调用。",
        "return_value": "x 和 y 每对对应元素相除结果，形状与 x 和 y 相同",
        "parameters": "x (Var): 输入数据\ny (Var): 输入数据，形状与 x 相同",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> x = jt.array([1, 2, 4])\n>>> y = jt.array([3, 4, 5])\n>>> x.divide(y)\njt.Var([0.3333333  0.49999997 0.8       ], dtype=float32)\n>>> x / y\njt.Var([0.3333333  0.49999997 0.8       ], dtype=float32)"
    },
    {
        "api_name": "jittor_core.Var.divide",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor_core.Var.divide",
        "api_signature": "jittor_core.Var.divide()",
        "api_description": "函数C++定义格式:\njt.Var divide(jt.Var x, jt.Var y)\n对两个张量中对应元素计算求商，也可以使用 / 运算符调用。",
        "return_value": "x 和 y 每对对应元素相除结果，形状与 x 和 y 相同",
        "parameters": "x (Var): 输入数据\ny (Var): 输入数据，形状与 x 相同",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> x = jt.array([1, 2, 4])\n>>> y = jt.array([3, 4, 5])\n>>> x.divide(y)\njt.Var([0.3333333  0.49999997 0.8       ], dtype=float32)\n>>> x / y\njt.Var([0.3333333  0.49999997 0.8       ], dtype=float32)"
    },
    {
        "api_name": "jittor_core.Var.double",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor_core.Var.double",
        "api_signature": "jittor_core.Var.double()",
        "api_description": "函数C++定义格式:\njt.Var float64_(jt.Var x)\n创建并返回一个 x 的张量副本，并将其类型转换为 float64 ，返回双精度的浮点数，即用64个比特(8个字节)表示一个数。",
        "return_value": "返回一个新的张量, 数据类型为 float64",
        "parameters": "x (Var): Var类型的张量",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> x = jt.randn((2,2))\n>>> x\njt.Var([[ 1.4581492   0.2615918 ]\n        [ 0.26567948 -0.34585235]], dtype=float32)\n>>> x.float64()\njt.Var([[ 1.45814919  0.26159179]\n        [ 0.26567948 -0.34585235]], dtype=float64)"
    },
    {
        "api_name": "jittor.nn.dropout",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.nn.html#jittor.nn.dropout",
        "api_signature": "jittor.nn.dropout(x, p=0.5, is_train=False)",
        "api_description": "该函数实现了dropout操作。此函数会在训练阶段随机将输入张量x中约p比例的元素设置为0，从而防止过拟合。在测试阶段，此函数会返回原始的输入张量x，而不进行dropout操作。在数学上，dropout操作表示如下，这里的 \\(\\frac{x}{1-p}\\) 操作确保了在训练和测试阶段，该层的输出的期望保持不变：\n\\[\\begin{split}y = \\begin{cases}\n0 & 概率 p \\\\\n\\frac{x}{1-p} & 概率1-p\n\\end{cases}\\end{split}\\]\n参数:\nx (Var): 输入张量\np (float, optional): dropout的概率。它是一个介于0和1之间的float值，表示每个元素被设置为0的概率。例如，如果p=0.5，那么输入张量中约有一半的元素会被设置为0。默认值: 0.5\nis_train (bool, optional): 一个布尔值，表示是否在训练模式下运行。如果 is_train==True ，那么会执行dropout操作；如果is_train=False，那么返回原始的输入张量x。默认值: False\n返回值:Var: 一个和输入张量x相同形状的张量，为x执行完dropout操作的结果。\n代码示例：>>> import jittor as jt\n>>> from jittor import nn\n>>> x = jt.ones(3,3)\n>>> nn.dropout(x)\njt.Var([[1. 1. 1.]\n[1. 1. 1.]\n[1. 1. 1.]], dtype=float32)\n>>> nn.dropout(x, is_train=True)\njt.Var([[0. 0. 0.]\n[0. 2. 2.]\n[0. 2. 2.]], dtype=float32)",
        "return_value": "Var: 一个和输入张量x相同形状的张量，为x执行完dropout操作的结果。",
        "parameters": "x (Var): 输入张量\np (float, optional): dropout的概率。它是一个介于0和1之间的float值，表示每个元素被设置为0的概率。例如，如果p=0.5，那么输入张量中约有一半的元素会被设置为0。默认值: 0.5\nis_train (bool, optional): 一个布尔值，表示是否在训练模式下运行。如果 is_train==True ，那么会执行dropout操作；如果is_train=False，那么返回原始的输入张量x。默认值: False",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> from jittor import nn\n>>> x = jt.ones(3,3)\n>>> nn.dropout(x)\njt.Var([[1. 1. 1.]\n        [1. 1. 1.]\n        [1. 1. 1.]], dtype=float32)\n>>> nn.dropout(x, is_train=True) \njt.Var([[0. 0. 0.]\n        [0. 2. 2.]\n        [0. 2. 2.]], dtype=float32)"
    },
    {
        "api_name": "jittor.nn.dropout2d",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.nn.html#jittor.nn.dropout2d",
        "api_signature": "jittor.nn.dropout2d(x, p=0.5, is_train=False)",
        "api_description": "对2D输入数据执行Dropout操作。此函数会在训练阶段随机将输入张量x中约p比例的元素设置为0，从而防止过拟合。通过这种方式，Dropout可以等同于对大量不同的神经网络进行模型平均。\n在测试阶段，此函数会返回原始的输入张量x，而不进行dropout操作。在数学上，dropout操作表示如下，这里的 \\(\\frac{x}{1-p}\\) 操作确保了在训练和测试阶段，该层的输出的期望保持不变，y是输出结果，x是输入数据：\n\\[\\begin{split}y = \\begin{cases}\n0 & 概率 p \\\\\n\\frac{x}{1-p} & 概率1-p\n\\end{cases}\\end{split}\\]\n参数:\nx (Var): 输入张量，应为2D的Jittor数组。\np (float, optional): dropout的概率。它是一个介于0和1之间的float值，表示每个元素被设置为0的概率。例如，如果p=0.5，那么输入张量中约有一半的元素会被设置为0。默认值: 0.5\nis_train (bool, optional): 一个布尔值，表示是否在训练模式下运行。如果is_train=True ，那么会执行dropout操作；如果is_train=False，那么返回原始的输入张量x。默认值: False\n返回值:Var: 一个和输入张量x相同形状的张量，为x执行完dropout操作的结果。\n代码示例：>>> import jittor as jt\n>>> from jittor import nn\n>>> x = jt.array([[0, 0.5, 1.0], [-0.3, 0.5, 0.8]])\n>>> nn.dropout2d(x, 0.2, True)\njt.Var([[[ 0.     0.625  1.25 ]\n[-0.375  0.625  1.   ]]], dtype=float32)\n>>> nn.dropout2d(x)\njt.Var([[ 0.   0.5  1. ]\n[-0.3  0.5  0.8]], dtype=float32)",
        "return_value": "Var: 一个和输入张量x相同形状的张量，为x执行完dropout操作的结果。",
        "parameters": "x (Var): 输入张量，应为2D的Jittor数组。\np (float, optional): dropout的概率。它是一个介于0和1之间的float值，表示每个元素被设置为0的概率。例如，如果p=0.5，那么输入张量中约有一半的元素会被设置为0。默认值: 0.5\nis_train (bool, optional): 一个布尔值，表示是否在训练模式下运行。如果is_train=True ，那么会执行dropout操作；如果is_train=False，那么返回原始的输入张量x。默认值: False",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> from jittor import nn\n>>> x = jt.array([[0, 0.5, 1.0], [-0.3, 0.5, 0.8]])\n>>> nn.dropout2d(x, 0.2, True) \njt.Var([[[ 0.     0.625  1.25 ]\n        [-0.375  0.625  1.   ]]], dtype=float32)\n>>> nn.dropout2d(x)            \njt.Var([[ 0.   0.5  1. ]\n        [-0.3  0.5  0.8]], dtype=float32)"
    },
    {
        "api_name": "jittor.nn.Dropout2d",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.nn.html#jittor.nn.Dropout2d",
        "api_signature": "jittor.nn.Dropout2d(p=0.5, is_train=False)",
        "api_description": "该类是一种用于减少神经网络过拟合的正则化技术，是一种特殊类型的 Dropout。按伯努利分布随机将输入的一些通道置零，其中每个通道的都是一个二维特征映射。这样的输出常常产生在二维卷积层之后。",
        "return_value": "",
        "parameters": "p (float, optional): 通道被置零的概率，默认值为 0.5。p 的值应在 0 到 1 之间（包含）\nis_train (bool, optional):  若设置为 True ，则 Dropout2d 层处于激活状态，会随机将输入张量的一些元素置零。若为 False，则 Dropout 层处于非激活状态，表现为恒等函数。默认值：False",
        "input_shape": "Input: \\((N, C, H, W)\\) 或 \\((N, C, L)\\), 其中 \\(N\\) 是批量大小，\\(C\\) 是通道数，\\(H\\) 是高度，\\(W\\) 是宽度\nOutput:  \\((N, C, H, W)\\) 或 \\((N, C, L)\\) (和输入保持一致)",
        "notes": "",
        "code_example": ">>> layer = nn.Dropout2d()\n>>> input = jt.randn(128, 20, 16, 16)\n>>> output = layer(input)\n>>> print(output.size())\n[128,20,16,16,]"
    },
    {
        "api_name": "jittor.nn.Dropout",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.nn.html#jittor.nn.Dropout",
        "api_signature": "jittor.nn.Dropout(p=0.5, is_train=False)",
        "api_description": "该类是一种用于减少神经网络过拟合的正则化技术。它通过在训练过程中以 p 概率丢弃（即置为零）网络层输出特征的一部分来工作，详细信息可参考论文 Improving neural networks by preventing co-adaptation of feature detectors.",
        "return_value": "",
        "parameters": "p (float, optional): 元素被置零的概率，默认值为 0.5。p 的值应在 0 到 1 之间（包含）\nis_train (bool, optional):  若设置为 True ，则 Dropout 层处于激活状态，会随机将输入张量的一些元素置零。若为 False，则 Dropout 层处于非激活状态，表现为恒等函数。默认值：False",
        "input_shape": "Input: \\(( *)\\)，其中 * 表示任意数量的附加维数。\nOutput: \\((*)\\), 和输入形状相同",
        "notes": "",
        "code_example": ">>> layer = nn.Dropout()\n>>> input = jt.randn(128, 20)\n>>> output = layer(input)\n>>> print(output.size())\n[128,20,]"
    },
    {
        "api_name": "jittor.nn.droppath",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.nn.html#jittor.nn.droppath",
        "api_signature": "jittor.nn.droppath(x, p=0.5, is_train=False)",
        "api_description": "这个函数通过DropPath算法实现对输入的影响，其中概率参数p和训练状态is_train可以被调整。其数学表示如下，其中 \\(y_i\\) 是输出张量, \\(x_i\\) 是输入张量, \\(z_i\\) 是从均匀分布[0, 1]中随机采样的一个随机值：\n\\[\\begin{split}y_i = \\begin{cases}\n0, & \\text{if } z_i < p \\text{ and } \\text{is_train} = \\text{True} \\\\\nx_i, & \\text{otherwise}\n\\end{cases}\\end{split}\\]\n参数:\nx (Var): 输入张量\np (float, optional): DropPath算法的概率参数。默认值: 0.5\nis_train (bool, optional): 一个布尔值，表示是否在训练模式下运行。如果is_train=True ，那么会执行dropout操作；如果is_train=False，那么返回原始的输入张量x。默认值: False\n返回值:Var: 一个和输入张量x相同形状的张量，为x执行完DropPath操作的结果。\n代码示例：>>> import jittor as jt\n>>> from jittor import nn\n>>> x = jt.array([[0, 0.5, 1.0], [-0.3, 0.5, 0.8]])\n>>> nn.droppath(x, is_train=True)\njt.Var([[ 0.  1.  2.]\n[-0.  0.  0.]], dtype=float32)\n>>> nn.droppath(x)\njt.Var([[ 0.   0.5  1. ]\n[-0.3  0.5  0.8]], dtype=float32)",
        "return_value": "Var: 一个和输入张量x相同形状的张量，为x执行完DropPath操作的结果。",
        "parameters": "x (Var): 输入张量\np (float, optional): DropPath算法的概率参数。默认值: 0.5\nis_train (bool, optional): 一个布尔值，表示是否在训练模式下运行。如果is_train=True ，那么会执行dropout操作；如果is_train=False，那么返回原始的输入张量x。默认值: False",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> from jittor import nn\n>>> x = jt.array([[0, 0.5, 1.0], [-0.3, 0.5, 0.8]])\n>>> nn.droppath(x, is_train=True) \njt.Var([[ 0.  1.  2.]\n        [-0.  0.  0.]], dtype=float32)\n>>> nn.droppath(x)  \njt.Var([[ 0.   0.5  1. ]\n        [-0.3  0.5  0.8]], dtype=float32)"
    },
    {
        "api_name": "jittor.nn.DropPath",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.nn.html#jittor.nn.DropPath",
        "api_signature": "jittor.nn.DropPath(p=0.5, is_train=False)",
        "api_description": "DropPath 类实现了随机深度（Stochastic Depth），通常在残差块的主路径中应用。\nDropPath 是一种正则化技术，通过随机丢弃训练过程中的部分计算路径来提高模型的泛化能力和训练稳定性，减少过拟合现象。当 is_train 为 False 或 p 为 0.0 时，DropPath 不会进行任何操作，直接返回输入数据。在训练模式下，根据保留概率 p 和随机张量进行操作，实现 DropPath 效果。\n参数:\np (float): 每批保留的概率。默认为 0.5。\nis_train (bool): 指定是否为训练模式。默认为 False。\n形状:\n输入: \\((*)\\)，其中 * 表示任意数量的附加维度。\n输出: \\((*)\\)，形状与输入相同。\n代码示例:>>> m = nn.DropPath(p=0.5, is_train=True)\n>>> input = jt.randn(3, 3)\n>>> m(input)\njt.Var([[-0.          0.         -0.        ]\n[ 0.7605061   3.3895922   0.35916936]\n[ 0.59844434  0.6205048  -0.18792158]], dtype=float32)",
        "return_value": "",
        "parameters": "p (float): 每批保留的概率。默认为 0.5。\nis_train (bool): 指定是否为训练模式。默认为 False。",
        "input_shape": "输入: \\((*)\\)，其中 * 表示任意数量的附加维度。\n输出: \\((*)\\)，形状与输入相同。",
        "notes": "",
        "code_example": ">>> m = nn.DropPath(p=0.5, is_train=True)\n>>> input = jt.randn(3, 3)\n>>> m(input)\njt.Var([[-0.          0.         -0.        ]\n        [ 0.7605061   3.3895922   0.35916936]\n        [ 0.59844434  0.6205048  -0.18792158]], dtype=float32)"
    },
    {
        "api_name": "jittor.loss3d.earth_mover_distance",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.loss3d.html#jittor.loss3d.earth_mover_distance",
        "api_signature": "jittor.loss3d.earth_mover_distance(pc1, pc2, reduction='mean', dims='BNC')",
        "api_description": "Earth Mover’s distance from pc1 to pc2. Only supports GPU.\nExample:\n>>> import jittor as jt\n>>> from jittor.loss3d import earth_mover_distance\n>>> jt.flags.use_cuda = True\n>>> pc1 = jt.rand([10, 100, 3], dtype=jt.float32)\n>>> pc2 = jt.rand([10, 100, 3], dtype=jt.float32)\n>>> emd = earth_mover_distance(pc1, pc2, dims='BNC')\n>>> print('EMD =', emd.item())",
        "return_value": "",
        "parameters": "pc1 (jittor array) – input point cloud\npc2 (jittor array) – input point cloud\nreduction (str, optional) – reduction method in batches, can be ‘mean’, ‘sum’, or None. Default: ‘mean’.\ndims (str, optional) – a string that represents each dimension, can be\n‘[BNC]’ ([batch, number of points, xyz]), or\n‘[BCN]’ ([batch, xyz, number of points]). Default: ‘BNC’.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "jittor.loss3d.EarthMoverDistance",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.loss3d.html#jittor.loss3d.EarthMoverDistance",
        "api_signature": "jittor.loss3d.EarthMoverDistance(*args, **kw)",
        "api_description": "A loss layer that computes Earth Mover’s distance from pc1 to pc2. Only supports GPU.\nExample:\n>>> import jittor as jt\n>>> from jittor.loss3d import EarthMoverDistance\n>>> jt.flags.use_cuda = True\n>>> pc1 = jt.rand([10, 100, 3], dtype=jt.float32)\n>>> pc2 = jt.rand([10, 100, 3], dtype=jt.float32)\n>>> EMD = EarthMoverDistance(dims='BNC')\n>>> emd = EMD(pc1, pc2)\n>>> print('EMD =', emd.item())",
        "return_value": "",
        "parameters": "pc1 (jittor array) – input point cloud\npc2 (jittor array) – input point cloud\nreduction (str, optional) – reduction method in batches, can be ‘mean’, ‘sum’, or None. Default: ‘mean’.\ndims (str, optional) – a string that represents each dimension, can be\n‘[BNC]’ ([batch, number of points, xyz]), or\n‘[BCN]’ ([batch, xyz, number of points]). Default: ‘BNC’.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "jittor.linalg.eigh",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.linalg.html#jittor.linalg.eigh",
        "api_signature": "jittor.linalg.eigh(x)",
        "api_description": "计算输入矩阵的特征值和特征向量。特征值和特征向量的计算公式：\n\\[A v = \\lambda v\\]\n其中,  \\(A\\) 是单位矩阵,  \\(v\\) 是特征向量,  \\(\\lambda\\) 是特征值。",
        "return_value": "返回两个变量的元组( \\(w\\) ,  \\(v\\) )(tuple ( Var , Var )), 其中 \\(w\\) 是特征值, 维度为(…,  \\(M\\) ),  \\(v\\) 是规范化后的特征向量, 维度为(…,  \\(M\\) ,  \\(M\\) )。",
        "parameters": "x (Var): 输入矩阵, 维度为(…,  \\(M\\) ,  \\(M\\) ), 其中 \\(M\\) 是矩阵的大小。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> x = jt.random((2, 2))\njt.Var([[ 0.9814584  -0.1916754 ]\n  [-0.8806686  -0.47373292]], dtype=float32)\n>>> jt.linalg.eigh(x)\n(jt.Var([-0.54820526  1.4303665 ], dtype=float32), jt.Var([[-0.1916754  -0.9814584 ]\n  [-0.47373292  0.8806686 ]], dtype=float32))"
    },
    {
        "api_name": "jittor.einops.EinopsError",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.einops.html#jittor.einops.EinopsError",
        "api_signature": null,
        "api_description": "Runtime error thrown by einops",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "jittor.linalg.einsum",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.linalg.html#jittor.linalg.einsum",
        "api_signature": "jittor.linalg.einsum(string, *args)",
        "api_description": "实现 \\(einsum(Einstein Summation)\\) 操作。",
        "return_value": "经过einsum操作后的Var。",
        "parameters": "string(str): 用以描述运算的字符串, 在 \\(einsum\\) 中指定输入、输出和对决定输出的维度上的操作。\nargs(Sequence[Var]): 待执行 \\(einsum\\) 操作的输入数据, 可以是一个或多个的数据。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> a = jt.random((1, 2, 1))\n>>> b = jt.random((1, 1, 2))\n>>> c = jt.linalg.einsum('bij,bjk->bik', a, b)\n>>> print(c)\njt.Var([[[0.10123717 0.6376321 ]\n [0.10257126 0.64603466]]], dtype=float32)"
    },
    {
        "api_name": "jittor.nn.elu",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.nn.html#jittor.nn.elu",
        "api_signature": "jittor.nn.elu(x: Var, alpha: float = 1.0)",
        "api_description": "该函数为Jittor的ELU激活函数，这是一个元素级别（element-wise）函数。与ReLU函数不同的是，输入值 x <= 0 时，不直接返回 0，而是返回关于x的自然指数的一个线性函数:\n\\[\\begin{split}\\text{ELU}(x) = \\begin{cases}\nx, & \\text{当} x > 0\\\\\\\\\n\\alpha * (\\exp(x) - 1), & \\text{当} x \\leq 0\n\\end{cases}\\end{split}\\]\n参数：\nx(Var): 输入的Var张量\nalpha(float，optional)：x<=0时公式中的 \\(\\alpha\\) 值。默认值：1.0\n返回值：Var张量。张量x应用ELU激活的结果\n代码示例：>>> import jittor as jt\n>>> from jittor import nn\n>>> x = jt.array([0.5, 6.5, -0.7])\n>>> nn.elu(x)\njt.Var([ 0.5        6.5       -0.5034147], dtype=float32)\n>>> nn.elu(x, 0.1)\njt.Var([ 0.5         6.5        -0.05034147], dtype=float32)",
        "return_value": "Var张量。张量x应用ELU激活的结果",
        "parameters": "x(Var): 输入的Var张量\nalpha(float，optional)：x<=0时公式中的 \\(\\alpha\\) 值。默认值：1.0",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> from jittor import nn\n>>> x = jt.array([0.5, 6.5, -0.7])\n>>> nn.elu(x)\njt.Var([ 0.5        6.5       -0.5034147], dtype=float32)\n>>> nn.elu(x, 0.1)\njt.Var([ 0.5         6.5        -0.05034147], dtype=float32)"
    },
    {
        "api_name": "jittor.nn.ELU",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.nn.html#jittor.nn.ELU",
        "api_signature": "jittor.nn.ELU(alpha=1.0)",
        "api_description": "ELU 类实现了ELU激活函数，如论文 Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs) 描述，它对每个输入元素应用以下变换：\n\\[\\begin{split}\\text{ELU}(x) = \\begin{cases}\nx, & \\text{ if } x > 0\\\\\n\\alpha * (\\exp(x) - 1), & \\text{ if } x \\leq 0\n\\end{cases}\\end{split}\\]",
        "return_value": "",
        "parameters": "alpha (float): 控制ELU负部分斜率的参数，默认值：1.0",
        "input_shape": "Input: \\((*)\\)，其中 * 表示任意数量的附加维数。\nOutput: \\((*)\\)，形状与输入相同。",
        "notes": "",
        "code_example": ">>> m = nn.ELU()\n>>> input = jt.randn(2)\n>>> output = m(input)\n>>> output\njt.Var([-0.8859823  -0.23584574], dtype=float32)"
    },
    {
        "api_name": "jittor.nn.embedding",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.nn.html#jittor.nn.embedding",
        "api_signature": "jittor.nn.embedding(input, weight)",
        "api_description": "这个函数实现了编译嵌入。接收一个矩阵索引作为输入，并返回相应的权重。\n参数:\ninput (Var) : 索引张量，表示要查询权重的索引\nweight (Var) : 权重张量，表示需要查询的权重\n返回值:Var: 返回一个同权重数据类型的张量，尺寸为(input.shape[0], weight.shape[1])，表示索引所代表的权重。\n代码示例：>>> import jittor as jt\n>>> from jittor import nn\n>>> weight = jt.array([[0,0,3,1],[2,0,3,1],[0,0,0,0]])\n>>> input = jt.array([0,1,2])\n>>> nn.embedding(input, weight)\njt.Var([[0 0 3 1]\n[2 0 3 1]\n[0 0 0 0]], dtype=int32)",
        "return_value": "Var: 返回一个同权重数据类型的张量，尺寸为(input.shape[0], weight.shape[1])，表示索引所代表的权重。",
        "parameters": "input (Var) : 索引张量，表示要查询权重的索引\nweight (Var) : 权重张量，表示需要查询的权重",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> from jittor import nn\n>>> weight = jt.array([[0,0,3,1],[2,0,3,1],[0,0,0,0]])\n>>> input = jt.array([0,1,2])  \n>>> nn.embedding(input, weight) \njt.Var([[0 0 3 1]\n        [2 0 3 1]\n        [0 0 0 0]], dtype=int32)"
    },
    {
        "api_name": "jittor.nn.Embedding",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.nn.html#jittor.nn.Embedding",
        "api_signature": "jittor.nn.Embedding(num_embeddings, embedding_dim, padding_idx=None, dtype='float32')",
        "api_description": "用于创建一个词嵌入的查找表，它将离散的词索引映射到连续的固定大小的向量空间。这种表示可以捕捉词与词之间的语义关系。",
        "return_value": "",
        "parameters": "num_embeddings (int) : 嵌入字典的大小\nembedding_dim (int) : 嵌入向量的维度\npadding_idx (int, optional) : 如果提供，该索引位置的向量会被初始化为零，并且在更新梯度时会被忽略。默认为 None ，表示不使用填充\ndtype (var.dtype, optional): 数据类型，默认为 float32",
        "input_shape": "Input: \\((*)\\)，其中 * 表示任意数量的额外维度\nOutput: \\((*, H)\\)，其中 * 为输入形状，H 是嵌入维度",
        "notes": "嵌入层的输入应该是整数类型，这些整数通常表示词汇表中词的索引\n在训练过程中，嵌入层的权重会被学习，以更好地捕捉词之间的关系和语义信息",
        "code_example": ">>> embedding_layer = nn.Embedding(num_embeddings=10, embedding_dim=3)\n>>> input = jt.array([[1,2,4,5],[4,3,2,9]])\n>>> output = embedding_layer(input)\n>>> print(output.size())\n[2,4,3,]\n>>> print(output)\njt.Var([[[ 0.21688631  0.20658202 -0.8409138 ]\n        [-1.4143792   1.2249023   0.31221074]\n        [ 0.69098186  0.42030936  1.6108662 ]\n        [-2.653321    0.7059287   1.8144118 ]]     \n        [[ 0.69098186  0.42030936  1.6108662 ]\n        [ 0.719435    0.0080323  -0.910858  ]\n        [-1.4143792   1.2249023   0.31221074]\n        [ 0.44668317 -0.8123236  -0.29966494]]], dtype=float32)"
    },
    {
        "api_name": "jittor.empty",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.empty",
        "api_signature": "jittor.empty(*shape, dtype='float32')",
        "api_description": "通过指定的形状和数据类型创建一个空的张量。",
        "return_value": "创建的空的张量(Var)",
        "parameters": "shape(Tuple[int]):创建的张量的形状。\ndtype(str,可选):创建的张量的数据类型，默认值：float32。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> jt.empty((2, 3))"
    },
    {
        "api_name": "jittor.enable_grad",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.enable_grad",
        "api_signature": "jittor.enable_grad(**jt_flags)",
        "api_description": "在scope类开启梯度。所有在这个scope内创建的变量都会记录梯度。\n示例代码:\nimport jittor as jt\nwith jt.enable_grad():\n...",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "jittor_core.Var.equal",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor_core.Var.equal",
        "api_signature": "jittor_core.Var.equal()",
        "api_description": "函数C++定义格式:\njt.Var equal(jt.Var x, jt.Var y)\n两个张量中对应元素比较是否相等，把结果布尔值放入输出张量的对应位置，也可以使用 = 运算符调用",
        "return_value": "形状与 x 和 y 相同的张量，数据类型是 bool。其中每个索引位置的布尔值表示该索引对应的 x 和 y 对应元素里是否 x 中的元素等于 y 中的元素",
        "parameters": "x (Var): 输入数据\ny (Var): 输入数据，形状与 x 相同",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> x = jt.array([3, 2, 1])\n>>> y = jt.array([1, 2, 3])\n>>> x.equal(y)\njt.Var([False  True False], dtype=bool)\n>>> x == y\njt.Var([False  True False], dtype=bool)"
    },
    {
        "api_name": "jittor_core.Var.erf",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor_core.Var.erf",
        "api_signature": "jittor_core.Var.erf()",
        "api_description": "函数C++定义格式:\njt.Var erf(jt.Var x)\n创建一个张量,  其为对输入变量 x 中的每一个数值进行误差变换得到的结果。 该函数为误差函数。\n\\[\\text{erf}(x) = \\frac{2}{\\sqrt{\\pi}} \\int_{0}^{x} e^{-t^2} \\, dt\\]",
        "return_value": "返回一个新的张量, 其数值是 x 中对应位置进行误差函数运算",
        "parameters": "x (Var): Var类型的张量",
        "input_shape": "",
        "notes": "该函数定义域是实数集，值域是 [-1,1]\n支持使用 jt.erf() 进行调用",
        "code_example": ">>> x = jt.randn(5) \n>>> x\njt.Var([ 2.7442768  -0.6135011  -0.09157802 -2.3600576   0.98465353], dtype=float32)\n>>> x.erf()\njt.Var([-0.91196376  0.19675992  0.9940873  -0.99938697  0.10797469], dtype=float32)"
    },
    {
        "api_name": "jittor.erf_",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.erf_",
        "api_signature": "jittor.erf_(x)",
        "api_description": "函数实现的是 对 x 实施原址元素级误差函数运算，并返回结果。\n误差函数是高斯函数积分，可以表示为：\n\\[\\text{erf}(x) = \\frac{2}{\\sqrt{\\pi}} \\int_{0}^{x} e^{-t^2} dt\\]",
        "return_value": "原位计算后的与 x 具有相同形状和数据类型的 Jittor 变量(Var)。",
        "parameters": "x(Var)：输入的需要进行原址元素级误差函数运算的 Jittor 变量，可以是标量，向量，矩阵或任意维度的张量。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> x = jt.array([0, 1., 2.])\n>>> print(jt.erf_(x))\njt.Var([0.        0.8427008 0.9953223], dtype=float32)"
    },
    {
        "api_name": "jittor_core.Var.erfinv",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor_core.Var.erfinv",
        "api_signature": "jittor_core.Var.erfinv()",
        "api_description": "函数C++定义格式:\njt.Var erfinv(jt.Var x)\n创建一个张量,  其为对输入变量 x 中的每一个数值进行反误差变换得到的结果。该函数为反误差函数。\n\\[\\text{erfinv}(x) = \\text{erf}^{-1}(x)\\]",
        "return_value": "返回一个新的张量, 其数值是 x 中对应位置进行反误差函数运算",
        "parameters": "x (Var): Var类型的张量",
        "input_shape": "",
        "notes": "该函数定义域是[-1,1]，值域是实数集\n支持使用 jt.erfinv() 进行调用",
        "code_example": ">>> x = jt.randn(5) \n>>> x\njt.Var([ 0.73859763  1.0348558   2.4863744  -0.36372563 -0.6050172 ], dtype=float32)\n>>> x.erfinv()\njt.Var([ 0.79413944         nan         nan -0.33440086 -0.6014762 ], dtype=float32)"
    },
    {
        "api_name": "jittor.erfinv_",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.erfinv_",
        "api_signature": "jittor.erfinv_(x)",
        "api_description": "函数实现的是数学中的误差反函数(erf)的就地版本 \\(erf^{-1}(x)\\)\n其中，\\(erf\\) 函数定义如下，x 为实数：\n\\[\\text{erf}(x) = \\frac{2}{\\sqrt{\\pi}} \\int_{0}^{x} e^{-t^{2}}\\, dt\\]\n\\(erfinv\\) 函数则是 \\(erf\\) 函数的反函数.",
        "return_value": "",
        "parameters": "x (float): 输入的 Jittor 变量。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> x = jt.array([0.5, 0.0, -0.5])\n>>> y = jt.erfinv_(x)\njt.Var([ 0.47693628  0.         -0.47693628], dtype=float32)"
    },
    {
        "api_name": "jittor.Module.eval",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.Module.eval",
        "api_signature": "eval()",
        "api_description": "将模块设置为评估eval模式。",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "jittor.ExitHooks",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.ExitHooks",
        "api_signature": null,
        "api_description": "该类主要用于捕获程序退出时的退出码和异常信息。在 Jittor 中，该类经常被用于添加一些在程序结束时需要进行的操作，比如资源清理等。其使用方式主要是将需要进行的操作添加到 exit 方法中。Jittor 中定义了该类的全局对象 jt.hooks 供使用。",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> import sys, atexit\n>>> hooks = jt.ExitHooks()\n>>> hooks.hook()\n>>> def exitfoo():\n...     if hooks.exception is not None:\n...         print(\"捕获到异常:\", hooks.exception)\n...     if hooks.exit_code is not None:\n...         print(\"程序退出，退出码为\", hooks.exit_code)\n... \n>>> atexit.register(exitfoo)\n>>> print(1/0) # 捕获到异常: division by zero\n>>> sys.exit(1) # 程序退出，退出码为 1"
    },
    {
        "api_name": "jittor_core.Var.exp",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor_core.Var.exp",
        "api_signature": "jittor_core.Var.exp()",
        "api_description": "函数C++定义格式:\njt.Var exp(jt.Var x)\n创建一个张量，其将 x 中的每一个数值的进行指数运算。\n\\[y_i = e^{x_i}\\]",
        "return_value": "返回一个新的张量, 其数值是 x 中对应位置进行指数运算后的值",
        "parameters": "x (Var): Var类型的张量",
        "input_shape": "",
        "notes": "可以使用 jt.exp() 进行调用",
        "code_example": ">>> x = jt.rand(5) * 5\n>>> x\njt.Var([2.8649285  0.854602   3.2959728  0.41697088 3.0296502 ], dtype=float32)\n>>> x.exp()\njt.Var([17.547798   2.3504386 27.003672   1.5173583 20.689995 ], dtype=float32)"
    },
    {
        "api_name": "jittor.misc.expand",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.misc.expand",
        "api_signature": "jittor.misc.expand(x, *shape)",
        "api_description": "扩展并广播一个张量到指定形状。\n指定的形状每一维必须为下列值之一：\n-1 表示不改变这维度的长度\n和 x 对应维长度一样，表示不改变这个维度的长度\n新的长度，要求 x 对应维长度为 1，表示在这个维度上扩展\n或者\nx (Var): 被扩展的张量\nn0, n1, … (int): 扩展到的形状",
        "return_value": "",
        "parameters": "x (Var): 被扩展的张量\nshape (tuple[int, …]): 扩展到的形状",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "jittor.lr_scheduler.ExponentialLR",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.optim.html#jittor.lr_scheduler.ExponentialLR",
        "api_signature": "jittor.lr_scheduler.ExponentialLR(optimizer, gamma, last_epoch=-1)",
        "api_description": "此类实现指数衰减学习率优化策略。在每个步骤中，将学习率乘以一个给定的系数 gamma。当 last_epoch 为 -1 时，将学习率设置为优化器的初始学习率。\n参数:\noptimizer (Optimizer): 优化器对象，需要进行学习率衰减的优化器。\ngamma (float): 在每个步骤中乘以学习率的系数。\nlast_epoch (int): 上一个周期（epoch）编号。默认值：-1\n代码示例：>>> optimizer = jt.optim.SGD(model.parameters(), lr=0.1)\n>>> scheduler = ExponentialLR(optimizer, gamma=0.9)\n>>> for epoch in range(100):\n...     train(...)\n...     validate(...)\n...     scheduler.step()",
        "return_value": "",
        "parameters": "optimizer (Optimizer): 优化器对象，需要进行学习率衰减的优化器。\ngamma (float): 在每个步骤中乘以学习率的系数。\nlast_epoch (int): 上一个周期（epoch）编号。默认值：-1",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> optimizer = jt.optim.SGD(model.parameters(), lr=0.1)\n>>> scheduler = ExponentialLR(optimizer, gamma=0.9)\n>>> for epoch in range(100):\n...     train(...)\n...     validate(...)\n...     scheduler.step()"
    },
    {
        "api_name": "jittor.init.eye",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.init.html#jittor.init.eye",
        "api_signature": "jittor.init.eye(shape, dtype='float32')",
        "api_description": "创建一个形状为 shape 的单位矩阵。\n单位矩阵的定义为：\n对于任何 \\(i, j (0 \\le i, j < dim)\\) ，当 \\(i\\) 等于 \\(j\\) 时，输出为 1 ；否则为 0 。\n\\[\\begin{split}eye(i, j) =\n\\begin{cases}\n1, & \\text{if  } i = j \\\\\n0, & \\text{else}\n\\end{cases}\\end{split}\\]",
        "return_value": "返回一个单位矩阵",
        "parameters": "shape (int, Tuple[int]): 规定返回矩阵的形状，如果 shape 为整数， 那么返回的矩阵的形状为 (shape, shape)。\ndtype (dtype, 可选): 数据类型，默认为 float32",
        "input_shape": "",
        "notes": "shape 如果是整数序列，只能传入两个维度的长度，否则会引起异常",
        "code_example": ">>> init.eye(2)\njt.Var([[1. 0.]\n        [0. 1.]], dtype=float32)\n>>> init.eye((2, 3), dtype='int32')\njt.Var([[1 0 0]\n        [0 1 0]], dtype=int32)"
    },
    {
        "api_name": "jittor.init.eye_",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.init.html#jittor.init.eye_",
        "api_signature": "jittor.init.eye_(var)",
        "api_description": "原地修改矩阵 var，将其成为单位矩阵。\n单位矩阵的定义为：\n对于任何 \\(i, j (0 \\le i, j < dim)\\) ，当 \\(i\\) 等于 \\(j\\) 时，输出为 1 ；否则为 0 。\n\\[\\begin{split}eye(i, j) =\n\\begin{cases}\n1, & \\text{if  } i = j \\\\\n0, & \\text{else}\n\\end{cases}\\end{split}\\]",
        "return_value": "返回一个形状和 var 相同的单位矩阵",
        "parameters": "var (Var): ``Var``类型的张量",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> x = jt.randn((3,2))\n>>> x\njt.Var([[-0.582047    1.1068922 ]\n        [ 0.08440255 -0.86142904]\n        [ 0.06269896 -0.979008  ]], dtype=float32)\n>>> init.eye_(x)\n>>> x\njt.Var([[1. 0.]\n        [0. 1.]\n        [0. 0.]], dtype=float32)"
    },
    {
        "api_name": "jittor.fetch",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.fetch",
        "api_signature": "jittor.fetch(*args)",
        "api_description": "注册一个在每个张量都计算完成并拿到数据后的回调函数。给出若干个参数，里面是 Var 的都计算完成后，调用回调函数。传入回调函数的参数与 fetch 除去最后回调函数的参数对应，但是所有 Var 都替换为其 data 。",
        "return_value": "无",
        "parameters": "前若干个参数为等待完成的 Var 和其它参数，最后一个参数是回调函数",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> a = jt.zeros((2, 3), dtype='float32')\n>>> jt.fetch(a, lambda x: print('got', repr(x)))\n>>> jt.sync_all()\n    got array([[0., 0., 0.],\n               [0., 0., 0.]], dtype=float32)"
    },
    {
        "api_name": "jittor.init.fill",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.init.html#jittor.init.fill",
        "api_signature": "jittor.init.fill(var, value=0.0)",
        "api_description": "原地修改张量 var ，将其修改为数值均为 value 的张量，默认填充零值。\n参数：\nvar (Var): Var类型的张量\nvalue (int or float): 填充的数值",
        "return_value": "就地修改 var ，返回一个填充值为 value ，形状和 var 相同的张量(Var)",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> x = jt.randn((3,2))\n>>> x\njt.Var([[ 0.0335454  -0.18658346]\n        [-0.53283346 -1.2073938 ]\n        [-1.344916   -1.6093305 ]], dtype=float32)\n>>> init.constant_(x, 3.8)\n>>> x\n>jt.Var([[3.8 3.8]\n         [3.8 3.8]\n         [3.8 3.8]], dtype=float32)"
    },
    {
        "api_name": "jittor.misc.finfo",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.misc.finfo",
        "api_signature": "jittor.misc.finfo(dtype)",
        "api_description": "根据指定的数据类型，返回其numpy浮点数类型的机器限制信息。numpy为每个浮点数提供了一个finfo对象，该对象提供了关于机器限制的信息，例如浮点数类型可以表示的最大和最小的正值、精度和“数字”的数量等，参见 https://numpy.org/doc/stable/reference/generated/numpy.finfo.html",
        "return_value": "返回对应指定数据类型的 numpy finfo 对象或者 ‘bfloat16_finfo’对象。",
        "parameters": "dtype (str) : 指定的数据类型。例如：’float16’。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> from jittor import misc\n>>> misc.finfo('float32')\nfinfo(resolution=1e-06, min=-3.4028235e+38, max=3.4028235e+38, dtype=float32)"
    },
    {
        "api_name": "jittor.misc.Finfo",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.misc.Finfo",
        "api_signature": null,
        "api_description": "浮点数类型相关参数。\n由 jt.finfo 返回，包括浮点数的相关参数。",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> jt.finfo('float32').min\n-3.4028235e+38\n>>> jt.finfo('bfloat16').min\n-1e+38"
    },
    {
        "api_name": "jittor.transform.FiveCrop",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.transform.html#jittor.transform.FiveCrop",
        "api_signature": "jittor.transform.FiveCrop(size)",
        "api_description": "在给定的PIL图像四个角和中心各截取一幅大小为 size 的图片",
        "return_value": "",
        "parameters": "size (sequence, int): 裁剪的期望输出大小。如果size是一个整数而不是像(h, w)这样的序列，会进行一个(size, size)的正方形裁剪",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> from jittor import transform\n>>> import numpy as np\n>>> from PIL import Image\n>>> data = np.random.rand(200, 200, 3)\n>>> img = Image.fromarray(data, 'RGB')\n>>> five_crop = transform.FiveCrop(16)\n>>> imgs = five_crop(img)\n>>> imgs[0].size, imgs[1].size, imgs[2].size, imgs[3].size, imgs[4].size\n((16, 16), (16, 16), (16, 16), (16, 16), (16, 16))"
    },
    {
        "api_name": "jittor.flag_scope",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.flag_scope",
        "api_signature": "jittor.flag_scope(**jt_flags)",
        "api_description": "该类用于构建 Jittor 中标志作用域，及将 Jittor 配置或标志在这个作用域内进行变动。该类只是一个上下文管理器类，用于临时设置标志位的值，并在离开作用域时恢复为原始值。",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "jittor.flatten",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.flatten",
        "api_signature": "jittor.flatten(input, start_dim=0, end_dim=-1)",
        "api_description": "将输入张量 input 在指定维度的范围 [start_dim, end_dim] 上进行展平操作。",
        "return_value": "经过展平操作后的新张量,类型为jittor.Var。",
        "parameters": "input (jt.Var) : 输入的jittor张量。\nstart_dim (int) : 展平操作的起始维度，默认值:0。\nend_dim (int) : 展平操作的结束维度，默认值:-1(代表最后一个维度)",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> x = jt.array([[[1, 2],\n            [3, 4]],\n            [[5, 6],\n            [7, 8]]])\n>>> y = jt.flatten(x)\njt.Var([1 2 3 4 5 6 7 8], dtype=int32)\n>>> y = jt.flatten(x,start_dim=1)\njt.Var([[1 2 3 4]\n        [5 6 7 8]], dtype=int32)"
    },
    {
        "api_name": "jittor.nn.Flatten",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.nn.html#jittor.nn.Flatten",
        "api_signature": "jittor.nn.Flatten(start_dim=1, end_dim=-1)",
        "api_description": "对一个 Var 的连续范围的数个维度进行扁平化处理 (Flatten)。\n扁平化是将多维数据转换为一维数据的过程。例如扁平化一个 shape 为 (2, 3, 4) 的 Var 中的后两个维度，得到的新 Var 的 shape 为 (2, 12)。\n默认情况下，此操作从第一维度开始到最后一维度。",
        "return_value": "",
        "parameters": "start_dim (int): 第一个需要被扁平化的维度，默认值：1\nend_dim (int): 最后一个需要被扁平化的维度，默认值：-1",
        "input_shape": "Input:  \\((*,S_{start},\\dots,S_i,\\dots,S_{end},*)\\)，其中 \\(S_i\\) 表示第 \\(i\\) 维大小， *  表示任意数量的附加维数。\nOutput:  \\((*,\\prod_{i=start}^{end}S_i,*)\\).",
        "notes": "",
        "code_example": ">>> m = nn.Flatten(1,2)\n>>> input = jt.array([[[1, 2],[3, 4]],[[5, 6],[7, 8]]])\n>>> output = m(input)\n>>> output\njt.Var([[1 2 3 4]\n        [5 6 7 8]], dtype=int32)"
    },
    {
        "api_name": "jittor.misc.flip",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.misc.flip",
        "api_signature": "jittor.misc.flip(x, dim=0)",
        "api_description": "将张量在指定维度镜像翻转",
        "return_value": "x 在指定维度翻转后的结果",
        "parameters": "x (Var): 需要翻转的张量\ndim (int | tuple[int, …]): 需要翻转的维度，负数表示从后往前数。默认值：0",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> x = jt.array([[1, 2], [3, 4]])\n>>> x.flip()\njt.Var([[3 4]\n        [1 2]], dtype=int32)\n>>> x.flip(1)\njt.Var([[2 1]\n        [4 3]], dtype=int32)\n>>> x.flip((0, 1))\njt.Var([[4 3]\n        [2 1]], dtype=int32)"
    },
    {
        "api_name": "jittor_core.Var.float",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor_core.Var.float",
        "api_signature": "jittor_core.Var.float()",
        "api_description": "函数C++定义格式:\njt.Var float32_(jt.Var x)\n创建并返回一个 x 的张量副本，并将其类型转换为 float32 ，返回单精度的浮点数，即用32个比特(4个字节)表示一个数。",
        "return_value": "返回一个新的张量, 数据类型为 float32",
        "parameters": "x (Var): Var类型的张量",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> x = jt.ones((2,2), dtype='int8')\n>>> x\njt.Var([[1 1]\n        [1 1]], dtype=int8)\n>>> x.float32()\njt.Var([[1. 1.]\n        1. 1.]], dtype=float32)"
    },
    {
        "api_name": "jittor.Module.float16",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.Module.float16",
        "api_signature": "float16()",
        "api_description": "将所有参数转换为float16。",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "jittor_core.Var.float16",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor_core.Var.float16",
        "api_signature": "jittor_core.Var.float16()",
        "api_description": "函数C++定义格式:\njt.Var float16_(jt.Var x)\n创建并返回一个 x 的张量副本，并将其类型转换为 float16 ，返回半精度的浮点数，即用16个比特(2个字节)表示一个数。",
        "return_value": "返回一个新的张量, 数据类型为 float16",
        "parameters": "x (Var): Var类型的张量",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> x = jt.randn((2,2))\n>>> x\njt.Var([[ 1.4581492   0.2615918 ]\n        [ 0.26567948 -0.34585235]], dtype=float32)\n>>> x.float16()\njt.Var([[ 1.458   0.2615]\n        [ 0.2656 -0.346 ]], dtype=float16)"
    },
    {
        "api_name": "jittor.Module.float32",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.Module.float32",
        "api_signature": "float32()",
        "api_description": "将所有参数转换为float32。",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "jittor_core.Var.float32",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor_core.Var.float32",
        "api_signature": "jittor_core.Var.float32()",
        "api_description": "函数C++定义格式:\njt.Var float32_(jt.Var x)\n创建并返回一个 x 的张量副本，并将其类型转换为 float32 ，返回单精度的浮点数，即用32个比特(4个字节)表示一个数。",
        "return_value": "返回一个新的张量, 数据类型为 float32",
        "parameters": "x (Var): Var类型的张量",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> x = jt.ones((2,2), dtype='int8')\n>>> x\njt.Var([[1 1]\n        [1 1]], dtype=int8)\n>>> x.float32()\njt.Var([[1. 1.]\n        1. 1.]], dtype=float32)"
    },
    {
        "api_name": "jittor.Module.float64",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.Module.float64",
        "api_signature": "float64()",
        "api_description": "将所有参数转换为float16。",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "jittor_core.Var.float64",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor_core.Var.float64",
        "api_signature": "jittor_core.Var.float64()",
        "api_description": "函数C++定义格式:\njt.Var float64_(jt.Var x)\n创建并返回一个 x 的张量副本，并将其类型转换为 float64 ，返回双精度的浮点数，即用64个比特(8个字节)表示一个数。",
        "return_value": "返回一个新的张量, 数据类型为 float64",
        "parameters": "x (Var): Var类型的张量",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> x = jt.randn((2,2))\n>>> x\njt.Var([[ 1.4581492   0.2615918 ]\n        [ 0.26567948 -0.34585235]], dtype=float32)\n>>> x.float64()\njt.Var([[ 1.45814919  0.26159179]\n        [ 0.26567948 -0.34585235]], dtype=float64)"
    },
    {
        "api_name": "jittor.Module.float_auto",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.Module.float_auto",
        "api_signature": "float_auto()",
        "api_description": "根据 jt.flags.auto_mixed_precision_level 和 jt.flags.amp_reg 自动将所有参数转换为 float16 或 float32。",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "jittor.float_auto",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.float_auto",
        "api_signature": "jittor.float_auto(x)",
        "api_description": "将输入的参数自动转化为浮点数。",
        "return_value": "转化后的浮点数。如果输入的是容器型数据，则返回的也是对应的容器型数据。比如输入的是列表，则返回的是浮点数列表；输入的是np.ndarray，则返回的是浮点数np.ndarray。",
        "parameters": "x (int, float, list, tuple, np.ndarray) : 输入的待转化的参数，可以是数值型数据或者是类似列表、数组这样的容器型数据。容器型数据中的每一个元素都会被转化为相应的浮点数。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> jt.float_auto(1) \n1.0\n>>> jt.float_auto([1, 2, 3]) \n[1.0, 2.0, 3.0]"
    },
    {
        "api_name": "jittor_core.Var.floor",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor_core.Var.floor",
        "api_signature": "jittor_core.Var.floor()",
        "api_description": "函数C++定义格式:\njt.Var floor(jt.Var x)\n创建一个张量, 其将 x 中的每一个数值转换为向下取整离其最近的整数。\n如：floor(3.7) 的结果是3, floor(-3.7) 的结果是-4。",
        "return_value": "返回一个新的张量, 其数值是 x 中对应位置向下取整离其最近的整数",
        "parameters": "x (Var): Var类型的张量",
        "input_shape": "",
        "notes": "支持使用 jt.floor() 进行调用",
        "code_example": ">>> x = jt.randn(5) \n>>> x\njt.Var([ 1.1675875  -0.36471805  2.0246403   1.1496693  -0.09650089], dtype=float32)\n>>> x.floor()\njt.Var([ 1. -1.  2.  1. -1.], dtype=float32)"
    },
    {
        "api_name": "jittor_core.Var.floor_divide",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor_core.Var.floor_divide",
        "api_signature": "jittor_core.Var.floor_divide()",
        "api_description": "函数C++定义格式:\njt.Var floor_divide(jt.Var x, jt.Var y)\n对两个张量中对应元素计算求商并向下取整，也可以使用 // 运算符调用",
        "return_value": "x 和 y 每对对应元素相除向下取整结果，形状与 x 和 y 相同，输入数据类型是整数的情况下，会返回整数",
        "parameters": "x (Var): 输入数据\ny (Var): 输入数据，形状与 x 相同",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> x = jt.array([3, 4, 5])\n>>> y = jt.array([1, 2, 4])\n>>> x.floor_divide(y)\njt.Var([3 2 1], dtype=int32)\n>>> x // y\njt.Var([3 2 1], dtype=int32)"
    },
    {
        "api_name": "jittor_core.Var.floor_int",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor_core.Var.floor_int",
        "api_signature": "jittor_core.Var.floor_int()",
        "api_description": "函数C++定义格式:\njt.Var floor_int(jt.Var x)\n创建一个张量, 其将 x 中的每一个数值转换为向下取整离其最近的整数。\n如：floor_int(3.7) 的结果是3, floor_int(-3.7) 的结果是-4。",
        "return_value": "返回一个新的张量，其数值是 x 中对应位置向下取整离其最近的整数，数据类型为 int32",
        "parameters": "x (Var): Var类型的张量",
        "input_shape": "",
        "notes": "支持使用 jt.floor_int() 进行调用",
        "code_example": ">>> x = jt.randn(5)\n>>> x\njt.Var([ 2.00422    -2.081075   -0.7260027   0.46558696 -0.1570169 ], dtype=float32)\n>>> x.floor_int()\njt.Var([ 2 -3 -1  0 -1], dtype=int32)"
    },
    {
        "api_name": "jittor.nn.fold",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.nn.html#jittor.nn.fold",
        "api_signature": "jittor.nn.fold(X, output_size, kernel_size, dilation=1, padding=0, stride=1)",
        "api_description": "将一系列滑动的局部块组合成一个大的包含性张量。\n考虑一个批量输入张量，其中包含滑动的局部块，例如图像的块，其形状为 (N, \\(C \\times \\prod(\\text{kernel_size})\\), L)，其中 N 是批次维度，\\(C \\times \\prod(\\text{kernel_size})\\) 是一个块内的值的数量（一个块有 \\(\\prod(\\text{kernel_size})\\) 个空间位置，每个位置包含一个 C 通道的向量），而 L 是块的总数。（这与 Unfold 操作的输出形状完全相同。）这个操作通过对重叠值求和，将这些局部块组合成一个大的输出张量，其形状为 (N, C, \\(\\text{output_size}[0]\\), \\(\\text{output_size}[1]\\), …)。\n\\[L = \\prod_{i=0}^{d-1} \\left\\lfloor\\frac{\\text{input_size}[i] + 2 \\times \\text{padding}[i] - \\text{dilation}[i]\n\\times (\\text{kernel_size}[i] - 1) - 1}{\\text{stride}[i]} + 1\\right\\rfloor\\]",
        "return_value": "output(Var): 输出的张量，形状为(N,C,output_size[0],output_size[1],…)",
        "parameters": "X (Var): 输入的张量\noutput_size （tuple）：期望的输出尺寸，格式为(height, width)。\nkernel_size （int, tuple）：折叠操作的块大小。如果输入为单一整数，则视为高度和宽度相同。默认值为1。\ndilation （int, tuple）：单元格之间的距离(沿着高度和宽度方向)。如果输入为单一整数，则视为高度和宽度相同。默认值为1。\npadding（int, tuple）：输入张量两侧填充的行数。如果输入为单一整数，则视为高度和宽度相同。默认值为0。\nstride （int, tuple）：滑动窗口大小(沿着高度和宽度方向)。如果输入为单一整数，则视为高度和宽度相同。默认值为1。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> input = jt.randn(1, 3 * 2 * 2, 12)\n>>> jt.nn.fold(input,(4,5),(2,2)).shape\n[1, 3, 4, 5]"
    },
    {
        "api_name": "jittor.format",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.format",
        "api_signature": "jittor.format(v, spec)",
        "api_description": "将输入的单个元素的张量转化为python数据类型的指定格式。",
        "return_value": "output (str): 根据设定格式显示的数值v的字符串形式。",
        "parameters": "v (jittor.Var): 包含单个元素的张量。\nspec (str， 可选): 设定的数值v的显示格式的描述。如果值为None，那么将使用默认的显示格式。默认值：None",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "jittor.nn.fp32_guard",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.nn.html#jittor.nn.fp32_guard",
        "api_signature": "jittor.nn.fp32_guard(func)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "jittor.misc.from_torch",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.misc.from_torch",
        "api_signature": "jittor.misc.from_torch(x)",
        "api_description": "将PyTorch Tensor转化为Jittor Var。",
        "return_value": "jt.Var: 转化后的Jittor Variable。",
        "parameters": "x (torch.Tensor): 需要转化的PyTorch Tensor。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import torch\n>>> import jittor as jt\n>>> a = torch.tensor([1.0, 2.0, 3.0])\n>>> b = from_torch(a)\n>>> print(b)\njt.Var([1.0, 2.0, 3.0])"
    },
    {
        "api_name": "jittor.full",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.full",
        "api_signature": "jittor.full(shape, val, dtype='float32')",
        "api_description": "创建一个张量(Var)，其中张量的形状由可变参数 shape 定义，\n张量中的各个标量的值由可变参数 val 定义，数据类型由 dtype 定义。如果不给定 dtype , 默认类型为 float32。",
        "return_value": "返回一个填充值为 val ，形状大小为 shape 的张量(Var)",
        "parameters": "shape (int…): 整数序列，定义了输出张量的形状\nval: 填充值，任意数值类型\ndtype (var.dtype, optional): 数据类型，默认为 float32",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> jt.full((3,2), 3.14)\njt.Var([[3.14 3.14]\n        [3.14 3.14]\n        [3.14 3.14]], dtype=float32)"
    },
    {
        "api_name": "jittor.full_like",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.full_like",
        "api_signature": "jittor.full_like(x, val, dtype=None)",
        "api_description": "创建一个张量(Var)，其中张量的形状由张量 x 的形状定义，张量中的各个标量的值由可变参数 val 定义，数据类型默认为 None ，即张量 x 的数据类型。",
        "return_value": "返回一个填充值为 val ，形状大小为和 x 一致的张量(Var)",
        "parameters": "x (Var): Var类型的张量\nval: 填充值，任意数值类型\ndtype (var.dtype, optional): 数据类型，默认为None",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> jt.array([1, 2, 3]).float()\njt.Var([1. 2. 3.], dtype=float32)\n>>> jt.full_like(x, 3.14)\njt.Var([3.14 3.14 3.14], dtype=float32)"
    },
    {
        "api_name": "jittor.Function",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.Function",
        "api_signature": "jittor.Function(*args, **kw)",
        "api_description": "用于自定义反向操作（backward operations）的函数模块。Function 类继承自 Jittor 的 Module 类。\n其派生类应实现 execute 和 grad 两个方法，作用分别是执行前向计算和计算反向梯度。当使用自定义算子进行前向计算时，应调用类方法 apply，而不是直接调用 execute。注意 grad 接收的参数和返回值都可为 None，表示无梯度。",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> from jittor import Function\n>>> class MyFunc(Function):\n...     def execute(self, x, y):\n...         self.x = x\n...         self.y = y\n...         return x*y, x/y\n...     def grad(self, grad0, grad1):\n...         return grad0 * self.y, grad1 * self.x\n... \n>>> a = jt.array(3.0)\n>>> b = jt.array(4.0)\n>>> func = MyFunc.apply\n>>> c,d = func(a, b)\n>>> da, db = jt.grad(c+d*3, [a, b])\n>>> print(da,db)\njt.Var([4.], dtype=float32) jt.Var([9.], dtype=float32)"
    },
    {
        "api_name": "jittor_core.Var.fuse_transpose",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor_core.Var.fuse_transpose",
        "api_signature": "jittor_core.Var.fuse_transpose()",
        "api_description": "函数C++定义格式:\njt.Var fuse_transpose(jt.Var x,  NanoVector axes=NanoVector())\n将输入的多维数组根据指定的轴进行转置。转置操作的数学公式为:\n.. math:\ny_{i_1,...,i_N} = x_{i_{\\pi(1)},...,i_{\\pi(N)}}\n其中, .. math::y_{i_1,…,i_N} = x_{j_1,…,j_N} 为 axes 给出的轴值列表",
        "return_value": "经转置操作后的多维数组(Var)。",
        "parameters": "x(Var): 待转置的多维数组。\naxes(Tuple[int]): 轴值的元组,用于指定转置操作的顺序。 默认值：空元组。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> from typing import Tuple\n>>> x = Var() \n>>> axes = (1, 0, 2) \n>>> fuse_transpose(x, axes)"
    },
    {
        "api_name": "jittor.distributions.GammaDistribution",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.distributions.html#jittor.distributions.GammaDistribution",
        "api_signature": "jittor.distributions.GammaDistribution(concentration, rate)",
        "api_description": "实现了 Gamma 分布类，支持随机采样、计算分布函数值、对数概率、熵、平均值和方差。Gamma 分布是一种主要用于连续概率分布的两参数家族，即集中度和比率参数，广泛应用于阵列模型等。其概率密度函数可以表示为:\n\\[p(x) = \\frac{x^{\\alpha-1}e^{-\\frac{x}{\\theta}}}{\\theta^\\alpha\\Gamma(\\alpha)}\\]\n其中， \\(\\alpha > 0\\) 是形状（shape）参数, 具有构成分布形状的主导作用； \\(\\theta > 0\\) 是比率（rate）参数； \\(\\Gamma(\\alpha)\\) 是 Gamma 函数。",
        "return_value": "",
        "parameters": "concentration (Var): Gamma 分布的形状参数。\nrate (Var):  Gamma 分布的比率参数。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> gamma_dist = GammaDistribution(concentration=2, rate=1.5)\n>>> gamma_dist.mean()  # 计算平均值\n>>> 1.3333333333333333"
    },
    {
        "api_name": "jittor.misc.gather",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.misc.gather",
        "api_signature": "jittor.misc.gather(x, dim, index)",
        "api_description": "该函数用给定的索引数组重新索引源数组（如果源数组是3-D数组）。\n对于索引数组中的每个元素，假设当前元素的坐标是(i,j,k)，则在结果数组中当前索引对应的元素取决于参数 dim 的值：\n如果dim == 0, out[i][j][k] = input[index[i][j][k]][j][k]\n如果dim == 1, out[i][j][k] = input[i][index[i][j][k]][k]\n如果dim == 2, out[i][j][k] = input[i][j][index[i][j][k]]",
        "return_value": "Var，通过指定的索引方式重新索引源数组，汇总得到的结果。",
        "parameters": "x (Var): 输入数组\ndim (int): 索引操作进行的轴\nindex (Var): 元素的索引数组",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> t = jt.array([[1, 2], [3, 4]])\n>>> jt.gather(t, 1, jt.array([[0, 0], [1, 0]]))\njt.Var([[1 1]\n        [4 3]], dtype=int32)\n>>> jt.gather(t, 0, jt.array([[0, 0], [1, 0]]))\njt.Var([[1 2]\n        [3 2]], dtype=int32)"
    },
    {
        "api_name": "jittor.init.gauss",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.init.html#jittor.init.gauss",
        "api_signature": "jittor.init.gauss(shape, dtype='float32', mean=0.0, std=1.0)",
        "api_description": "创建一个 Var，其形状由 shape 指定，数据类型为  dtype ，\n元素值遵循均值为 mean 和标准差为 std 的高斯分布（正态分布）的张量。\n\\[out_i \\sim \\mathcal{N}(\\text{mean}, \\text{std}^2)\\]",
        "return_value": "返回一个数值符合高斯分布的 Var",
        "parameters": "shape (Tuple[int]): 整型序列，定义了输出的形状\ndtype (dtype): 数据类型，默认为 float32\nmean (float or Var, 可选): 高斯分布的均值， 默认值为 0.0\nstd (float or Var, 可选): 高斯分布的标准差，默认值为 1.0",
        "input_shape": "",
        "notes": "确保指定的 std 是非负的，因为标准差不能是负数",
        "code_example": ">>> init.gauss((3,2))\njt.Var([[-1.09277    -0.22924843]\n        [-0.5264394  -0.13242662]\n        [-1.1316705   1.2506602 ]], dtype=float32)\n>>> init.gauss((2,2), 'float32', 0, 10)\njt.Var([[ 3.457805   5.8171034]\n        [-1.6440934  2.1744032]], dtype=float32)"
    },
    {
        "api_name": "jittor.init.gauss_",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.init.html#jittor.init.gauss_",
        "api_signature": "jittor.init.gauss_(var, mean=0.0, std=1.0)",
        "api_description": "原地修改张量 var ，将其数值填充满足以均值为 mean 和标准差为 std 的高斯分布（正态分布）的随机数。\n\\[out_i \\sim \\mathcal{N}(\\text{mean}, \\text{std}^2)\\]",
        "return_value": "就地修改张量 var ，返回一个数值符合高斯分布的张量",
        "parameters": "var (Var): 改变的 var\nmean (float or Var, 可选): 高斯分布的均值， 默认值为 0.0\nstd (float or Var, 可选): 高斯分布的标准差，默认值为 1.0",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> x = init.zero((3,3))\n>>> x\njt.Var([[0. 0. 0.]\n        [0. 0. 0.]\n        [0. 0. 0.]], dtype=float32)\n>>> init.gauss_(x, 0, 5.0)\n>>> x\njt.Var([[-0.34221977 -8.056475    2.6251674 ]\n        [-0.81275284  2.00393    -2.4397573 ]\n        [ 0.7867035  -8.159389    7.100675  ]], dtype=float32)"
    },
    {
        "api_name": "jittor.nn.gelu",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.nn.html#jittor.nn.gelu",
        "api_signature": "jittor.nn.gelu(x)",
        "api_description": "该函数为Jittor的GELU激活函数，它是非线性激活函数，在输入张量的每个元素乘上对应的高斯分布CDF（累积分布函数）：\\(\\text{GELU}(x) = x * \\Phi(x)\\) ，其中 \\(\\Phi(x)\\) 是高斯分布的累积分布函数。这是一个元素级别（element-wise）函数。\n参数：\nx(Var): 输入的Var张量\n返回值：Var张量。张量x应用GELU激活的结果\n代码示例：>>> import jittor as jt\n>>> from jittor import nn\n>>> x = jt.array([0, 6.5, -0.7])\n>>> nn.gelu(x)\njt.Var([ 0.          6.5        -0.16937456], dtype=float32)",
        "return_value": "Var张量。张量x应用GELU激活的结果",
        "parameters": "x(Var): 输入的Var张量",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> from jittor import nn\n>>> x = jt.array([0, 6.5, -0.7])\n>>> nn.gelu(x)\njt.Var([ 0.          6.5        -0.16937456], dtype=float32)"
    },
    {
        "api_name": "jittor.distributions.Geometric",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.distributions.html#jittor.distributions.Geometric",
        "api_signature": "jittor.distributions.Geometric(p=None, logits=None)",
        "api_description": "这是一个处理几何概率分布的类，几何概率分布是离散概率分布的一种，描述了在进行一系列独立的、具有相同成功概率的伯努利试验中，首次成功需要的试验次数。类 Geometric 初始化时，需要提供成功概率或者对数几率（logit）其中之一，且成功概率 \\(p\\) 的取值范围在 0 和 1 之间。",
        "return_value": "",
        "parameters": "p (float): 几何概率分布成功概率。默认值：None\nlogits (float): 几何概率分布的对数几率。默认值：None",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> from jittor.distributions import Geometric\n>>> geom = Geometric(p=0.5)\n>>> geom.entropy()\njt.Var([1.3862944], dtype=float32)"
    },
    {
        "api_name": "jittor.nn.get_init_var_rand",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.nn.html#jittor.nn.get_init_var_rand",
        "api_signature": "jittor.nn.get_init_var_rand(shape, dtype)",
        "api_description": "在给定形状和数据类型下，返回随机数初始化一个张量。\n随机数初始化如下所示，均值为（0.0），标准差为（1.0）：\n\\[X_i \\sim  N(0,1)\\]",
        "return_value": "output(Var): 随机初始化的张量",
        "parameters": "shape(tuple): 张量的形状\ndtype(string): 张量的数据类型",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> x = jt.get_init_var_rand([2, 3],'float32')\njt.Var([[ 0.5034227   0.75092447 -0.7876699 ]\n        [-0.7334006  -0.69090897 -2.2373345 ]], dtype=float32)"
    },
    {
        "api_name": "jittor.get_len",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.get_len",
        "api_signature": "jittor.get_len(var)",
        "api_description": "获取输入张量的第一个维度的长度。",
        "return_value": "output (int): 返回输入张量的第一个维度的长度。",
        "parameters": "var (jittor.Var): 输入的张量(jittor.Var)。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> v = jt.float32([1,2,3])\n>>> len = jt.get_len(v)\n>>> print(len)\n3"
    },
    {
        "api_name": "jittor.misc.get_max_memory_treemap",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.misc.get_max_memory_treemap",
        "api_signature": "jittor.misc.get_max_memory_treemap(build_by=0, do_print=True)",
        "api_description": "显示最大内存消耗的树图。",
        "return_value": "最大内存消耗的树图",
        "parameters": "build_by (int, 可选): 0或1，0表示按照名称构造树，1表示按照路径构造树。默认值: 0\ndo_print (bool, 可选): 是否打印输出。如果do_print=True，那么会在函数中打印输出。默认值: True",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "jittor.models.googlenet",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.models.html#jittor.models.googlenet",
        "api_signature": "jittor.models.googlenet(pretrained=False, **kwargs)",
        "api_description": "构建一个GoogLeNet模型\nGoogLeNet源自论文 Going Deeper with Convolutions , 它由多个称为Inception模块的子网络构成。\n参数:\npretrained (bool, optional): 表示是否预加载预训练模型。默认为 False。\nkwargs: 其他optional参数。\n返回值:\n返回构建好的googlenet模型实例。如果 pretrained 为 True, 则返回在ImageNet上预训练的模型。\n代码示例:>>> import jittor as jt\n>>> from jittor.models.googlenet import *\n>>> net = googlenet(pretrained=False)\n>>> x = jt.rand(1, 3, 224, 224)\n>>> y = net(x)\n>>> y.shape\n[1, 1000]",
        "return_value": "返回构建好的googlenet模型实例。如果 pretrained 为 True, 则返回在ImageNet上预训练的模型。",
        "parameters": "pretrained (bool, optional): 表示是否预加载预训练模型。默认为 False。\nkwargs: 其他optional参数。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> from jittor.models.googlenet import *\n>>> net = googlenet(pretrained=False)\n>>> x = jt.rand(1, 3, 224, 224)\n>>> y = net(x)\n>>> y.shape\n[1, 1000]"
    },
    {
        "api_name": "jittor.models.GoogLeNet",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.models.html#jittor.models.GoogLeNet",
        "api_signature": "jittor.models.GoogLeNet(num_classes=1000, aux_logits=True, init_weights=True, blocks=None)",
        "api_description": "GoogLeNet源自论文 Going Deeper with Convolutions , 也称为Inception v1, 它由多个称为Inception模块的子网络构成。\n注意:\n输入数据的维度应当是[batch_size, 3, height, width], 且height和width应该足够大, 以保证在网络结构中可以经历多次池化操作后依然保有空间分辨率。\n如果关闭了aux_logits, aux1和aux2的输出将为None。\n参数:\nnum_classes (int, optional): 分类的类别数量, 默认值: 1000。\naux_logits (bool, optional): 若为True, 则添加辅助分支以帮助训练。默认值: 1000。\ninit_weights (bool, optional): 若为True, 则对模型的权重进行初始化。默认值: True。\nblocks (list[nn.Module]): 由三个模块组成的列表, 依次为[conv_block, inception_block, inception_aux_block]。若为None, 则使用[BasicConv2d, Inception, InceptionAux]作为默认值。默认值: None。\n属性:\nconv1 (nn.Module): 输入层的卷积层。\nmaxpool1 (nn.Pool): 输入层的最大池化层。\nconv2 (nn.Module): 第二个卷积层。\nconv3 (nn.Module): 第三个卷积层。\nmaxpool2 (nn.Pool): 第二个最大池化层。\ninception3a (nn.Module): 第一个Inception模块。\ninception3b (nn.Module): 第二个Inception模块。\nmaxpool3 (nn.Pool): 第三个最大池化层。\ninception4a (nn.Module): 第三个Inception模块。\ninception4b (nn.Module): 第四个Inception模块。\ninception4c (nn.Module): 第五个Inception模块。\ninception4d (nn.Module): 第六个Inception模块。\ninception4e (nn.Module): 第七个Inception模块。\nmaxpool4 (nn.Pool): 第四个最大池化层。\ninception5a (nn.Module): 第八个Inception模块。\ninception5b (nn.Module): 第九个Inception模块。\naux1 (nn.Module): 第一个辅助分类器。\naux2 (nn.Module): 第二个辅助分类器。\navgpool (nn.AdaptiveAvgPool2d): 全局平均池化层。\ndropout (nn.Dropout): 丢弃层。\nfc (nn.Linear): 全连接层。\n代码示例:>>> import jittor as jt\n>>> from jittor.models.googlenet import GoogLeNet\n>>> model = GoogLeNet(num_classes=1000, aux_logits=True, init_weights=True)\n>>> input_tensor = jt.randn(1, 3, 224, 224)\n>>> model(input_tensor).shape\n[1,1000,]",
        "return_value": "",
        "parameters": "num_classes (int, optional): 分类的类别数量, 默认值: 1000。\naux_logits (bool, optional): 若为True, 则添加辅助分支以帮助训练。默认值: 1000。\ninit_weights (bool, optional): 若为True, 则对模型的权重进行初始化。默认值: True。\nblocks (list[nn.Module]): 由三个模块组成的列表, 依次为[conv_block, inception_block, inception_aux_block]。若为None, 则使用[BasicConv2d, Inception, InceptionAux]作为默认值。默认值: None。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> from jittor.models.googlenet import GoogLeNet\n>>> model = GoogLeNet(num_classes=1000, aux_logits=True, init_weights=True)\n>>> input_tensor = jt.randn(1, 3, 224, 224)\n>>> model(input_tensor).shape    \n[1,1000,]"
    },
    {
        "api_name": "jittor.loss3d.EarthMoverDistance.grad",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.loss3d.html#jittor.loss3d.EarthMoverDistance.grad",
        "api_signature": "grad(grad)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "jittor.grad",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.grad",
        "api_signature": "jittor.grad(loss, targets, retain_graph=True)",
        "api_description": "计算损失函数对目标变量的梯度。对于每个目标变量，求取损失函数对该变量的偏导数，这个偏导数就是该变量的梯度。",
        "return_value": "返回计算得到的梯度张量(Var)。",
        "parameters": "loss (Var) : 损失函数值。\ntargets (Var): 目标变量，可以是一个变量或变量的列表。\nretain_graph (bool) : 是否保持计算图。默认值为False。如果设为True，那么在计算梯度之后，计算图不会被删除，可以进行多次反向传播。",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "jittor.transform.gray",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.transform.html#jittor.transform.gray",
        "api_signature": "jittor.transform.gray(img, num_output_channels)",
        "api_description": "将任意模式（如 RGB、HSV、LAB 等）的 PIL 图片转换为灰度版本。该函数支持将灰度图像转换为单通道或三通道图像。",
        "return_value": "返回图片的灰度版本。如果 num_output_channels 为 1，返回单通道图像；如果为 3，返回三通道图像，其中 R、G、B 值相同。",
        "parameters": "img (PIL.Image.Image): 输入的 PIL 图片。\nnum_output_channels (int, 可选): 输出图片的通道数。值可以为 1 或 3。默认值:  1。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import numpy as np\n>>> from PIL import Image\n>>> from jittor.transform import gray\n>>> img = np.ones((100, 100, 3)).astype(np.uint8)\n>>> img = Image.fromarray(img)\n>>> img_gray_1_channel = gray(img, 1)\n>>> print(np.array(img_gray_1_channel).shape)\n(100, 100)\n>>> img_gray_3_channel = gray(img, 3)\n>>> print(np.array(img_gray_3_channel).shape)\n(100, 100, 3)"
    },
    {
        "api_name": "jittor.transform.Gray",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.transform.html#jittor.transform.Gray",
        "api_signature": "jittor.transform.Gray(num_output_channels=1)",
        "api_description": "将图像转换为灰度图的类。",
        "return_value": "",
        "parameters": "num_output_channels (int, 可选): 输出通道数（1表示单通道灰度图，3表示三通道灰度图）,默认值为1。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> from jittor import transform\n>>> gray = transform.Gray()\n>>> img_ = gray(img)"
    },
    {
        "api_name": "jittor_core.Var.greater",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor_core.Var.greater",
        "api_signature": "jittor_core.Var.greater()",
        "api_description": "函数C++定义格式:\njt.Var greater(jt.Var x, jt.Var y)\n两个张量中对应元素比较是否为大于，把结果布尔值放入输出张量的对应位置，也可以使用 > 运算符调用",
        "return_value": "形状与 x 和 y 相同的张量，数据类型是 bool。其中每个索引位置的布尔值表示 x 和 y 对应元素里是否 x 中的元素大于 y 中的元素",
        "parameters": "x (Var): 输入数据\ny (Var): 输入数据，形状与 x 相同",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> x = jt.array([3, 2, 1])\n>>> y = jt.array([1, 2, 3])\n>>> x.greater(y)\njt.Var([ True False False], dtype=bool)\n>>> x > y\njt.Var([ True False False], dtype=bool)"
    },
    {
        "api_name": "jittor_core.Var.greater_equal",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor_core.Var.greater_equal",
        "api_signature": "jittor_core.Var.greater_equal()",
        "api_description": "函数C++定义格式:\njt.Var greater_equal(jt.Var x, jt.Var y)\n两个张量中对应元素比较是否为大于或等于，把结果布尔值放入输出张量的对应位置，也可以使用 >= 运算符调用",
        "return_value": "形状与 x 和 y 相同的张量，数据类型是 bool。其中每个索引位置的布尔值表示该索引对应的 x 和 y 对应元素里是否 x 中的元素大于或等于 y 中的元素",
        "parameters": "x (Var): 输入数据\ny (Var): 输入数据，形状与 x 相同",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> x = jt.array([3, 2, 1])\n>>> y = jt.array([1, 2, 3])\n>>> x.greater_equal(y)\njt.Var([ True  True False], dtype=bool)\n>>> x >= y\njt.Var([ True  True False], dtype=bool)"
    },
    {
        "api_name": "jittor.nn.grid_sample",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.nn.html#jittor.nn.grid_sample",
        "api_signature": "jittor.nn.grid_sample(input, grid, mode='bilinear', padding_mode='zeros', align_corners=False)",
        "api_description": "给定一个输入和一个流场网格（flow-field grid），通过使用输入值和网格中的像素位置来计算输出。\n对于每个输出位置 output[n, :, h, w]，大小为2的向量 grid[n, h, w] 指定了输入像素的位置 x 和 y，这些位置被用来插值计算输出值 output[n, :, h, w]。在5D输入 的情况下，grid[n, d, h, w] 指定了用于插值计算 output[n, :, d, h, w] 的 x, y, z 像素位置。mode 参数指定了用于采样输入像素的最近邻或双线性插值方法。grid 指定 了通过输入空间维度归一化的采样像素位置。因此，它应该有大多数值在 [-1, 1] 的范围内。例如，值 x = -1, y = -1 是输入的左上像素，而值 x = 1, y = 1 是输入的右下 像素。如果 grid 有超出 [-1, 1] 范围的值，相应的输出将按照 padding_mode 定义的方式处理。",
        "return_value": "output(Var): 输出图像张量，形状为(N,C,Ho,Wo)",
        "parameters": "input (Var): 输入图像张量，形状为(N,C,Hi,Wi)\ngrid (Var): 流场网格，形状为(N,Ho,Wo,2)\nmode (str): 插值模式，可选’bilinear’和’nearest’，默认为’bilinear’\npadding_mode (str): 填充模式，可选’zeros’,’border’和’reflection’，默认为’zeros’",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> x = jt.array([[[[1.,2,3],[4,5,6],[7,8,9]]]])\n>>> grid = jt.array([[[[0.5,0.5],[0.5,0.5],[0.5,0.5]]]])\n>>> jt.nn.grid_sample(x, grid, mode='nearest')\njt.Var([[[[9. 9. 9.]]]], dtype=float32)"
    },
    {
        "api_name": "jittor.nn.grid_sample_v0",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.nn.html#jittor.nn.grid_sample_v0",
        "api_signature": "jittor.nn.grid_sample_v0(input, grid, mode='bilinear', padding_mode='zeros')",
        "api_description": "给定一个输入和一个流场网格（flow-field grid），通过使用输入值和网格中的像素位置来计算输出。\n对于每个输出位置 output[n, :, h, w]，大小为2的向量 grid[n, h, w] 指定了输入像素的位置 x 和 y，这些位置被用来插值计算输出值 output[n, :, h, w]。在5D输入 的情况下，grid[n, d, h, w] 指定了用于插值计算 output[n, :, d, h, w] 的 x, y, z 像素位置。mode 参数指定了用于采样输入像素的最近邻或双线性插值方法。grid 指定 了通过输入空间维度归一化的采样像素位置。因此，它应该有大多数值在 [-1, 1] 的范围内。例如，值 x = -1, y = -1 是输入的左上像素，而值 x = 1, y = 1 是输入的右下 像素。如果 grid 有超出 [-1, 1] 范围的值，相应的输出将按照 padding_mode 定义的方式处理。仅支持’zeros’模式。",
        "return_value": "output(Var): 输出图像张量，形状为(N,C,Ho,Wo)",
        "parameters": "input (Var): 输入图像张量，形状为(N,C,Hi,Wi)\ngrid (Var): 流场网格，形状为(N,Ho,Wo,2)\nmode (str): 插值模式，可选’bilinear’和’nearest’，默认为’bilinear’\npadding_mode (str): 填充模式，可选’zeros’",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> x = jt.array([[[[1.,2,3],[4,5,6],[7,8,9]]]])\n>>> grid = jt.array([[[[0.5,0.5],[0.5,0.5],[0.5,0.5]]]])\n>>> jt.nn.grid_sample_v0(x, grid, mode='nearest')\njt.Var([[[[5. 5. 5.]]]], dtype=float32)"
    },
    {
        "api_name": "jittor.nn.grid_sampler",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.nn.html#jittor.nn.grid_sampler",
        "api_signature": "jittor.nn.grid_sampler(X, grid, mode, padding_mode, align_corners)",
        "api_description": "对数据张量 X 进行基于网格的采样。该函数根据输入张量的维度自动选择使用二维或三维采样。\n给定输入张量 X 和流场 grid，使用 X 的值和 grid 中指定的像素位置计算输出。\n当前仅支持空间（二维）和体积（三维）的 X。\n在空间（二维）情况下，对于形状为 (N, C, inp_H, inp_W) 的 X 和形状为 (N, H, W, 2) 的 grid，输出将具有形状 (N, C, H, W)\n对于每个输出位置 output[n, :, h, w]，大小为 2 的向量 grid[n, h, w] 指定了 X 中的像素位置 x 和 y，这些位置用于插值计算输出值 output[n, :, h, w]。在 5D 输入的情况下，grid[n, d, h, w] 指定了用于插值计算 output[n, :, d, h, w] 的 x、y、z 像素位置。mode 参数指定了用于采样输入像素的 nearest 或 bilinear 插值方法。\n如果 grid 的值超出 [-1, 1] 范围，相应的输出将按照 padding_mode 定义的方式处理。选项包括：\npadding_mode=\"zeros\"：对于越界的网格位置使用 0，\npadding_mode=\"border\"：对于越界的网格位置使用边界值\npadding_mode=\"reflection\"：对于越界的网格位置使用边界反射的值。对于远离边界的位置，将继续反射直到变为界内",
        "return_value": "采样后的数据张量(Var)，形状为 [N, C, H, W]（二维）或 [N, C, D, H, W]（三维）",
        "parameters": "X(Var)：输入的数据张量，维度为4或5，形状为 [N, C, H, W]（二维）或 [N, C, D, H, W]（三维）\ngrid(Var)：采样网格，维度与 X 相同，形状为 [N, H, W, 2]（二维）或 [N, D, H, W, 3]（三维）\nmode(str)：采样模式，’nearest’ 或 ‘bilinear’。\npadding_mode(str)：填充模式，’border’ 或 ‘reflection’\nalign_corners(bool)：是否对齐角点。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> from jittor.nn import grid_sampler\n>>> N, C, D, H, W = 1, 1, 2, 2, 2\n>>> X = jt.array([[[[[1, 2], [3, 4]], [[5, 6], [7, 8]]]]])\n>>> grid = jt.array([[[[[0, 0, 0], [1, 1, 1]], [[0, 1, 0], [1, 0, 1]]]]])\n>>> mode = 'bilinear'\n>>> padding_mode = 'border'\n>>> align_corners = True\n>>> sampled_X = grid_sampler(X, grid, mode, padding_mode, align_corners)\n>>> print(sampled_X)\njt.Var([[[[[0.16508093 0.25176302]\n        [0.12954207 0.27189574]]  \n        [[0.24352701 0.32800347]\n        [0.30734745 0.38710332]]]]], dtype=float32)\n>>> print(sampled_X) \njt.Var([[[[[4.5 8. ]\n       [5.5 7. ]]]]], dtype=float32)\n>>> print(sampled_X.shape)\n[1, 1, 1, 2, 2]\n\n\n>>> N, C, H, W = 1, 1, 2, 2\n>>> X = jt.array([[[[1, 2], [3, 4]]]])\n>>> grid = jt.array([[[[0, 0], [1, 1]], [[0, 1], [1, 0]]]])\n>>> mode = 'bilinear'\n>>> padding_mode = 'border'\n>>> align_corners = True\n>>> sampled_X = grid_sampler_2d(X, grid, mode, padding_mode, align_corners)\n>>> print(sampled_X)\njt.Var([[[[[4.5 8. ]\n       [5.5 7. ]]]]], dtype=float32)\n>>> print(sampled_X.shape)\n[1, 1, 2, 2]"
    },
    {
        "api_name": "jittor.nn.grid_sampler_2d",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.nn.html#jittor.nn.grid_sampler_2d",
        "api_signature": "jittor.nn.grid_sampler_2d(X, grid, mode, padding_mode, align_corners)",
        "api_description": "对二维数据张量 X 进行基于网格的采样。\n根据提供的 grid，以及指定的采样模式、填充模式和对齐角点选项，对 X 进行采样。",
        "return_value": "采样后的数据张量(Var)，形状为 [N, C, H, W]。",
        "parameters": "X(Var)：输入的二维数据张量，形状为 [N, C, inp_H, inp_W]。\ngrid(Var)：采样网格，形状为 [N, H, W, 2]。\nmode(str)：采样模式，’nearest’ 或 ‘bilinear’。\npadding_mode(str)：填充模式，’border’ 或 ‘reflection’。\nalign_corners(bool)：是否对齐角点。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> from jittor.nn import grid_sampler_2d\n>>> N, C, H, W = 1, 1, 2, 2\n>>> X = jt.array([[[[1, 2], [3, 4]]]])\n>>> grid = jt.array([[[[0, 0], [1, 1]], [[0, 1], [1, 0]]]])\n>>> mode = 'bilinear'\n>>> padding_mode = 'border'\n>>> align_corners = True\n>>> sampled_X = grid_sampler_2d(X, grid, mode, padding_mode, align_corners)\n>>> print(sampled_X)\njt.Var([[[[[4.5 8. ]\n       [5.5 7. ]]]]], dtype=float32)\n>>> print(sampled_X.shape)\n[1, 1, 2, 2]"
    },
    {
        "api_name": "jittor.nn.grid_sampler_3d",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.nn.html#jittor.nn.grid_sampler_3d",
        "api_signature": "jittor.nn.grid_sampler_3d(X, grid, mode, padding_mode, align_corners)",
        "api_description": "对三维数据张量 X 进行基于网格的采样。\n根据提供的 grid，以及指定的采样模式、填充模式和对齐角点选项，对 X 进行采样。",
        "return_value": "采样后的数据张量(Var)，形状为 [N, C, D, H, W]",
        "parameters": "X(Var)：输入的三维数据张量，形状为 [N, C, inp_D, inp_H, inp_W]\ngrid(Var)：采样网格，形状为 [N, D, H, W, 3]\nmode(str)：采样模式，’nearest’ 或 ‘bilinear’\npadding_mode(str)：填充模式，’border’ 或 ‘reflection’\nalign_corners(bool)：是否对齐角点",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> from jittor.nn import grid_sampler_3d\n>>> N, C, D, H, W = 1, 1, 2, 2, 2\n>>> X = jt.array([[[[[1, 2], [3, 4]], [[5, 6], [7, 8]]]]])\n>>> grid = jt.array([[[[[0, 0, 0], [1, 1, 1]], [[0, 1, 0], [1, 0, 1]]]]])\n>>> mode = 'bilinear'\n>>> padding_mode = 'border'\n>>> align_corners = True\n>>> sampled_X = grid_sampler_3d(X, grid, mode, padding_mode, align_corners)\n>>> print(sampled_X)\njt.Var([[[[[0.16508093 0.25176302]\n       [0.12954207 0.27189574]]   \n       [[0.24352701 0.32800347]\n       [0.30734745 0.38710332]]]]], dtype=float32)\n>>> print(sampled_X) \njt.Var([[[[[4.5 8. ]\n       [5.5 7. ]]]]], dtype=float32)\n>>> print(sampled_X.shape)\n[1, 1, 1, 2, 2]"
    },
    {
        "api_name": "jittor.nn.grid_sampler_compute_source_index",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.nn.html#jittor.nn.grid_sampler_compute_source_index",
        "api_signature": "jittor.nn.grid_sampler_compute_source_index(coord, size, padding_mode, align_corners)",
        "api_description": "计算网格采样器的源索引。\n首先将标准化坐标转化为原始坐标，然后进行裁剪或反射并裁剪以适应原始图像的边界。",
        "return_value": "调整后的坐标(Var)，其形状与输入 coord 相同",
        "parameters": "coord(Var)：归一化的坐标\nsize(int)：目标尺寸\npadding_mode(str)：填充模式，’border’ 或 ‘reflection’\nalign_corners(bool)：是否对齐角点",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> coord = jt.array([0.5, 0.5, 0.5, 0.5])\n>>> size = 5\n>>> padding_mode = 'border'\n>>> align_corners = True\n>>> source_index = grid_sampler_compute_source_index(coord, size, padding_mode, align_corners)\n>>> print(source_index)\njt.Var([3. 3. 3. 3.], dtype=float32)"
    },
    {
        "api_name": "jittor.nn.grid_sampler_unnormalize",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.nn.html#jittor.nn.grid_sampler_unnormalize",
        "api_signature": "jittor.nn.grid_sampler_unnormalize(coord, size, align_corners)",
        "api_description": "将归一化坐标转换为未归一化的坐标。该函数根据 align_corners 参数的值来决定坐标转换的方式。\n当 align_corners 为 True 时，转换公式为 ((coord + 1) / 2) * (size - 1)。\n当 align_corners 为 False 时，转换公式为 ((coord + 1) * size - 1) / 2。",
        "return_value": "Var: 转换后的未归一化坐标，形状与输入 coord 相同",
        "parameters": "coord (Var): 归一化坐标，可以是任意形状的张量\nsize (int,list, tuple of int): 目标尺寸，可以是单个整数或整数列表/元组\nalign_corners (bool): 控制坐标转换方式的布尔值",
        "input_shape": "",
        "notes": "",
        "code_example": "import jittor as jt\ncoord = jt.array([-1., 0, 1])\nsize = 100\nalign_corners = True\nunnormalized_coord = grid_sampler_unnormalize(coord, size, align_corners)\nprint(unnormalized_coord)  # Output: [  0.  49.5  99. ]"
    },
    {
        "api_name": "jittor.nn.group_norm",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.nn.html#jittor.nn.group_norm",
        "api_signature": "jittor.nn.group_norm(x, num_groups, weight=1, bias=0, eps=1e-05)",
        "api_description": "group normalization操作函数。应用组归一化（Group Normalization）到一小批输入上，如论文《Group Normalization》所描述。\n输入通道被分成 num_groups 组，每组包含 num_channels / num_groups 个通道。num_channels 必须能被 num_groups 整除。均值和标准差分别在每组上单独计算。\n\\[y = \\frac{x - \\mathrm{E}[x]}{\\sqrt{\\mathrm{Var}[x] + \\epsilon}} * \\gamma + \\beta\\]",
        "return_value": "output(Var): 组归一化后的张量，和输入张量的形状相同",
        "parameters": "x (Var): 输入张量, 形状为 \\((N, C, *)\\)\nnum_groups (int): 分组数\nweight (float): 乘法权重, 默认为1\nbias (float): 加法权重, 默认为0\neps (float): 除数中的常数项, 默认为1e-05",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> x = jt.randn([8,16,32,32])\n>>> num_groups = 4\n>>> jt.nn.group_norm(inputs,num_groups).shape\n[8,16,32,32]"
    },
    {
        "api_name": "jittor.nn.GroupNorm",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.nn.html#jittor.nn.GroupNorm",
        "api_signature": "jittor.nn.GroupNorm(num_groups, num_channels, eps=1e-05, affine=True, is_train=True)",
        "api_description": "对输入进行分组归一化\n将输入的通道分为 num_groups 组，每组计算 \\(\\mu\\) 和方差 \\(\\sigma^2\\) ，将输入归一化，然后进行一个线性变换：\n\\[x_i' = \\frac{x_i - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} \\cdot w_i + b_i\\]\n训练和测试的时候都使用输入数据计算方差进行归一化",
        "return_value": "",
        "parameters": "num_groups (int): 输入的通道分成的组数，必须整除 num_channels\nnum_channels (int): 输入的通道个数\neps (float): 给方差加上的小量，避免除以 0。默认值：1e-5\naffine (bool): 是否对输入进行线性变换。默认值：True\nis_train (bool): 没有含义。默认值：True",
        "input_shape": "Input: (N, num_channels, ...)，其中 N 是批次中的数据个数。\nOutput: (N, num_channels, ...)，与输入相同。",
        "notes": "",
        "code_example": ">>> x = jt.array([[1, 2, 3, 4, 5, 6], [5, 6, 7, 8, 9, 10]])\n>>> gn1 = nn.GroupNorm(2, 6)\n>>> gn1(x)\njt.Var([[-1.2247355  0.         1.2247355 -1.2247343  0.         1.2247348]\n        [-1.2247348  0.         1.2247348 -1.2247314  0.         1.2247305]], dtype=float32)\n>>> gn2 = nn.GroupNorm(3, 6)\n>>> gn2(x)\njt.Var([[-0.99998    0.99998   -0.99998    0.99998   -0.99998    0.99998  ]\n        [-0.99998    0.99998   -0.99998    0.99998   -0.999979   0.9999809]], dtype=float32)"
    },
    {
        "api_name": "jittor.nn.GRUCell",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.nn.html#jittor.nn.GRUCell",
        "api_signature": "jittor.nn.GRUCell(input_size, hidden_size, bias=True)",
        "api_description": "一个门控循环单元 (GRU) 单元\n\\[\\begin{split}\\begin{array}{ll}\nr = \\sigma(W_{ir} x + b_{ir} + W_{hr} h + b_{hr}) \\\\\nz = \\sigma(W_{iz} x + b_{iz} + W_{hz} h + b_{hz}) \\\\\nn = \\tanh(W_{in} x + b_{in} + r * (W_{hn} h + b_{hn})) \\\\\nh' = (1 - z) * n + z * h\n\\end{array}\\end{split}\\]\n参数:\ninput_size(int): 输入特征的数量\nhidden_size(int): 隐藏状态的数量\nbias(bool, optional): 如果为 False ，则模型不会使用偏置权重. 默认值: True\n代码示例:>>> rnn = nn.GRUCell(10, 20)\n>>> input = jt.randn(6, 3, 10)\n>>> hx = jt.randn(3, 20)\n>>> output = []\n>>> for i in range(6):\nhx = rnn(input[i], hx)\noutput.append(hx)",
        "return_value": "",
        "parameters": "input_size(int): 输入特征的数量\nhidden_size(int): 隐藏状态的数量\nbias(bool, optional): 如果为 False ，则模型不会使用偏置权重. 默认值: True",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> rnn = nn.GRUCell(10, 20)\n>>> input = jt.randn(6, 3, 10)\n>>> hx = jt.randn(3, 20)\n>>> output = []\n>>> for i in range(6):\n        hx = rnn(input[i], hx)\n        output.append(hx)"
    },
    {
        "api_name": "jittor.nn.GRU",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.nn.html#jittor.nn.GRU",
        "api_signature": "jittor.nn.GRU(input_size: int, hidden_size: int, num_layers: int = 1, bias: bool = True, batch_first: bool = False, dropout: float = 0, bidirectional: bool = False)",
        "api_description": "一个门控循环单元 (GRU) 单元\n\\[\\begin{split}\\begin{array}{ll}\nr = \\sigma(W_{ir} x + b_{ir} + W_{hr} h + b_{hr}) \\\\\nz = \\sigma(W_{iz} x + b_{iz} + W_{hz} h + b_{hz}) \\\\\nn = \\tanh(W_{in} x + b_{in} + r * (W_{hn} h + b_{hn})) \\\\\nh' = (1 - z) * n + z * h\n\\end{array}\\end{split}\\]",
        "return_value": "",
        "parameters": "input_size(int): 输入特征的数量\nhidden_size(int): 隐藏状态的数量\nbias(bool, optional): 如果为 False ，则模型不会使用偏置权重. 默认值: True",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> rnn = nn.GRUCell(10, 20)\n>>> input = jt.randn(6, 3, 10)\n>>> hx = jt.randn(3, 20)\n>>> output = []\n>>> for i in range(6):\n        hx = rnn(input[i], hx)\n        output.append(hx)"
    },
    {
        "api_name": "jittor_core.Var.half",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor_core.Var.half",
        "api_signature": "jittor_core.Var.half()",
        "api_description": "函数C++定义格式:\njt.Var float16_(jt.Var x)\n创建并返回一个 x 的张量副本，并将其类型转换为 float16 ，返回半精度的浮点数，即用16个比特(2个字节)表示一个数。",
        "return_value": "返回一个新的张量, 数据类型为 float16",
        "parameters": "x (Var): Var类型的张量",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> x = jt.randn((2,2))\n>>> x\njt.Var([[ 1.4581492   0.2615918 ]\n        [ 0.26567948 -0.34585235]], dtype=float32)\n>>> x.float16()\njt.Var([[ 1.458   0.2615]\n        [ 0.2656 -0.346 ]], dtype=float16)"
    },
    {
        "api_name": "jittor.nn.hardtanh",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.nn.html#jittor.nn.hardtanh",
        "api_signature": "jittor.nn.hardtanh(x, min_val=-1, max_val=1)",
        "api_description": "在指定范围内对输入张量进行hardtanh剪裁。定义为:\n\\[\\begin{split}\\text{hardtanh}(x) = \\begin{cases}\n\\text{min_val} & \\text{ if } x < \\text{min_val} \\\\\nx & \\text{ if } \\text{min_val} \\leq x \\leq \\text{max_val} \\\\\n\\text{max_val} & \\text{ if } x > \\text{max_val}\n\\end{cases}\\end{split}\\]",
        "return_value": "output(Var): 计算后的张量，与输入张量形状相同",
        "parameters": "x (Var): 输入张量\nmin_val (float): 下界, 默认值为 -1\nmax_val (float): 上界, 默认值为 1",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> x = jt.randn(5)\njt.Var([ 1.0286063   0.66291064 -0.7988304   0.26159737 -0.5073038 ], dtype=float32)\n>>> jt.nn.hardtanh(input,-0.5,0.5)\njt.Var([ 0.5         0.5        -0.5         0.26159737 -0.5       ], dtype=float32)"
    },
    {
        "api_name": "jittor.transform.hflip",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.transform.html#jittor.transform.hflip",
        "api_signature": "jittor.transform.hflip(img)",
        "api_description": "对给定的图像进行水平翻转。",
        "return_value": "PIL.Image.Image: 水平翻转后的图像",
        "parameters": "img (PIL.Image.Image): 输入图像",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> import numpy as np\n>>> from PIL import Image\n>>> data = np.random.rand(200,200)\n>>> img = Image.fromarray(data, 'L')       \n>>> img.size\n(200, 200)\n>>> img_flipped = jt.transform.hflip(img) # 水平翻转"
    },
    {
        "api_name": "jittor.misc.histc",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.misc.histc",
        "api_signature": "jittor.misc.histc(input, bins, min=0.0, max=0.0)",
        "api_description": "计算输入的N维数组的直方图。\n元素被排序进入最小值和最大值之间等宽的箱中。若 min 和 max 都为0，则会取输入数组的最小值和最大值作为范围。返回的结果中，每个箱的值表示输入数组中值落在该 箱范围的元素数量。\n代码示例:\n>>> inputs = jt.randn((40,40))\n>>> jt.histc(inputs, bins=10)\njt.Var([  1.  12.  40. 143. 323. 398. 386. 202.  72.  23.], dtype=float32)",
        "return_value": "计算得到的直方图（Var）",
        "parameters": "input(Var): 输入的数组\nbins(int): 直方图的分隔宽度数量\nmin(float,optional): 箱的最小边界，默认值：0\nmax(float,optional): 箱的最大边界，默认值：0",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "jittor.misc.hypot",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.misc.hypot",
        "api_signature": "jittor.misc.hypot(a, b)",
        "api_description": "计算输入平方之和的平方根 \\(\\sqrt{a^2 + b^2}\\)",
        "return_value": "a 和 b 平方之和的平方根",
        "parameters": "a (Var): 输入参数\nb (Var): 输入参数，形状和 b 一样",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> a = jt.array([3.0, 5.0])\n>>> b = jt.array([4.0, 12.0])\n>>> jt.hypot(a, b)\njt.Var([ 5. 13.], dtype=float32)"
    },
    {
        "api_name": "jittor.nn.identity",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.nn.html#jittor.nn.identity",
        "api_signature": "jittor.nn.identity(input)",
        "api_description": "该函数返回输入的同一份拷贝。",
        "return_value": "输入变量的拷贝。结果并不是原地(in-place)操作, 操作后的结果和原来的input并不共享存储空间。",
        "parameters": "input (Var): 输入变量",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> from jittor import nn\n>>> x = jt.ones(3,3)\n>>> nn.idnetity(x)\njt.Var([[1. 1. 1.]\n        [1. 1. 1.]\n        [1. 1. 1.]], dtype=float32)"
    },
    {
        "api_name": "jittor.nn.Identity",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.nn.html#jittor.nn.Identity",
        "api_signature": "jittor.nn.Identity(*args, **kwargs)",
        "api_description": "该类用于占位，即它会输出与输入相同的张量。这个模块不会对数据进行任何改变或计算。",
        "return_value": "",
        "parameters": "*args: 可变参数，用于兼容可能传入的参数，但在实际中不会使用\n**kwargs: 关键字参数，同样用于兼容性，但不会在类中使用",
        "input_shape": "Input: \\((*)\\)，其中 * 表示任意数量的附加维数。\nOutput: \\((*)\\)，与输入形状相同。",
        "notes": "",
        "code_example": ">>> layer = nn.Identity()\n>>> input = jt.randn(128, 20)\n>>> output = layer(input)\n>>> print(output.size())\n[128, 20,]"
    },
    {
        "api_name": "jittor.misc.iinfo",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.misc.iinfo",
        "api_signature": "jittor.misc.iinfo(dtype)",
        "api_description": "此函数返回给定数据类型可表示的最大/最小整数的信息。此函数仅支持整数类型，如果dtype不是整数类型，会引发ValueError。",
        "return_value": "返回一个iinfo对象，它有以下属性：min, max, dtype等。",
        "parameters": "dtype (str)：期望获得其信息的数据类型的名称。如 ‘int32’ 、 ‘int16’ 等。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> from jittor import misc\n>>> misc.iinfo('int32')\niinfo(min=-2147483648, max=2147483647, dtype=int32)"
    },
    {
        "api_name": "jittor.transform.image_normalize",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.transform.html#jittor.transform.image_normalize",
        "api_signature": "jittor.transform.image_normalize(img, mean, std)",
        "api_description": "对图像进行归一化。",
        "return_value": "归一化后的图像",
        "parameters": "img (PIL Image.Image, jt.Var, np.ndarray): 输入的图像。如果输入的图像类型是 np.ndarray，则它应该具有形状 (C, H, W)\nmean (list): 归一化的平均值\nstd (list): 归一化的标准差值",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> import numpy as np\n>>> from PIL import Image\n>>> data = np.random.rand(200,200, 3)\n>>> img = Image.fromarray(data, 'RGB')   \n>>> mean = jt.ones(200,200) / 2\n>>> std = jt.ones(200,200) / 2  \n>>> jt.transform.image_normalize(img, mean, std)"
    },
    {
        "api_name": "jittor.dataset.ImageFolder",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.dataset.html#jittor.dataset.ImageFolder",
        "api_signature": "jittor.dataset.ImageFolder(root, transform=None)",
        "api_description": "从目录中加载图像及其标签用于图像分类的数据集。\n数据集的目录结构应如下所示:\nroot/label1/img1.png\nroot/label1/img2.png\n…\nroot/label2/img1.png\nroot/label2/img2.png\n…\n参数:\nroot (str): 包含图像和标签子目录的根目录的路径\ntransform (callable, optional): 用于对样本进行转换的optional转换操作(例如, 数据增强)。默认值: None\n属性:\nclasses (list): 类名的列表\nclass_to_idx (dict): 从类名映射到类索引的字典\nimgs (list): 包含(image_path, class_index)元组的列表\n代码示例:  >>> from jittor.dataset import ImageFolder\n>>> dataset = ImageFolder(root=\"path_to_your_dataset\")",
        "return_value": "",
        "parameters": "root (str): 包含图像和标签子目录的根目录的路径\ntransform (callable, optional): 用于对样本进行转换的optional转换操作(例如, 数据增强)。默认值: None",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> from jittor.dataset import ImageFolder\n>>> dataset = ImageFolder(root=\"path_to_your_dataset\")"
    },
    {
        "api_name": "jittor.transform.ImageNormalize",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.transform.html#jittor.transform.ImageNormalize",
        "api_signature": "jittor.transform.ImageNormalize(mean, std)",
        "api_description": "对图像进行标准化处理的类。给定n个通道的均值(mean[1], …, mean[n])和标准差(std[1], .., std[n])，此转换将标准化输入张量的每个通道，即：\n\\[output[channel] = (input[channel] - mean[channel]) / std[channel]\\]",
        "return_value": "",
        "parameters": "mean(list): 标准化使用的平均值列表\nstd(list): 标准化使用的标准差列表",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> from jittor import transform\n>>> img_normalize = transform.ImageNormalize(mean=[0.5], std=[0.5])\n>>> img_ = img_normalize(img)"
    },
    {
        "api_name": "jittor.models.Inception3",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.models.html#jittor.models.Inception3",
        "api_signature": "jittor.models.Inception3(num_classes=1000, aux_logits=True, inception_blocks=None, init_weights=True)",
        "api_description": "Inception v3源自论文 Rethinking the Inception Architecture for Computer Vision。inception模块通过不同尺寸的卷积核和池化层并行处理输入, 并合并输出, 旨在在不同的尺度上有效提取特征。\n参数:\nnum_classes (int, optional): 分类的类别数目。默认值: 1000\naux_logits (bool, optional): 若为True, 则添加一个辅助分支, 可以改善训练。默认值: True\ninception_blocks (List[nn.Module], optional): 模型的七个块的列表, 顺序为[conv_block, inception_a, inception_b, inception_c, inception_d, inception_e, inception_aux], 若为None, 则会使用默认的基础块 [BasicConv2d, InceptionA, InceptionB, InceptionC, InceptionD, InceptionE, InceptionAux]。默认值: None\ninit_weights (bool, optional): 是否初始化权重。默认值: True\n属性:\nConv2d_1a_3x3 (nn.Module): 输入层的卷积层。\nConv2d_2a_3x3 (nn.Module): 第二个卷积层。\nConv2d_2b_3x3 (nn.Module): 第三个卷积层。\nConv2d_3b_1x1 (nn.Module): 第四个卷积层。\nConv2d_4a_3x3 (nn.Module): 第五个卷积层。\nMixed_5b (nn.Module): 第一个Inception模块。\nMixed_5c (nn.Module): 第二个Inception模块。\nMixed_5d (nn.Module): 第三个Inception模块。\nMixed_6a (nn.Module): 第四个Inception模块。\nMixed_6b (nn.Module): 第五个Inception模块。\nMixed_6c (nn.Module): 第六个Inception模块。\nMixed_6d (nn.Module): 第七个Inception模块。\nMixed_6e (nn.Module): 第八个Inception模块。\nAuxLogits (nn.Module): 辅助分支。\nMixed_7a (nn.Module): 第九个Inception模块。\nMixed_7b (nn.Module): 第十个Inception模块。\nMixed_7c (nn.Module): 第十一个Inception模块。\nfc (nn.Linear): 全连接层。\n代码示例:>>> import jittor as jt\n>>> from jittor.models.inception import Inception3\n>>> model = Inception3(num_classes=1000, aux_logits=True)\n>>> inputs = jt.rand([1, 3, 299, 299])\n>>> model(inputs).shape\n[1,1000,]",
        "return_value": "",
        "parameters": "num_classes (int, optional): 分类的类别数目。默认值: 1000\naux_logits (bool, optional): 若为True, 则添加一个辅助分支, 可以改善训练。默认值: True\ninception_blocks (List[nn.Module], optional): 模型的七个块的列表, 顺序为[conv_block, inception_a, inception_b, inception_c, inception_d, inception_e, inception_aux], 若为None, 则会使用默认的基础块 [BasicConv2d, InceptionA, InceptionB, InceptionC, InceptionD, InceptionE, InceptionAux]。默认值: None\ninit_weights (bool, optional): 是否初始化权重。默认值: True",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> from jittor.models.inception import Inception3\n>>> model = Inception3(num_classes=1000, aux_logits=True)\n>>> inputs = jt.rand([1, 3, 299, 299])\n>>> model(inputs).shape\n[1,1000,]"
    },
    {
        "api_name": "jittor.models.inception_v3",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.models.html#jittor.models.inception_v3",
        "api_signature": "jittor.models.inception_v3(pretrained=False, progress=True, **kwargs)",
        "api_description": "构建一个Inception v3模型\nInception v3源自论文 Rethinking the Inception Architecture for Computer Vision, inception模块通过不同尺寸的卷积核和池化层并行处理输入, 并合并输出, 旨在在不同的尺度上有效提取特征。\n参数:\npretrained (bool, optional): 表示是否预加载预训练模型。默认为 False。\n返回值:\n返回构建好的inception_v3模型实例。如果 pretrained 为 True, 则返回在ImageNet上预训练的模型。\n代码示例:>>> import jittor as jt\n>>> from jittor.models.inception import *\n>>> net = inception_v3(pretrained=False)\n>>> x = jt.rand(1, 3, 224, 224)\n>>> y = net(x)\n>>> y.shape\n[1, 1000]",
        "return_value": "返回构建好的inception_v3模型实例。如果 pretrained 为 True, 则返回在ImageNet上预训练的模型。",
        "parameters": "pretrained (bool, optional): 表示是否预加载预训练模型。默认为 False。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> from jittor.models.inception import *\n>>> net = inception_v3(pretrained=False)\n>>> x = jt.rand(1, 3, 224, 224)\n>>> y = net(x)\n>>> y.shape\n[1, 1000]"
    },
    {
        "api_name": "jittor_core.Var.index",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor_core.Var.index",
        "api_signature": "jittor_core.Var.index()",
        "api_description": "函数C++定义格式:\njt.Var index__(jt.Var a,  int64 dim,  String dtype=ns_int32)\n获得 a 中每个元素在每一维的下标",
        "return_value": "tuple[Var] 包含 a.ndims 个张量，每个张量形状和 a 一样，第 \\(k\\) 个张量下标 \\((i_0, ..., i_k, ...)\\) 处元素值是 \\(i_k\\) ，即第 \\(k\\) 维的下标",
        "parameters": "a (Var): 一个张量\ndtype (str，可选): 返回的数据类型。默认值： 'int32'",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> x = jt.zeros((2, 3, 4))\n>>> jt.index_var(x)\n(jt.Var([[[0 0 0 0]\n            [0 0 0 0]\n            [0 0 0 0]]\n            [[1 1 1 1]\n            [1 1 1 1]\n            [1 1 1 1]]], dtype=int32),\n    jt.Var([[[0 0 0 0]\n            [1 1 1 1]\n            [2 2 2 2]]\n            [[0 0 0 0]\n            [1 1 1 1]\n            [2 2 2 2]]], dtype=int32),\n    jt.Var([[[0 1 2 3]\n            [0 1 2 3]\n            [0 1 2 3]]\n            [[0 1 2 3]\n            [0 1 2 3]\n            [0 1 2 3]]], dtype=int32))"
    },
    {
        "api_name": "jittor.misc.index_add_",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.misc.index_add_",
        "api_signature": "jittor.misc.index_add_(x, dim, index, tensor)",
        "api_description": "对于 index 中的每个下标和 tensor 里对应的每个张量，在 x 的 dim 维对应的下标处加上这个张量\n参数：\nx (Var): 形状 (..., k, ...)\ndim (int): 上面 x 中 k 对应的是第几维\nindex (Var): 形状 (n) 选取要加上张量的下标\ntensor (Var): 形状 (n, ..., ...) 每个下标处要加上的张量\n代码示例：>>> x = jt.ones(2, 3, 4)\n>>> jt.index_add_(x, 0, jt.array([0, 1]), jt.array([jt.ones(3, 4), jt.ones(3, 4) * 2]))\n>>> jt.index_add_(x, 1, jt.array([0, 1]), jt.array([jt.ones(2, 4), jt.ones(2, 4) * 2]))\n>>> x\njt.Var([[[3. 3. 3. 3.]\n[3. 3. 3. 3.]\n[2. 2. 2. 2.]]\n[[5. 5. 5. 5.]\n[5. 5. 5. 5.]\n[3. 3. 3. 3.]]], dtype=float32)",
        "return_value": "",
        "parameters": "x (Var): 形状 (..., k, ...)\ndim (int): 上面 x 中 k 对应的是第几维\nindex (Var): 形状 (n) 选取要加上张量的下标\ntensor (Var): 形状 (n, ..., ...) 每个下标处要加上的张量",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> x = jt.ones(2, 3, 4)\n>>> jt.index_add_(x, 0, jt.array([0, 1]), jt.array([jt.ones(3, 4), jt.ones(3, 4) * 2]))\n>>> jt.index_add_(x, 1, jt.array([0, 1]), jt.array([jt.ones(2, 4), jt.ones(2, 4) * 2]))\n>>> x\njt.Var([[[3. 3. 3. 3.]\n         [3. 3. 3. 3.]\n         [2. 2. 2. 2.]]\n        [[5. 5. 5. 5.]\n         [5. 5. 5. 5.]\n         [3. 3. 3. 3.]]], dtype=float32)"
    },
    {
        "api_name": "jittor.misc.index_fill_",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.misc.index_fill_",
        "api_signature": "jittor.misc.index_fill_(x, dim, indexs, val)",
        "api_description": "根据给定的索引 indexs 在指定维度 dim 上替换为给定元素值 val",
        "return_value": "进行索引替换之后的 Var，shape 与输入的 Var 一致",
        "parameters": "x (Var): 输入的 Var， 任意形状\ndim (int): 进行索引替换的维度\nindexs (Var): 需要替换的索引，类型为 Var，shape 为 (N, )，N 代表需要替换的索引个数\nval (float): 替换的元素值",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> x = jt.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=\"float32\")\n>>> jt.index_fill_(x, 0, jt.array([0, 2]), -1)\njt.Var([[-1. -1. -1.]\n        [ 4.  5.  6.]\n        [-1. -1. -1.]], dtype=float32)\n>>> jt.index_fill_(x, 1, jt.array([0, 2]), -1)\njt.Var([[-1.  2. -1.]\n        [-1.  5. -1.]\n        [-1.  8. -1.]], dtype=float32)"
    },
    {
        "api_name": "jittor.misc.index_select",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.misc.index_select",
        "api_signature": "jittor.misc.index_select(x: Var, dim: int, index: Var)",
        "api_description": "该方法返回一个新的张量，该张量通过使用索引在dim维度上对输入张量x进行索引。\n返回的张量具有与原始张量（x）相同的维度数。dim维度的大小与索引的长度相同; 其他维度的大小与原始张量中的大小相同。",
        "return_value": "jt.Var类型，新的通过索引获得的张量。",
        "parameters": "x: jt.Var类型，输入的张量。\ndim: int类型，需要进行索引的维度。\nindex: jt.Var类型，包含要索引的索引的1-D张量。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> x = jt.randn(3, 4)\n>>> indices = torch.tensor([2, 1])\n>>> y = jt.index_select(x, 0, indices)\n>>> assert jt.all_equal(y, x[indices])\n>>> y = jt.index_select(x, 1, indices)\n>>> assert jt.all_equal(y, x[:, indices])"
    },
    {
        "api_name": "jittor_core.Var.index_var",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor_core.Var.index_var",
        "api_signature": "jittor_core.Var.index_var()",
        "api_description": "函数C++定义格式:\njt.Var index__(jt.Var a,  int64 dim,  String dtype=ns_int32)\n获得 a 中每个元素在每一维的下标",
        "return_value": "tuple[Var] 包含 a.ndims 个张量，每个张量形状和 a 一样，第 \\(k\\) 个张量下标 \\((i_0, ..., i_k, ...)\\) 处元素值是 \\(i_k\\) ，即第 \\(k\\) 维的下标",
        "parameters": "a (Var): 一个张量\ndtype (str，可选): 返回的数据类型。默认值： 'int32'",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> x = jt.zeros((2, 3, 4))\n>>> jt.index_var(x)\n(jt.Var([[[0 0 0 0]\n            [0 0 0 0]\n            [0 0 0 0]]\n            [[1 1 1 1]\n            [1 1 1 1]\n            [1 1 1 1]]], dtype=int32),\n    jt.Var([[[0 0 0 0]\n            [1 1 1 1]\n            [2 2 2 2]]\n            [[0 0 0 0]\n            [1 1 1 1]\n            [2 2 2 2]]], dtype=int32),\n    jt.Var([[[0 1 2 3]\n            [0 1 2 3]\n            [0 1 2 3]]\n            [[0 1 2 3]\n            [0 1 2 3]\n            [0 1 2 3]]], dtype=int32))"
    },
    {
        "api_name": "jittor.nn.instance_norm",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.nn.html#jittor.nn.instance_norm",
        "api_signature": "jittor.nn.instance_norm(x, running_mean=None, running_var=None, weight=1, bias=0, momentum=0.1, eps=1e-05)",
        "api_description": "实例归一化(Instance Normalization)函数。均值和标准差是在每个小批量中的每个对象上，按维度单独计算的。\n在每一个单独的实例上进行归一化, 会改变数据分布使之接近标准正态分布，可以在训练神经网络时保证网络的稳定性。计算公式如下：\n\\[y = \\frac{x - \\mathrm{E}[x]}{\\sqrt{\\mathrm{Var}[x] + \\epsilon}} * \\gamma + \\beta\\]",
        "return_value": "output(Var): 实例归一化后的张量，与输入张量形状相同",
        "parameters": "x (Var): 输入张量，形状为 \\((N, *)\\)\nrunning_mean (Var): 保存的均值，默认不使用\nrunning_var (Var): 保存的方差，默认不使用\nweight (float): 缩放参数, 默认为1\nbias (float): 偏移参数, 默认为0\nmomentum (float): 动量参数, 默认为0.1\neps (float): 防止分母为0的参数, 默认为1e-5",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> x = jt.randn([3,32,32])\n>>> jt.nn.instance_norm(x).shape\n[3, 32, 32]"
    },
    {
        "api_name": "jittor.nn.InstanceNorm1d",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.nn.html#jittor.nn.InstanceNorm1d",
        "alias": "InstanceNorm"
    },
    {
        "api_name": "jittor.nn.InstanceNorm2d",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.nn.html#jittor.nn.InstanceNorm2d",
        "alias": "InstanceNorm"
    },
    {
        "api_name": "jittor.nn.InstanceNorm3d",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.nn.html#jittor.nn.InstanceNorm3d",
        "alias": "InstanceNorm"
    },
    {
        "api_name": "jittor.nn.InstanceNorm",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.nn.html#jittor.nn.InstanceNorm",
        "api_signature": "jittor.nn.InstanceNorm(num_features, eps=1e-05, momentum=0.1, affine=True, is_train=True, sync=True)",
        "api_description": "对输入进行实例归一化\n计算输入各项的均值 \\(\\mu\\) 和方差 \\(\\sigma^2\\) ，将输入归一化，然后进行一个线性变换：\n\\[x_i^{\\prime} = \\frac{x_i - \\mu}{\\sqrt{\\sigma^2 + \\varepsilon}} \\cdot w_i + b_i\\]\n训练和测试的时候都使用输入数据计算方差进行归一化",
        "return_value": "",
        "parameters": "num_features (int): 输入的特性个数\neps (float): 给方差加上的小量，避免除以 0。默认值：1e-5\nmomentum (float): 没有含义。默认值：0.1\naffine (bool): 是否对输入进行线性变换。默认值：True\nis_train (bool): 没有含义。默认值：True\nsync (bool): 没有含义。默认值：True",
        "input_shape": "Input: (N, num_features, ...)，其中 N 是批次中的数据个数。\nOutput: (N, num_features, ...)，与输入相同。",
        "notes": "",
        "code_example": ">>> x = jt.array([1, 2, 3])\n>>> y = jt.array([4, 5, 6])\n>>> n = nn.InstanceNorm(3)\n>>> n(x), n(y)\n(jt.Var([-1.2247354  0.         1.2247355], dtype=float32),\n jt.Var([-1.2247343  0.         1.2247348], dtype=float32))"
    },
    {
        "api_name": "jittor_core.Var.int",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor_core.Var.int",
        "api_signature": "jittor_core.Var.int()",
        "api_description": "函数C++定义格式:\njt.Var int32_(jt.Var x)\n创建并返回一个 x 的张量副本，并将其类型转换为 int32 。",
        "return_value": "返回一个新的张量, 数据类型为 int32",
        "parameters": "x (Var): Var类型的张量",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> x = jt.randn((2,3))\n>>> x\njt.Var([[-1.8707825  -0.5307898  -2.2583888 ]\n        [-0.1800911   0.10568491  0.0891161 ]], dtype=float32)\n>>> x.int32()\njt.Var([[-1  0 -2]\n        [ 0  0  0]], dtype=int32)"
    },
    {
        "api_name": "jittor_core.Var.int16",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor_core.Var.int16",
        "api_signature": "jittor_core.Var.int16()",
        "api_description": "函数C++定义格式:\njt.Var int16_(jt.Var x)\n创建并返回一个 x 的张量副本，并将其类型转换为 int16 。",
        "return_value": "返回一个新的张量, 数据类型为 int16",
        "parameters": "x (Var): Var类型的张量",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> x = jt.randn((2,3))\n>>> x\njt.Var([[ 1.8623732  -1.060122    0.82836497]\n        [-0.17111613  1.0357065  -0.4598468 ]], dtype=float32)\n>>> x.int16()\njt.Var([[ 1 -1  0]\n        [ 0  1  0]], dtype=int16)"
    },
    {
        "api_name": "jittor_core.Var.int32",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor_core.Var.int32",
        "api_signature": "jittor_core.Var.int32()",
        "api_description": "函数C++定义格式:\njt.Var int32_(jt.Var x)\n创建并返回一个 x 的张量副本，并将其类型转换为 int32 。",
        "return_value": "返回一个新的张量, 数据类型为 int32",
        "parameters": "x (Var): Var类型的张量",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> x = jt.randn((2,3))\n>>> x\njt.Var([[-1.8707825  -0.5307898  -2.2583888 ]\n        [-0.1800911   0.10568491  0.0891161 ]], dtype=float32)\n>>> x.int32()\njt.Var([[-1  0 -2]\n        [ 0  0  0]], dtype=int32)"
    },
    {
        "api_name": "jittor_core.Var.int64",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor_core.Var.int64",
        "api_signature": "jittor_core.Var.int64()",
        "api_description": "函数C++定义格式:\njt.Var int64_(jt.Var x)\n创建并返回一个 x 的张量副本，并将其类型转换为 int64 。",
        "return_value": "返回一个新的张量, 数据类型为 int64",
        "parameters": "x (Var): Var类型的张量",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> x = jt.randn((2,3))\n>>> x\njt.Var([[-1.4805598  -0.34178254 -0.3623456 ]\n        [-0.31025547  0.3924162  -1.5397007 ]], dtype=float32)\n>>> x.int64()\njt.Var([[-1  0  0]\n        [ 0  0 -1]], dtype=int64)"
    },
    {
        "api_name": "jittor_core.Var.int8",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor_core.Var.int8",
        "api_signature": "jittor_core.Var.int8()",
        "api_description": "函数C++定义格式:\njt.Var int8_(jt.Var x)\n创建并返回一个 x 的张量副本，并将其类型转换为 int8 。",
        "return_value": "返回一个新的张量, 数据类型为 int8",
        "parameters": "x (Var): Var类型的张量",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> x = jt.randn((2,3))\n>>> x\njt.Var([[-0.3373416   0.28905255  1.496737  ]\n        [-1.0225579  -1.0431367   0.3423414 ]], dtype=float32)\n>>> x.int8()\njt.Var([[ 0  0  1]\n        [-1 -1  0]], dtype=int8)"
    },
    {
        "api_name": "jittor.nn.interpolate",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.nn.html#jittor.nn.interpolate",
        "api_signature": "jittor.nn.interpolate(X, size=None, scale_factor=None, mode='bilinear', align_corners=False, tf_mode=False)",
        "api_description": "根据设定的模式(mode)对给定的图像进行大小调整。\n如果 scale_factor 是给定的，那么根据 scale_factor 进行调整，否则根据 size 进行调整。",
        "return_value": "output(Var): 调整后的图像张量",
        "parameters": "img (Var): 输入图像张量，形状为 \\((N, C, H, W)\\)\nsize (Union[int, Tuple[int, int]]): 输出图像的大小，可以是整数或者整数元组\nscale_factor (Union[float, Tuple[float, float]]): 缩放因子，可以是浮点数或者浮点数元组\nmode (str): 插值模式，可选 ‘bilinear’ (默认), ‘bicubic’, ‘area’, ‘nearest’\nalign_corners (bool): 默认为False, 如果设置为 True，输入和输出张量通过其角像素的中心点对齐，保留角像素处的值。如果设置为 False，输入和输出张量通过其角像素的角点对齐，插值使用边缘值填充来处理边界外的值。\ntf_mode (bool): 默认为False",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> x = jt.randn(4,3,32,32)\n>>> output_size = (64, 64)\n>>> jt.nn.interpolate(x, output_size,scale_factor=0.5).shape\n[4, 3, 16, 16]"
    },
    {
        "api_name": "jittor.linalg.inv",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.linalg.html#jittor.linalg.inv",
        "api_signature": "jittor.linalg.inv(x)",
        "api_description": "计算输入矩阵的逆。",
        "return_value": "逆矩阵(Var) 。",
        "parameters": "x (Var): 输入Var。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> x = jt.random((2, 2))\njt.Var([[ 0.9814584  -0.1916754 ]\n  [-0.8806686  -0.47373292]], dtype=float32)\n>>> jt.linalg.inv(x)\njt.Var([[ 2.520024   -0.4921519 ]\n  [-1.1624513  -0.62531066]], dtype=float32)"
    },
    {
        "api_name": "jittor.init.invariant_uniform",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.init.html#jittor.init.invariant_uniform",
        "api_signature": "jittor.init.invariant_uniform(shape, dtype='float32', mode='fan_in')",
        "api_description": "返回由 invariant_uniform 初始化的 Var。",
        "return_value": "由 invariant_uniform 初始化的 Var。",
        "parameters": "shape (int or Tuple[int]): 输出Var的形状\ndtype (str): 输出Var的 dtype ，默认 float32\nmode (str): 模式选择，应为 fan_in 或 fan_out。选择 'fan_in' 保留正向传递中权重方差的大小。选择 'fan_out' 保留反向传递中的大小。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> from jittor import init\n>>> from jittor import nn\n>>> a = init.invariant_uniform_((2,2))\n>>> print(a)\njt.Var([[ 0.08833592 -0.04776876]\n                [-0.04776876  0.08833592]], dtype=float32)"
    },
    {
        "api_name": "jittor.init.invariant_uniform_",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.init.html#jittor.init.invariant_uniform_",
        "api_signature": "jittor.init.invariant_uniform_(var, mode='fan_in')",
        "api_description": "使用不变均匀分布初始化 Var。",
        "return_value": "由不变均匀分布初始化的 Var",
        "parameters": "var (Var): 用不变均匀分布初始化的Var\nmode (str): 模式选择，应为 fan_in 或 fan_out。选择 'fan_in' 保留正向传递中权重方差的大小。选择 'fan_out' 保留反向传递中的大小。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> from jittor import init\n>>> from jittor import nn\n>>> linear = nn.Linear(2,2)\n>>> init.invariant_uniform_(linear.weight)\n>>> print(linear.weight)\njt.Var([[ 0.66421247 -0.74773896]\n                [ 0.74773896  0.66421247]], dtype=float32)\n>>> linear.weight.invariant_uniform_() # This is ok too"
    },
    {
        "api_name": "jittor_core.Var.is_stop_fuse",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor_core.Var.is_stop_fuse",
        "api_signature": "jittor_core.Var.is_stop_fuse()",
        "api_description": "函数C++定义格式:\ninline bool is_stop_fuse()\n若算子的fusion已经关闭返回 True ，否则返回 False 。",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "jittor_core.Var.is_stop_grad",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor_core.Var.is_stop_grad",
        "api_signature": "jittor_core.Var.is_stop_grad()",
        "api_description": "函数C++定义格式:\ninline bool is_stop_grad()\n若当前变量已关闭梯度记录则返回 True ，否则返回 False 。",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "jittor.Module.is_training",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.Module.is_training",
        "api_signature": "is_training()",
        "api_description": "返回模块是否处于training模式。",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "jittor.is_var",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.is_var",
        "api_signature": "jittor.is_var(v)",
        "api_description": "检查输入是否为Jittor的张量。",
        "return_value": "output (bool): 如果输入对象是Jittor变量，返回True，否则返回False。",
        "parameters": "v (Any): 待检查的输入对象。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> a = jt.array([1, 2, 3])\n>>> print(jt.is_var(a))\nTrue\n>>> b = 30\n>>> print(jt.is_var(b))\nFalse"
    },
    {
        "api_name": "jittor.misc.isfinite",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.misc.isfinite",
        "api_signature": "jittor.misc.isfinite(x)",
        "api_description": "检查输入 Var 中的元素是否为有限数值。\n如果 x 中元素既不是NaN（Not a Number）也不是无穷大或无穷小（Infinity 或 -Infinity）,则返回True，否则返回False。",
        "return_value": "Var, 返回一个与输入张量x同形状的张量，每个元素是判断x对应元素是否为有限数值的结果。",
        "parameters": "x (Var): 输入的张量。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> x = jt.array([1.0, float('Infinity'), float('nan'), 3])\n>>> jt.isfinite(x)\njt.Var([ True False False  True], dtype=bool)"
    },
    {
        "api_name": "jittor.misc.isinf",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.misc.isinf",
        "api_signature": "jittor.misc.isinf(x)",
        "api_description": "检查输入 Var 中的元素是否为无穷大值。\n如果 x 中元素是无穷大或无穷小（Infinity 或 -Infinity）,则返回True，否则返回False。",
        "return_value": "Var, 返回一个与输入张量x同形状的张量，每个元素是判断x对应元素是否为无穷大值的结果。",
        "parameters": "x (Var): 输入的张量。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> x = jt.array([1.0, float('inf'), float('nan'), float('-inf'), 3])\n>>> jt.isinf(x)\njt.Var([False  True False  True False], dtype=bool)"
    },
    {
        "api_name": "jittor.misc.isnan",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.misc.isnan",
        "api_signature": "jittor.misc.isnan(x)",
        "api_description": "检查输入 Var 中的元素是否为 NaN。",
        "return_value": "Var, 返回一个与输入张量x同形状的张量，每个元素是判断x对应元素是否为NaN的结果。",
        "parameters": "x (Var): 输入的张量。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> x = jt.array([1.0, float('nan'), 3])\n>>> jt.isnan(x)\njt.Var([False  True False], dtype=bool)"
    },
    {
        "api_name": "jittor.misc.isneginf",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.misc.isneginf",
        "api_signature": "jittor.misc.isneginf(x)",
        "api_description": "检查输入 Var 中的元素是否为负无穷。\n如果 x 中元素是无穷小（-Infinity）,则返回True，否则返回False。",
        "return_value": "Var, 返回一个与输入张量x同形状的张量，每个元素是判断x对应元素是否为负无穷的结果。",
        "parameters": "x (Var): 输入的张量。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> x = jt.array([1.0, float('inf'), float('nan'), float('-inf'), 3])\n>>> jt.isneginf(x)\njt.Var([False False False True False], dtype=bool)"
    },
    {
        "api_name": "jittor.misc.isposinf",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.misc.isposinf",
        "api_signature": "jittor.misc.isposinf(x)",
        "api_description": "检查输入 Var 中的元素是否为正无穷。\n如果 x 中元素是无穷大（+Infinity）,则返回True，否则返回False。",
        "return_value": "Var, 返回一个与输入张量x同形状的张量，每个元素是判断x对应元素是否为正无穷的结果。",
        "parameters": "x (Var): 输入的张量。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> x = jt.array([1.0, float('inf'), float('nan'), float('-inf'), 3])\n>>> jt.isposinf(x)\njt.Var([False True False False False], dtype=bool)"
    },
    {
        "api_name": "jittor_core.Var.item",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor_core.Var.item",
        "api_signature": "jittor_core.Var.item()",
        "api_description": "函数C++定义格式:\nItemData item()\n如果此张量只包含一个元素，返回python对应类型数值。否则请使用data()。",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "jittor.jittor_exit",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.jittor_exit",
        "api_signature": "jittor.jittor_exit()",
        "api_description": "主要作用是关闭Jittor，即在完成Jittor绑定的任务后，安全地退出Jittor运行环境。此函数无需任何输入参数并且没有返回值。",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> jt.jittor_exit()"
    },
    {
        "api_name": "jittor.init.kaiming_normal_",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.init.html#jittor.init.kaiming_normal_",
        "api_signature": "jittor.init.kaiming_normal_(var, a=0, mode='fan_in', nonlinearity='leaky_relu')",
        "api_description": "将 Var 通过 kaiming normal 随机初始化。",
        "return_value": "由 kaiming uniform 随机初始化的 Var",
        "parameters": "var (Var):需要被 kaiming normal 初始化的 Var\na (float):此层后使用的整流器的负斜率（仅在非线性函数是 leaky_relu 时使用）。\nmode (str): 模式选择，应为 fan_in 或 fan_out。选择 'fan_in' 保留正向传递中权重方差的大小。选择 'fan_out' 保留反向传递中的大小。\nnonlinearity (str):此层后使用的非线性函数。默认值：leaky_ relu 。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> from jittor import init\n>>> from jittor import nn\n>>> linear = nn.Linear(2,2)\n>>> init.kaiming_normal_(linear.weight)\n>>> linear.weight"
    },
    {
        "api_name": "jittor.init.kaiming_uniform_",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.init.html#jittor.init.kaiming_uniform_",
        "api_signature": "jittor.init.kaiming_uniform_(var, a=0, mode='fan_in', nonlinearity='leaky_relu')",
        "api_description": "使用kaiming_uniform随机初始化Jittor Var。",
        "return_value": "由 kaiming uniform 随机初始化的 Var",
        "parameters": "var (Var): 需要使用 kaiming uniform 随机初始化的 Var\na (float): leaky_relu 的负斜率，默认为 0\nmode (str): 模式选择，应为 fan_in 或 fan_out。选择 'fan_in' 保留正向传递中权重方差的大小。选择 'fan_out' 保留反向传递中的大小。\nnonlinearity (str): 在此层之后使用的非线性，默认使用 leaky_relu 。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> from jittor import init\n>>> from jittor import nn\n>>> linear = nn.Linear(2,2)\n>>> init.kaiming_uniform_(linear.weight)\n>>> print(linear.weight)\njt.Var([[-0.48337215  0.1855056 ]\n                [-0.35389122 -0.07890949]], dtype=float32)\n>>> linear.weight.kaiming_uniform_() # This is ok too"
    },
    {
        "api_name": "jittor.distributions.kl_divergence",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.distributions.html#jittor.distributions.kl_divergence",
        "api_signature": "jittor.distributions.kl_divergence(cur_dist, old_dist)",
        "api_description": "计算两个概率分布之间的 KL 散度。 KL 散度，也称为 Kullback-Leibler 散度，是一种测量两个概率分布之间差异的方法。如果两个分布完全相同，则 KL 散度为0。请注意， KL 散度不是对称的，也就是说， KL 散度( cur_dist , old_dist )并不等于 KL 散度( old_dist , cur_dist )。数学公式:\n对于 Normal 分布， KL 散度为\n\\[$0.5 * \\left(\\frac{\\sigma_{cur}^2}{\\sigma_{old}^2} + \\left(\\frac{\\mu_{cur} - \\mu_{old}}{\\sigma_{old}}\\right)^2 - 1 - \\log\\left(\\frac{\\sigma_{cur}^2}{\\sigma_{old}^2}\\right)\\right)$\\]\n对于 Categorical 或 OneHotCategorical 分布， KL 散度是\n\\[$prob_{cur}*(logits_{cur}-logits_{old})$\\]\n对于 Uniform 分布，如果满足条件 old_dist.low  >  cur_dist.low or  old_dist.high < cur_dist.high ，则结果为正无穷，否则为\n\\[$log\\left(\\frac{old\\_high - old\\_low}{cur\\_high - cur_low}\\right)$\\]\n对于 Geometric 分布,  KL 散度为\n\\[$-entropy_{cur} - \\log(1-prob_{old})/prob_{cur} - logits_{old}\\]",
        "return_value": "根据提供的分布类型返回两分布间的 KL 散度。",
        "parameters": "cur_dist (Distribution类型): 当前的概率分布。必须是 Normal ,  Categorical ,  OneHotCategorical ,  Uniform 或 Geometric 的一个实例。\nold_dist(Distribution类型): 用于比较的旧概率分布。必须是与 cur_dist 同类型的实例。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> from jittor.distributions import kl_divergence    \n>>> from jittor.distributions import OneHotCategorical\n>>> cur_onehot = OneHotCategorical(jt.array([0.3, 0.7])) \n>>> old_onehot = OneHotCategorical(jt.array([0.5, 0.5])) \n>>> kl_divergence(cur_onehot, old_onehot) \njt.Var([0.08228284], dtype=float32)"
    },
    {
        "api_name": "jittor.nn.KLDivLoss",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.nn.html#jittor.nn.KLDivLoss",
        "api_signature": "jittor.nn.KLDivLoss(reduction: str = 'mean', log_target: bool = False)",
        "api_description": "KLDivLoss 实现了 Kullback-Leibler 散度损失，用于衡量两个概率分布之间的差异。\n这个损失函数对于比较模型输出的概率分布（预测分布）和目标分布（真实分布）非常有用。\n对于相同形状的张量 \\(y_{\\text{pred}},\\ y_{\\text{true}}\\)，\n其中 \\(y_{\\text{pred}}\\) 是 input ，而 \\(y_{\\text{true}}\\) 是 target ，\n输入和目标值的之间的差异可被定义为\n\\[L(y_{\\text{pred}},\\ y_{\\text{true}})\n= y_{\\text{true}} \\cdot \\log \\frac{y_{\\text{true}}}{y_{\\text{pred}}}\n= y_{\\text{true}} \\cdot (\\log y_{\\text{true}} - \\log y_{\\text{pred}})\\]\n为了避免计算时的下溢问题，此损失函数期望输入 input 为对数空间。\n如果 log_target 设置为 True，则 target 也应该提供在对数空间中。\n简而言之，此代码大致等价于\nif not self.log_target:\nloss_pointwise = target * (target.log() - input)\nelse:\nloss_pointwise = target.exp() * (target - input)\n然后可以根据 reduction 参数来对这个结果进行处理：\nif self.reduction == 'mean':\nloss = loss_pointwise.mean()\nelif self.reduction == 'batchmean':\nloss = loss_pointwise.sum() / input.size(0)\nelif self.reduction == 'sum':\nloss = loss_pointwise.sum()\nelse:\nloss = loss_pointwise",
        "return_value": "",
        "parameters": "reduction (str, optional): 指定损失计算的方式。默认为 'mean'， 该参数可以被设置为 'mean'、 'batchmean' 、 'sum'、 None\nlog_target (bool, optional): 指定 target 是否为对数空间。默认为 False",
        "input_shape": "Input:\ninput: \\((*)\\)，模型输出，其中 \\(*\\) 表示任意数量的维度。\ntarget \\((*)\\)，目标值，与输入形状相同。\n\n\n\n\nOutput: 默认为标量。如果 reduction 为 'None'，则为 \\((*)\\)，维数与输入相同。",
        "notes": "",
        "code_example": ">>> kl_loss = nn.KLDivLoss(reduction=\"batchmean\")\n>>> input = jt.randn(3, 5)\n>>> target = jt.rand(3, 5)\n>>> output = kl_loss(input, target)\n>>> print(output)\njt.Var([-0.30870536], dtype=float32)"
    },
    {
        "api_name": "jittor.misc.knn",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.misc.knn",
        "api_signature": "jittor.misc.knn(unknown, known, k)",
        "api_description": "对未知向量寻找与已知向量里中的欧氏距离最近的 k 个向量\n参数：\nunknown (Var): 形状 (b, n, c) ，其中 b 是批次数， n 是每批次未知向量个数， c 是每个向量维数\nknown (Var): 形状 (b, m, c) ，其中 b 和 c 上同， m 是每批次已知向量个数\nk (int): 对于每个未知向量寻找多少个最近向量\n返回值：\ntuple[Var, Var] ，其中第一个元素是形状 (b, n, k) 表示对每个未知向量，在当前批次中找到的最近 k 个已知向量的欧式距离的平方，第二个元素是形状 (b, n, k) 表示找到最近向量的下标\n代码示例：>>> unknown = jt.randn(1, 4, 3)\n>>> known = jt.randn(1, 10, 3)\n>>> jt.misc.knn(unknown, known, 2)\n(jt.Var([[[0.04125667 1.0006377 ]\n[1.2794693  2.05327   ]\n[1.4255599  3.4547305 ]\n[0.849751   1.5978208 ]]], dtype=float32),\njt.Var([[[8 4]\n[9 4]\n[4 8]\n[1 3]]], dtype=int32))\n这个函数只在 c 为 3 的时候正确工作。",
        "return_value": "tuple[Var, Var] ，其中第一个元素是形状 (b, n, k) 表示对每个未知向量，在当前批次中找到的最近 k 个已知向量的欧式距离的平方，第二个元素是形状 (b, n, k) 表示找到最近向量的下标",
        "parameters": "unknown (Var): 形状 (b, n, c) ，其中 b 是批次数， n 是每批次未知向量个数， c 是每个向量维数\nknown (Var): 形状 (b, m, c) ，其中 b 和 c 上同， m 是每批次已知向量个数\nk (int): 对于每个未知向量寻找多少个最近向量",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> unknown = jt.randn(1, 4, 3)\n>>> known = jt.randn(1, 10, 3)\n>>> jt.misc.knn(unknown, known, 2)\n(jt.Var([[[0.04125667 1.0006377 ]\n          [1.2794693  2.05327   ]\n          [1.4255599  3.4547305 ]\n          [0.849751   1.5978208 ]]], dtype=float32),\n jt.Var([[[8 4]\n          [9 4]\n          [4 8]\n          [1 3]]], dtype=int32))"
    },
    {
        "api_name": "jittor.misc.kthvalue",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.misc.kthvalue",
        "api_signature": "jittor.misc.kthvalue(input, k, dim=None, keepdim=False, keepdims=False)",
        "api_description": "在指定维度上保留最小的第 k 个元素\n在输入 Var input 的指定维度上找出最小的第 k 个元素，并返回这些元素的值以及其在原始 Var 中的索引。返回的 Var 的大小在指定的维度上大小为 1，其他维度与 input 一致。",
        "return_value": "Tuple of Var, 其中包含 input 在指定维度上仅保留第 k 小元素后的张量 (Var)，以及这些元素在 input 中的索引 (Var)。",
        "parameters": "input (Var): 输入张量 Var, 任意形状\nk (int):  指定寻找第 k 小的元素\ndim (int): 指定在哪个维度上寻找第 k 小的元素，默认为最后一个维度\nkeepdim (bool): 为 True 时，则在输出中保持缩减的维度。默认为 False\nkeepdims (bool): 和 keepdim 的作用相同。旨在与numpy对齐，其在numpy中采用的参数名称是keepdims。默认为 False",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> var = jt.arange(1, 7).reshape(2, 3)\njt.Var([[1 2 3]\n        [4 5 6]], dtype=int32)\n>>> jt.misc.kthvalue(var, 2, dim=0, keepdim=True)\n(jt.Var([[4 5 6]], dtype=int32), jt.Var([[1 1 1]], dtype=int32))\n>>> jt.misc.kthvalue(var, 2, dim=0, keepdim=False)\n(jt.Var([4 5 6], dtype=int32), jt.Var([1 1 1], dtype=int32))"
    },
    {
        "api_name": "jittor.nn.l1_loss",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.nn.html#jittor.nn.l1_loss",
        "api_signature": "jittor.nn.l1_loss(output, target)",
        "api_description": "计算给定输出和目标之间的L1损失（平均绝对误差）。L1损失是预测值output和真实值target之间差值的绝对值的均值：\n\\[L = \\frac{1}{n} ∑|output_i - target_i|\\]\n参数:\noutput (Var): 预测值，模型输出。可以是任意形状的张量\ntarget (Var): 目标值，实际值或者是标签。形状应与output保持一致\n返回值:Var: 一个标量张量，表示L1损失。\n代码示例：>>> import jittor as jt\n>>> from jittor import nn\n>>> output = jt.array([1.0, 1.0, 1.0])\n>>> target = jt.array([0.5, 0.6, -2.0])\n>>> nn.l1_loss(output, target)\njt.Var([1.3], dtype=float32)",
        "return_value": "Var: 一个标量张量，表示L1损失。",
        "parameters": "output (Var): 预测值，模型输出。可以是任意形状的张量\ntarget (Var): 目标值，实际值或者是标签。形状应与output保持一致",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> from jittor import nn\n>>> output = jt.array([1.0, 1.0, 1.0])\n>>> target = jt.array([0.5, 0.6, -2.0])\n>>> nn.l1_loss(output, target)\njt.Var([1.3], dtype=float32)"
    },
    {
        "api_name": "jittor.nn.L1Loss",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.nn.html#jittor.nn.L1Loss",
        "api_signature": null,
        "api_description": "该类用于计算输出值和目标值的绝对误差损失。对单个元素的误差计算如下：\n\\[\\ell(x, y) = L = \\{l_1,\\dots,l_N\\}^\\top, \\quad\nl_n = \\left| x_n - y_n \\right|,\\]\n其中 \\(N\\) 为批处理数值。",
        "return_value": "",
        "parameters": "",
        "input_shape": "Input:\noutput: \\((*)\\)，模型输出的预测值，其中 * 表示任意数量的附加维数。\ntarget: \\((*)\\)，目标值，和输入形状相同。\n\n\n\n\nOutput: 一个标量，表示计算的 L1 损失。",
        "notes": "",
        "code_example": ">>> loss = nn.L1Loss()\n>>> output = jt.randn((3,2))\n>>> target = jt.randn((3,2))\n>>> loss_var = loss(output, target)\n>>> loss_var\njt.Var([0.6522219], dtype=float32)"
    },
    {
        "api_name": "jittor.optim.LambdaLR",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.optim.html#jittor.optim.LambdaLR",
        "api_signature": "jittor.optim.LambdaLR(optimizer, lr_lambda, last_epoch=-1)",
        "api_description": "用于实现学习率调度功能的类，其根据使用者设置的学习率迭代公式（可用 lambda 表达式作为参数给出），对基本学习率进行调整。",
        "return_value": "",
        "parameters": "optimizer (Optimizer): 已经初始化的优化器。\nlr_lambda (callable or list): 一个函数或者一个函数列表，用以定义学习率策略. 对应公式为: \\(lr = \\text{init_lr} \\times \\text{lr_lambda(last_epoch)}\\)\nlast_epoch (int, 可选): 最后一次迭代的 epoch 数。默认值: -1。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> from jittor.optim import SGD, LambdaLR\n>>> optimizer = SGD(model.parameters(), lr=0.1)\n>>> scheduler = LambdaLR(optimizer, lr_lambda=lambda epoch: 0.95 ** epoch)\n>>> for epoch in range(100):\n...     train(...)\n...     validate(...)\n...     scheduler.step()"
    },
    {
        "api_name": "jittor.transform.Lambda",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.transform.html#jittor.transform.Lambda",
        "api_signature": "jittor.transform.Lambda(lambd)",
        "api_description": "此类用于将用户定义的lambda函数作为变换应用到对象上。",
        "return_value": "",
        "parameters": "lambd (function): 用于变换的lambda函数。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> from jittor import transform\n>>> lambda = transform.Lambda(lambda x: x + 10)\n>>> result = lambda(5)"
    },
    {
        "api_name": "jittor.nn.layer_norm",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.nn.html#jittor.nn.layer_norm",
        "api_signature": "jittor.nn.layer_norm(*args, **kw)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "jittor.nn.LayerNorm1d",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.nn.html#jittor.nn.LayerNorm1d",
        "alias": "LayerNorm"
    },
    {
        "api_name": "jittor.nn.LayerNorm2d",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.nn.html#jittor.nn.LayerNorm2d",
        "alias": "LayerNorm"
    },
    {
        "api_name": "jittor.nn.LayerNorm3d",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.nn.html#jittor.nn.LayerNorm3d",
        "alias": "LayerNorm"
    },
    {
        "api_name": "jittor.nn.LayerNorm",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.nn.html#jittor.nn.LayerNorm",
        "api_signature": "jittor.nn.LayerNorm(normalized_shape, eps: float = 1e-05, elementwise_affine: bool = True)",
        "api_description": "对输入在某些维度归一化\n计算输入在某些维度的均值 \\(\\mu\\) 和方差 \\(\\sigma^2\\) ，将输入归一化，然后进行一个线性变换：\n\\[x_i^{\\prime} = \\frac{x_i - \\mu}{\\sqrt{\\sigma^2 + \\varepsilon}} \\cdot w_i + b_i\\]\n训练和测试的时候都使用输入数据计算方差进行归一化。",
        "return_value": "",
        "parameters": "normalized_shape (int ,  tuple[int, …]): 需要被归一化的维度的长度\neps (float): 给方差加上的小量，避免除以 0。默认值：1e-5\nelementwise_affine (bool): 是否对输入进行线性变换。默认值：True",
        "input_shape": "Input: (N, ..., normalized_shape)，其中 N 是批次中的数据个数。\nOutput: (N, ..., normalized_shape)，与输入相同。",
        "notes": "",
        "code_example": ">>> x = jt.array([[1, 2, 3], [4, 5, 6]])\n>>> n1 = nn.LayerNorm(3)\n>>> n1(x)\njt.Var([[-1.2247354  0.         1.2247354]\n        [-1.2247345  0.         1.2247345]], dtype=float32)\n>>> n2 = nn.LayerNorm((2, 3))\n>>> n2(x)\njt.Var([[-1.4638475  -0.87830853 -0.29276955]\n        [ 0.29276943  0.8783083   1.4638474 ]], dtype=float32)"
    },
    {
        "api_name": "jittor.nn.leaky_relu",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.nn.html#jittor.nn.leaky_relu",
        "api_signature": "jittor.nn.leaky_relu(x, scale=0.01)",
        "api_description": "该函数为Jittor的Leaky ReLU激活函数，与ReLU函数不同的是，输入值 x < 0 时，不直接返回 0，而是返回输入值进行scale之后的结果：\n\\[\\begin{split}\\text{LeakyRELU}(x) =\n\\begin{cases}\nx, &  x \\geq 0 \\\\\\\\\n\\text{scale} * x, & x < 0\n\\end{cases}\\end{split}\\]\n参数：\nx(Var): 输入的Var张量\nscale(float，optional)：x<0情况下的放缩比例。默认值：0.01\n返回值：输入的张量x应用Leaky ReLU激活的结果\n代码示例：>>> import jittor as jt\n>>> from jittor import nn\n>>> x = jt.array([0.5, -0.5, -0.7])\n>>> nn.leaky_relu(x)\njt.Var([ 0.5   -0.005 -0.007], dtype=float32)",
        "return_value": "输入的张量x应用Leaky ReLU激活的结果",
        "parameters": "x(Var): 输入的Var张量\nscale(float，optional)：x<0情况下的放缩比例。默认值：0.01",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> from jittor import nn\n>>> x = jt.array([0.5, -0.5, -0.7])\n>>> nn.leaky_relu(x)\njt.Var([ 0.5   -0.005 -0.007], dtype=float32)"
    },
    {
        "api_name": "jittor_core.Var.left_shift",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor_core.Var.left_shift",
        "api_signature": "jittor_core.Var.left_shift()",
        "api_description": "函数C++定义格式:\njt.Var left_shift(jt.Var x, jt.Var y)\n对两个张量中对应元素计算左移移位运算，也可以使用 << 运算符调用",
        "return_value": "形状与 x 和 y 相同的张量，其元素为 x 和 y 对应索引位置的元素计算左移操作的结果数",
        "parameters": "x (Var): 被左移的张量，元素的数据类型是 int32 或 int64\ny (Var): 左移的位数张量，元素数据类型是 int32 或 int64 ，形状与 x 相同",
        "input_shape": "",
        "notes": "支持使用 jt.left_shift() 进行调用",
        "code_example": ">>> x = jt.array([1, 2, 4])\n>>> y = jt.array([3, 4, 5])\n>>> x.left_shift(y)\njt.Var([  8  32 128], dtype=int32)\n>>> x << y\njt.Var([  8  32 128], dtype=int32)"
    },
    {
        "api_name": "jittor_core.Var.less",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor_core.Var.less",
        "api_signature": "jittor_core.Var.less()",
        "api_description": "函数C++定义格式:\njt.Var less(jt.Var x, jt.Var y)\n两个张量中对应元素比较是否为小于，把结果布尔值放入输出张量的对应位置，也可以使用 < 运算符调用",
        "return_value": "输出张量，形状与 x 和 y 相同，数据类型是 bool。其中每个索引位置的布尔值表示该索引对应的 x 和 y 对应元素里是否 x 中的元素小于 y 中的元素",
        "parameters": "x (Var): 输入数据\ny (Var): 输入数据，形状与 x 相同",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> x = jt.array([3, 2, 1])\n>>> y = jt.array([1, 2, 3])\n>>> x.less(y)\njt.Var([False False  True], dtype=bool)\n>>> x < y\njt.Var([False False  True], dtype=bool)"
    },
    {
        "api_name": "jittor_core.Var.less_equal",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor_core.Var.less_equal",
        "api_signature": "jittor_core.Var.less_equal()",
        "api_description": "函数C++定义格式:\njt.Var less_equal(jt.Var x, jt.Var y)\n两个张量中对应元素比较是否为小于或等于，把结果布尔值放入输出张量的对应位置，也可以使用 <= 运算符调用",
        "return_value": "形状与 x 和 y 相同的张量，数据类型是 bool。其中每个索引位置的布尔值表示该索引对应的 x 和 y 对应元素里是否 x 中的元素小于或等于 y 中的元素",
        "parameters": "x (Var): 输入数据\ny (Var): 输入数据，形状与 x 相同",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> x = jt.array([3, 2, 1])\n>>> y = jt.array([1, 2, 3])\n>>> x.less_equal(y)\njt.Var([False  True  True], dtype=bool)\n>>> x <= y\njt.Var([False  True  True], dtype=bool)"
    },
    {
        "api_name": "jittor.nn.linear",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.nn.html#jittor.nn.linear",
        "api_signature": "jittor.nn.linear(x, weight, bias=None)",
        "api_description": "对输入x进行线性变换。此函数返回x与权重weight的矩阵乘法的结果，如果传入了偏置bias，则在进行矩阵乘法后还会加上偏置: \\(x\\ *\\ weight^T + bias\\)\n参数:\nx (Var): 输入张量，大小可以是(batch_size, input_dim)\nweight (Var): 权重矩阵，大小可以是(batch_size, input_dim)\nbias (Var, optional): 偏置向量，大小可以是(output_dim,)。默认值: None\n返回值:Var: 线性变换后的结果，大小可以是(batch_size, output_dim)\n代码示例：>>> import jittor as jt\n>>> from jittor import nn\n>>> x = jt.array([[0, 0.5, 1.0], [-0.3, 0.5, 0.8]])\n>>> weight = jt.ones(3,3)\n>>> nn.linear(x, weight)\njt.Var([[1.5 1.5 1.5]\n[1.  1.  1. ]], dtype=float32)",
        "return_value": "Var: 线性变换后的结果，大小可以是(batch_size, output_dim)",
        "parameters": "x (Var): 输入张量，大小可以是(batch_size, input_dim)\nweight (Var): 权重矩阵，大小可以是(batch_size, input_dim)\nbias (Var, optional): 偏置向量，大小可以是(output_dim,)。默认值: None",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> from jittor import nn\n>>> x = jt.array([[0, 0.5, 1.0], [-0.3, 0.5, 0.8]])\n>>> weight = jt.ones(3,3)     \n>>> nn.linear(x, weight)   \njt.Var([[1.5 1.5 1.5]\n        [1.  1.  1. ]], dtype=float32)"
    },
    {
        "api_name": "jittor.nn.Linear",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.nn.html#jittor.nn.Linear",
        "api_signature": "jittor.nn.Linear(in_features, out_features, bias=True)",
        "api_description": "对输入作用线性变换\n\\[y = A x + b\\]\n其中 \\(A\\) 和 \\(b\\) 是可学习的参数。\n默认有偏置值 \\(b\\)，如果 bias = False 则 \\(b = 0\\)。",
        "return_value": "",
        "parameters": "in_features (int): 每个输入向量的维数\nout_features (int): 每个输出向量的维数\nbias (bool): 是否使用偏置。默认值：True",
        "input_shape": "Input: \\((*, \\text{in_features})\\)，其中 * 表示任意数量的附加维数。\nOutput: \\((*, \\text{out_features})\\)，其中 * 表示附加维数，这部分与输入相同。",
        "notes": "",
        "code_example": ">>> l = nn.Linear(2, 3)\n>>> input = jt.randn(5, 2)\n>>> output = l(input)\n>>> print(output.size())\n[5,3,]"
    },
    {
        "api_name": "jittor.misc.linspace",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.misc.linspace",
        "api_signature": "jittor.misc.linspace(start, end, steps)",
        "api_description": "生成一个由 start 到 end 等距的一维张量，其中共有 steps 个元素，并且包含end。也就是说输出张量为：\n\\[\\left(\\text { start, } \\text { start }+\\frac{\\text { end }- \\text { start }}{\\text { steps }-1}, \\ldots, \\text { start }+(\\text { steps }-2) * \\frac{\\text { end }- \\text { start }}{\\text { steps }-1}, \\text { end }\\right)\\]",
        "return_value": "Var，一个一维张量，包含在 start 和 end 之间的等间隔步数。",
        "parameters": "start (int, float): 起始值\nend (int, float): 结束值\nsteps (int): 构造的 Var 的长度",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> jt.linspace(3, 10, 5)\njt.Var([ 3.    4.75  6.5   8.25 10.  ], dtype=float32)"
    },
    {
        "api_name": "jittor.nn.linspace_from_neg_one",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.nn.html#jittor.nn.linspace_from_neg_one",
        "api_signature": "jittor.nn.linspace_from_neg_one(grid, num_steps, align_corners)",
        "api_description": "创建一个以-1和1为端点的等差数列。",
        "return_value": "output(Var): 等差数列张量",
        "parameters": "grid (Var): 输入图像张量，形状为(N,C,H,W)\nnum_steps (int): 等差数列的长度\nalign_corners (bool): 是否将-1和1作为端点",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> grid = jt.rand(3,3)\n>>> jt.nn.linspace_from_neg_one(grid, 5, True)\njt.Var([-1.  -0.5  0.   0.5  1. ], dtype=float32)"
    },
    {
        "api_name": "jittor.liveness_info",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.liveness_info",
        "api_signature": "jittor.liveness_info()",
        "api_description": "获取Jittor库中的存活信息。",
        "return_value": "包含存活信息的字典(dict)，包括 hold_vars , lived_vars , lived_ops 。",
        "parameters": "无",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> jt.liveness_info()\n{'hold_vars': 0, 'lived_vars': 0, 'lived_ops': 0}"
    },
    {
        "api_name": "jittor.Module.load",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.Module.load",
        "api_signature": "load(path: str)",
        "api_description": "从文件中加载参数。\n代码示例:\n>>> class Net(nn.Module):\n>>> ...\n>>> net = Net()\n>>> net.save('net.pkl')\n>>> net.load('net.pkl')\n本方法也支持从pytorch .pth文件中加载状态字典。\n备注\n当载入的参数与模型定义不一致时, jittor 会输出错误信息, 但是不会抛出异常.\n若载入参数出现模型定义中没有的参数名, 则会输出如下信息, 并忽略此参数:\n>>> [w 0205 21:49:39.962762 96 __init__.py:723] load parameter w failed ...\n若载入参数的 shape 与模型定义不一致, 则会输出如下信息, 并忽略此参数:\n>>> [e 0205 21:49:39.962822 96 __init__.py:739] load parameter w failed: expect the shape of w to be [1000,100,], but got [3,100,100,]\n如载入过程中出现错误, jittor 会输出概要信息, 您需要仔细核对错误信息\n>>> [w 0205 21:49:39.962906 96 __init__.py:741] load total 100 params, 3 failed",
        "return_value": "",
        "parameters": "path (str) – 模型位置",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "jittor.load",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.load",
        "api_signature": "jittor.load(path: str)",
        "api_description": "从指定路径 path 加载一个模型。",
        "return_value": "返回加载后的模型。",
        "parameters": "path (str) :需要加载模型的具体路径，此参数为必需。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> model = jt.load(\"/path/to/model\")"
    },
    {
        "api_name": "jittor.Module.load_parameters",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.Module.load_parameters",
        "api_signature": "load_parameters(params)",
        "api_description": "将参数加载到模块中。",
        "return_value": "",
        "parameters": "params – 一个记录模型函数类名称和对应参数的字典。",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "jittor.Module.load_state_dict",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.Module.load_state_dict",
        "api_signature": "load_state_dict(params)",
        "api_description": "从字典中加载模块的参数。",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "jittor.optim.Optimizer.load_state_dict",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.optim.html#jittor.optim.Optimizer.load_state_dict",
        "api_signature": "load_state_dict(state)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "jittor_core.Var.log",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor_core.Var.log",
        "api_signature": "jittor_core.Var.log()",
        "api_description": "函数C++定义格式:\njt.Var log(jt.Var x)\n创建一个张量，其将输入变量 x 中的每一个数值的进行自然对数运算。\n\\[y_i = \\ln(x_i)\\]",
        "return_value": "返回一个新的张量, 其数值是 x 中对应位置进行自然对数运算后的值",
        "parameters": "x (Var): Var类型的张量",
        "input_shape": "",
        "notes": "自然对数是以常数e（数学常量，大约等于2.71828）为底的对数\n可以使用 jt.log() 进行调用",
        "code_example": ">>> x = jt.rand(5) * 5\n>>> x\njt.Var([2.8649285  0.854602   3.2959728  0.41697088 3.0296502 ], dtype=float32)\n>>> x.log()\njt.Var([ 1.0525434  -0.15711944  1.1927013  -0.8747389   1.1084472 ], dtype=float32)"
    },
    {
        "api_name": "jittor.misc.log2",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.misc.log2",
        "api_signature": "jittor.misc.log2(x)",
        "api_description": "计算 x 中每个元素的以 2 为底的对数值。\n\\[y_i=\\log _2\\left(x_i\\right)\\]",
        "return_value": "返回与输入形状相同的 Var, 其元素为输入元素的以2为底的对数。",
        "parameters": "x(Var): 任意形状的输入 Var",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> jt.array([1,2,4,8,10]).log2()\njt.Var([0. 1. 2.  3.  3.321928], dtype=float32)"
    },
    {
        "api_name": "jittor.log_capture_scope",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.log_capture_scope",
        "api_signature": "jittor.log_capture_scope(**jt_flags)",
        "api_description": "该类是一个上下文管理器，用于捕获并记录 log 信息。所有在该类的上下文管理器内的日志记录信息都将被捕获并保存。该类继承自 _call_no_record_scope 类，通过在代码段中临时地改变 jt_flags 标志来实现记录 LOG 调用信息。",
        "return_value": "",
        "parameters": "**jt_flags (dict):用于修改 Jittor 运行时的参数，接受任意数量的键值对作为输入。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> with jt.log_capture_scope(log_v=1) as logs:\n...     jt.LOG.v(\"...\")\n>>> print(logs)\n[{'level': 'i', 'lineno': '2', 'msg': '...', 'name': '<stdin>', 'verbose': '1'}]"
    },
    {
        "api_name": "jittor.nn.log_sigmoid",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.nn.html#jittor.nn.log_sigmoid",
        "api_signature": "jittor.nn.log_sigmoid(x)",
        "api_description": "将输入的张量x传入sigmoid函数后，然后对其求对数。使用此函数可以帮助我们在进行深度学习或者神经网络计算中，更好地平滑输入值。\n\\[log_sigmoid(x) = log\\left(\\frac{1}{1+e^{-x}}\\right)\\]\n参数:\nx (Var): 输入张量，可以是任意维度。\n返回值:Var: 与输入 x 形状相同的张量，其各元素等于输入x在dim维度上的log_sigmoid的结果。\n代码示例：>>> import jittor as jt\n>>> from jittor import nn\n>>> x = jt.array([0.8, 0.2, 0.5])\n>>> nn.log_sigmoid(x)\njt.Var([-0.37110066 -0.5981389  -0.47407696], dtype=float32)",
        "return_value": "Var: 与输入 x 形状相同的张量，其各元素等于输入x在dim维度上的log_sigmoid的结果。",
        "parameters": "x (Var): 输入张量，可以是任意维度。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> from jittor import nn\n>>> x = jt.array([0.8, 0.2, 0.5])\n>>> nn.log_sigmoid(x)\njt.Var([-0.37110066 -0.5981389  -0.47407696], dtype=float32)"
    },
    {
        "api_name": "jittor.nn.log_softmax",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.nn.html#jittor.nn.log_softmax",
        "api_signature": "jittor.nn.log_softmax(x, dim=None)",
        "api_description": "对输入的张量进行softmax操作并取对数:\n\\[\\text{log_softmax}(x_i) = \\log(\\frac{\\exp(x_i)}{\\sum_j \\exp(x_j)})\\]\n参数:\nx (Var): 输入张量，可以是任意维度。\ndim (int, tuple(int), optional): 指定softmax操作的维度。不指定时，操作会应用于所有的元素上。默认值：None\n返回值:Var: 与输入 x 形状相同的张量，其各元素 \\(y_i\\) 等于输入x在dim维度上的log_softmax的结果。\n代码示例：>>> import jittor as jt\n>>> from jittor import nn\n>>> x = jt.array([0.8, 0.2, 0.5])\n>>> nn.log_softmax(x)\njt.Var([-0.8283902 -1.4283903 -1.1283902], dtype=float32)",
        "return_value": "Var: 与输入 x 形状相同的张量，其各元素 \\(y_i\\) 等于输入x在dim维度上的log_softmax的结果。",
        "parameters": "x (Var): 输入张量，可以是任意维度。\ndim (int, tuple(int), optional): 指定softmax操作的维度。不指定时，操作会应用于所有的元素上。默认值：None",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> from jittor import nn\n>>> x = jt.array([0.8, 0.2, 0.5])\n>>> nn.log_softmax(x)\njt.Var([-0.8283902 -1.4283903 -1.1283902], dtype=float32)"
    },
    {
        "api_name": "jittor_core.Var.logical_and",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor_core.Var.logical_and",
        "api_signature": "jittor_core.Var.logical_and()",
        "api_description": "函数C++定义格式:\njt.Var logical_and(jt.Var x, jt.Var y)\n对两个张量中对应元素计算逻辑与",
        "return_value": "形状与 x 和 y 相同的张量，元素数据类型为 bool 。其元素为 x 和 y 对应索引位置上的元素进行逻辑与操作的结果，即：如果都为非零值或 True 则输出为 True ，否则为 False 。",
        "parameters": "x (Var): 输入数据\ny (Var): 输入数据，形状与 x 相同",
        "input_shape": "",
        "notes": "支持使用 jt.logical_and() 进行调用",
        "code_example": ">>> x = jt.array([0, 0, 1, 1])\n>>> y = jt.array([0, 1, 0, 1])\n>>> x.logical_and(y)\njt.Var([False False False  True], dtype=bool)"
    },
    {
        "api_name": "jittor_core.Var.logical_not",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor_core.Var.logical_not",
        "api_signature": "jittor_core.Var.logical_not()",
        "api_description": "函数C++定义格式:\njt.Var logical_not(jt.Var x)\n创建一个张量，其将 x 中的每一个数值转换为其逻辑非, 数据类型为 bool。 如果输入张量 x 不是数据类型为 bool 的张量，则零被视为 False ，非零被视为 True。",
        "return_value": "返回一个新的张量, 其数值是 x 中对应位置的逻辑非.",
        "parameters": "x (Var): Var类型的张量",
        "input_shape": "",
        "notes": "可以使用 jt.logical_not() 进行调用",
        "code_example": ">>> (jt.float32([-1, 0, 1])).logical_not()\njt.Var([False  True False], dtype=bool)"
    },
    {
        "api_name": "jittor_core.Var.logical_or",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor_core.Var.logical_or",
        "api_signature": "jittor_core.Var.logical_or()",
        "api_description": "函数C++定义格式:\njt.Var logical_or(jt.Var x, jt.Var y)\n对两个张量中对应元素计算逻辑或",
        "return_value": "形状与 x 和 y 相同的张量，元素数据类型为 bool 。其元素为 x 和 y 对应索引位置上的元素进行逻辑或操作的结果，即：如果两者有非零值或 True 则输出为 True ，否则为 False",
        "parameters": "x (Var): 输入数据\ny (Var): 输入数据，形状与 x 相同",
        "input_shape": "",
        "notes": "支持使用 jt.logical_or() 进行调用",
        "code_example": ">>> x = jt.array([0, 0, 1, 1])\n>>> y = jt.array([0, 1, 0, 1])\n>>> x.logical_or(y)\njt.Var([False  True  True  True], dtype=bool)"
    },
    {
        "api_name": "jittor_core.Var.logical_xor",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor_core.Var.logical_xor",
        "api_signature": "jittor_core.Var.logical_xor()",
        "api_description": "函数C++定义格式:\njt.Var logical_xor(jt.Var x, jt.Var y)\n两个张量的元素级逻辑异或",
        "return_value": "形状与 x 和 y 相同的张量，元素数据类型为 bool 。其元素为 x 和 y 对应索引位置上的元素进行逻辑异或操作的结果，即：如果两者有且仅有一个为非零值或 True 则输出为 True ，否则为 False ，数据类型为 bool ，形状与 x 和 y 相同",
        "parameters": "x (Var): 输入数据\ny (Var): 输入数据，形状与 x 相同",
        "input_shape": "",
        "notes": "支持使用 jt.logical_xor() 进行调用",
        "code_example": ">>> x = jt.array([0, 0, 1, 1])\n>>> y = jt.array([0, 1, 0, 1])\n>>> x.logical_xor(y)\njt.Var([False  True  True False], dtype=bool)"
    },
    {
        "api_name": "jittor.nn.logsumexp",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.nn.html#jittor.nn.logsumexp",
        "api_signature": "jittor.nn.logsumexp(x, dim, keepdims=False, keepdim=False)",
        "api_description": "计算输入张量x在给定维度上的对数和指数。实现的是下列公式，其中，\\(x_{i}\\) 是 x 的元素：\n\\[log(\\sum_{i}exp(x_{i}))\\]\n参数:\nx (Var): 输入张量，可以是任意维度。\ndim (int, tuple of int): 用于指定计算logsumexp的行或列轴的编号或元组\nkeepdims (bool, optional): 如果此选项设为 True，那么求和张量的选定维度将被保留。默认值：False\nkeepdim (bool, optional): 与keepdims相同。默认值：False\n返回值:Var: x在给定维度上计算对数和指数得到的结果\n代码示例：>>> import jittor as jt\n>>> from jittor import nn\n>>> x = jt.ones(3,3)\n>>> nn.logsumexp(x, dim=0)\njt.Var([2.0986123 2.0986123 2.0986123], dtype=float32)\n>>> nn.logsumexp(x, dim=0, keepdims=True)\njt.Var([[2.0986123 2.0986123 2.0986123]], dtype=float32)",
        "return_value": "Var: x在给定维度上计算对数和指数得到的结果",
        "parameters": "x (Var): 输入张量，可以是任意维度。\ndim (int, tuple of int): 用于指定计算logsumexp的行或列轴的编号或元组\nkeepdims (bool, optional): 如果此选项设为 True，那么求和张量的选定维度将被保留。默认值：False\nkeepdim (bool, optional): 与keepdims相同。默认值：False",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> from jittor import nn\n>>> x = jt.ones(3,3)\n>>> nn.logsumexp(x, dim=0)\njt.Var([2.0986123 2.0986123 2.0986123], dtype=float32)\n>>> nn.logsumexp(x, dim=0, keepdims=True)\njt.Var([[2.0986123 2.0986123 2.0986123]], dtype=float32)"
    },
    {
        "api_name": "jittor_core.Var.long",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor_core.Var.long",
        "api_signature": "jittor_core.Var.long()",
        "api_description": "函数C++定义格式:\njt.Var int32_(jt.Var x)\n创建并返回一个 x 的张量副本，并将其类型转换为 int32 。",
        "return_value": "返回一个新的张量, 数据类型为 int32",
        "parameters": "x (Var): Var类型的张量",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> x = jt.randn((2,3))\n>>> x\njt.Var([[-1.8707825  -0.5307898  -2.2583888 ]\n        [-0.1800911   0.10568491  0.0891161 ]], dtype=float32)\n>>> x.int32()\njt.Var([[-1  0 -2]\n        [ 0  0  0]], dtype=int32)"
    },
    {
        "api_name": "jittor.optim.LRScheduler",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.optim.html#jittor.optim.LRScheduler",
        "api_signature": "jittor.optim.LRScheduler(optimizer, last_epoch=-1)",
        "api_description": "学习率调度器的基类，用来根据训练的轮次调整学习率。",
        "return_value": "",
        "parameters": "optimizer (Optimizer): 优化器，用来进行模型参数的优化。\nlast_epoch (int, 可选): 最后的轮次（epoch）。该值默认为 -1，代表使用优化器的学习率来初始化。否则，需要确保每个参数组中都有 initial_lr 这个值来进行学习率的初始化。",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "jittor.nn.LSTMCell",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.nn.html#jittor.nn.LSTMCell",
        "api_signature": "jittor.nn.LSTMCell(input_size, hidden_size, bias=True)",
        "api_description": "一个长短期记忆（LSTM）单元。\n\\[\\begin{split}\\begin{array}{ll}\ni = \\sigma(W_{ii} x + b_{ii} + W_{hi} h + b_{hi}) \\\\\nf = \\sigma(W_{if} x + b_{if} + W_{hf} h + b_{hf}) \\\\\ng = \\tanh(W_{ig} x + b_{ig} + W_{hg} h + b_{hg}) \\\\\no = \\sigma(W_{io} x + b_{io} + W_{ho} h + b_{ho}) \\\\\nc' = f * c + i * g \\\\\nh' = o \\tanh(c') \\\\\n\\end{array}\\end{split}\\]\n其中 \\(\\sigma\\) 是 sigmoid 函数, \\(*\\) 是逐元素乘法。\n参数:\ninput_size(int): 输入的特征维度\nhidden_size(int): 隐藏层的特征维度\nbias(bool): 如果为 False ，则模型不会使用偏置权重, 默认值:True\n代码示例:>>> rnn = nn.LSTMCell(10, 20)  # (input_size, hidden_size)\n>>> input = torch.randn(2, 3, 10)  # (time_steps, batch, input_size)\n>>> hx = jt.randn(3, 20)  # (batch, hidden_size)\n>>> cx = jt.randn(3, 20)\n>>> output = []\n>>> for i in range(input.size()[0]):\nhx, cx = rnn(input[i], (hx, cx))\noutput.append(hx)\n>>> output = jt.stack(output, dim=0)",
        "return_value": "",
        "parameters": "input_size(int): 输入的特征维度\nhidden_size(int): 隐藏层的特征维度\nbias(bool): 如果为 False ，则模型不会使用偏置权重, 默认值:True",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> rnn = nn.LSTMCell(10, 20)  # (input_size, hidden_size)\n>>> input = torch.randn(2, 3, 10)  # (time_steps, batch, input_size)\n>>> hx = jt.randn(3, 20)  # (batch, hidden_size)\n>>> cx = jt.randn(3, 20)\n>>> output = []\n>>> for i in range(input.size()[0]):\n        hx, cx = rnn(input[i], (hx, cx))\n        output.append(hx)\n>>> output = jt.stack(output, dim=0)"
    },
    {
        "api_name": "jittor.nn.LSTM",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.nn.html#jittor.nn.LSTM",
        "api_signature": "jittor.nn.LSTM(input_size, hidden_size, num_layers=1, bias=True, batch_first=False, dropout=0, bidirectional=False, proj_size=0)",
        "api_description": "将多层长短期记忆（LSTM）RNN应用于输入序列。\n对于输入序列中的每个元素，每一层都计算以下函数：\n\\[\\begin{split}\\begin{array}{ll}\ni_t = \\sigma(W_{ii} x_t + b_{ii} + W_{hi} h_{(t-1)} + b_{hi}) \\\\\nf_t = \\sigma(W_{if} x_t + b_{if} + W_{hf} h_{(t-1)} + b_{hf}) \\\\\ng_t = \\tanh(W_{ig} x_t + b_{ig} + W_{hc} h_{(t-1)} + b_{hg}) \\\\\no_t = \\sigma(W_{io} x_t + b_{io} + W_{ho} h_{(t-1)} + b_{ho}) \\\\\nc_t = f_t * c_{(t-1)} + i_t * g_t \\\\\nh_t = o_t * \\tanh(c_t)\n\\end{array}\\end{split}\\]\n其中， \\(h_t\\) 是时间t的隐藏状态， \\(c_t\\) 是时间t的单元状态， \\(x_t\\) 是时间t的输入，\\(h_{(t-1)}\\) 是时间t-1的隐藏状态或时间0的初始隐藏状态，且 \\(i_t, f_t, g_t, o_t\\) 分别是输入门、遗忘门、单元门和输出门。 \\(\\sigma\\) 是sigmoid函数， \\(*\\) 是哈达玛德积（逐元素乘积）。\n在多层LSTM中，第 \\(l\\) 层（ \\(l \\geq 2\\)）的输入 \\(x_t^{(l)}\\) 是前一层的隐藏状态 \\(h_t^{(l-1)}\\) 乘以dropout \\(\\delta_t^{(l-1)}\\) ，其中每个 \\(\\delta_t^{(l-1)}\\) 是一个伯努利随机变量，在dropout的概率下为0。\n参数：\ninput_size(int): 输入的特征维度\nhidden_size(int): 隐藏状态的特征维度\nnum_layers(int): RNN层数。默认值：1\nbias(bool): 如果为False，则层不会使用偏置b_ih和b_hh。默认值：True\nbatch_first(bool): 如果为True，则输入和输出张量的形状为（batch，seq，feature）。默认值：False\ndropout(float): 如果非零，则除了最后一层外，将在每个RNN层的输出上应用丢弃，使用dropout概率。默认值：0\nbidirectional(bool): 如果为True，则RNN层将是双向的，且输出将是双向隐状态的拼接。默认值：False\nproj_size(int): 如果大于0，则将每个隐藏状态投影到proj_size维空间。默认值：0\n代码示例：>>> rnn = nn.LSTM(10, 20, 2)\n>>> input = jt.randn(5, 3, 10)\n>>> h0 = jt.randn(2, 3, 20)\n>>> c0 = jt.randn(2, 3, 20)\n>>> output, (hn, cn) = rnn(input, (h0, c0))",
        "return_value": "",
        "parameters": "input_size(int): 输入的特征维度\nhidden_size(int): 隐藏状态的特征维度\nnum_layers(int): RNN层数。默认值：1\nbias(bool): 如果为False，则层不会使用偏置b_ih和b_hh。默认值：True\nbatch_first(bool): 如果为True，则输入和输出张量的形状为（batch，seq，feature）。默认值：False\ndropout(float): 如果非零，则除了最后一层外，将在每个RNN层的输出上应用丢弃，使用dropout概率。默认值：0\nbidirectional(bool): 如果为True，则RNN层将是双向的，且输出将是双向隐状态的拼接。默认值：False\nproj_size(int): 如果大于0，则将每个隐藏状态投影到proj_size维空间。默认值：0",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> rnn = nn.LSTM(10, 20, 2)\n>>> input = jt.randn(5, 3, 10)\n>>> h0 = jt.randn(2, 3, 20)\n>>> c0 = jt.randn(2, 3, 20)\n>>> output, (hn, cn) = rnn(input, (h0, c0))"
    },
    {
        "api_name": "jittor.nn.make_base_grid_4D",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.nn.html#jittor.nn.make_base_grid_4D",
        "api_signature": "jittor.nn.make_base_grid_4D(theta, N, C, H, W, align_corners)",
        "api_description": "创建一个4D的基本grid。",
        "return_value": "output(Var): 4D的基本grid张量",
        "parameters": "theta (Var): 基础矩阵\nN (int): batch size\nC (int): 通道数\nH (int): 高度\nW (int): 宽度\nalign_corners (bool): 是否对齐角点\n[0 0 1]]]], dtype=int32)",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> jt.nn.make_base_grid_4D(jt.array([[[1,0,0],[0,1,0]]]), 1, 2, 3, 3, False)\njt.Var([[[[0 0 1]\n  [0 0 1]\n  [0 0 1]]\n [[0 0 1]\n  [0 0 1]\n  [0 0 1]]\n [[0 0 1]\n  [0 0 1]"
    },
    {
        "api_name": "jittor.nn.make_base_grid_5D",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.nn.html#jittor.nn.make_base_grid_5D",
        "api_signature": "jittor.nn.make_base_grid_5D(theta, N, C, D, H, W, align_corners)",
        "api_description": "创建一个5D的基本grid。",
        "return_value": "output(Var): 5D的基本grid张量",
        "parameters": "theta (Var): 基础矩阵\nN (int): batch size\nC (int): 通道数\nD (int): 深度\nH (int): 高度\nW (int): 宽度\nalign_corners (bool): 是否对齐角点",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> jt.nn.make_base_grid_5D(jt.array([[[1,0,0],[0,1,0]]]), 1, 2, 2,3, 3, False)\njt.Var([[[[[0 0 0 1]\n   [0 0 0 1]\n   [0 0 0 1]]\n  [[0 0 0 1]\n   [0 0 0 1]\n   [0 0 0 1]]\n  [[0 0 0 1]\n   [0 0 0 1]\n   [0 0 0 1]]]\n [[[0 0 0 1]\n   [0 0 0 1]\n   [0 0 0 1]]\n  [[0 0 0 1]\n   [0 0 0 1]\n   [0 0 0 1]]\n  [[0 0 0 1]\n   [0 0 0 1]\n   [0 0 0 1]]]]], dtype=int32)"
    },
    {
        "api_name": "jittor.misc.make_grid",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.misc.make_grid",
        "api_signature": "jittor.misc.make_grid(x, nrow=8, padding=2, normalize=False, range=None, scale_each=False, pad_value=0)",
        "api_description": "将多个图片按网格状拼成一个图片",
        "return_value": "形状 \\((c, h', w')\\) 的图片，由 x 中的图片组合成网格状而成",
        "parameters": "x (Var): 图片数据，形状 \\((n, c, h, w)\\)\nnrow (int): 每行多少个图片，最终拼成的网格形状是 \\((n / \\mathtt{nrow}, nrow)\\) 。默认值：8\npadding (int): 图片间的间隔。默认值：2\nnormalize (bool): 是否将图片值线性映射到 \\([0, 1]\\) 。默认值：False\nrange (None | tuple[float, float]): 映射到 \\([0, 1]\\) 的范围，或者不指定则使用图片数据中的最小最大值。默认值：None\nscale_each (bool): 不支持，必须为 False。默认值：False\npad_value (float): 图片间填充的值。默认值：0",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "jittor.make_module",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.make_module",
        "api_signature": "jittor.make_module(func, exec_n_args=1)",
        "api_description": "用于根据给定的函数和执行参数的数量，创建一个Jittor模块。",
        "return_value": "module (Jittor.Module): 根据给定的函数和执行参数的数量创建的Jittor模块。",
        "parameters": "func (callable): 输入的函数，用于创建Jittor模块。\nexec_n_args (int): 默认为…执行参数的数量。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> def add_func(a, b):\n>>>     return a + b\n>>> module = jt.make_module(add_func, 2)\n>>> print(module)\n<class 'jittor.make_module.<locals>.MakeModule'>"
    },
    {
        "api_name": "jittor.masked_fill",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.masked_fill",
        "api_signature": "jittor.masked_fill(x, mask, value)",
        "api_description": "对张量 x 中 mask 为 True 的位置进行填充值 value。需要保证 mask 的形状必须和 x 相匹配。",
        "return_value": "x 填充后的张量。类型为jittor.Var。",
        "parameters": "x (jt.Var) : 输入的张量。\nmask (bool Var) : 和 x 形状相同的布尔类型张量。\nvalue (float) : 用于填充张量 x 中对应 mask 为 True 的位置的值。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> x = jt.array([1, 2, 3])\n>>> mask = jt.array([False, True, False])\n>>> print(jt.ops.masked_fill(x, mask, -1))\njt.Var([ 1 -1  3], dtype=int32)"
    },
    {
        "api_name": "jittor.nn.matmul",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.nn.html#jittor.nn.matmul",
        "api_signature": "jittor.nn.matmul(a, b)",
        "api_description": "矩阵乘法。此函数接收两个参数，执行矩阵乘法操作，并且返回结果。输入矩阵a，b的尺寸必须匹配。具体来说，a的最后一维的大小必须和b的倒数第二维的大小相同。\n参数：\na :(Var)，形状为 (…, M, N)的第一个输入矩阵.\nb : (Var), 形状为 (…, N, K)的第二个输入矩阵.\n返回值：Var: 结果矩阵, 形状为 (…, M, K)\n代码示例：>>> a = jt.random([3])\n>>> b = jt.random([3])\n>>> c = jt.matmul(a, b)\n>>> c.shape\n[1]\n>>> a = jt.random([3, 4])\n>>> b = jt.random([4])\n>>> c = jt.matmul(a, b)\n>>> c.shape\n[3]\n>>> a = jt.random([10, 3, 4])\n>>> b = jt.random([4])\n>>> c = jt.matmul(a, b)\n>>> c.shape\n[10, 3]\n>>> a = jt.random([10, 3, 4])\n>>> b = jt.random([4, 5])\n>>> c = jt.matmul(a, b)\n>>> c.shape\n[10, 3, 5]\n>>> a = jt.random([10, 3, 4])\n>>> b = jt.random([10, 4, 5])\n>>> c = jt.matmul(a, b)\n>>> c.shape\n[10, 3, 5]\n>>> a = jt.random([8, 1, 3, 4])\n>>> b = jt.random([10, 4, 5])\n>>> c = jt.matmul(a, b)\n>>> c.shape\n[8, 10, 3, 5]",
        "return_value": "Var: 结果矩阵, 形状为 (…, M, K)",
        "parameters": "a :(Var)，形状为 (…, M, N)的第一个输入矩阵.\nb : (Var), 形状为 (…, N, K)的第二个输入矩阵.",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> a = jt.random([3])\n>>> b = jt.random([3])\n>>> c = jt.matmul(a, b)\n>>> c.shape\n[1]\n>>> a = jt.random([3, 4])\n>>> b = jt.random([4])\n>>> c = jt.matmul(a, b)\n>>> c.shape \n[3]\n>>> a = jt.random([10, 3, 4])\n>>> b = jt.random([4])\n>>> c = jt.matmul(a, b)\n>>> c.shape \n[10, 3]\n>>> a = jt.random([10, 3, 4])\n>>> b = jt.random([4, 5])\n>>> c = jt.matmul(a, b)\n>>> c.shape \n[10, 3, 5]\n>>> a = jt.random([10, 3, 4])\n>>> b = jt.random([10, 4, 5])\n>>> c = jt.matmul(a, b)\n>>> c.shape \n[10, 3, 5]\n>>> a = jt.random([8, 1, 3, 4])\n>>> b = jt.random([10, 4, 5])\n>>> c = jt.matmul(a, b)\n>>> c.shape\n[8, 10, 3, 5]"
    },
    {
        "api_name": "jittor.nn.matmul_transpose",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.nn.html#jittor.nn.matmul_transpose",
        "api_signature": "jittor.nn.matmul_transpose(a, b)",
        "api_description": "对于给定的两个矩阵 a 和 b ，这个函数首先将 b 转置，然后再对 a 和转置后的 b 进行矩阵乘法运算，返回该运算结果 \\(ab^T\\)\n参数:\na (Var) : 输入的第一个矩阵\nb (Var) : 输入的第二个矩阵\n返回值:矩阵 a 与转置后的矩阵 b 的矩阵乘法运算结果。如果 a 的最后一个维度与 b 的最后一个维度不同，将引发 AssertionError ；如果 a 的形状与 b 的形状不同，这个函数会使用Jittor的广播规则来进行矩阵乘法运算。\n代码示例：>>> import jittor as jt\n>>> from jittor import nn\n>>> nn.matmul_transpose(jt.ones(3,4),jt.ones(3,4))\njt.Var([[4. 4. 4.]\n[4. 4. 4.]\n[4. 4. 4.]], dtype=float32)",
        "return_value": "矩阵 a 与转置后的矩阵 b 的矩阵乘法运算结果。如果 a 的最后一个维度与 b 的最后一个维度不同，将引发 AssertionError ；如果 a 的形状与 b 的形状不同，这个函数会使用Jittor的广播规则来进行矩阵乘法运算。",
        "parameters": "a (Var) : 输入的第一个矩阵\nb (Var) : 输入的第二个矩阵",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> from jittor import nn\n>>> nn.matmul_transpose(jt.ones(3,4),jt.ones(3,4))\njt.Var([[4. 4. 4.]\n        [4. 4. 4.]\n        [4. 4. 4.]], dtype=float32)"
    },
    {
        "api_name": "jittor_core.Var.max",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor_core.Var.max",
        "api_signature": "jittor_core.Var.max()",
        "api_description": "函数C++定义格式:\njt.Var reduce_maximum(jt.Var x,  int dim,  bool keepdims=false)\n返回输入张量中的最大元素。",
        "return_value": "对应维度的最大元素。",
        "parameters": "x (Var): 输入的jt.Var.\ndims_mask (int，可选): 指定的维度，如果指定，则沿给定的维度进行缩减。默认值： None\nkeepdims_mask (int，可选): 输出是否保留 dim 。默认值： None",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> x = jt.randint(10, shape=(2, 3))\n>>> x\njt.Var([[4 1 2]\n    [0 2 4]], dtype=int32)\n>>> jt.max(x)\njt.Var([4], dtype=int32)\n>>> x.max()\njt.Var([4], dtype=int32)\n>>> x.max(dim=1)\njt.Var([4 4], dtype=int32)\n>>> x.max(dim=1, keepdims=True)\njt.Var([[4]\n    [4]], dtype=int32)"
    },
    {
        "api_name": "jittor.nn.max_pool2d",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.nn.html#jittor.nn.max_pool2d",
        "api_signature": "jittor.nn.max_pool2d(x, kernel_size, stride=None, padding=0, dilation=None, return_indices=None, ceil_mode=False)",
        "api_description": "对输入张量进行2D最大池化操作。\n参数:\nx(Var): 输入张量, 形状为 \\((N, C, H_{in}, W_{in})\\)。\nkernel_size (int or tuple): 池化核的大小。\nstride(int or tuple, optional): 步长。\npadding(int, optional): 在输入张量的所有边界上隐式零填充。默认值: 0。\nceil_mode(bool, optional): 当设置为True时, 会使用 \\(ceil\\) 函数计算输出形状。默认值: False。\ncount_include_pad(bool, optional): 当设置为True时, 将在计算平均值时包含零填充。默认值: True。\n返回:\n进行2D最大池化后的张量。\n代码示例:>>> import jittor as jt\n>>> x = jt.random((1, 1, 4, 4))\n>>> jt.nn.max_pool2d(x, kernel_size=2, stride=2)",
        "return_value": "",
        "parameters": "x(Var): 输入张量, 形状为 \\((N, C, H_{in}, W_{in})\\)。\n\n\n\nkernel_size (int or tuple): 池化核的大小。\nstride(int or tuple, optional): 步长。\npadding(int, optional): 在输入张量的所有边界上隐式零填充。默认值: 0。\nceil_mode(bool, optional): 当设置为True时, 会使用 \\(ceil\\) 函数计算输出形状。默认值: False。\ncount_include_pad(bool, optional): 当设置为True时, 将在计算平均值时包含零填充。默认值: True。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> x = jt.random((1, 1, 4, 4))\n>>> jt.nn.max_pool2d(x, kernel_size=2, stride=2)"
    },
    {
        "api_name": "jittor.nn.max_pool3d",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.nn.html#jittor.nn.max_pool3d",
        "api_signature": "jittor.nn.max_pool3d(x, kernel_size, stride=None, padding=0, dilation=None, return_indices=None, ceil_mode=False)",
        "api_description": "对输入张量进行3D最大池化操作。\n参数:\nx(Var): 输入张量, 形状为 \\((N, C, D_{in}, H_{in}, W_{in})\\)。\nkernel_size (int or tuple): 池化核的大小。\nstride(int or tuple, optional): 步长。\npadding(int, optional): 在输入张量的所有边界上隐式零填充。默认值: 0。\nceil_mode(bool, optional): 当设置为True时, 会使用 \\(ceil\\) 函数计算输出形状。默认值: False。\ncount_include_pad(bool, optional): 当设置为True时, 将在计算平均值时包含零填充。默认值: True。\n返回值: 3D最大池化操作后的输出张量(Var)\n代码示例: >>> import torch\n>>> x = torch.randn(1, 1, 4, 4, 4)  # 输入张量的形状为 [batch_size, channels, depth, height, width]\n>>> torch.nn.functional.max_pool3d(x, kernel_size=3, stride=2, padding=1)",
        "return_value": "3D最大池化操作后的输出张量(Var)",
        "parameters": "x(Var): 输入张量, 形状为 \\((N, C, D_{in}, H_{in}, W_{in})\\)。\nkernel_size (int or tuple): 池化核的大小。\nstride(int or tuple, optional): 步长。\npadding(int, optional): 在输入张量的所有边界上隐式零填充。默认值: 0。\nceil_mode(bool, optional): 当设置为True时, 会使用 \\(ceil\\) 函数计算输出形状。默认值: False。\ncount_include_pad(bool, optional): 当设置为True时, 将在计算平均值时包含零填充。默认值: True。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import torch\n>>> x = torch.randn(1, 1, 4, 4, 4)  # 输入张量的形状为 [batch_size, channels, depth, height, width]\n>>> torch.nn.functional.max_pool3d(x, kernel_size=3, stride=2, padding=1)"
    },
    {
        "api_name": "jittor_core.Var.maximum",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor_core.Var.maximum",
        "api_signature": "jittor_core.Var.maximum()",
        "api_description": "函数C++定义格式:\njt.Var maximum(jt.Var x, jt.Var y)\n对两个张量中对应位置元素计算最大值",
        "return_value": "x 和 y 每对对应元素中的较大者，形状与 x 和 y 相同",
        "parameters": "x (Var): 输入数据\ny (Var): 输入数据，形状与 x 相同",
        "input_shape": "",
        "notes": "支持使用 jt.maximum() 进行调用",
        "code_example": ">>> x = jt.array([1, 2, 4])\n>>> y = jt.array([3, 2, 0])\n>>> x.maximum(y)\njt.Var([3 2 4], dtype=int32)"
    },
    {
        "api_name": "jittor.nn.MaxPool2d",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.nn.html#jittor.nn.MaxPool2d",
        "api_signature": "jittor.nn.MaxPool2d(kernel_size, stride=None, padding=0, dilation=None, return_indices=None, ceil_mode=False)",
        "api_description": "二维最大池化 (pooling) 类。对二维输入的高度和宽度进行最大池化计算。\n参数:\nkernel_size (int or tuple): 池化核的大小。\nstride (int or tuple, optional): 池化操作的步长。\npadding (int or tuple, optional): 在输入数据的高度和宽度上各边缘处添加的零填充的大小。\ndilation (int or tuple, optional): 控制卷积核中元素的间距。\nreturn_indices (bool, optional): 如果为True, 则返回最大值的位置索引。\nceil_mode (bool, optional): 是否使用 ceil 函数计算输出的高度和宽度。默认值: False。\n形状:\nInput: \\((N,C,H_{in},W_{in})\\)\nOutput: \\((N,C,H_{out},W_{out})\\), 其中\n\\[\\begin{split}& \\qquad H_{\\text {out }}=\\left\\lfloor\\frac{H_{\\text {in }}+2 \\times \\text { padding }[0]-\\text { dilation }[0] \\times(\\text { kernel_size }[0]-1)-1}{\\text { stride }[0]}+1\\right\\rfloor \\\\\n& \\qquad W_{\\text {out }}=\\left\\lfloor\\frac{W_{\\text {in }}+2 \\times \\text { padding }[1]-\\text { dilation }[1] \\times(\\text { kernel_size }[1]-1)-1}{\\text { stride }[1]}+1\\right\\rfloor\\end{split}\\]\n属性:\nlayer (jt.Module): 用于执行池化操作的模块。\n代码示例: >>> import jittor as jt\n>>> from jittor import nn\n>>> m = nn.MaxPool2d(2, stride=1, padding=1)\n>>> input = jt.random([1, 3, 5, 5])\n>>> output = m(input)\n>>> print(output.shape)\n[1,3,6,6,]",
        "return_value": "",
        "parameters": "kernel_size (int or tuple): 池化核的大小。\nstride (int or tuple, optional): 池化操作的步长。\npadding (int or tuple, optional): 在输入数据的高度和宽度上各边缘处添加的零填充的大小。\ndilation (int or tuple, optional): 控制卷积核中元素的间距。\nreturn_indices (bool, optional): 如果为True, 则返回最大值的位置索引。\nceil_mode (bool, optional): 是否使用 ceil 函数计算输出的高度和宽度。默认值: False。",
        "input_shape": "Input: \\((N,C,H_{in},W_{in})\\)\nOutput: \\((N,C,H_{out},W_{out})\\), 其中\n\n\n\\[\\begin{split}& \\qquad H_{\\text {out }}=\\left\\lfloor\\frac{H_{\\text {in }}+2 \\times \\text { padding }[0]-\\text { dilation }[0] \\times(\\text { kernel_size }[0]-1)-1}{\\text { stride }[0]}+1\\right\\rfloor \\\\\n& \\qquad W_{\\text {out }}=\\left\\lfloor\\frac{W_{\\text {in }}+2 \\times \\text { padding }[1]-\\text { dilation }[1] \\times(\\text { kernel_size }[1]-1)-1}{\\text { stride }[1]}+1\\right\\rfloor\\end{split}\\]",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> from jittor import nn\n>>> m = nn.MaxPool2d(2, stride=1, padding=1)\n>>> input = jt.random([1, 3, 5, 5])\n>>> output = m(input)\n>>> print(output.shape)\n[1,3,6,6,]"
    },
    {
        "api_name": "jittor.nn.MaxPool3d",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.nn.html#jittor.nn.MaxPool3d",
        "api_signature": "jittor.nn.MaxPool3d(kernel_size, stride=None, padding=0, dilation=None, return_indices=None, ceil_mode=False)",
        "api_description": "三维最大池化 (pooling) 类。对三维输入的深度、高度和宽度进行最大池化计算。\n参数:\nkernel_size (int or tuple): 池化核的大小。\nstride (int or tuple, optional): 池化操作的步长。\npadding (int or tuple, optional): 在输入数据的高度和宽度上各边缘处添加的零填充的大小。\ndilation (int or tuple, optional): 控制卷积核中元素的间距。\nreturn_indices (bool, optional): 如果为True, 则返回最大值的位置索引。\nceil_mode (bool, optional): 是否使用 ceil 函数计算输出的高度和宽度。默认值: False。\n形状:\nInput: \\((N,C,D_{in},H_{in},W_{in})\\)\nOutput: \\((N,C,D_{out},H_{out},W_{out})\\), 其中\n\\[\\begin{split}& \\qquad D_{\\text {out }}=\\left\\lfloor\\frac{D_{\\text {in }}+2 \\times \\text { padding }[0]-\\text { dilation }[0] \\times(\\text { kernel_size }[0]-1)-1}{\\text { stride }[0]}+1\\right\\rfloor \\\\\n& \\qquad H_{\\text {out }}=\\left\\lfloor\\frac{H_{\\text {in }}+2 \\times \\text { padding }[1]-\\text { dilation }[1] \\times(\\text { kernel_size }[1]-1)-1}{\\text { stride }[1]}+1\\right\\rfloor \\\\\n& \\qquad W_{\\text {out }}=\\left\\lfloor\\frac{W_{\\text {in }}+2 \\times \\text { padding }[2]-\\text { dilation }[2] \\times(\\text { kernel_size }[2]-1)-1}{\\text { stride }[2]}+1\\right\\rfloor\\end{split}\\]\n属性:\nlayer (jt.Module): 用于执行池化操作的模块。\n代码示例: >>> import jittor as jt\n>>> from jittor import nn\n>>> m = nn.MaxPool3d(2, stride=1, padding=1)\n>>> input = jt.random([1, 3, 5, 5, 5])\n>>> output = m(input)\n>>> print(output.shape)\n[1,3,6,6,6,]",
        "return_value": "",
        "parameters": "kernel_size (int or tuple): 池化核的大小。\nstride (int or tuple, optional): 池化操作的步长。\npadding (int or tuple, optional): 在输入数据的高度和宽度上各边缘处添加的零填充的大小。\ndilation (int or tuple, optional): 控制卷积核中元素的间距。\nreturn_indices (bool, optional): 如果为True, 则返回最大值的位置索引。\nceil_mode (bool, optional): 是否使用 ceil 函数计算输出的高度和宽度。默认值: False。",
        "input_shape": "Input: \\((N,C,D_{in},H_{in},W_{in})\\)\nOutput: \\((N,C,D_{out},H_{out},W_{out})\\), 其中\n\n\n\\[\\begin{split}& \\qquad D_{\\text {out }}=\\left\\lfloor\\frac{D_{\\text {in }}+2 \\times \\text { padding }[0]-\\text { dilation }[0] \\times(\\text { kernel_size }[0]-1)-1}{\\text { stride }[0]}+1\\right\\rfloor \\\\\n& \\qquad H_{\\text {out }}=\\left\\lfloor\\frac{H_{\\text {in }}+2 \\times \\text { padding }[1]-\\text { dilation }[1] \\times(\\text { kernel_size }[1]-1)-1}{\\text { stride }[1]}+1\\right\\rfloor \\\\\n& \\qquad W_{\\text {out }}=\\left\\lfloor\\frac{W_{\\text {in }}+2 \\times \\text { padding }[2]-\\text { dilation }[2] \\times(\\text { kernel_size }[2]-1)-1}{\\text { stride }[2]}+1\\right\\rfloor\\end{split}\\]",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> from jittor import nn\n>>> m = nn.MaxPool3d(2, stride=1, padding=1)\n>>> input = jt.random([1, 3, 5, 5, 5])\n>>> output = m(input)\n>>> print(output.shape)\n[1,3,6,6,6,]"
    },
    {
        "api_name": "jittor.nn.MaxUnpool2d",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.nn.html#jittor.nn.MaxUnpool2d",
        "api_signature": "jittor.nn.MaxUnpool2d(kernel_size, stride=None)",
        "api_description": "MaxPool2d 的逆运算。\n参数:\nkernel_size(int,  Tuple[int, int]]): 窗口大小。\nstride(int,  Tuple[int, int], optional, 默认为None): 步长。\n形状:\n输入: \\((N,C,H_{in},W_{in})\\) 。\n输出: \\((N,C,H_{out},W_{out})\\), 其中后两维被 output_size 指定。若 output_size 为 None, 则\n\\[\\begin{split}& \\qquad H_{\\text {out }}=\\left\\lfloor\\frac{H_{\\text {in }}+2 \\times \\text { padding }[0]-\\text { kernel_size }[0]}{\\text { stride }[0]}+1\\right\\rfloor \\\\\n& \\qquad W_{\\text {out }}=\\left\\lfloor\\frac{W_{\\text {in }}+2 \\times \\text { padding }[1]-\\text { kernel_size }[1]}{\\text { stride }[1]}+1\\right\\rfloor\\end{split}\\]\n属性:\nkernel_size (int, tuple): 窗口大小。\nstride (int, tuple, optional): 步长。\n代码示例:>>> import jittor as jt\n>>> from jittor import nn\n>>> pool = nn.MaxPool2d(2, stride=2, return_indices=True)\n>>> unpool = nn.MaxUnpool2d(2, stride=2)\n>>> input = jt.array([[[[1., 2, 3, 4, 0],\n...                     [5, 6, 7, 8, 0],\n...                     [9, 10, 11, 12, 0],\n...                     [13, 14, 15, 16, 0],\n...                     [0, 0, 0, 0, 0]]]])\n>>> output1, indices = pool(input)\n>>> output2= unpool(output1, indices, output_size=input.shape)\n>>> print(output2)\njt.Var([[[[ 0.  0.  0.  0.  0.]\n[ 0.  6.  0.  8.  0.]\n[ 0.  0.  0.  0.  0.]\n[ 0. 14.  0. 16.  0.]\n[ 0.  0.  0.  0.  0.]]]], dtype=float32)",
        "return_value": "",
        "parameters": "kernel_size(int,  Tuple[int, int]]): 窗口大小。\nstride(int,  Tuple[int, int], optional, 默认为None): 步长。",
        "input_shape": "输入: \\((N,C,H_{in},W_{in})\\) 。\n输出: \\((N,C,H_{out},W_{out})\\), 其中后两维被 output_size 指定。若 output_size 为 None, 则\n\n\n\\[\\begin{split}& \\qquad H_{\\text {out }}=\\left\\lfloor\\frac{H_{\\text {in }}+2 \\times \\text { padding }[0]-\\text { kernel_size }[0]}{\\text { stride }[0]}+1\\right\\rfloor \\\\\n& \\qquad W_{\\text {out }}=\\left\\lfloor\\frac{W_{\\text {in }}+2 \\times \\text { padding }[1]-\\text { kernel_size }[1]}{\\text { stride }[1]}+1\\right\\rfloor\\end{split}\\]",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> from jittor import nn\n>>> pool = nn.MaxPool2d(2, stride=2, return_indices=True)\n>>> unpool = nn.MaxUnpool2d(2, stride=2)\n>>> input = jt.array([[[[1., 2, 3, 4, 0],\n...                     [5, 6, 7, 8, 0],\n...                     [9, 10, 11, 12, 0],\n...                     [13, 14, 15, 16, 0],\n...                     [0, 0, 0, 0, 0]]]])\n>>> output1, indices = pool(input)\n>>> output2= unpool(output1, indices, output_size=input.shape)\n>>> print(output2)\njt.Var([[[[ 0.  0.  0.  0.  0.]\n        [ 0.  6.  0.  8.  0.]\n        [ 0.  0.  0.  0.  0.]\n        [ 0. 14.  0. 16.  0.]\n        [ 0.  0.  0.  0.  0.]]]], dtype=float32)"
    },
    {
        "api_name": "jittor.nn.MaxUnpool3d",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.nn.html#jittor.nn.MaxUnpool3d",
        "api_signature": "jittor.nn.MaxUnpool3d(kernel_size, stride=None)",
        "api_description": "MaxPool3d 的逆运算。\n参数:\nkernel_size(int,  Tuple[int, int, int]]): 窗口大小。 如果是一个整数, 大小为(kernel_size, kernel_size, kernel_size)。\nstride(int,  Tuple[int, int, int], optional, 默认为None): 步长。\n形状:\n输入: \\((N,C,D_{in},H_{in},W_{in})\\) 。\n输出: \\((N,C,D_{in},H_{out},W_{out})\\), 其中后三维被 output_size 指定。若 output_size 为 None, 则\n\\[\\begin{split}& \\qquad D_{\\text {out }}=\\left\\lfloor\\frac{D_{\\text {in }}+2 \\times \\text { padding }[0]-\\text { kernel_size }[0]}{\\text { stride }[0]}+1\\right\\rfloor \\\\\n& \\qquad H_{\\text {out }}=\\left\\lfloor\\frac{H_{\\text {in }}+2 \\times \\text { padding }[1]-\\text { kernel_size }[1]}{\\text { stride }[1]}+1\\right\\rfloor \\\\\n& \\qquad W_{\\text {out }}=\\left\\lfloor\\frac{W_{\\text {in }}+2 \\times \\text { padding }[2]-\\text { kernel_size }[2]}{\\text { stride }[2]}+1\\right\\rfloor\\end{split}\\]\n属性:\nkernel_size (int, tuple): 窗口大小。\nstride (int, tuple, optional): 步长。\n代码示例:>>> import jittor as jt\n>>> from jittor import nn\n>>> pool = nn.MaxPool3d(3, stride=2, return_indices=True)\n>>> unpool = nn.MaxUnpool3d(3, stride=2)\n>>> output, indices = pool(jt.randn(20, 16, 51, 33, 15))\n>>> unpooled_output = unpool(output, indices)\n>>> print(unpooled_output.size())\n[20,16,50,32,14,]",
        "return_value": "",
        "parameters": "kernel_size(int,  Tuple[int, int, int]]): 窗口大小。 如果是一个整数, 大小为(kernel_size, kernel_size, kernel_size)。\nstride(int,  Tuple[int, int, int], optional, 默认为None): 步长。",
        "input_shape": "输入: \\((N,C,D_{in},H_{in},W_{in})\\) 。\n输出: \\((N,C,D_{in},H_{out},W_{out})\\), 其中后三维被 output_size 指定。若 output_size 为 None, 则\n\n\n\\[\\begin{split}& \\qquad D_{\\text {out }}=\\left\\lfloor\\frac{D_{\\text {in }}+2 \\times \\text { padding }[0]-\\text { kernel_size }[0]}{\\text { stride }[0]}+1\\right\\rfloor \\\\\n& \\qquad H_{\\text {out }}=\\left\\lfloor\\frac{H_{\\text {in }}+2 \\times \\text { padding }[1]-\\text { kernel_size }[1]}{\\text { stride }[1]}+1\\right\\rfloor \\\\\n& \\qquad W_{\\text {out }}=\\left\\lfloor\\frac{W_{\\text {in }}+2 \\times \\text { padding }[2]-\\text { kernel_size }[2]}{\\text { stride }[2]}+1\\right\\rfloor\\end{split}\\]",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> from jittor import nn\n>>> pool = nn.MaxPool3d(3, stride=2, return_indices=True)\n>>> unpool = nn.MaxUnpool3d(3, stride=2)\n>>> output, indices = pool(jt.randn(20, 16, 51, 33, 15))\n>>> unpooled_output = unpool(output, indices)\n>>> print(unpooled_output.size())\n[20,16,50,32,14,]"
    },
    {
        "api_name": "jittor_core.Var.mean",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor_core.Var.mean",
        "api_signature": "jittor_core.Var.mean()",
        "api_description": "函数C++定义格式:\njt.Var reduce_mean(jt.Var x,  int dim,  bool keepdims=false)\n计算输入张量的均值。",
        "return_value": "计算得到的均值(Var)",
        "parameters": "x (Var): 输入的 Jittor 的变量。\ndims_mask (int): 如果给定，沿着指定的维度缩减。\nkeepdims_mask (bool): 如果为True，则输出结果会保留维度。默认值：False",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> x = jt.randint(10, shape=(2, 3))\n>>> x\njt.Var([[9 4 4]\n    [1 9 6]], dtype=int32)\n>>> jt.mean(x)\njt.Var([5.5000005], dtype=float32)\n>>> x.mean(dim=1)\njt.Var([5.666667  5.3333335], dtype=float32)\n>>> x.mean(dim=1, keepdims=True)\njt.Var([[5.666667 ]\n    [5.3333335]], dtype=float32)"
    },
    {
        "api_name": "jittor.misc.meshgrid",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.misc.meshgrid",
        "api_signature": "jittor.misc.meshgrid(*tensors)",
        "api_description": "创建由 *tensors 这个输入的 Var 列表指定的坐标网格列表。\n给定 \\(N\\) 个 1D 张量 \\(T_0 \\ldots T_{N-1}\\) 作为输入，输入对应的尺寸分别为 \\(S_0 \\ldots S_{N-1}\\)，输出 \\(N\\) 个 \\(N\\) 维张量 \\(G_0 \\ldots G_{N-1}\\)，每个形状为 \\(\\left(S_0, \\ldots, S_{N-1}\\right)\\)，其中输出 \\(G_i\\) 是通过将 \\(T_i\\) 扩展到结果形状而构建的。",
        "return_value": "grids(Var): 包含了 N 个 Var 的列表, 由输入张量列表决定。",
        "parameters": "*tensors: (jt.Var, 不限个数)- 输入的张量, 每个张量需要是一维的jt.Var向量。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> a = jt.array([1, 2, 3])\n>>> b = jt.array([4, 5, 6, 7])\n>>> jt.meshgrid(a, b)\n[jt.Var([[1 1 1 1]\n[2 2 2 2]\n[3 3 3 3]], dtype=int32), \n jt.Var([[4 5 6 7]\n[4 5 6 7]\n[4 5 6 7]], dtype=int32)]"
    },
    {
        "api_name": "jittor_core.Var.min",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor_core.Var.min",
        "api_signature": "jittor_core.Var.min()",
        "api_description": "函数C++定义格式:\njt.Var reduce_minimum(jt.Var x,  int dim,  bool keepdims=false)\n对张量的 dims 这些维度计算最小值",
        "return_value": "计算最小值的结果。如果 keepdims 为 True 则形状为 x 将 dims 里给出的维度的长度变成 1，否则是 x 的形状去掉 dims 给出的维度",
        "parameters": "x (Var): 输入数据\ndim (tuple[int]): 在哪些维度计算最小值。默认是全部元素的最小值。默认值： ()\nkeepdims (bool): 是否在结果中保留 dim 维度。默认值：False",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> x = jt.randint(0, 10, shape=(2, 3))\n>>> x\njt.Var([[2 7 9]\n        [2 6 3]], dtype=int32)\n>>> x.min()\njt.Var([2], dtype=int32)\n>>> x.min((0, 1))\njt.Var([2], dtype=int32)"
    },
    {
        "api_name": "jittor_core.Var.minimum",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor_core.Var.minimum",
        "api_signature": "jittor_core.Var.minimum()",
        "api_description": "函数C++定义格式:\njt.Var minimum(jt.Var x, jt.Var y)\n对两个张量中对应元素计算最小值",
        "return_value": "x 和 y 每对对应元素中的较小者，形状与 x 和 y 相同",
        "parameters": "x (Var): 输入数据\ny (Var): 输入数据，形状与 x 相同",
        "input_shape": "",
        "notes": "支持使用 jt.minimum() 进行调用",
        "code_example": ">>> x = jt.array([1, 2, 4])\n>>> y = jt.array([3, 2, 0])\n>>> x.minimum(y)\njt.Var([1 2 0], dtype=int32)"
    },
    {
        "api_name": "jittor.nn.mish",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.nn.html#jittor.nn.mish",
        "api_signature": "jittor.nn.mish(x, inplace=False)",
        "api_description": "mish函数是一个在神经网络中使用的激活函数，其定义为 \\(x * tanh(softplus(x))\\) 。其中 \\(softplus\\) 函数的定义为 \\(softplus(x) = log(1 + e^x)\\)\n参数:\nx（Var）：输入的张量\ninplace（bool, optional）：用来决定是否在原位进行运算。当inplace=True时，函数会将结果直接保存在输入张量x上，这将节省存储空间但可能会改变输入的值。 默认值是False。当inplace=False时，函数将会创建一个新的张量来保存运算结果，并且不会影响输入张量x的值。\n返回值:Var: 一个与输入同形状、同数据类型的张量，代表运算的结果\n代码示例：>>> import jittor as jt\n>>> from jittor import nn\n>>> x = jt.array([1, 2, 3, 4, 5])\n>>> y = jt.nn.functional.mish(x, inplace=False)\n>>> y = jt.nn.mish(x, inplace=False)\n>>> y\njt.Var([0.86509836 1.9439589  2.986535   3.997413   4.9995522 ], dtype=float32)",
        "return_value": "Var: 一个与输入同形状、同数据类型的张量，代表运算的结果",
        "parameters": "x（Var）：输入的张量\ninplace（bool, optional）：用来决定是否在原位进行运算。当inplace=True时，函数会将结果直接保存在输入张量x上，这将节省存储空间但可能会改变输入的值。 默认值是False。当inplace=False时，函数将会创建一个新的张量来保存运算结果，并且不会影响输入张量x的值。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> from jittor import nn\n>>> x = jt.array([1, 2, 3, 4, 5])\n>>> y = jt.nn.functional.mish(x, inplace=False)\n>>> y = jt.nn.mish(x, inplace=False)            \n>>> y\njt.Var([0.86509836 1.9439589  2.986535   3.997413   4.9995522 ], dtype=float32)"
    },
    {
        "api_name": "jittor.nn.Mish",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.nn.html#jittor.nn.Mish",
        "api_signature": "jittor.nn.Mish(inplace=False)",
        "api_description": "对每个元素应用Mish函数。Mish是一种自我调节的非单调神经激活函数。\n\\[Mish(x) = x * tanh(Softplus(x))\\]\n其中，Softplus函数定义为 \\(softplus(x) = \\ln(1 + e^x)\\)，然后将Softplus的结果用于tanh函数，并与原始输入 \\(x\\) 相乘得到Mish函数的输出。\n对每个元素应用Mish函数。Mish是一种自我调节的非单调神经激活函数。\n参数：\ninplace(bool): 是否进行原地操作\n代码示例：>>> m = nn.Mish()\n>>> input = jt.randn(2)\n>>> output = m(input)",
        "return_value": "",
        "parameters": "inplace(bool): 是否进行原地操作",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> m = nn.Mish()\n>>> input = jt.randn(2)\n>>> output = m(input)"
    },
    {
        "api_name": "jittor.models.mnasnet0_5",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.models.html#jittor.models.mnasnet0_5",
        "api_signature": "jittor.models.mnasnet0_5(pretrained=False, **kwargs)",
        "api_description": "构建一个MnasNet 0_5模型\nMnasNet源自论文 MnasNet: Platform-Aware Neural Architecture Search for Mobile, 其使用深度可分卷积和线性瓶颈层来减少计算复杂性, 同时仍然保持了表示能力。\n参数:\npretrained (bool, optional): 表示是否预加载预训练模型。默认为 False。\nkwargs: 其他optional参数。\n返回值:\n返回构建好的MnasNet 0_5模型实例。如果 pretrained 为 True, 则返回在ImageNet上预训练的模型。\n代码示例:>>> import jittor as jt\n>>> from jittor.models.mnasnet import *\n>>> net = mnasnet0_5(pretrained=False)\n>>> x = jt.rand(1, 3, 224, 224)\n>>> y = net(x)\n>>> y.shape\n[1, 1000]",
        "return_value": "返回构建好的MnasNet 0_5模型实例。如果 pretrained 为 True, 则返回在ImageNet上预训练的模型。",
        "parameters": "pretrained (bool, optional): 表示是否预加载预训练模型。默认为 False。\nkwargs: 其他optional参数。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> from jittor.models.mnasnet import *\n>>> net = mnasnet0_5(pretrained=False)\n>>> x = jt.rand(1, 3, 224, 224)\n>>> y = net(x)\n>>> y.shape\n[1, 1000]"
    },
    {
        "api_name": "jittor.models.mnasnet0_75",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.models.html#jittor.models.mnasnet0_75",
        "api_signature": "jittor.models.mnasnet0_75(pretrained=False, **kwargs)",
        "api_description": "构建一个MnasNet 0_75模型\nMnasNet源自论文 MnasNet: Platform-Aware Neural Architecture Search for Mobile, 其使用深度可分卷积和线性瓶颈层来减少计算复杂性, 同时仍然保持了表示能力。\n参数:\npretrained (bool, optional): 表示是否预加载预训练模型。默认为 False。\nkwargs: 其他optional参数。\n返回值:\n返回构建好的MnasNet 0_75模型实例。如果 pretrained 为 True, 则返回在ImageNet上预训练的模型。\n代码示例:>>> import jittor as jt\n>>> from jittor.models.mnasnet import *\n>>> net = mnasnet0_75(pretrained=False)\n>>> x = jt.rand(1, 3, 224, 224)\n>>> y = net(x)\n>>> y.shape\n[1, 1000]",
        "return_value": "返回构建好的MnasNet 0_75模型实例。如果 pretrained 为 True, 则返回在ImageNet上预训练的模型。",
        "parameters": "pretrained (bool, optional): 表示是否预加载预训练模型。默认为 False。\nkwargs: 其他optional参数。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> from jittor.models.mnasnet import *\n>>> net = mnasnet0_75(pretrained=False)\n>>> x = jt.rand(1, 3, 224, 224)\n>>> y = net(x)\n>>> y.shape\n[1, 1000]"
    },
    {
        "api_name": "jittor.models.mnasnet1_0",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.models.html#jittor.models.mnasnet1_0",
        "api_signature": "jittor.models.mnasnet1_0(pretrained=False, **kwargs)",
        "api_description": "构建一个MnasNet 1_0模型\nMnasNet源自论文 《MnasNet: Platform-Aware Neural Architecture Search for Mobile》\n参数:\npretrained (bool, optional): 表示是否预加载预训练模型。默认为 False。\nkwargs: 其他optional参数。\n返回值:\n返回构建好的MnasNet 1_0模型实例。如果 pretrained 为 True, 则返回在ImageNet上预训练的模型。\n代码示例:>>> import jittor as jt\n>>> from jittor.models.mnasnet import *\n>>> net = mnasnet1_0(pretrained=False)\n>>> x = jt.rand(1, 3, 224, 224)\n>>> y = net(x)\n>>> y.shape\n[1, 1000]",
        "return_value": "返回构建好的MnasNet 1_0模型实例。如果 pretrained 为 True, 则返回在ImageNet上预训练的模型。",
        "parameters": "pretrained (bool, optional): 表示是否预加载预训练模型。默认为 False。\nkwargs: 其他optional参数。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> from jittor.models.mnasnet import *\n>>> net = mnasnet1_0(pretrained=False)\n>>> x = jt.rand(1, 3, 224, 224)\n>>> y = net(x)\n>>> y.shape\n[1, 1000]"
    },
    {
        "api_name": "jittor.models.mnasnet1_3",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.models.html#jittor.models.mnasnet1_3",
        "api_signature": "jittor.models.mnasnet1_3(pretrained=False, **kwargs)",
        "api_description": "构建一个MnasNet 1_3模型\nMnasNet源自论文 MnasNet: Platform-Aware Neural Architecture Search for Mobile, 其使用深度可分卷积和线性瓶颈层来减少计算复杂性, 同时仍然保持了表示能力。\n参数:\npretrained (bool, optional): 表示是否预加载预训练模型。默认为 False。\nkwargs: 其他optional参数。\n返回值:\n返回构建好的MnasNet 1_3模型实例。如果 pretrained 为 True, 则返回在ImageNet上预训练的模型。\n代码示例:>>> import jittor as jt\n>>> from jittor.models.mnasnet import *\n>>> net = mnasnet1_3(pretrained=False)\n>>> x = jt.rand(1, 3, 224, 224)\n>>> y = net(x)\n>>> y.shape\n[1, 1000]",
        "return_value": "返回构建好的MnasNet 1_3模型实例。如果 pretrained 为 True, 则返回在ImageNet上预训练的模型。",
        "parameters": "pretrained (bool, optional): 表示是否预加载预训练模型。默认为 False。\nkwargs: 其他optional参数。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> from jittor.models.mnasnet import *\n>>> net = mnasnet1_3(pretrained=False)\n>>> x = jt.rand(1, 3, 224, 224)\n>>> y = net(x)\n>>> y.shape\n[1, 1000]"
    },
    {
        "api_name": "jittor.models.MNASNet",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.models.html#jittor.models.MNASNet",
        "api_signature": "jittor.models.MNASNet(alpha, num_classes=1000, dropout=0.2)",
        "api_description": "MnasNet源自论文 MnasNet: Platform-Aware Neural Architecture Search for Mobile, 其使用深度可分卷积和线性瓶颈层来减少计算复杂性, 同时仍然保持了表示能力。\n在MNASNet中的深度可分离卷积操作可通过以下方式表达:\n\\[y = DConv(x) * PWConv(x)\\]\n其中 \\(DConv\\) 表示逐深度卷积, \\(PWConv\\) 表示点卷积操作。\n参数:\nalpha (float): 深度乘数, 用于控制网络层的深度缩放因子。在初始化时传入的 alpha 参数必须大于0。\nnum_classes (int, optional): 分类类别的数量。默认值: 1000。\ndropout (float, optional): Dropout层的丢弃概率。默认值: 0.2。\n属性:\nlayers (nn.Sequential): MNASNet的特征提取部分。\nclassifier (nn.Sequential): MNASNet的分类部分。\n代码示例:>>> import jittor as jt\n>>> from jittor.models.mnasnet import MNASNet\n>>> network = MNASNet(alpha=1.0, num_classes=1000, dropout=0.2)\n>>> input = jt.randn(10, 3, 256, 256)  # 假设是一个形状为[批次大小, 通道数, 高度, 宽度]的张量\n>>> network(input).shape\n[10,1000,]",
        "return_value": "",
        "parameters": "alpha (float): 深度乘数, 用于控制网络层的深度缩放因子。在初始化时传入的 alpha 参数必须大于0。\nnum_classes (int, optional): 分类类别的数量。默认值: 1000。\ndropout (float, optional): Dropout层的丢弃概率。默认值: 0.2。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> from jittor.models.mnasnet import MNASNet\n>>> network = MNASNet(alpha=1.0, num_classes=1000, dropout=0.2)\n>>> input = jt.randn(10, 3, 256, 256)  # 假设是一个形状为[批次大小, 通道数, 高度, 宽度]的张量\n>>> network(input).shape\n[10,1000,]"
    },
    {
        "api_name": "jittor.dataset.MNIST",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.dataset.html#jittor.dataset.MNIST",
        "api_signature": "jittor.dataset.MNIST(data_root='/home/zwy/.cache/jittor/dataset/mnist_data/', train=True, download=True, batch_size=16, shuffle=False, transform=None)",
        "api_description": "MNIST 数据集。\n参数:\ndata_root (str): 数据根目录的路径, 默认值: ‘dataset_root/mnist_data/’\ntrain (bool, optional): 选择是训练模式还是验证模式, 默认值: True。\ndownload (bool, optional): 如果设置为 True, 则会自动下载数据。默认值: True。\nbatch_size (int, optional): 数据批次大小, 默认值: 16。\nshuffle (bool, optional): 若为 True, 则会在每个 epoch 对数据进行打乱, 默认值: False。\ntransform (jittor.transform, optional): 数据的转换操作, 默认值: None。\n属性:\ndata_root (str): 数据根目录的路径。\nis_train (bool): 选择是训练模式还是验证模式。\ntransform (jittor.transform): 数据的转换操作。\nbatch_size (int): 数据批次大小。\nshuffle (bool): 是否打乱数据。\nmnist (dict): mnist 数据集。\ntotal_len (int): 数据集的长度。\n代码示例:  >>> from jittor.dataset.mnist import MNIST\n>>> train_loader = MNIST(train=True, batch_size=16, shuffle=True)\n>>> for i, (imgs, target) in enumerate(train_loader):\n>>> ...     # 处理训练数据",
        "return_value": "",
        "parameters": "data_root (str): 数据根目录的路径, 默认值: ‘dataset_root/mnist_data/’\ntrain (bool, optional): 选择是训练模式还是验证模式, 默认值: True。\ndownload (bool, optional): 如果设置为 True, 则会自动下载数据。默认值: True。\nbatch_size (int, optional): 数据批次大小, 默认值: 16。\nshuffle (bool, optional): 若为 True, 则会在每个 epoch 对数据进行打乱, 默认值: False。\ntransform (jittor.transform, optional): 数据的转换操作, 默认值: None。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> from jittor.dataset.mnist import MNIST\n>>> train_loader = MNIST(train=True, batch_size=16, shuffle=True)\n>>> for i, (imgs, target) in enumerate(train_loader):\n>>> ...     # 处理训练数据"
    },
    {
        "api_name": "jittor.models.mobilenet_v2",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.models.html#jittor.models.mobilenet_v2",
        "api_signature": "jittor.models.mobilenet_v2(pretrained=False)",
        "api_description": "构建一个MobileNetV2模型\nMobileNetV2源自论文 Mobilenetv2: Inverted residuals and linear bottlenecks。mobilenet_v2基于倒置残差结构, 其中使用线性瓶颈在压缩表示中传递消息。\n参数:\npretrained (bool, optional): 表示是否预加载预训练模型。默认为 False。\n返回值:\n返回构建好的MobileNetV2模型实例。如果 pretrained 为 True, 则返回在ImageNet上预训练的模型。\n代码示例:>>> import jittor as jt\n>>> from jittor.models.mobilenet import *\n>>> net = mobilenet_v2(pretrained=False)\n>>> x = jt.rand(1, 3, 224, 224)\n>>> y = net(x)\n>>> y.shape\n[1, 1000]",
        "return_value": "返回构建好的MobileNetV2模型实例。如果 pretrained 为 True, 则返回在ImageNet上预训练的模型。",
        "parameters": "pretrained (bool, optional): 表示是否预加载预训练模型。默认为 False。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> from jittor.models.mobilenet import *\n>>> net = mobilenet_v2(pretrained=False)\n>>> x = jt.rand(1, 3, 224, 224)\n>>> y = net(x)\n>>> y.shape\n[1, 1000]"
    },
    {
        "api_name": "jittor.models.MobileNetV2",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.models.html#jittor.models.MobileNetV2",
        "api_signature": "jittor.models.MobileNetV2(num_classes=1000, width_mult=1.0, inverted_residual_setting=None, round_nearest=8, block=None)",
        "api_description": "MobileNetV2源自论文 Mobilenetv2: Inverted residuals and linear bottlenecks。mobilenet_v2基于倒置残差结构, 其中使用线性瓶颈在压缩表示中传递消息。\n参数:\nnum_classes (int, optional): 类别数。默认值: 1000。\nwidth_mult (float, optional): 宽度乘数 - 按此比例调整每层中通道数。默认值: 1.0。\ninit_weights (bool, optional): 是否初始化权重。默认值: True。\ninverted_residual_setting (List[List[int]], optional): 定义网络结构的参数列表, 其中每个元素是一个拥有4个整数的列表, 分别表示扩展因子(t)、输出通道数(c)、重复次数(n)和步幅(s)。默认值: None, 会自动采用MobileNetV2的标准配置。\nround_nearest (int, optional): 将每层的通道数四舍五入为该数的倍数。设为1则取消四舍五入。默认值: 8。\nblock (callable[…, nn.Module], optional): 用于指定MobileNet中倒置残差构建块的模块。如果为None, 则使用InvertedResidual。默认值: None。\n属性:\nfeatures (nn.Sequential): MobileNetV2的特征提取部分。\nclassifier (nn.Sequential): MobileNetV2的分类部分。\n代码示例:  >>> import jittor as jt\n>>> from jittor.models.mobilenet import MobileNetV2\n>>> model = MobileNetV2(num_classes=1000, width_mult=1.0)\n>>> print(model)\nMobileNetV2(\nfeatures: Sequential(\n0: ConvBNReLU(\n...)))",
        "return_value": "",
        "parameters": "num_classes (int, optional): 类别数。默认值: 1000。\nwidth_mult (float, optional): 宽度乘数 - 按此比例调整每层中通道数。默认值: 1.0。\ninit_weights (bool, optional): 是否初始化权重。默认值: True。\ninverted_residual_setting (List[List[int]], optional): 定义网络结构的参数列表, 其中每个元素是一个拥有4个整数的列表, 分别表示扩展因子(t)、输出通道数(c)、重复次数(n)和步幅(s)。默认值: None, 会自动采用MobileNetV2的标准配置。\nround_nearest (int, optional): 将每层的通道数四舍五入为该数的倍数。设为1则取消四舍五入。默认值: 8。\nblock (callable[…, nn.Module], optional): 用于指定MobileNet中倒置残差构建块的模块。如果为None, 则使用InvertedResidual。默认值: None。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> from jittor.models.mobilenet import MobileNetV2\n>>> model = MobileNetV2(num_classes=1000, width_mult=1.0)\n>>> print(model)\nMobileNetV2(\n    features: Sequential(\n        0: ConvBNReLU(\n        ...)))"
    },
    {
        "api_name": "jittor_core.Var.mod",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor_core.Var.mod",
        "api_signature": "jittor_core.Var.mod()",
        "api_description": "函数C++定义格式:\njt.Var mod(jt.Var x, jt.Var y)\n对两个张量中对应元素计算求余数，也可以使用 % 运算符调用",
        "return_value": "x 和 y 每对对应元素相除的余数，形状与 x 和 y 相同。非零余数的符号和 y 中元素的符号相同，该行为与 Python 的 x % y 运算一致",
        "parameters": "x (Var): 输入数据\ny (Var): 输入数据，形状与 x 相同",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> x = jt.array([3, 4, 5, -5, 5])\n>>> y = jt.array([1, 2, 3, 3, -3])\n>>> x.mod(y)\njt.Var([ 0  0  2 -2  2], dtype=int32)\n>>> x % y\njt.Var([ 0  0  2 -2  2], dtype=int32)"
    },
    {
        "api_name": "jittor.nn.ModuleList",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.nn.html#jittor.nn.ModuleList",
        "alias": "Sequential"
    },
    {
        "api_name": "jittor.Module.modules",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.Module.modules",
        "api_signature": "modules()",
        "api_description": "递归地返回模块中子模块的列表。\n示例代码:\n>>> net = nn.Sequential(nn.Linear(2, 10), nn.ReLU(), nn.Linear(10, 2))\n>>> net.modules()\n[Sequential(\n0: Linear(2, 10, float32[10,], None)\n1: relu()\n2: Linear(10, 2, float32[2,], None)\n), Linear(2, 10, float32[10,], None), relu(), Linear(10, 2, float32[2,], None)]",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "jittor.Module",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.Module",
        "api_signature": "jittor.Module(*args, **kw)",
        "api_description": "Module 类通常用于构建神经网络模型。Module 是 Jittor 深度学习框架的基础模块，是所有神经网络层和模型的基类。提供了构建神经网络模型所需的各种基础方法，包括前向计算( execute 方法)、设置混合精度计算(比如 float16/32 方法)、设置模型的训练/评估状态( train/eval 方法)、获取模型参数( parameters 方法) 等等。同时，由于 Jittor 是一个  JIT 编译框架，它还使用了很多 C++ 代码和编译技术来提高执行效率，这使得 Jittor 的 Module 类在运行速度上具有优势。",
        "return_value": "",
        "parameters": "初始化参数：初始化  Module 对象的参数。例如权重初始化方式、偏置项初始化方式等。\n超参数：定义模型结构和行为的参数。例如隐藏层的维度、激活函数的类型、优化器的学习率等。\n输入数据相关参数：定义输入数据的特征维度、输入数据的形状等。\n其他配置参数：指定其他模型相关的配置。例如是否使用  GPU 加速、是否打印训练过程中的日志等。",
        "input_shape": "Input：输入形状根据具体的模型结构和网络架构而定。\nOutput：输出形状根据具体的模型结构和网络架构而确定。",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> from jittor import nn\n>>> class LinearModel(nn.Module):\n...     def __init__(self):\n...         self.linear = nn.Linear(10, 2)\n...     def execute(self, x):\n...         x = self.linear(x)\n...         return x\n>>> net = LinearModel()\n>>> x = jt.random((10, 10))\n>>> out = net(x)\n>>> print(out.shape)\n[10,2,]"
    },
    {
        "api_name": "jittor.nn.mse_loss",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.nn.html#jittor.nn.mse_loss",
        "api_signature": "jittor.nn.mse_loss(output, target, reduction='mean')",
        "api_description": "计算均方误差（Mean Squared Error）损失。对于给定的类别标签 target 和网络输出结果 output ，计算均方误差损失。该函数主要用于分类问题的损失计算。可以选择损失函数输出方式，例如如果选择  'mean'  ，则计算方式如下：\n\\[L = \\frac{1}{n} \\sum_i^n (output[i] - target[i])^2\\]\n参数:\noutput (Var): 预测值，模型输出。如果output和target的类型不是Var，会抛出TypeError。\ntarget (Var): 目标值，实际值或者是标签。\nreduction (str, optional): 控制损失函数的输出方式，可以是’mean’ （输出平均损失）, ‘sum’, ‘none’之一。如果reduction的值不是’mean’, ‘sum’, ‘none’之一，会抛出ValueError。默认值：’mean’\n返回值:Var: 均方误差损失。\n代码示例：>>> import jittor as jt\n>>> from jittor import nn\n>>> output = jt.array([1.0, 1.0, 1.0])\n>>> target = jt.array([0.5, 0.6, -2.0])\n>>> nn.mse_loss(output, target)\njt.Var([3.1366668], dtype=float32)",
        "return_value": "Var: 均方误差损失。",
        "parameters": "output (Var): 预测值，模型输出。如果output和target的类型不是Var，会抛出TypeError。\ntarget (Var): 目标值，实际值或者是标签。\nreduction (str, optional): 控制损失函数的输出方式，可以是’mean’ （输出平均损失）, ‘sum’, ‘none’之一。如果reduction的值不是’mean’, ‘sum’, ‘none’之一，会抛出ValueError。默认值：’mean’",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> from jittor import nn\n>>> output = jt.array([1.0, 1.0, 1.0])\n>>> target = jt.array([0.5, 0.6, -2.0])\n>>> nn.mse_loss(output, target)\njt.Var([3.1366668], dtype=float32)"
    },
    {
        "api_name": "jittor.nn.MSELoss",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.nn.html#jittor.nn.MSELoss",
        "api_signature": "jittor.nn.MSELoss(reduction='mean')",
        "api_description": "用于计算输出值和目标值的均方误差。创建一个衡量 x 和目标 y 之间均方误差标准\n\\[\\ell(x, y) = L = \\{l_1,\\dots,l_N\\}^\\top, \\quad\nl_n = \\left( x_n - y_n \\right)^2,\\]\n其中 \\(N\\) 是批处理数量。\n如果缩减操作类型（reduction）不是 'none' (默认为 'mean'), 则有以下计算:\n\\[\\begin{split}\\ell(x, y) =\n\\begin{cases}\n\\mathrm{mean}(L), &  \\text{if reduction} = \\text{'mean';}\\\\\n\\mathrm{sum}(L),  &  \\text{if reduction} = \\text{'sum'.}\n\\end{cases}\\end{split}\\]\n其中，\\(x\\) 和 \\(y\\) 是任意形状的张量（Var)， \\(n\\) 是元素数量。",
        "return_value": "",
        "parameters": "reduction (str, optional): 指定应用于输出的缩减操作类型。可选值有   'mean'、 'sum' 或 'none' 。默认值： 'mean'",
        "input_shape": "output: \\((*)\\)，其中 * 表示任意数量的附加维数。\ntarget: \\((*)\\)，和输入形状相同",
        "notes": "",
        "code_example": ">>> m = nn.MSELoss()\n>>> output = jt.array([[1.5, 2.3, 0.7], [1.8, 0.5, 2.2]])\n>>> target = jt.zeros((2,3))\n>>> loss_var = m(output, target)\n>>> loss_var\njt.Var([2.7266667], dtype=float32)"
    },
    {
        "api_name": "jittor_core.Var.mul",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor_core.Var.mul",
        "api_signature": "jittor_core.Var.mul()",
        "api_description": "函数C++定义格式:\njt.Var multiply(jt.Var x, jt.Var y)\n对两个张量中对应元素计算求积，也可以使用 * 运算符调用",
        "return_value": "x 和 y 每对对应元素相乘结果，形状与 x 和 y 相同",
        "parameters": "x (Var): 输入数据\ny (Var): 输入数据，形状与 x 相同",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> x = jt.array([1, 2, 4])\n>>> y = jt.array([3, 4, 5])\n>>> x.multiply(y)\njt.Var([ 3  8 20], dtype=int32)\n>>> x * y\njt.Var([ 3  8 20], dtype=int32)"
    },
    {
        "api_name": "jittor.attention.MultiheadAttention",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.attention.html#jittor.attention.MultiheadAttention",
        "api_signature": "jittor.attention.MultiheadAttention(embed_dim, num_heads, kdim=None, vdim=None, dropout=0.0, bias=True, add_bias_kv=False, add_zero_attn=False, self_attention=False, encoder_decoder_attention=False, q_noise=0.0, qn_block_size=8)",
        "api_description": "多头注意力机制（multi-head attention）源自论文 Attention Is All You Need ，其定义为：\n\\[\\text{MultiHead}(Q, K, V) = \\text{Concat}(head_1,\\dots,head_h)W^O\\]\n其中 \\(head_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)\\).\n参数:\nembed_dim (int): 输入的维度。\nnum_heads (int): 多头的数量。\nkdim (int, optional): 键值的维度。默认值为None，表示与embed_dim相同。\nvdim (int, optional): 值的维度。默认值为None，表示与embed_dim相同。\ndropout (float, optional): dropout概率。默认值为0.0。\nbias (bool, optional): 是否使用偏置。默认值为True。\nadd_bias_kv (bool, optional): 是否添加偏置。默认值为False。\nadd_zero_attn (bool, optional): 是否添加零注意力。默认值为False。\nself_attention (bool, optional): 是否自注意力。默认值为False。\nencoder_decoder_attention (bool, optional): 是否编码器解码器注意力。默认值为False。\nq_noise (float, optional): query的噪声。默认值为0.0。\nqn_block_size (int, optional): 块大小。默认值为8。\n属性:\nembed_dim (int): 输入的维度。\nkdim (int): 键值的维度。\nvdim (int): 值的维度。\nnum_heads (int): 多头的数量。\nhead_dim (int): 头的维度。\nscaling (float): 缩放因子。\nself_attention (bool): 是否自注意力。\nencoder_decoder_attention (bool): 是否编码器解码器注意力。\nk_proj (nn.Linear): 键的线性变换。\nv_proj (nn.Linear): 值的线性变换。\nq_proj (nn.Linear): 查询的线性变换。\nout_proj (nn.Linear): 输出的线性变换。\nbias_k (None): 键的偏置。\nbias_v (None): 值的偏置。\nadd_zero_attn (bool): 是否添加零注意力。\n形状:\n输入: \\((L, N, E)\\) 其中 \\(L\\) 表示序列长度， \\(N\\) 表示batch大小， \\(E\\) 表示输入的维度。\n输出: \\((L, N, E)\\) 其中 \\(L\\) 表示序列长度， \\(N\\) 表示batch大小， \\(E\\) 表示输入的维度。\n代码示例:  >>> multihead_attn = jt.attention.MultiheadAttention(embed_dim, num_heads)\n>>> attn, attn_weights = multihead_attn(query, key, value)",
        "return_value": "",
        "parameters": "embed_dim (int): 输入的维度。\nnum_heads (int): 多头的数量。\nkdim (int, optional): 键值的维度。默认值为None，表示与embed_dim相同。\nvdim (int, optional): 值的维度。默认值为None，表示与embed_dim相同。\ndropout (float, optional): dropout概率。默认值为0.0。\nbias (bool, optional): 是否使用偏置。默认值为True。\nadd_bias_kv (bool, optional): 是否添加偏置。默认值为False。\nadd_zero_attn (bool, optional): 是否添加零注意力。默认值为False。\nself_attention (bool, optional): 是否自注意力。默认值为False。\nencoder_decoder_attention (bool, optional): 是否编码器解码器注意力。默认值为False。\nq_noise (float, optional): query的噪声。默认值为0.0。\nqn_block_size (int, optional): 块大小。默认值为8。",
        "input_shape": "输入: \\((L, N, E)\\) 其中 \\(L\\) 表示序列长度， \\(N\\) 表示batch大小， \\(E\\) 表示输入的维度。\n输出: \\((L, N, E)\\) 其中 \\(L\\) 表示序列长度， \\(N\\) 表示batch大小， \\(E\\) 表示输入的维度。",
        "notes": "",
        "code_example": ">>> multihead_attn = jt.attention.MultiheadAttention(embed_dim, num_heads)\n>>> attn, attn_weights = multihead_attn(query, key, value)"
    },
    {
        "api_name": "jittor.misc.multinomial",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.misc.multinomial",
        "api_signature": "jittor.misc.multinomial(weights: Var, num_samples: int, replacement: bool = False)",
        "api_description": "返回一个var，其中每一行包含从对应输入权重行的多项分布中抽取的 num_samples 个索引。",
        "return_value": "返回一个var，其中每一行包含从对应输入权重行的多项分布中抽取的 num_samples 个索引。",
        "parameters": "weights(Var): 输入概率。（如：weights = jt.float32([0, 10, 3, 0]) ）\nnum_samples(int): 抽取的样本数量（如：num_samples = 4）。这个参数必须小于等于weights的个数。换句话说如果你在不放回的情况下设置num_samples为比weights个数还多，将会报错。\nreplacement(bool): 是否放回地抽样，默认为 False。如果设置为 True ，那么每一次抽样权重不变，可以多次抽到同一个。反之，每抽一次，权重会进行相应改变，不会抽到同一个。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> weights = jt.float32([0, 10, 3, 0])\n... x = jt.multinomial(weights, 2)\n... assert jt.all_equal(x, [1, 2]) or jt.all_equal(x, [2, 1])\n... x = jt.multinomial(weights, 4, replacement=True)\n... assert x.shape == (4, )\n\n\n>>> weights = jt.float32([[0,0,2],[0,1,0], [0.5,0,0]])\n... x = jt.multinomial(weights, 1)\n... assert jt.all_equal(x, [[2],[1],[0]])"
    },
    {
        "api_name": "jittor_core.Var.multiply",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor_core.Var.multiply",
        "api_signature": "jittor_core.Var.multiply()",
        "api_description": "函数C++定义格式:\njt.Var multiply(jt.Var x, jt.Var y)\n对两个张量中对应元素计算求积，也可以使用 * 运算符调用",
        "return_value": "x 和 y 每对对应元素相乘结果，形状与 x 和 y 相同",
        "parameters": "x (Var): 输入数据\ny (Var): 输入数据，形状与 x 相同",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> x = jt.array([1, 2, 4])\n>>> y = jt.array([3, 4, 5])\n>>> x.multiply(y)\njt.Var([ 3  8 20], dtype=int32)\n>>> x * y\njt.Var([ 3  8 20], dtype=int32)"
    },
    {
        "api_name": "jittor.multiply_",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.multiply_",
        "api_signature": "jittor.multiply_(x, y)",
        "api_description": "函数是 \\(multiply()\\)  的就地版本，即在原始变量上进行操作，不会创建新的变量。该函数主要进行两个输入参数的乘法运算。\n数学公式：\n\\(z = x \\times y\\)\n其中 x 和 y 都是输入变量。",
        "return_value": "output(Var)： x 和 y 的乘法结果。它的形状和数据类型与输入变量 x 相同。",
        "parameters": "x (int,flot or Jittor.Variable): 输入变量。\ny (int,flot or Jittor.Variable)：输入变量。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> x = jt.array([1.0, 2.0, 3.0])\n>>> y = jt.array([4.0, 5.0, 6.0])\n>>> z = jt.multiply_(x, y)\n>>> print(z)\narray([4., 10., 18.], dtype=float32)"
    },
    {
        "api_name": "jittor.lr_scheduler.MultiStepLR",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.optim.html#jittor.lr_scheduler.MultiStepLR",
        "api_signature": "jittor.lr_scheduler.MultiStepLR(optimizer, milestones=[], gamma=0.1, last_epoch=-1)",
        "api_description": "实现了一个常见的学习率调节策略，即在预设的多个阶段（milestones）对学习率进行衰减。",
        "return_value": "",
        "parameters": "optimizer (Optimizer): 被包装的优化器实例。\nmilestones (list): 预设的学习率衰减阶段。默认值：[]\ngamma (float): 学习率衰减系数，每次衰减时，学习率都会乘以这个系数。默认值：0.1\nlast_epoch (int): 上一个 epoch 的标记，默认值：-1。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> # 当 epoch 达到 30 和 80 时学习率乘以 0.1\n>>> scheduler = MultiStepLR(optimizer, milestones=[30,80], gamma=0.1)\n>>> for epoch in range(100):\n...     train(...)\n...     validate(...)\n...     scheduler.step()"
    },
    {
        "api_name": "jittor_core.Var.name",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor_core.Var.name",
        "api_signature": "jittor_core.Var.name()",
        "api_description": "函数C++定义格式:\ninline jt.Var name(const char* s)\n给对应变量命名。\n函数C++定义格式:\ninline const char* name()\n返回该变量的名称。",
        "return_value": "str: 返回该变量的名称。",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "jittor.Module.named_modules",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.Module.named_modules",
        "api_signature": "named_modules()",
        "api_description": "递归地返回子模块及其名称的列表。\n代码示例:\n>>> net = nn.Sequential(nn.Linear(2, 10), nn.ReLU(), nn.Linear(10, 2))\n>>> net.named_modules()\n[('', Sequential(\n0: Linear(2, 10, float32[10,], None)\n1: relu()\n2: Linear(10, 2, float32[2,], None)\n)), ('0', Linear(2, 10, float32[10,], None)), ('1', relu()), ('2', Linear(10, 2, float32[2,], None))]",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "jittor.Module.named_parameters",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.Module.named_parameters",
        "api_signature": "named_parameters(recurse=True)",
        "api_description": "返回模块参数及其名称的列表。\n示例代码:\n>>> net = nn.Linear(2, 5)\n>>> net.named_parameters()\n[('weight', jt.Var([[ 0.5964666  -0.3175258 ]\n[ 0.41493994 -0.66982657]\n[-0.32677156  0.49614117]\n[-0.24102807 -0.08656466]\n[ 0.15868133 -0.12468725]], dtype=float32)),\n('bias', jt.Var([-0.38282675  0.36271113 -0.7063226   0.02899247  0.52210844], dtype=float32))]",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "jittor.ne",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.ne",
        "api_signature": "jittor.ne()",
        "api_description": "函数C++定义格式:\njt.Var not_equal(jt.Var x, jt.Var y)\n两个张量中对应元素比较是否不相等，把结果布尔值放入输出张量的对应位置，也可以使用 != 运算符调用",
        "return_value": "形状与 x 和 y 相同的张量，数据类型是 bool。其中每个索引位置的布尔值表示该索引对应的 x 和 y 对应元素里是否 x 中的元素不等于 y 中的元素",
        "parameters": "x (Var): 输入数据\ny (Var): 输入数据，形状与 x 相同",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> x = jt.array([3, 2, 1])\n>>> y = jt.array([1, 2, 3])\n>>> x.not_equal(y)\njt.Var([ True False  True], dtype=bool)\n>>> x != y\njt.Var([ True False  True], dtype=bool)"
    },
    {
        "api_name": "jittor.misc.ne",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.misc.ne",
        "api_signature": "jittor.misc.ne()",
        "api_description": "函数C++定义格式:\njt.Var not_equal(jt.Var x, jt.Var y)\n两个张量中对应元素比较是否不相等，把结果布尔值放入输出张量的对应位置，也可以使用 != 运算符调用",
        "return_value": "形状与 x 和 y 相同的张量，数据类型是 bool。其中每个索引位置的布尔值表示该索引对应的 x 和 y 对应元素里是否 x 中的元素不等于 y 中的元素",
        "parameters": "x (Var): 输入数据\ny (Var): 输入数据，形状与 x 相同",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> x = jt.array([3, 2, 1])\n>>> y = jt.array([1, 2, 3])\n>>> x.not_equal(y)\njt.Var([ True False  True], dtype=bool)\n>>> x != y\njt.Var([ True False  True], dtype=bool)"
    },
    {
        "api_name": "jittor_core.Var.ne",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor_core.Var.ne",
        "api_signature": "jittor_core.Var.ne()",
        "api_description": "函数C++定义格式:\njt.Var not_equal(jt.Var x, jt.Var y)\n两个张量中对应元素比较是否不相等，把结果布尔值放入输出张量的对应位置，也可以使用 != 运算符调用",
        "return_value": "形状与 x 和 y 相同的张量，数据类型是 bool。其中每个索引位置的布尔值表示该索引对应的 x 和 y 对应元素里是否 x 中的元素不等于 y 中的元素",
        "parameters": "x (Var): 输入数据\ny (Var): 输入数据，形状与 x 相同",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> x = jt.array([3, 2, 1])\n>>> y = jt.array([1, 2, 3])\n>>> x.not_equal(y)\njt.Var([ True False  True], dtype=bool)\n>>> x != y\njt.Var([ True False  True], dtype=bool)"
    },
    {
        "api_name": "jittor_core.Var.negative",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor_core.Var.negative",
        "api_signature": "jittor_core.Var.negative()",
        "api_description": "函数C++定义格式:\njt.Var negative(jt.Var x)\n创建一个张量，其将输入变量 x 中的每一个数值转换为其相反数。\n\\[out_i = - input_i\\]",
        "return_value": "返回一个新的张量, 其数值是 x 中对应位置的相反数",
        "parameters": "x (Var): Var类型的张量",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> (jt.float32([-1, 0, 1])).negative()\njt.Var([1. 0. -1.], dtype=float32)"
    },
    {
        "api_name": "jittor.misc.new",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.misc.new",
        "api_signature": "jittor.misc.new(x, *args)",
        "api_description": "创建一个和 x 类型相同的张量",
        "return_value": "如果给的参数是一个张量，则将其转换为和 x 类型相同的张量；否则新创建一个给定形状，类型与 x 相同的未初始化张量",
        "parameters": "x (Var): 一个张量\na (Var): 待转换类型的张量\n\n\n\n或者\nn1, n2, … (int): 新张量每维的长度",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> x = jt.array([1.0, 2.0, 3.0])\n>>> x\njt.Var([1. 2. 3.], dtype=float32)\n>>> x.new(3, 4)\njt.Var([[-0.62324923  0.81981313  0.49348482 -0.80826735]\n        [-2.185655   -0.12619412 -0.36160922 -0.28803992]\n        [-1.3812838  -0.3991161   0.9999802  -0.11363947]], dtype=float32)\n>>> a = jt.array([4, 5])\n>>> a\njt.Var([4 5], dtype=int32)\n>>> x.new(a)\njt.Var([4. 5.], dtype=float32)"
    },
    {
        "api_name": "jittor.new_empty",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.new_empty",
        "api_signature": "jittor.new_empty(x, size)",
        "api_description": "返回一个用未初始化数据填充的张量(Var)，形状和元素类型与输入张量 x 定义保持一致。",
        "return_value": "返回一个用初始化数据填充的张量(Var)且数据类型 dtype 和输入张量 x 保持一致",
        "parameters": "x (Var): Var类型的张量\nsize (int…): 整数序列，定义了输出的形状",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> jt.array([1, 2, 3]).float()\njt.Var([1. 2. 3.], dtype=float32)\n>>> jt.new_empty(x, (5,))\njt.Var([0. 0. 0. 0. 0.], dtype=float32)"
    },
    {
        "api_name": "jittor.new_full",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.new_full",
        "api_signature": "jittor.new_full(x, size, val)",
        "api_description": "创建一个张量(Var)，其中张量的形状由可变参数 shape 定义，张量中的各个标量的值由可变参数 val 定义，数据类型由张量 x 的数据类型定义。",
        "return_value": "返回一个填充值为 val ，形状大小为 size 的张量(Var)。",
        "parameters": "x (Var): Var类型的张量\nsize(int…): 整数序列，定义了输出张量的形状\nval: 填充值，任意数值类型",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> jt.array([1, 2, 3]).float()\njt.Var([1. 2. 3.], dtype=float32)\n>>> jt.new_full(x, (3,2), 3)\njt.Var([[3. 3.]\n        [3. 3.]\n        [3. 3.]], dtype=float32)"
    },
    {
        "api_name": "jittor.new_ones",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.new_ones",
        "api_signature": "jittor.new_ones(x, size)",
        "api_description": "返回一个全为1的张量(Var)，形状和元素类型与输入张量 x 定义保持一致。",
        "return_value": "全为1的张量(Var)且 dtype 和 x 保持一致",
        "parameters": "x (Var): Var类型的张量",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> jt.array([1, 2, 3]).float()\njt.Var([1. 2. 3.], dtype=float32)\n>>> jt.new_ones(x, (5,))\njt.Var([1. 1. 1. 1. 1.], dtype=float32)"
    },
    {
        "api_name": "jittor.new_zeros",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.new_zeros",
        "api_signature": "jittor.new_zeros(x, size)",
        "api_description": "返回一个全为0的张量(Var)，数据类型由张量 x 定义，形状由可变参数 size 定义。",
        "return_value": "全为0的张量(Var)且数据类型 dtype 和输入张量 x 保持一致",
        "parameters": "x (Var): Var类型的张量\nsize (int…): 整数序列，定义了输出的形状",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> jt.array([1, 2, 3]).float()\njt.Var([1. 2. 3.], dtype=float32)\n>>> jt.new_zeros(x, (5,))\njt.Var([0. 0. 0. 0. 0.], dtype=float32)"
    },
    {
        "api_name": "jittor.nn.nll_loss",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.nn.html#jittor.nn.nll_loss",
        "api_signature": "jittor.nn.nll_loss(output, target, weight=None, ignore_index=-100, reduction='mean')",
        "api_description": "根据给定的目标概率密度函数(target),计算负对数似然损失(negative log likelihood loss)。对于输入中的每个元素，计算该元素在目标处的负对数似然概率： \\(loss(x, class) = -weight[class] * log(x[class])\\) ，其中log(x[class]) 是预测为class这一类的概率取对数。如果 reduction=='mean' ，那么 \\(Out(n, c) = -weight[c]/\\sum(weight) * log(input_{n, c})\\)\n参数:\noutput (Var) : 输出张量，具体维度或形状取决于具体的损失函数。对于nll_loss，input的形状是(minibatch, C)。\ntarget (Var) : 目标张量。其定义在[0，class_num-1]之间的取值表示对应输入数据的类别标签。\nweight (Var, optional) : 一个手动指定每个类别的权重的张量。默认值：None\nignore_index (int, optional) : 指定一个值，在计算损失函数时忽略目标值中等于指定值的元素。默认值：-100\nreduction (string, optional) : 指定如何减少损失：’none’ (不减少) | ‘mean’ (加权减少) | ‘sum’ (简单元素相加)。默认值： 'mean'\n返回值:Var: 一个标量张量，表示负对数似然损失。\n代码示例：>>> import jittor as jt\n>>> from jittor import nn\n>>> output = jt.array([[0.1, 0.1, 0.8], [0.2, 0.4, 0.4], [0.2, 0.6, 0.2]])\n>>> target = jt.array([3, 2, 2])\n>>> nn.nll_loss(output, target)\njt.Var([-0.3], dtype=float32)",
        "return_value": "Var: 一个标量张量，表示负对数似然损失。",
        "parameters": "output (Var) : 输出张量，具体维度或形状取决于具体的损失函数。对于nll_loss，input的形状是(minibatch, C)。\ntarget (Var) : 目标张量。其定义在[0，class_num-1]之间的取值表示对应输入数据的类别标签。\nweight (Var, optional) : 一个手动指定每个类别的权重的张量。默认值：None\nignore_index (int, optional) : 指定一个值，在计算损失函数时忽略目标值中等于指定值的元素。默认值：-100\nreduction (string, optional) : 指定如何减少损失：’none’ (不减少) | ‘mean’ (加权减少) | ‘sum’ (简单元素相加)。默认值： 'mean'",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> from jittor import nn\n>>> output = jt.array([[0.1, 0.1, 0.8], [0.2, 0.4, 0.4], [0.2, 0.6, 0.2]])\n>>> target = jt.array([3, 2, 2])\n>>> nn.nll_loss(output, target)\njt.Var([-0.3], dtype=float32)"
    },
    {
        "api_name": "jittor.misc.nms",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.misc.nms",
        "api_signature": "jittor.misc.nms(dets, thresh)",
        "api_description": "对给定的一个未排序的矩阵，执行非极大值抑制(non-maximum suppression, NMS)操作。\n输入应当是一个二维矩阵，矩阵的每一行代表一个边界框，格式为： [x1, y1, x2, y2, score] ，其中， [x1, y1] 为左上角的坐标， [x2, y2] 为右下角的坐标， score 是该边界框对应的置信度分数。\n使用的是贪婪策略，首先将所有边界框按照置信度得分降序排序，然后从中选取分数最高的边界框并移除所有与该边界框的 IoU 值大于阈值的边界框。然后对剩余的边界框重复上述过程直到没有边界框，最后返回被选出的边界框的索引。\nIoU 是衡量两个边界框重叠程度的指标，其定义为两个边界框的交集面积除以并集面积。",
        "return_value": "被选出的边界框的索引，类型为 Var，在原 dets 中的行索引。",
        "parameters": "dets (Var): 输入的 Var， shape 为 (N, 5), N代表边界框的个数，5列分别代表 [x1, y1, x2, y2, score]\nthresh (float): IoU 阈值，IoU 值大于此的边界框将被移除",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> a = dets = jt.array([[20, 20, 40, 40, 0.9],[22, 22, 42, 42, 0.8],[200, 200, 240, 240, 0.7]], dtype=\"float32\")\n>>> jt.nms(dets, 0.5)\njt.Var([0 2], dtype=int32)"
    },
    {
        "api_name": "jittor.no_grad",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.no_grad",
        "api_signature": "jittor.no_grad(**jt_flags)",
        "api_description": "取消梯度范围，所有在此范围内创建的变量都将停止梯度。\n继承自 flag_scope 类，作为上下文管理器，通过将 jt_flags 的 no_grad 键值设为 1，使优化器取消计算其范围内每个运算的梯度。",
        "return_value": "",
        "parameters": "jt_flags (dict): 包含若干 Jittor 标记的字典。默认值：空。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> x = jt.array([1, 2, 3], dtype=float)\n>>> y = jt.array([4, 5, 6], dtype=float)\n>>> with jt.no_grad():\n...     z = x * y\n...     w = z.sum()\n>>> optimizer = jt.optim.SGD(params=[x, y, z, w], lr=0.1)\n>>> optimizer.step(w)\n>>> x.opt_grad(optimizer)\njt.Var([0. 0. 0.], dtype=float64)"
    },
    {
        "api_name": "jittor.misc.nonzero",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.misc.nonzero",
        "api_signature": "jittor.misc.nonzero(x)",
        "api_description": "返回输入Var中每一个非零元素的索引。",
        "return_value": "返回一个二维 Var, 每一行代表一个非零元素的索引。注意每一行的长度取决于输入 Var 的维度。",
        "parameters": "x (Var): 任意形状的输入 Var",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> x = jt.array([0, 1, 2, 0, 3, 0])\n>>> y = jt.array([[0, 1, 2], [0, 3, 4], [0, 0, 1]])\n>>> jt.nonzero(x)\njt.Var([[1]\n        [2]\n        [4]], dtype=int32)\n>>> jt.nonzero(y)\njt.Var([[0 1]\n        [0 2]\n        [1 1]\n        [1 2]\n        [2 2]], dtype=int32)"
    },
    {
        "api_name": "jittor.norm",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.norm",
        "api_signature": "jittor.norm(x, p=2, dim=-1, keepdims=False, eps=1e-30, keepdim=False)",
        "api_description": "计算输入向量 x 的 p 范数(默认为2范数), p 只能为1或2。\n如果p=1，计算公式如下:\n\\[\\|x\\|_1=\\left|x_1\\right|+\\left|x_2\\right|+\\ldots+\\left|x_n\\right|\\]\n如果p=2，计算公式如下：\n\\[\\|x\\|_2=\\left(\\left|x_1\\right|^2+\\left|x_2\\right|^2+\\ldots+\\left|x_n\\right|^2\\right)^{1 / 2}\\]",
        "return_value": "计算的p范数的结果,类型为jittor.Var。",
        "parameters": "x (jt.Var) : 输入的jittor张量。\np (int) : 需要计算的范数的类型，只能为1或2，默认值:2。\ndim (int) : 指定需要计算范数的维度，如果为-1,则计算整个张量的范数。默认值:-1。\nkeepdims (bool) : 指定是否在输出的结果中保留原始的维度。默认值:False。\neps (float) : 用于保持数值稳定性的微小值。默认值:1e-12。\nkeepdim (bool) : 指定是否在输出的结果中保留原始的维度。默认值:False。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> x = jt.array([1.0, 2.0, 3.0])\n>>> jt.norm(x)\njt.Var([3.7416575], dtype=float32)"
    },
    {
        "api_name": "jittor.normal",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.normal",
        "api_signature": "jittor.normal(mean, std, size=None, dtype='float32')",
        "api_description": "生成产生满足正态分布的随机数的张量。",
        "return_value": "生成的产生满足正态分布的随机数的张量(Var)。",
        "parameters": "mean (int or jt.Var) : 正态分布的均值，是标量或者一个与需要生成的数组shape相同的张量。\nstd (int or jt.Var) : 正态分布的标准差，是标量或者一个与需要生成的数组shape相同的张量。\nsize (tuple, optional) : 生成的随机张量的形状。默认值: None 。\ndtype (str,optional) : 生成的随机张量的数据类型。默认值: float32 。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> jt.normal(5, 3, size=(2,3))\njt.Var([[ 8.070848   7.654219  10.252696 ]\n        [ 6.383718   7.8817277  3.0786133]], dtype=float32)\n>>> mean = jt.randint(low=0, high=10, shape=(10,))\n>>> jt.normal(mean, 0.1)\njt.Var([1.9524184 1.0749301 7.9864206 5.9407325 8.1596155 4.824019  7.955083\n        8.972998  6.0674286 8.88026  ], dtype=float32)"
    },
    {
        "api_name": "jittor.misc.normalize",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.misc.normalize",
        "api_signature": "jittor.misc.normalize(input, p=2, dim=1, eps=1e-30)",
        "api_description": "在指定维度对张量 \\(L_p\\) 归一化\n\\[v' = \\frac{v}{\\max(\\lVert v \\rVert_p, \\epsilon)}\\]",
        "return_value": "在 dim 指定维度归一化后的张量",
        "parameters": "input (Var): 被归一化的张量\np (float): 计算向量范数所用的指数。默认值：2\ndim (int or tuple[int, ...]): 被归一化的维度。默认值：1\neps (float): 避免除以零所用的小量 \\(\\epsilon\\) 。默认值： 1e-30",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> x = jt.randn(2, 3)\n>>> x, x.normalize()\n(jt.Var([[ 0.6452728  -1.7699949   0.7562031 ]\n         [ 0.32050335  0.7093985  -0.44102713]], dtype=float32),\n jt.Var([[ 0.31786057 -0.8718973   0.3725047 ]\n         [ 0.35822764  0.792897   -0.49293745]], dtype=float32))\n>>> x.normalize().norm()\njt.Var([1. 1.], dtype=float32)"
    },
    {
        "api_name": "jittor.distributions.Normal",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.distributions.html#jittor.distributions.Normal",
        "api_signature": "jittor.distributions.Normal(mu, sigma)",
        "api_description": "定义了一个正态分布对象，提供了样本抽样、计算对数概率以及熵计算等 。",
        "return_value": "",
        "parameters": "mu (Var)： 正态分布的均值。没有默认值，必须明确给出。\nsigma (Var)： 正态分布的标准差。没有默认值，必须明确给出。",
        "input_shape": "mu 和 sigma 为形状相同的任意维数 Var，表示一系列服从正态分布的变量。",
        "notes": "",
        "code_example": ">>> n = Normal(mu=jt.array([1.1, 4.]), sigma=jt.array([5., 1.4]))\n>>> n.sample()\njt.Var([8.488743 4.159627], dtype=float32)"
    },
    {
        "api_name": "jittor_core.Var.not_equal",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor_core.Var.not_equal",
        "api_signature": "jittor_core.Var.not_equal()",
        "api_description": "函数C++定义格式:\njt.Var not_equal(jt.Var x, jt.Var y)\n两个张量中对应元素比较是否不相等，把结果布尔值放入输出张量的对应位置，也可以使用 != 运算符调用",
        "return_value": "形状与 x 和 y 相同的张量，数据类型是 bool。其中每个索引位置的布尔值表示该索引对应的 x 和 y 对应元素里是否 x 中的元素不等于 y 中的元素",
        "parameters": "x (Var): 输入数据\ny (Var): 输入数据，形状与 x 相同",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> x = jt.array([3, 2, 1])\n>>> y = jt.array([1, 2, 3])\n>>> x.not_equal(y)\njt.Var([ True False  True], dtype=bool)\n>>> x != y\njt.Var([ True False  True], dtype=bool)"
    },
    {
        "api_name": "jittor.dataset.VOC.NUM_CLASSES",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.dataset.html#jittor.dataset.VOC.NUM_CLASSES",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "jittor_core.Var.numpy",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor_core.Var.numpy",
        "api_signature": "jittor_core.Var.numpy()",
        "api_description": "函数C++定义格式:\nArrayArgs fetch_sync()\n返回此变量的对应 numpy.ndarray 类型变量。",
        "return_value": "Var: 返回自身对应的numpy类型变量。",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> x = jt.randn(5)\n>>> x\njt.Var([-0.39287043  1.3877422  -1.2779838   0.968416    1.6573125 ], dtype=float32)\n>>> x.numpy()\narray([-0.39287043,  1.3877422 , -1.2779838 ,  0.968416  ,  1.6573125 ],\n    dtype=float32)"
    },
    {
        "api_name": "jittor.misc.numpy_cumprod",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.misc.numpy_cumprod",
        "api_signature": "jittor.misc.numpy_cumprod(a, dim)",
        "api_description": "沿着指定维度计算输入张量在这个维度上的累乘，基于numpy实现。\n累乘是指，在指定方向上：\n\\[y_i=x_1 \\times x_2 \\times x_3 \\times \\cdots \\times x_i\\]",
        "return_value": "Var，输出的张量，累乘后的结果，形状和输入一致",
        "parameters": "a (Var): 输入张量 Var, 任意形状\ndim (int): 指定进行乘法的轴。默认为最后一维。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> a = jt.arange(-3, 6).reshape(3, 3)\njt.Var([[-3 -2 -1]\n        [ 0  1  2]\n        [ 3  4  5]], dtype=int32)\n>>> jt.numpy_cumprod(a, 1)\njt.Var([[-3  6 -6]\n        [ 0  0  0]\n        [ 3 12 60]], dtype=int32)\n>>> jt.numpy_cumprod(a, 0)\njt.Var([[ -3  -2  -1]\n        [  0  -2  -2]\n        [  0  -8 -10]], dtype=int32)"
    },
    {
        "api_name": "jittor.misc.numpy_cumsum",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.misc.numpy_cumsum",
        "api_signature": "jittor.misc.numpy_cumsum(x, dim=None)",
        "api_description": "沿着指定维度计算输入张量在这个维度上的累加求和。\n累加求和是指，在指定方向上：\n\\[y_i=x_1+x_2+x_3+\\cdots+x_i\\]\n注意，该函数不应该直接调用。推荐使用jittor.misc.cumsum。",
        "return_value": "Var，输出的张量，累加求和后的结果",
        "parameters": "x (Var): 输入张量 Var, 任意形状\ndim (int): 指定进行累加求和的轴。默认为最后一维。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> a = jt.randn(4, 2)\njt.Var([[ 0.7793252  -2.1805465 ]\n        [-0.46900165  2.0724497 ]\n        [-1.2677554   0.31553593]\n        [ 0.14260457 -1.4511695 ]], dtype=float32)\n>>> jt.numpy_cumsum(a, 1)\njt.Var([[ 0.7793252  -1.4012213 ]\n        [-0.46900165  1.603448  ]\n        [-1.2677554  -0.9522195 ]\n        [ 0.14260457 -1.3085649 ]], dtype=float32)\n>>> jt.numpy_cumsum(a, 0)\njt.Var([[ 0.7793252  -2.1805465 ]\n        [ 0.31032354 -0.10809684]\n        [-0.95743185  0.2074391 ]\n        [-0.81482726 -1.2437304 ]], dtype=float32)"
    },
    {
        "api_name": "jittor.init.one",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.init.html#jittor.init.one",
        "api_signature": "jittor.init.one(shape, dtype='float32')",
        "api_description": "返回一个全为 1 的张量(Var)，形状由可变参数 shape 定义, 数据类型由可变参数 dtype 定义，默认类型  float32。",
        "return_value": "全为 1 的张量(Var)",
        "parameters": "shape (Tuple[int]): 整型序列，定义了输出的形状\ndtype (var.dtype, 可选): 数据类型，默认为 float32",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> init.one((3, 4))\njt.Var([[1. 1. 1. 1.]\n        [1. 1. 1. 1.]\n        [1. 1. 1. 1.]], dtype=float32)\n>>> init.one(5)\njt.Var([1. 1. 1. 1. 1.], dtype=float32)"
    },
    {
        "api_name": "jittor.init.one_",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.init.html#jittor.init.one_",
        "api_signature": "jittor.init.one_(var)",
        "api_description": "原地修改张量 var ，将其数值全部填充为1，数据类型保持不变。",
        "return_value": "就地修改张量 var ，返回一个数值全为1的张量",
        "parameters": "var (Var): Var类型的张量",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> x = jt.randn((3, 2))\n>>> x\njt.Var([[ 0.8584159  -1.1204817 ]\n        [ 0.5418147  -0.62170196]\n        [-0.91137475 -0.13982968]], dtype=float32)\n>>> init.one_(x)\n>>> x\njt.Var([[1. 1.]\n        [1. 1.]\n        [1. 1.]], dtype=float32)"
    },
    {
        "api_name": "jittor.nn.one_hot",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.nn.html#jittor.nn.one_hot",
        "api_signature": "jittor.nn.one_hot(x: Var, num_classes: int = -1)",
        "api_description": "返回输入的 one-hot 编码。如果 x 中的值大于 num_class 或小于0，返回的 one-hot 将全为零。\n参数:\nx（Var）：输入的张量，元素类型为bool或者int\nnum_classes（bool, optional）：类总数。如果设为-1，那么类的数量将被推断为输入张量中最大的类值加一。默认值: -1\n返回值:Var: 其形状由输入维度加一得到。在最后一个维度的索引处该 Var 的值为1，其他处为0。\n代码示例：>>> jt.nn.one_hot(jt.arange(5) % 3)\njt.Var([[1 0 0]\n[0 1 0]\n[0 0 1]\n[1 0 0]\n[0 1 0]], dtype=int32)\n>>> jt.nn.one_hot(jt.arange(5) % 3, num_classes=5)\njt.Var([[1 0 0 0 0]\n[0 1 0 0 0]\n[0 0 1 0 0]\n[1 0 0 0 0]\n[0 1 0 0 0]], dtype=int32)\n>>> jt.nn.one_hot(jt.arange(6).reshape(3,2) % 3)\njt.Var([[[1 0 0]\n[0 1 0]]\n[[0 0 1]\n[1 0 0]]\n[[0 1 0]\n[0 0 1]]], dtype=int32)",
        "return_value": "Var: 其形状由输入维度加一得到。在最后一个维度的索引处该 Var 的值为1，其他处为0。",
        "parameters": "x（Var）：输入的张量，元素类型为bool或者int\nnum_classes（bool, optional）：类总数。如果设为-1，那么类的数量将被推断为输入张量中最大的类值加一。默认值: -1",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> jt.nn.one_hot(jt.arange(5) % 3)\njt.Var([[1 0 0]\n    [0 1 0]\n    [0 0 1]\n    [1 0 0]\n    [0 1 0]], dtype=int32)\n>>> jt.nn.one_hot(jt.arange(5) % 3, num_classes=5)\njt.Var([[1 0 0 0 0]\n    [0 1 0 0 0]\n    [0 0 1 0 0]\n    [1 0 0 0 0]\n    [0 1 0 0 0]], dtype=int32)\n>>> jt.nn.one_hot(jt.arange(6).reshape(3,2) % 3)\njt.Var([[[1 0 0]\n    [0 1 0]]\n    [[0 0 1]\n    [1 0 0]]\n    [[0 1 0]\n    [0 0 1]]], dtype=int32)"
    },
    {
        "api_name": "jittor.distributions.OneHotCategorical",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.distributions.html#jittor.distributions.OneHotCategorical",
        "api_signature": "jittor.distributions.OneHotCategorical(probs=None, logits=None)",
        "api_description": "初始化一个“独热分类”的模型, 它是Categorical的一个子类。",
        "return_value": "",
        "parameters": "probs (Var): 是一个概率列表, 默认值为 None。表示类别出现的概率。\nlogits (Var): 是一个实数列表, 默认值为 None。表示属于不同类别的 logit 值(未标准化的对数概率)。\n\nlogits 参数和 probs 参数应至少有一个不为 None。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> from jittor.distributions import OneHotCategorical\n>>> onehot = OneHotCategorical(jt.array([0.3, 0.7]))\n>>> onehot.sample()\njt.Var([0. 1.], dtype=float32)"
    },
    {
        "api_name": "jittor.ones",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.ones",
        "api_signature": "jittor.ones(*shape, dtype='float32')",
        "api_description": "创建一个指定形状和数据类型的全 1 矩阵。",
        "return_value": "返回一个给定形状和数据类型的全 1 矩阵，类型为 jt.Var。",
        "parameters": "shape (int or sequence of ints): 形成全 1 矩阵的形状。可以是单个整数（生成一维数组）或整数序列（生成多维数组）。\ndtype (str, 可选): 矩阵的数据类型，默认为 float。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> jt.ones(5)\njt.Var([1. 1. 1. 1. 1.], dtype=float32)\n\n\n>>> jt.ones((5,), dtype='int')\njt.Var([1 1 1 1 1], dtype=int32)\n\n\n>>> jt.ones((2, 1), dtype='float')\njt.Var([[1.]\n [1.]], dtype=float32)"
    },
    {
        "api_name": "jittor.ones_like",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.ones_like",
        "api_signature": "jittor.ones_like(x)",
        "api_description": "返回一个全为1的张量(Var)，形状和元素类型与输入张量  x 保持一致。",
        "return_value": "全为1的张量(Var)，其形状和元素类型与输入张量 x 保持一致",
        "parameters": "x (Var): Var类型的张量",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> jt.array([1, 2, 3]).float()\njt.Var([1. 2. 3.], dtype=float32)\n>>> jt.ones_like(x)\njt.Var([1. 1. 1.], dtype=float32)"
    },
    {
        "api_name": "jittor.optim.opt_grad",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.optim.html#jittor.optim.opt_grad",
        "api_signature": "jittor.optim.opt_grad(v: Var, opt: Optimizer)",
        "api_description": "获取优化器中某个变量的梯度，",
        "return_value": "优化器中某个变量的梯度（Var）",
        "parameters": "v(Var): 优化器中的变量\nopt (Optimizer): 优化器",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> model = Model()\n>>> optimizer = SGD(model.parameters())\n>>> ...\n>>> optimizer.backward(loss)\n>>> for p in model.parameters():\n>>>     grad = p.opt_grad(optimizer)"
    },
    {
        "api_name": "jittor.optim.Optimizer",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.optim.html#jittor.optim.Optimizer",
        "api_signature": "jittor.optim.Optimizer(params, lr, param_sync_iter=10000)",
        "api_description": "优化器的基础类。这个基类可以用来实现各种优化算法，比如随机梯度下降等。",
        "return_value": "",
        "parameters": "params (list): 模型参数。\nlr (float): 学习速率。\nparam_sync_iter (int, optional): 参数同步的迭代次数。默认值：10000",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> from jittor.optim import Optimizer\n>>> class MyOptimizer(Optimizer):\n...     def __init__(self, params, lr):\n...         super(MyOptimizer, self).__init__(params, lr)     \n...     def step(self, loss):\n...         self.zero_grad()\n...         self.backward(loss)\n...             for group in self.param_groups:\n...             for param, grad in zip(group['params'], group['grads']):\n...                 if not param.is_stop_grad():\n...                     param.update(param - self.lr * grad)\n... \n>>> x = jt.randn([2,3])\n>>> optimizer = MyOptimizer([x], lr=0.1)\n>>> loss = x.sum()\n>>> optimizer.step(loss)\n>>> print(x)\njt.Var([[-0.481019    0.01914055 -0.74143946]\n        [ 0.33761212 -1.7029546  -0.8524694 ]], dtype=float32)"
    },
    {
        "api_name": "jittor.outer",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.outer",
        "api_signature": "jittor.outer(x, y)",
        "api_description": "计算两个一维向量的外积。具体公式如下:\n\\[res_{i,j} = x_{i} \\times y_{j}\\]",
        "return_value": "两个一维向量的外积(Var)",
        "parameters": "x(Var, numpy array, python sequence)：输入的 Var 或者 numpy 数组 或者 python 序列，表示第一个向量。\ny(Var, numpy array, python sequence): 输入的 Var 或者 numpy 数组 或者 python 序列，表示第二个向量。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> x = jt.arange(3)\n>>> y = jt.arange(4)\n>>> jt.outer(x, y)\njt.Var([[0 0 0 0]\n      [0 1 2 3]\n      [0 2 4 6]], dtype=int32)\n>>> x.outer(y)\njt.Var([[0 0 0 0]\n      [0 1 2 3]\n      [0 2 4 6]], dtype=int32)"
    },
    {
        "api_name": "jittor.nn.pad",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.nn.html#jittor.nn.pad",
        "api_signature": "jittor.nn.pad(x, padding, mode='constant', value=0)",
        "api_description": "对给定的输入张量进行填充。填充方法有四种方式:\n常数填充（’constant’，在每个维度的两侧用固定值填充）\n复制填充（’replicate’，在每个维度的两侧用输入张量的反射（无重复）进行填充）\n反射填充（’reflect’，在每个维度的两侧用输入张量的边缘值进行填充）\n环形填充（’circular’，在每个维度的两侧用输入张量的环形复制进行填充）。\n参数:\nx (Var) : 输入的张量\npadding (list[int]) : 填充的尺寸，list长度必须为偶数，且必须小于等于输入张量的维度的两倍。偶数索引为左填充，奇数索引为右填充。\nmode (str, optional) : 填充模式，有‘constant’，‘replicate’，‘reflect’和‘circular’四种选择，默认值: ‘constant’\nvalue (int, float，optional) : 当填充模式为’constant’时，使用此值进行填充。默认值: 0\n返回值:Var: 填充后的张量\n代码示例：>>> import jittor as jt\n>>> from jittor import nn\n>>> x = jt.ones(3,3)\n>>> nn.pad(x, [1,2])\njt.Var([[0. 1. 1. 1. 0. 0.]\n[0. 1. 1. 1. 0. 0.]\n[0. 1. 1. 1. 0. 0.]], dtype=float32)",
        "return_value": "Var: 填充后的张量",
        "parameters": "x (Var) : 输入的张量\npadding (list[int]) : 填充的尺寸，list长度必须为偶数，且必须小于等于输入张量的维度的两倍。偶数索引为左填充，奇数索引为右填充。\nmode (str, optional) : 填充模式，有‘constant’，‘replicate’，‘reflect’和‘circular’四种选择，默认值: ‘constant’\nvalue (int, float，optional) : 当填充模式为’constant’时，使用此值进行填充。默认值: 0",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> from jittor import nn\n>>> x = jt.ones(3,3)\n>>> nn.pad(x, [1,2]) \njt.Var([[0. 1. 1. 1. 0. 0.]\n        [0. 1. 1. 1. 0. 0.]\n        [0. 1. 1. 1. 0. 0.]], dtype=float32)"
    },
    {
        "api_name": "jittor.nn.Parameter",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.nn.html#jittor.nn.Parameter",
        "api_signature": "jittor.nn.Parameter(data, requires_grad=True)",
        "api_description": "在Jittor中不需要Parameter接口，这个接口并没有实际作用，仅仅用于兼容性。\n在Jittor中，当一个Var是Module的成员时，它就是一个Parameter。如果你不希望一个Jittor Var成员被当作Parameter处理，只需将其名称以下划线 _ 开头即可。",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "jittor.nn.ParameterDict",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.nn.html#jittor.nn.ParameterDict",
        "alias": "ParameterList"
    },
    {
        "api_name": "jittor.Module.parameters",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.Module.parameters",
        "api_signature": "parameters(recurse=True)",
        "api_description": "返回模块参数的列表。\n示例代码:\n>>> net = nn.Sequential(nn.Linear(2, 10), nn.ReLU(), nn.Linear(10, 2))\n>>> for p in net.parameters():\n...     print(p.name)\n...\n>>> for p in net.parameters():\n...     print(p.name())\n...\n0.weight\n0.bias\n2.weight\n2.bias",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "jittor.einops.parse_shape",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.einops.html#jittor.einops.parse_shape",
        "api_signature": "jittor.einops.parse_shape(x, pattern: str)",
        "api_description": "将张量形状解析为字典，将轴名称映射到其长度。",
        "return_value": "output (dict): 字典，将轴名称映射到它们的长度",
        "parameters": "x (Var): 输入张量\npattern (str): 字符串， 轴的空格分隔名称，下划线表示跳过轴",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> x = jt.zeros([2, 3, 5, 7]) # 使用下划线 _ 来在解析时跳过维度\n>>> parse_shape(x, 'batch _ h w')\n{'batch': 2, 'h': 5, 'w': 7}\n# `parse_shape`输出可用于指定其他操作的axes_lalength：\n>>> y = jt.zeros([700])\n>>> rearrange(y, '(b c h w) -> b c h w', **parse_shape(x, 'b _ h w')).shape\n(2, 10, 5, 7)"
    },
    {
        "api_name": "jittor.misc.peek",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.misc.peek",
        "api_signature": "jittor.misc.peek(x)",
        "api_description": "输出一个Var的构成的字符串。",
        "return_value": "",
        "parameters": "x (Var): 输入的张量。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> a = [jt.ones((5,4))]\n>>> jt.peek(a)\n[float32[5,4,], ]"
    },
    {
        "api_name": "jittor.misc.peek_s",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.misc.peek_s",
        "api_signature": "jittor.misc.peek_s(x)",
        "api_description": "返回一个Var的构成的字符串。",
        "return_value": "str，返回一个递归组成的字符串。",
        "parameters": "x (Var): 输入的张量。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> a = [jt.ones((5,4))]\n>>> jt.peek_s(a)\n'[float32[5,4,], ]'"
    },
    {
        "api_name": "jittor.permute",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.permute",
        "api_signature": "jittor.permute(x, *dim)",
        "api_description": "返回一个张量，其数据与原始张量 x 相同，其指定的维度已经根据 dim 参数中的指示进行了交换。",
        "return_value": "返回改变维度的张量（Var）",
        "parameters": "x (Var): Var类型的张量\ndim (int…): 整数序列，表示需要交换的维度, 默认为空，表示对所有轴进行转置",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> x = jt.randn((3,4))\n>>> x\njt.Var([[ 1.7929314   0.8375565   0.13141492  1.4283884 ]\n        [ 0.0627789   1.4741095  -0.3331003   0.34370992]\n        [ 0.405448    1.6529875   0.19640142 -0.09192007]], dtype=float32)\n>>> x.permute(1,0)\njt.Var([[ 1.7929314   0.0627789   0.405448  ]\n        [ 0.8375565   1.4741095   1.6529875 ]\n        [ 0.13141492 -0.3331003   0.19640142]\n        [ 1.4283884   0.34370992 -0.09192007]], dtype=float32)"
    },
    {
        "api_name": "jittor.transform.pil_to_tensor",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.transform.html#jittor.transform.pil_to_tensor",
        "api_signature": "jittor.transform.pil_to_tensor(pic)",
        "api_description": "将Image.Image对象转换为格式为CHW的np.array",
        "return_value": "经格式转换、规范化等操作后的np.array对象",
        "parameters": "pic (PIL.Image.Image): 输入图像",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> import numpy as np\n>>> from PIL import Image\n>>> data = np.random.rand(200, 200, 3)\n>>> img = Image.fromarray(data, 'RGB')   \n>>> arr = jt.transform.to_tensor(img) \n>>> type(arr), arr.shape\n(<class 'numpy.ndarray'>, (3, 200, 200))"
    },
    {
        "api_name": "jittor.linalg.pinv",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.linalg.html#jittor.linalg.pinv",
        "api_signature": "jittor.linalg.pinv(x)",
        "api_description": "实现对输入矩阵的伪逆运算, 该算子支持梯度反向传播。",
        "return_value": "返回 \\(x\\) 的伪逆矩阵(numpy.ndarray) (…, \\(N\\) , \\(M\\) )；对于每个高阶张量的最后两个维度进行计算, 这两个维度被视为矩阵, 其他高阶维度被视为批次。",
        "parameters": "x (numpy.ndarray): (…, \\(M\\) , \\(N\\) )类型的维度数组或矩阵。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> x = jt.array([[1.0,2.0],[3.0,4.0]])\n>>> y = jt.linalg.pinv(x)\n>>> print(y)\njt.Var([[-1.9999999  1.       ]\n        [ 1.5       -0.5      ]], dtype=float32)"
    },
    {
        "api_name": "jittor.nn.PixelShuffle",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.nn.html#jittor.nn.PixelShuffle",
        "api_signature": "jittor.nn.PixelShuffle(upscale_factor)",
        "api_description": "该操作将形状为 \\((..., C \\times r^2, H, W)\\) 的张量重新排列为形状 \\((..., C, H \\times r, W \\times r)\\)  的张量。的张量，其中 r 是放大因子。\n这个过程通常用于上采样或图像尺寸放大，通过调整元素位置来增加空间维度的分辨率。\n\\[\\begin{split}C_{out} = C_{in} / (\\text{upscale_factor})^2 \\\\\nH_{out} = H_{in} * \\text{upscale_factor} \\\\\nW_{out} = W_{in} * \\text{upscale_factor}\\end{split}\\]",
        "return_value": "",
        "parameters": "upscale_factor (int): 上采样因子，即每个空间维度的放大因子",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> pixel_shuffle = nn.PixelShuffle(3)\n>>> input = jt.randn(1,9,4,4)\n>>> output = pixel_shuffle(input)\n>>> output.shape\n[1, 1, 12, 12]"
    },
    {
        "api_name": "jittor.nn.pool",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.nn.html#jittor.nn.pool",
        "api_signature": "jittor.nn.pool(x, kernel_size, op, padding=0, stride=None)",
        "api_description": "对输入的张量进行池化操作。此函数将对输入应用指定的池化操作, 池化的方式由参数 op 指定。\n参数:\nx (Var): 输入的张量。\nkernel_size (int,  tuple of int): 池化窗口的大小。\nop (str): 池化方式,  \\('max'\\) 表示最大值池化, \\('avg'\\) 表示平均池化。\npadding (int,  tuple of int, optional): 在输入的张量的各边填充0的宽度, 默认值: 0。\nstride (int,  tuple of int, optional): 池化窗口移动的步长。默认值: None。\n返回值: 池化后的张量。\n代码示例: >>> import jittor as jt\n>>> x = jt.ones((1,3,4,4))\n>>> y = pool(x, (2, 2), 'max')\n>>> y.shape\n(1, 3, 2, 2)",
        "return_value": "池化后的张量。",
        "parameters": "x (Var): 输入的张量。\nkernel_size (int,  tuple of int): 池化窗口的大小。\nop (str): 池化方式,  \\('max'\\) 表示最大值池化, \\('avg'\\) 表示平均池化。\npadding (int,  tuple of int, optional): 在输入的张量的各边填充0的宽度, 默认值: 0。\nstride (int,  tuple of int, optional): 池化窗口移动的步长。默认值: None。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> x = jt.ones((1,3,4,4))\n>>> y = pool(x, (2, 2), 'max')\n>>> y.shape\n(1, 3, 2, 2)"
    },
    {
        "api_name": "jittor.nn.pool3d",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.nn.html#jittor.nn.pool3d",
        "api_signature": "jittor.nn.pool3d(x, kernel_size, op, padding=0, stride=None)",
        "api_description": "对输入的张量进行三维池化操作。此函数将对输入应用指定的池化操作, 池化的方式由参数 op 指定。\n参数:\nx (Var or jt.Module): 输入的3维张量。\nkernel_size (int or tuple of int): 池化窗口的尺寸。\nop (str): 池化操作的类型。可以是 \\('''max'''\\) 表示最大值池化。\npadding (int or tuple of int, optional): 输入的每一维在每个方向上的补0层数。默认值: 0。\nstride (int or tuple of int, optional): 池化窗口的步长。默认值: 等于 \\(kernel\\) _ \\(size\\) 。\n返回值: 池化后的输出。\n代码示例: >>> import jittor as jt\n>>> x = jt.random([2,3,10,10,10])\n>>> y = jt.nn.pool3d(x, 2, 'max')\n>>> y.shape\n[2,3,5,5,5]",
        "return_value": "池化后的输出。",
        "parameters": "x (Var or jt.Module): 输入的3维张量。\nkernel_size (int or tuple of int): 池化窗口的尺寸。\nop (str): 池化操作的类型。可以是 \\('''max'''\\) 表示最大值池化。\npadding (int or tuple of int, optional): 输入的每一维在每个方向上的补0层数。默认值: 0。\nstride (int or tuple of int, optional): 池化窗口的步长。默认值: 等于 \\(kernel\\) _ \\(size\\) 。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> x = jt.random([2,3,10,10,10])\n>>> y = jt.nn.pool3d(x, 2, 'max')\n>>> y.shape \n[2,3,5,5,5]"
    },
    {
        "api_name": "jittor.nn.Pool3d",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.nn.html#jittor.nn.Pool3d",
        "api_signature": "jittor.nn.Pool3d(kernel_size, stride=None, padding=0, dilation=None, return_indices=None, ceil_mode=False, count_include_pad=True, op='maximum')",
        "api_description": "三维池化 (pooling) 类。对三维输入的深度, 高度和宽度进行池化计算。\n参数:\nkernel_size (int, tuple): 池化核的大小。\nstride (int, tuple): 池化操作的步长。\npadding (int, tuple): 输入的填充大小。\ndilation (int, tuple): 内核之间元素的距离。\nreturn_indices (bool): 如果是 True, 则返回输出的最大值的索引。\nceil_mode (bool): 如果是 True, 则在计算输出大小时会使用向上取整, 而不是默认的向下取整。\ncount_include_pad (bool): 如果是 True, 在计算平均值时, 填充位置会被计入总数。\nop (str): 池化操作的类型。\n属性:\nkernel_size (int,  tuple): 池化核的大小。\nstride (int,  tuple): 池化操作的步长。\npadding (int,  tuple): 输入的填充大小。\ndilation (int,  tuple): 内核之间元素的距离。\nreturn_indices (bool): 如果是 True, 则返回输出的最大值的索引。\nceil_mode (bool): 如果是 True, 则在计算输出大小时会使用向上取整, 而不是默认的向下取整。\ncount_include_pad (bool): 如果是 True, 在计算平均值时, 填充位置会被计入总数。\nop (str): 池化操作的类型。\n形状:\n输入: \\((N, C, D_{in}, H_{in}, W_{in})\\)。\n输出: \\((N, C, D_{out}, H_{out}, W_{out})\\)。\n代码示例:>>> pool = nn.Pool3d(kernel_size=2, stride=2)\n>>> input = jt.randn(20, 16, 50, 32, 32)\n>>> output = pool(input)\n>>> print(output.shape)\n[20,16,25,16,16,]",
        "return_value": "",
        "parameters": "kernel_size (int, tuple): 池化核的大小。\nstride (int, tuple): 池化操作的步长。\npadding (int, tuple): 输入的填充大小。\ndilation (int, tuple): 内核之间元素的距离。\nreturn_indices (bool): 如果是 True, 则返回输出的最大值的索引。\nceil_mode (bool): 如果是 True, 则在计算输出大小时会使用向上取整, 而不是默认的向下取整。\ncount_include_pad (bool): 如果是 True, 在计算平均值时, 填充位置会被计入总数。\nop (str): 池化操作的类型。",
        "input_shape": "输入: \\((N, C, D_{in}, H_{in}, W_{in})\\)。\n输出: \\((N, C, D_{out}, H_{out}, W_{out})\\)。",
        "notes": "",
        "code_example": ">>> pool = nn.Pool3d(kernel_size=2, stride=2)\n>>> input = jt.randn(20, 16, 50, 32, 32)\n>>> output = pool(input)\n>>> print(output.shape)\n[20,16,25,16,16,]"
    },
    {
        "api_name": "jittor.nn.Pool",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.nn.html#jittor.nn.Pool",
        "api_signature": "jittor.nn.Pool(kernel_size, stride=None, padding=0, dilation=None, return_indices=None, ceil_mode=False, count_include_pad=True, op='maximum')",
        "api_description": "池化(Pooling)类。根据op参数的不同, 可以实现最大池化、最小池化和平均池化操作。\n参数:\nkernel_size (int, tuple): 池化窗口的大小。\nstride (int, tuple): 池化窗口移动的步长。\npadding (int, tuple): 输入图像四周的零填充数量。\ndilation (None): 控制池化窗口中各点的间距。\nreturn_indices (bool): 如果是True, 那么在前向过程中, 返回额外的一个输出, 为每个窗口中最大值的索引。\nceil_mode (bool): 当为True时, 会对output的大小进行向上取整的操作。\ncount_include_pad (bool): 当进行平均池化操作时, 该参数定义是否把零填充区域计算在内。\nop (str): 池化操作的类型。\n形状:\n输入: \\((N, C, H_{in}, W_{in})\\)。\n输出: \\((N, C, H_{out}, W_{out})\\)。\n属性:\nkernel_size (int, tuple): 池化窗口的大小。\nstride (int, tuple): 池化窗口移动的步长。\npadding (int, tuple): 输入图像四周的零填充数量。\ndilation (None): 控制池化窗口中各点的间距。\nreturn_indices (bool): 如果是True, 那么在前向过程中, 返回额外的一个输出, 为每个窗口中最大值的索引。\nceil_mode (bool): 当为True时, 会对output的大小进行向上取整的操作。\ncount_include_pad (bool): 当进行平均池化操作时, 该参数定义是否把零填充区域计算在内。\nop (str): 池化操作的类型。\n代码示例:>>> input = jt.random([50,3,32,32])  #初始化一个随机张量\n>>> pool = nn.Pool(2,2)  #创建一个实现2x2最大池化操作的Pool对象\n>>> output = pool(input)  #对张量input进行池化操作\n>>> print(output.shape)\n[50,3,16,16,]",
        "return_value": "",
        "parameters": "kernel_size (int, tuple): 池化窗口的大小。\nstride (int, tuple): 池化窗口移动的步长。\npadding (int, tuple): 输入图像四周的零填充数量。\ndilation (None): 控制池化窗口中各点的间距。\nreturn_indices (bool): 如果是True, 那么在前向过程中, 返回额外的一个输出, 为每个窗口中最大值的索引。\nceil_mode (bool): 当为True时, 会对output的大小进行向上取整的操作。\ncount_include_pad (bool): 当进行平均池化操作时, 该参数定义是否把零填充区域计算在内。\nop (str): 池化操作的类型。",
        "input_shape": "输入: \\((N, C, H_{in}, W_{in})\\)。\n输出: \\((N, C, H_{out}, W_{out})\\)。",
        "notes": "",
        "code_example": ">>> input = jt.random([50,3,32,32])  #初始化一个随机张量\n>>> pool = nn.Pool(2,2)  #创建一个实现2x2最大池化操作的Pool对象\n>>> output = pool(input)  #对张量input进行池化操作\n>>> print(output.shape)\n[50,3,16,16,]"
    },
    {
        "api_name": "jittor.optim.Optimizer.post_step",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.optim.html#jittor.optim.Optimizer.post_step",
        "api_signature": "post_step()",
        "api_description": "在 step 之后执行的操作，比如更新参数值。\n例子：\n>>> class MyOptimizer(Optimizer):\n...     def step(self, loss):\n...         self.pre_step(loss)\n...         ...\n...         self.post_step()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "jittor.pow",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.pow",
        "api_signature": "jittor.pow(x, y)",
        "api_description": "对两个张量逐个元素计算乘方，也可以使用 ** 运算符调用",
        "return_value": "乘方结果(Var)，形状与 x 和 y 相同",
        "parameters": "x (Var): 乘方运算的底数\ny (Var): 乘方运算的幂次，形状与 x 相同",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> x = jt.array([1, 2, 3])\n>>> y = jt.array([2, 3, 4])\n>>> jt.pow(x, y)\n    jt.Var([ 1  8 81], dtype=int32)\n>>> x ** y\n    jt.Var([ 1  8 81], dtype=int32)"
    },
    {
        "api_name": "jittor.optim.Optimizer.pre_step",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.optim.html#jittor.optim.Optimizer.pre_step",
        "api_signature": "pre_step(loss, retain_graph=False)",
        "api_description": "在 step 之前执行的操作，比如计算梯度，retain_graph 等。\n例子：\n>>> class MyOptimizer(Optimizer):\n...     def step(self, loss):\n...         self.pre_step(loss)\n...         ...\n...         self.post_step()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "jittor.nn.PReLU",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.nn.html#jittor.nn.PReLU",
        "api_signature": "jittor.nn.PReLU(num_parameters=1, init_=0.25)",
        "api_description": "应用 element-wise 函数：\n\\[\\begin{split}\\text{PReLU}(x) = \\max(0,x) + a * \\min(0,x) \\\\\\end{split}\\]\n这里 \\(a\\) 是一个可学习的参数。当不带参数调用时，nn.PReLU() 在所有输入通道中使用单个参数 \\(a\\)。如果使用 nn.PReLU(nChannels) 调用，则每个输入通道使用单独的 \\(a\\)。",
        "return_value": "",
        "parameters": "num_parameters (int): 要学习的 \\(a\\) 数量。尽管它接受 int 作为输入，但只有两个值是合法的：1或者输入张量的通道数。默认值：1\ninit_ (float): a 的初始值。默认值：0.25",
        "input_shape": "Input: \\(( *)\\)，其中 * 表示任意数量的附加维数。\nOutput: \\((*)\\)，形状与输入相同。",
        "notes": "",
        "code_example": ">>> m = nn.PReLU()\n>>> input = jt.randn(2)\n>>> output = m(input)\n>>> output\njt.Var([-0.3737674 -0.6461646], dtype=float32)"
    },
    {
        "api_name": "jittor_core.Var.prod",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor_core.Var.prod",
        "api_signature": "jittor_core.Var.prod()",
        "api_description": "函数C++定义格式:\njt.Var reduce_multiply(jt.Var x,  int dim,  bool keepdims=false)\n对输入的数组进行降维求积操作，计算指定维度上元素的乘积。",
        "return_value": "Var: 返回一个新的张量，进行过一次或多次降维求积后的结果。",
        "parameters": "x(Var): 输入的张量\ndims(int, or Tuple of int): 要计算乘法的轴，如果是 Tuple 则在这些维度上依次进行降维求积计算，如果是 int 则在这一维度上进行降维求积计算。如果为空，则对输入张量整体计算求积\nkeepdims(bool or int): 如果为 True，则进行求积的维度将被保留，否则消除这一维度。如果输入的是 int, 0 被认为是 False，非 0 被认为是 True。默认值：False 。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> a = jt.arange(1, 7).reshape(2, 3)\njt.Var([[1 2 3]\n        [4 5 6]], dtype=int32)\n>>> jt.prod(a, 1)\njt.Var([ 6 120], dtype=int32)\n>>> jt.prod(a, 1, True)\njt.Var([[ 6]\n        [120]], dtype=int32)\n>>> jt.prod(a, (1, 0))\njt.Var([ 720], dtype=int32)\n>>> jt.prod(a, 0)\njt.Var([ 4 10 18], dtype=int32)"
    },
    {
        "api_name": "jittor.profile_mark",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.profile_mark",
        "api_signature": "jittor.profile_mark(mark_name: str)",
        "api_description": "profile_mark 类用于分析部分代码的性能。该类继承了 _call_no_record_scope 类，作为上下文管理器，通过临时改变全局变量 flags.compile_options 来实现统计其范围内代码的运行时间和内存等性能参数。",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> a = jt.rand(1000,1000)\n>>> b = jt.rand(1000,1000)\n>>> jt.sync_all()\n>>> results = []\n>>> with jt.profile_scope() as rep:\n...     results.append(jt.matmul(a, b))\n...     with jt.profile_mark(\"mark1\"):\n...         results.append(jt.matmul(a, b))\n...         with jt.profile_mark(\"mark2\"):\n...             results.append(jt.matmul(a, b))\n...     with jt.profile_mark(\"mark3\"):\n...         results.append(jt.matmul(a, b))\n...     results.append(jt.matmul(a, b))\n# 输出：性能分析结果，包括每个 mark 段落的性能评估报告"
    },
    {
        "api_name": "jittor.profile_scope",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.profile_scope",
        "api_signature": "jittor.profile_scope(warmup=0, rerun=0, **jt_flags)",
        "api_description": "profile_scope 类是用来测量代码执行时间的一个工具，封装了自动化性能分析的功能。使用此上下文管理器来记录 Jittor 程序的性能数据。",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "jittor.misc.python_pass_wrapper",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.misc.python_pass_wrapper",
        "api_signature": "jittor.misc.python_pass_wrapper(mod_func, args, kw)",
        "api_description": "使用给定的模块函数 mod_func 、参数 args 和关键字参数 kw 通过 Python 的\nimportlib 导入模块函数并执行它，最后返回执行结果。",
        "return_value": "Any: 返回模块函数调用的结果。",
        "parameters": "mod_func (str): 完全限定名称的模块函数，例如 numpy.matmul。\nargs (Tuple[str]): 需要传递给模块函数的参数，这些参数应以元组的形式提供，元组中的每一项都要是字符串。 例如，如果函数需要两个参数 a 和 b，调用应该为 python_pass_wrapper('func', ('a', 'b'), kw)\nkw (dict): 需要传递给模块函数的关键字参数，应以字典的形式提供。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> jt.python_pass_wrapper('math.sqrt', ('25',), {})\n5\n>>> jt.python_pass_wrapper('builtins.len', ('[1, 2, 3]',), {})\n3"
    },
    {
        "api_name": "jittor.linalg.qr",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.linalg.html#jittor.linalg.qr",
        "api_signature": "jittor.linalg.qr(x)",
        "api_description": "对输入矩阵进行 \\(QR\\) 分解。\n参数:x (array): 要进行 \\(QR\\) 分解的矩阵, 形状为(M,M)。\n返回值:\\(QR\\) 分解的结果, 是形状为(M,M)的矩阵。\n代码示例:>>> import jittor as jt\n>>> x = jt.random((2, 2))\n>>> q, r = jt.linalg.qr(x)\n>>> print(q, r)\njt.Var([[-0.9639901 -0.2659382]\n[-0.2659382  0.9639901]], dtype=float32) jt.Var([[-1.0051305  -1.0211498 ]\n[ 0.          0.29402444]], dtype=float32)",
        "return_value": "\\(QR\\) 分解的结果, 是形状为(M,M)的矩阵。",
        "parameters": "x (array): 要进行 \\(QR\\) 分解的矩阵, 形状为(M,M)。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> x = jt.random((2, 2))\n>>> q, r = jt.linalg.qr(x)\n>>> print(q, r)\njt.Var([[-0.9639901 -0.2659382]\n[-0.2659382  0.9639901]], dtype=float32) jt.Var([[-1.0051305  -1.0211498 ]\n[ 0.          0.29402444]], dtype=float32)"
    },
    {
        "api_name": "jittor.misc.rad2deg",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.misc.rad2deg",
        "api_signature": "jittor.misc.rad2deg(x)",
        "api_description": "将弧度转换为角度\n\\[x' = \\frac{180 x}{\\pi}\\]",
        "return_value": "x 中每个元素换成角度",
        "parameters": "x (Var): 弧度值组成的张量",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> x = jt.array([[3.142, -3.142], [6.283, -6.283], [1.570, -1.570]])\n>>> x.rad2deg()\njt.Var([[ 180.02333 -180.02333]\n        [ 359.98935 -359.98935]\n        [  89.95437  -89.95437]], dtype=float32)"
    },
    {
        "api_name": "jittor.rand",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.rand",
        "api_signature": "jittor.rand(*size, dtype='float32', requires_grad=True)",
        "api_description": "返回一个元素值用从[0, 1)均匀分布中抽样取得的随机Var形式张量。",
        "return_value": "返回一个 *size 维Var形式张量，元素值用从[0, 1)均匀分布中抽样取得的随机Var形式张量。",
        "parameters": "*size (序列的整数) – 输出张量的期望形状，例如 (m, n, k)。\ndtype (str, 可选) – 输出张量的数据类型，默认为 float32 。\nrequires_grad (bool, 可选) – 是否需要返回梯度，默认为 False 。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> jt.random.seed(3)\n>>> jt.rand(3,3)\njt.Var([[0.5509497 , 0.6197729 , 0.909781  ],\n        [0.04279451, 0.10358319, 0.9024445 ],\n        [0.09957159, 0.95744824, 0.546594  ]], dtype=float32)"
    },
    {
        "api_name": "jittor.rand_like",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.rand_like",
        "api_signature": "jittor.rand_like(x, dtype=None)",
        "api_description": "生成一个元素值从[0,1]均匀分布中抽样取得的张量，并且形状与输入的张量形状相同。",
        "return_value": "与输入的张量 x 形状相同，值为[0,1]均匀分布随机抽样的张量。",
        "parameters": "x (jt.Var) : 输入的张量。\ndtype (str,optional) : 生成的随机张量的数据类型。如果此参数为None, 则生成的张量的数据类型将与输入的张量的数据类型相同。默认值: None 。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> x = jt.zeros((2, 3))\n>>> jt.rand_like(x)\njt.Var([[0.6164821  0.21476883 0.61959815]\n        [0.58626485 0.35345772 0.5638483 ]], dtype=float32)"
    },
    {
        "api_name": "jittor.randint",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.randint",
        "api_signature": "jittor.randint(low, high=None, shape=(1,)",
        "api_description": "用于在一个指定的范围 [ low , high)内生成随机整数张量。",
        "return_value": "生成的随机张量(Var)。",
        "parameters": "low (int, optional) : 生成随机数的范围的下限。默认值: 0 。\nhigh (int) : 生成随机数的范围的上限。\nshape (tuple, optional) : 生成的随机张量的形状。默认值: (1,) 。\ndtype (str,optional) : 生成的随机张量的数据类型。默认值: int32 。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> jt.randint(3, shape=(3, 3))\njt.Var([[2 0 2]\n        [2 1 2]\n        [2 0 1]], dtype=int32)\n>>> jt.randint(1, 3, shape=(3, 3))\njt.Var([[2 2 2]\n        [1 1 2]\n        [1 1 1]], dtype=int32)"
    },
    {
        "api_name": "jittor.randint_like",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.randint_like",
        "api_signature": "jittor.randint_like(x, low, high=None)",
        "api_description": "用于在一个指定的范围[ low , high)内生成随机整数张量，并且形状与输入变量 x 相同。",
        "return_value": "生成的形状与输入变量 x 相同的随机张量(Var)。",
        "parameters": "x (jt.Var) : 输入的张量。\nlow (int, optional) : 生成随机数的范围的下限。默认值: 0 。\nhigh (int) : 生成随机数的范围的上限。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> x = jt.zeros((2, 3))\n>>> jt.randint_like(x, 10)\njt.Var([[9. 3. 4.]\n        [4. 8. 5.]], dtype=float32)\n>>> jt.randint_like(x, 10, 20)\njt.Var([[17. 11. 18.]\n        [14. 17. 15.]], dtype=float32)"
    },
    {
        "api_name": "jittor.randn",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.randn",
        "api_signature": "jittor.randn(*size, dtype='float32', requires_grad=True)",
        "api_description": "返回一个具有标准正态分布的张量。",
        "return_value": "生成的具有标准正态分布的张量。类型为jittor.Var。",
        "parameters": "size (int or a sequence of int) : 生成张量的形状。\ndtype(str,optional) : 生成张量的数据类型，默认值: float32 。\nrequires_grad (bool) : 是否能够进行梯度反向传播。默认值: True 。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> jt.randn(3)\njt.Var([-1.019889   -0.30377278 -1.4948598 ], dtype=float32)\n>>> jt.randn(2, 3)\njt.Var([[-0.15989183 -1.5010914   0.5476955 ]\n [-0.612632   -1.1471151  -1.1879086 ]], dtype=float32)"
    },
    {
        "api_name": "jittor.randn_like",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.randn_like",
        "api_signature": "jittor.randn_like(x, dtype=None)",
        "api_description": "生成一个元素服从标准正态分布的张量，并且形状与输入的张量形状相同的随机张量。",
        "return_value": "与输入的张量 x 形状相同，元素服从标准正态分布的张量(Var)。",
        "parameters": "x (jt.Var) : 输入的张量。\ndtype (str,optional) : 生成的随机张量的数据类型。如果此参数为None, 则生成的张量的数据类型将与输入的张量的数据类型相同。默认值: None 。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> x = jt.zeros((2, 3))\n>>> jt.randn_like(x)\njt.Var([[-1.1647032   0.34847224 -1.3061888 ]\n    [ 1.068085   -0.34366122  0.13172573]], dtype=float32)"
    },
    {
        "api_name": "jittor.random",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.random",
        "api_signature": "jittor.random(shape, dtype='float32', type='uniform')",
        "api_description": "生成一个给定形状，值为随机数的张量",
        "return_value": "生成的随机数",
        "parameters": "shape (tuple[int]): 生成张量的形状\ndtype (str): 数据类型。默认值： 'float32'\ntype (Literal[‘uniform’, ‘normal’]): 'uniform' ：生成 0 到 1 之间的均匀随机数； 'normal' ：生成均值为 0 方差为 1 的正态分布随机数。默认值： 'uniform'",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> jt.random((3,))\njt.Var([0.7427739  0.80093205 0.2652795 ], dtype=float32)\n>>> jt.random((3,), type='normal')\njt.Var([ 1.2703565   0.20411628 -1.1991016 ], dtype=float32)"
    },
    {
        "api_name": "jittor.init.random_",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.init.html#jittor.init.random_",
        "api_signature": "jittor.init.random_(var)",
        "api_description": "该函数将输入变量(var)重新赋值为随机值，生成的随机数范围在 0 到 1 之间。\n参数:\nvar(Var): 需要被重新赋值的变量",
        "return_value": "Var: 重新赋值后的变量",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> x = jt.init.one(5)\n>>> jt.init.random_(x)\n>>> print(x)\njt.Var([0.9079071  0.1955278  0.2359613  0.8015607  0.83047885], dtype=float32)"
    },
    {
        "api_name": "jittor.transform.RandomAffine",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.transform.html#jittor.transform.RandomAffine",
        "api_signature": "jittor.transform.RandomAffine(degrees, translate=None, scale=None, shear=None, resample=False, fillcolor=0)",
        "api_description": "随机仿射变换，输入保持中心不变。",
        "return_value": "",
        "parameters": "degrees (序列或int) : 可供选择的度数范围。如果度数是数字而不是序列(min, max) ，则度数范围将为 (-degrees, +degrees)。\ntranslate (元组, 可选): 例如 translate=(a, b)，那么水平平移将在 img_width * a < dx < img_width * a 的范围内随机采样，垂直平移将在 -img_height * b < dy < img_height * b 的范围内随机采样。默认情况下不进行平移。默认值: None\nscale (元组, 可选): 缩放因子区间，例如 (a, b)，则缩放比例在 a <= scale <= b 的范围内随机采样。默认值: None\nshear (序列, float或int, 可选): 从中选取剪切角度的范围。 如果 shear 是一个数字，则将应用范围是 (-shear, +shear) 并且平行于 x 轴的剪切;若是一个元组或列表且包含2个值，则应用范围是 (shear[0], shear[1]) 并且平行于 x 轴的剪切; 若是一个元组或列表且包含4个值，则应用 x 轴剪切范围 (shear[0], shear[1]) 以及y 轴剪切范围 (shear[2], shear[3])。默认情况下不应用剪切。默认值: None\nresample ({PIL.Image.NEAREST, PIL.Image.BILINEAR, PIL.Image.BICUBIC}, 可选): 可选的重采样滤波器。请参阅 这里 以获取更多信息。默认值: False\nfillcolor (元组或int, 可选): 对输出图像中变换区域外的区域填充的颜色（RGB 图像用元组表示，灰度图像用整数表示）。默认值: 0",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> from jittor import transform\n>>> random_affine = transform.RandomAffine(degrees=45)\n>>> img_ = random_affine(img)"
    },
    {
        "api_name": "jittor.transform.RandomApply",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.transform.html#jittor.transform.RandomApply",
        "api_signature": "jittor.transform.RandomApply(transforms, p=0.5)",
        "api_description": "随机应用具有给定概率的变换列表。",
        "return_value": "",
        "parameters": "transforms (list, tuple): 需要随机应用的变换操作列表\np (float): 概率，默认值是 0.5",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> from jittor import transform\n>>> transform_list = [transform.RandomHorizontalFlip(), transform.RandomCrop(size=(32, 32))] \n>>> random_apply = transform.RandomApply(transform_list, p=0.5)\n>>> img_= random_apply(img)"
    },
    {
        "api_name": "jittor.transform.RandomChoice",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.transform.html#jittor.transform.RandomChoice",
        "api_signature": "jittor.transform.RandomChoice(transforms)",
        "api_description": "从给定的变换列表中随机选择一个变换应用于图像。",
        "return_value": "",
        "parameters": "transforms (list, tuple): 变换列表。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> from jittor import transform\n>>> transform_list = [transform.RandomHorizontalFlip(), transform.RandomCrop(size=(32, 32))] \n>>> random_choice = transform.RandomChoice(transform_list)\n>>> img_ = random_choice(img)"
    },
    {
        "api_name": "jittor.transform.RandomCropAndResize",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.transform.html#jittor.transform.RandomCropAndResize",
        "api_signature": "jittor.transform.RandomCropAndResize(size, scale: tuple = (0.08, 1.0)",
        "api_description": "对给定的PIL图像进行随机裁剪和尺寸调整。",
        "return_value": "",
        "parameters": "size (int, tuple): 输出图像的[高度,宽度]\nscale (tuple): 裁剪区域的面积比例范围。默认值: (0.08, 1.0)\nratio (tuple): 裁剪图像的长宽比范围。默认值: (3.0/4.0, 4.0/3.0)\ninterpolation (InterpolationMode): 图像大小调整时采用的插值类型。默认使用双线性插值。默认值: PIL.Image.BILINEAR",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> from jittor import transform\n>>> import numpy as np  \n>>> from PIL import Image\n>>> data = np.random.rand(200, 200, 3)\n>>> img = Image.fromarray(data, 'RGB')\n>>> transform = transform.RandomCropAndResize(128)\n>>> transform(img).size\n(133, 128)"
    },
    {
        "api_name": "jittor.transform.RandomCrop",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.transform.html#jittor.transform.RandomCrop",
        "api_signature": "jittor.transform.RandomCrop(size)",
        "api_description": "随机裁剪输入图像的类。",
        "return_value": "",
        "parameters": "size (tuple, int): 欲裁剪的目标大小。如果是int，将裁剪出一个正方形；如果是tuple，则按照(tuple_height, tuple_width)的形式给出。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> from jittor import transform\n>>> import numpy as np\n>>> from PIL import Image\n>>> data = np.random.rand(200, 200, 3)\n>>> img = Image.fromarray(data, 'RGB')\n>>> random_crop = transform.RandomCrop(128)\n>>> random_crop(img).size\n(128, 128)"
    },
    {
        "api_name": "jittor.transform.RandomGray",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.transform.html#jittor.transform.RandomGray",
        "api_signature": "jittor.transform.RandomGray(p=0.1)",
        "api_description": "随机将图片转换为灰度图。该类用于以给定的概率p随机将输入图像转换为灰度图像，转换后的图像可能保持不变或则转换为灰度图，这取决于一个随机事件。输出灰度图与输入图的通道数保持一致。",
        "return_value": "",
        "parameters": "p (float): 图像被转换为灰度图的概率。默认值: 0.1",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> from jittor import transform\n>>> random_gray = transform.RandomGray()\n>>> img_ = random_gray(img)"
    },
    {
        "api_name": "jittor.transform.RandomHorizontalFlip",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.transform.html#jittor.transform.RandomHorizontalFlip",
        "api_signature": "jittor.transform.RandomHorizontalFlip(p=0.5)",
        "api_description": "对图像进行随机水平翻转的变换类，以一定的概率水平翻转输入的图像。",
        "return_value": "",
        "parameters": "p (float): 图像翻转的概率。默认值为0.5。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> from jittor import transform\n>>> random_hflip = transform.RandomHorizontalFlip(0.6)\n>>> img_ = random_hflip(img)"
    },
    {
        "api_name": "jittor.transform.RandomOrder",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.transform.html#jittor.transform.RandomOrder",
        "api_signature": "jittor.transform.RandomOrder(transforms)",
        "api_description": "对一系列的变换操作以随机的顺序进行应用。",
        "return_value": "",
        "parameters": "transforms (list): 需要应用的变换操作的列表",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> from jittor import transform\n>>> transform_list = [transform.RandomHorizontalFlip(), transform.RandomCrop(size=(32, 32))] \n>>> random_order = transform.RandomOrder(transform_list)\n>>> img_ = random_order(img)"
    },
    {
        "api_name": "jittor.transform.RandomPerspective",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.transform.html#jittor.transform.RandomPerspective",
        "api_signature": "jittor.transform.RandomPerspective(distortion_scale=0.5, p=0.5, interpolation=3)",
        "api_description": "对图像以一定的概率进行随机透视变换。",
        "return_value": "",
        "parameters": "distortion_scale(float, 可选): 控制变形程度的比例系数，值域为 [0, 1]。默认值: 0.5\np (float, 可选): 图片被透视变换的概率。默认值: 0.5\ninterpolation (PIL.Image.Interpolation method, 可选): 插值方法。默认值: PIL.Image.BICUBIC",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> from jittor import transform\n>>> random_perspective = transform.RandomPerspective(distortion_scale=0.5, p=0.5)\n>>> img_ = random_perspective(img)"
    },
    {
        "api_name": "jittor.transform.RandomResizedCrop",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.transform.html#jittor.transform.RandomResizedCrop",
        "api_signature": "jittor.transform.RandomResizedCrop(size, scale=(0.08, 1.0)",
        "api_description": "对给定的PIL图像执行随机尺寸与宽高比的裁剪并将其调整为给定大小。\n同 jittor.transform.RandomSizedCrop",
        "return_value": "",
        "parameters": "size (int, tuple[int]): 裁剪后每边的期望输出尺寸。若为单一整数，输出将是正方形；若为(int, int)元组，则输出为指定的宽高。\nscale (tuple[float], 可选): 原始尺寸裁剪范围。默认值: (0.08, 1.0)。\nratio (tuple[float], 可选): 原始宽高比裁剪范围。默认值: (3/4, 4/3)。\ninterpolation (PIL.Image.Interpolation method, 可选): 图像插值方法。默认值: PIL.Image.BILINEAR",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> from jittor import transform\n>>> random_resized_crop = transform.RandomResizedCrop(size=(224, 224))\n>>> img_ = random_resized_crop(img)"
    },
    {
        "api_name": "jittor.transform.RandomRotation",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.transform.html#jittor.transform.RandomRotation",
        "api_signature": "jittor.transform.RandomRotation(degrees, resample=False, expand=False, center=None, fill=None)",
        "api_description": "对图像进行随机旋转。",
        "return_value": "",
        "parameters": "degrees (sequence, float, int): 选择的角度范围，如果degrees是一个数字而不是像(min, max)这样的序列，角度的范围将为(-degrees, +degrees)。\nresample ({PIL.Image.NEAREST, PIL.Image.BILINEAR, PIL.Image.BICUBIC}, 可选):可选的重采样滤波器。请参阅 链接 以获取更多信息。如果省略或图像的模式为1或P，则设置为 PIL.Image.NEAREST 。默认值: False\nexpand (bool, 可选): 可选的扩展标志。若为真，则扩展输出以使其足够大以容纳整个旋转后的图像；为假或省略，则使输出图像与输入图像的尺寸相同。请注意，扩展标志假定围绕中心旋转且不进行平移。默认值: False\ncenter (2-tuple, 可选): 可选的旋转中心，原点是左上角。默认值: 图像的中心。默认值: None\nfill (n-tuple, int, float, 可选): 旋转后图像之外区域的像素填充值。如果是整数或浮点数，分别用于所有通道。默认情况下，所有通道的值都为0。此选项仅在`pillow>=5.2.0`版本中可用。默认值: None",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> from jittor import transform\n>>> random_rotation = transform.RandomRotation(degrees=45)\n>>> rotated_image = random_rotation(input_image)"
    },
    {
        "api_name": "jittor.dataset.RandomSampler",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.dataset.html#jittor.dataset.RandomSampler",
        "api_signature": "jittor.dataset.RandomSampler(dataset, replacement=False, num_samples=None)",
        "api_description": "随机对元素进行采样。\n参数:\ndataset (jittor.dataset.Dataset): 需要采样的数据集。\nreplacement (bool, optional): 是否采用替换的方式进行采样。False为不替换, 即每个样本仅被采样一次。默认值: False。\nnum_samples (int or None, optional): 期望采样的样本总数。如果为None, 则采样数量为数据集的实际长度。默认值: None。\n属性:\ndataset (Dataset): 被采样的数据集。\nrep (bool): 采样时是否允许替换。\n_num_samples (int or None): 采样的样本总数。\n_shuffle_rng (np.random.Generator): 随机数生成器, 用于生成随机序列。\n代码示例:  >>> import jittor as jt\n>>> from jittor.dataset import Dataset\n>>> from jittor.dataset import RandomSampler\n>>>\n>>> class MyDataset(Dataset):\n>>>      def __len__(self):\n>>>          return 5\n>>>\n>>>      def __getitem__(self, index):\n>>>          return index\n>>>\n>>>\n>>> dataset = MyDataset()\n>>> sampler = RandomSampler(dataset)\n>>> list(iter(sampler))\n[2, 1, 0, 3, 4]",
        "return_value": "",
        "parameters": "dataset (jittor.dataset.Dataset): 需要采样的数据集。\nreplacement (bool, optional): 是否采用替换的方式进行采样。False为不替换, 即每个样本仅被采样一次。默认值: False。\nnum_samples (int or None, optional): 期望采样的样本总数。如果为None, 则采样数量为数据集的实际长度。默认值: None。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> from jittor.dataset import Dataset\n>>> from jittor.dataset import RandomSampler\n>>>\n>>> class MyDataset(Dataset):\n>>>      def __len__(self):\n>>>          return 5\n>>>\n>>>      def __getitem__(self, index):\n>>>          return index\n>>>\n>>>\n>>> dataset = MyDataset()\n>>> sampler = RandomSampler(dataset)\n>>> list(iter(sampler))\n[2, 1, 0, 3, 4]"
    },
    {
        "api_name": "jittor.transform.RandomSizedCrop",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.transform.html#jittor.transform.RandomSizedCrop",
        "alias": "RandomResizedCrop"
    },
    {
        "api_name": "jittor.transform.RandomVerticalFlip",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.transform.html#jittor.transform.RandomVerticalFlip",
        "api_signature": "jittor.transform.RandomVerticalFlip(p=0.5)",
        "api_description": "随机垂直翻转图像的类。该类以一定概率沿垂直方向翻转给定的图像。这是数据增强过程中的一个常用技术，旨在提高模型对图像方向变化的鲁棒性。",
        "return_value": "",
        "parameters": "p (float, 可选): 图像翻转的概率。默认值:  0.5。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> from jittor.transform import RandomVerticalFlip\n>>> from PIL import Image\n>>> random_vflip = RandomVerticalFlip(0.6)\n>>> img = Image.open('path/to/image.jpg')\n>>> img_transformed = random_vflip(img)"
    },
    {
        "api_name": "jittor.misc.randperm",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.misc.randperm",
        "api_signature": "jittor.misc.randperm(n, dtype='int32')",
        "api_description": "生成一个随机排列张量 Var，其中元素为从 0 到 n - 1 这 n 个整数。",
        "return_value": "一个从 0 到 n-1 的随机排列的 Var 对象。",
        "parameters": "n (int): 要生成的随机排列数组的长度。\ndtype (str): 返回的张量中的数据类型，默认为 int32",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> print(jt.randperm(7))\njt.Var([5 3 6 0 4 2 1], dtype=int32)"
    },
    {
        "api_name": "jittor.einops.rearrange",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.einops.html#jittor.einops.rearrange",
        "api_signature": "jittor.einops.rearrange(tensor: Tensor | List[Tensor], pattern: str, **axes_lengths)",
        "api_description": "对多维张量进行智能、易读的元素重排的操作。此操作包括转置（轴置换）、重塑（视图）、挤压、展开、堆叠、连接等操作。\n参数:\ntensor (Union[Var, List[Var]]): 支持的任何库（例如 numpy.ndarray, jittor.Var）的张量，或相同类型和形状的张量列表。\npattern (str): 重排模式的字符串描述。\naxes_lengths: 对维度的额外说明，可选。\n返回值:返回与输入相同类型的张量。尽可能返回原始张量的视图。\n代码示例:>>> from jittor import einops\n>>> import numpy as np\n# 假设有32张 30x40x3 大小的图像\n>>> images = [np.random.randn(30, 40, 3) for _ in range(32)]\n# 沿批量轴堆叠，输出单一数组\n>>> einops.rearrange(images, 'b h w c -> b h w c').shape\n(32, 30, 40, 3)\n# 沿高度轴拼接，输出 960x40x3\n>>> einops.rearrange(images, 'b h w c -> (b h) w c').shape\n(960, 40, 3)\n# 沿宽度轴拼接，输出 30x1280x3\n>>> einops.rearrange(images, 'b h w c -> h (b w) c').shape\n(30, 1280, 3)\n# 轴重排为 'b c h w' 格式\n>>> einops.rearrange(images, 'b h w c -> b c h w').shape\n(32, 3, 30, 40)\n# 每个图像展平为矢量，输出 32x3600\n>>> einops.rearrange(images, 'b h w c -> b (c h w)').shape\n(32, 3600)\n# 将图像分为4个较小的(左上、右上、左下、右下)部分，输出 128x15x20x3\n>>> einops.rearrange(images, 'b (h1 h) (w1 w) c -> (b h1 w1) h w c', h1=2, w1=2).shape\n(128, 15, 20, 3)",
        "return_value": "返回与输入相同类型的张量。尽可能返回原始张量的视图。",
        "parameters": "tensor (Union[Var, List[Var]]): 支持的任何库（例如 numpy.ndarray, jittor.Var）的张量，或相同类型和形状的张量列表。\npattern (str): 重排模式的字符串描述。\naxes_lengths: 对维度的额外说明，可选。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> from jittor import einops \n>>> import numpy as np\n# 假设有32张 30x40x3 大小的图像\n>>> images = [np.random.randn(30, 40, 3) for _ in range(32)]\n# 沿批量轴堆叠，输出单一数组\n>>> einops.rearrange(images, 'b h w c -> b h w c').shape\n(32, 30, 40, 3)\n# 沿高度轴拼接，输出 960x40x3\n>>> einops.rearrange(images, 'b h w c -> (b h) w c').shape\n(960, 40, 3)\n# 沿宽度轴拼接，输出 30x1280x3\n>>> einops.rearrange(images, 'b h w c -> h (b w) c').shape\n(30, 1280, 3)\n# 轴重排为 'b c h w' 格式\n>>> einops.rearrange(images, 'b h w c -> b c h w').shape\n(32, 3, 30, 40)\n# 每个图像展平为矢量，输出 32x3600\n>>> einops.rearrange(images, 'b h w c -> b (c h w)').shape\n(32, 3600)\n# 将图像分为4个较小的(左上、右上、左下、右下)部分，输出 128x15x20x3\n>>> einops.rearrange(images, 'b (h1 h) (w1 w) c -> (b h1 w1) h w c', h1=2, w1=2).shape\n(128, 15, 20, 3)"
    },
    {
        "api_name": "jittor.einops.reduce",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.einops.html#jittor.einops.reduce",
        "api_signature": "jittor.einops.reduce(tensor: Tensor, pattern: str, reduction: str | Callable[[Tensor, List[int]], Tensor], **axes_lengths: int)",
        "api_description": "重新排序和reduce的组合操作。",
        "return_value": "output (Var): 重塑后的张量",
        "parameters": "tensor (Var): 输入张量\npattern (str): 字符串， 减少模式\nreduction (str): 减少操作,可用约简（’min’、’max’、’sum’、’mean’、’prod’）之一\naxes_lengths (int): 轴长度",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> x = jt.randn(100, 32, 64)\n# 在第一个轴上执行最大值归约\n>>> y = reduce(x, 't b c -> b c', 'max')\n# 与前面相同，但轴的含义更清晰\n>>> y = reduce(x, 'time batch channel -> batch channel', 'max')\n>>> x = jt.randn(10, 20, 30, 40)\n# 使用2*2的核大小进行2D最大池化，用于图像处理\n>>> y1 = reduce(x, 'b c (h1 h2) (w1 w2) -> b c h1 w1', 'max', h2=2, w2=2)\n# 如果想恢复到原始的高度和宽度，可以应用深度到空间的技巧\n>>> y2 = rearrange(y1, 'b (c h2 w2) h1 w1 -> b c (h1 h2) (w1 w2)', h2=2, w2=2)\n>>> assert parse_shape(x, 'b _ h w') == parse_shape(y2, 'b _ h w')\n# 自适应2D最大池化到3*4的网格\n>>> reduce(x, 'b c (h1 h2) (w1 w2) -> b c h1 w1', 'max', h1=3, w1=4).shape\n(10, 20, 3, 4)\n# 全局平均池化\n>>> reduce(x, 'b c h w -> b c', 'mean').shape\n(10, 20)\n# 为每个通道减去批次上的均值\n>>> y = x - reduce(x, 'b c h w -> () c () ()', 'mean')\n# 为每个图像的每个通道减去均值\n>>> y = x - reduce(x, 'b c h w -> b c () ()', 'mean')"
    },
    {
        "api_name": "jittor_core.Var.reduce",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor_core.Var.reduce",
        "api_signature": "jittor_core.Var.reduce()",
        "api_description": "函数C++定义格式:\njt.Var reduce(jt.Var x,  String op,  int dim,  bool keepdims=false)\n对张量的 dims 这些维度计算一个聚合函数",
        "return_value": "计算聚合函数的结果。如果 keepdims 为 True 则形状为 x 将 dims 里给出的维度的长度变成 1，否则是 x 的形状去掉 dims 给出的维度",
        "parameters": "x (Var): 输入数据\nop (str): 计算的聚合函数，可从下列函数中选择： 'maximum', 'minimum', 'add', 'multiply', 'logical_and', 'logical_or', 'logical_xor', 'bitwise_and', 'bitwise_or', 'bitwise_xor', 'mean'\ndims (tuple[int]): 在哪些维度计算聚合函数。默认是全部元素都聚合。默认值： ()\nkeepdims (bool): 是否在结果中保留 dim 维度。默认值：False",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> x = jt.randint(0, 10, shape=(2, 3))\n>>> x\njt.Var([[2 7 9]\n        [2 6 3]], dtype=int32)\n>>> x.reduce('maximum')\njt.Var([9], dtype=int32)\n>>> x.reduce('maximum', (0, 1))\njt.Var([9], dtype=int32)\n>>> x.reduce('add')\njt.Var([29], dtype=int32)"
    },
    {
        "api_name": "jittor_core.Var.reduce_add",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor_core.Var.reduce_add",
        "api_signature": "jittor_core.Var.reduce_add()",
        "api_description": "函数C++定义格式:\njt.Var reduce_add(jt.Var x,  int dim,  bool keepdims=false)\n对张量的 dims 这些维度求和",
        "return_value": "求和的结果。如果 keepdims 为 True 则形状为 x 将 dims 里给出的维度的长度变成 1，否则是 x 的形状去掉 dims 给出的维度",
        "parameters": "x (Var): 输入数据\ndim (tuple[int]): 在哪些维度求和。默认是全部元素的最大值。默认值： ()\nkeepdims (bool): 是否在结果中保留 dim 维度。默认值：False",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> x = jt.randint(0, 10, shape=(2, 3))\n>>> x\njt.Var([[2 7 9]\n        [2 6 3]], dtype=int32)\n>>> x.sum()\njt.Var([29], dtype=int32)\n>>> x.sum((0, 1))\njt.Var([29], dtype=int32)"
    },
    {
        "api_name": "jittor_core.Var.reduce_bitwise_and",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor_core.Var.reduce_bitwise_and",
        "api_signature": "jittor_core.Var.reduce_bitwise_and()",
        "api_description": "函数C++定义格式:\njt.Var reduce_bitwise_and(jt.Var x,  int dim,  bool keepdims=false)\n对输入的张量计算指定维度上所有元素进行按位与 (bitwise and) 计算。输入的张量必须是 int 或 bool 类型。",
        "return_value": "Var: 返回一个新的张量，进行过一次或多次降维求按位与后的结果。",
        "parameters": "x(Var): 输入的张量\ndims(int或Tuple[int]，可选): 要计算按位与的轴，如果是 Tuple 则在这些维度上依次进行降维求按位与计算，如果是 int 则在这一个维度上进行降维求按位与计算。如果为空则对张量整体求按位与运算\nkeepdims(bool或int，可选): 如果为 True，则进行求按位与的维度将被保留，否则消除这一维度。如果输入的是 int, 0 被认为是 False，非 0 被认为是 True。默认值：False",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> a = jt.array([[3,2,3],[4,5,6],[7,8,7]])\n>>> jt.reduce_bitwise_and(a, 1)\njt.Var([2 4 0], dtype=int32)\n>>> jt.reduce_bitwise_and(a, 1, True)\njt.Var([[2]\n        [4]\n        [0]], dtype=int32)\n>>> jt.reduce_bitwise_and(a, (1, 0))\njt.Var([0], dtype=int32)\n>>> jt.reduce_bitwise_and(a, 0)\njt.Var([0 0 2], dtype=int32)"
    },
    {
        "api_name": "jittor_core.Var.reduce_bitwise_or",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor_core.Var.reduce_bitwise_or",
        "api_signature": "jittor_core.Var.reduce_bitwise_or()",
        "api_description": "函数C++定义格式:\njt.Var reduce_bitwise_or(jt.Var x,  int dim,  bool keepdims=false)\n对输入的张量计算指定维度上所有元素进行按位或 (bitwise or) 计算。输入的张量必须是 int 或 bool 类型。",
        "return_value": "Var: 返回一个新的张量，进行过一次或多次降维求按位或后的结果。",
        "parameters": "x(Var): 输入的张量\ndims(int或Tuple[int]，可选): 要计算按位或的轴，如果是 Tuple 则在这些维度上依次进行降维求按位或计算，如果是 int 则在这一个维度上进行降维求按位或计算。如果为空则对张量整体求按位或运算\nkeepdims(bool或int，可选): 如果为 True，则进行求按位或的维度将被保留，否则消除这一维度。如果输入的是 int, 0 被认为是 False，非 0 被认为是 True。默认值：False",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> x = jt.array([[1,2,4],[8,16,32]])\n>>> jt.reduce_bitwise_or(x)\njt.Var([63], dtype=int32)\n>>> jt.reduce_bitwise_or(x, 1)\njt.Var([ 7 56], dtype=int32)\n>>> jt.reduce_bitwise_or(x, 1, True)\njt.Var([[ 7]\n        [56]], dtype=int32)\n>>> jt.reduce_bitwise_or(x, (1, 0))\njt.Var([63], dtype=int32)"
    },
    {
        "api_name": "jittor_core.Var.reduce_bitwise_xor",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor_core.Var.reduce_bitwise_xor",
        "api_signature": "jittor_core.Var.reduce_bitwise_xor()",
        "api_description": "函数C++定义格式:\njt.Var reduce_bitwise_xor(jt.Var x,  int dim,  bool keepdims=false)\n将输入的张量在指定的dims上进行按位异或操作。\n\\[\\text{out} = x_1 \\oplus x_2 \\oplus x_3 \\oplus ... \\oplus x_n\\]\n其中，\\(x_i\\) 是输入张量的元素，\\(\\oplus\\) 是按位异或操作。",
        "return_value": "进行按位异或操作后的张量(Var)",
        "parameters": "x(Var): 输入的张量\ndims(Tuple[int], optional): 需要做异或操作的维度，默认值：None\nkeepdims(bool, optional): 是否保持原始的张量维度。默认值：False 。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> x = jt.array([0, 1, 2, 3, 4])\n>>> jt.reduce_bitwise_xor(x).item()\n4\n>>> jt.reduce_bitwise_xor(x, keepdims=True)\njt.Var([4])"
    },
    {
        "api_name": "jittor_core.Var.reduce_logical_xor",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor_core.Var.reduce_logical_xor",
        "api_signature": "jittor_core.Var.reduce_logical_xor()",
        "api_description": "函数C++定义格式:\njt.Var reduce_logical_xor(jt.Var x,  int dim,  bool keepdims=false)\n对输入的数组计算指定维度上所有元素进行逻辑异或 (XOR) 计算。\n逻辑异或有这样的性质：如果参与计算的元素中有奇数个 True，则结果为 True；如果有偶数个 True，结果则为 False。",
        "return_value": "Var: 返回一个新的张量，进行过一次或多次降维求逻辑异或后的结果。",
        "parameters": "x(Var): 输入的张量\ndims(int或Tuple[int]，可选): 要计算逻辑异或的轴，如果是 Tuple 则在这些维度上依次进行降维求逻辑异或计算，如果是 int 则在这一个维度上进行降维求逻辑异或计算。如果为空则对张量整体求逻辑异或运算\nkeepdims(bool或int，可选): 如果为 True，则进行求逻辑异或的维度将被保留，否则消除这一维度。如果输入的是 int, 0 被认为是 False，非 0 被认为是 True。默认值：False",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> a = jt.array([[0,1,0],[0,1,1],[1,1,1]])\n>>> jt.reduce_logical_xor(a, 1)\njt.Var([1 0 1], dtype=int32)\n>>> jt.reduce_logical_xor(a, 1, True)\njt.Var([[1]\n        [0]\n        [1]], dtype=int32)\n>>> jt.reduce_logical_xor(a, (1, 0))\njt.Var([0], dtype=int32)\n>>> jt.reduce_logical_xor(a, 0)\njt.Var([1 1 0], dtype=int32)"
    },
    {
        "api_name": "jittor_core.Var.reduce_maximum",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor_core.Var.reduce_maximum",
        "api_signature": "jittor_core.Var.reduce_maximum()",
        "api_description": "函数C++定义格式:\njt.Var reduce_maximum(jt.Var x,  int dim,  bool keepdims=false)\n返回输入张量中的最大元素。",
        "return_value": "对应维度的最大元素。",
        "parameters": "x (Var): 输入的jt.Var.\ndims_mask (int，可选): 指定的维度，如果指定，则沿给定的维度进行缩减。默认值： None\nkeepdims_mask (int，可选): 输出是否保留 dim 。默认值： None",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> x = jt.randint(10, shape=(2, 3))\n>>> x\njt.Var([[4 1 2]\n    [0 2 4]], dtype=int32)\n>>> jt.max(x)\njt.Var([4], dtype=int32)\n>>> x.max()\njt.Var([4], dtype=int32)\n>>> x.max(dim=1)\njt.Var([4 4], dtype=int32)\n>>> x.max(dim=1, keepdims=True)\njt.Var([[4]\n    [4]], dtype=int32)"
    },
    {
        "api_name": "jittor.lr_scheduler.ReduceLROnPlateau",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.optim.html#jittor.lr_scheduler.ReduceLROnPlateau",
        "api_signature": "jittor.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, verbose=False, threshold=0.0001, threshold_mode='rel', cooldown=0, min_lr=0, eps=1e-08)",
        "api_description": "在训练深度学习模型时，如果训练过程中的损失（Loss）一直不再降低则表明已进入平台期，将学习率 (Learning Rate) 降低可以有助于模型跳出局部最小值点。\nReduceLROnPlateau 类提供了这样一种策略：当损失在一定期限 (patience) 内没有降低时，降低学习率。在一定的慢冷期 (cooldown) 内，损失的变化不再触发学习率的减小。",
        "return_value": "",
        "parameters": "optimizer (Optimizer): 要使用的优化器.\nmode (str): 有两种模式 'min' 和 'max'。默认为 'min'，当损失不再降低时减小学习率。如果设置为 'max'，当损失不再增加时减小学习率。\nfactor (float): 学习率每次降低的倍数。默认为 0.1\npatience (int): 当损失在这么多次迭代后没有下降时，学习率进行调整。默认为 10\nverbose (bool): 设为 True 会在每次更新学习率时打印信息。默认为 False\nthreshold (float): 判断损失是否不再减少的阈值。只有损失减少的幅度大于该值才算作损失的减小。默认为 0.0001\nthreshold_mode (str): 损失阈值模式，'rel' 或 'abs' 。在 'rel' 模式下，只要相较于上一次的损失减少了相对于上一次损失的 threshold 个百分比即算作损失的减小；在 ‘abs’ 模式下，只有 相较于上一次的损失减少了 threshold 即算作损失的减小。默认为 'rel'\ncooldown (int): 每次降低学习率后，会有一段慢冷期，在该期间内的损失变化不再触发学习率的减小。默认为 0\nmin_lr (float or list): 学习率的下限，可为单一浮点数或浮点数的列表。如果是浮点数，则所有参数组的学习率下限都是这个值，如果是列表，则每个参数组的学习率下限是列表中的对应值。默认为 0\neps (float): 学习率的最小减少值。如果新旧学习率之间的差异小于 eps ，则忽略这次更新。默认为 1e-8",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> optimizer = jt.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n>>> scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10)\n>>> for epoch in range(100):\n...     train(...)\n...     loss = validate(...)\n...     scheduler.step(loss)"
    },
    {
        "api_name": "jittor.nn.reflect_coordinates",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.nn.html#jittor.nn.reflect_coordinates",
        "api_signature": "jittor.nn.reflect_coordinates(x, twice_low, twice_high)",
        "api_description": "对输入坐标 x 进行反射操作。\n首先将输入 x 减去 twice_low / 2 ，使其落在零和 twice_high - twice_low 的范围内，然后使用绝对值，取余数和取反操作来反射。\n\\[ \\begin{align}\\begin{aligned}\\begin{array}{ll}\nm  = \\frac{twice\\_low}{2} \\\\    span = \\frac{twice\\_high - twice\\_low}{2} \\\\    flips = \\left\\lfloor \\frac{x - m}{span} \\right\\rfloor \\\\    result1 = |(x - m) \\mod span| + m \\\\    result2 = span - |(x - m) \\mod span| + m\n\\end{array}\\end{aligned}\\end{align} \\]",
        "return_value": "反射后的坐标(Var):其形状与输入 x 相同。",
        "parameters": "x (Var)：需要进行反射操作的坐标\ntwice_low (float)：反射区间两倍低点的值\ntwice_high (float)：反射区间两倍高点的值",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> x = jt.array([1, 2, 3, 4, 5])\n>>> twice_low = 2\n>>> twice_high = 8\n>>> reflected_x = reflect_coordinates(x, twice_low, twice_high)\n>>> print(reflected_x)  \njt.Var([1. 2. 3. 4. 3.], dtype=float32)"
    },
    {
        "api_name": "jittor.nn.ReflectionPad2d",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.nn.html#jittor.nn.ReflectionPad2d",
        "api_signature": "jittor.nn.ReflectionPad2d(padding)",
        "api_description": "使用输入边界的反射来填充输入张量。\n输入为 \\((N, C, H_{in}, W_{in})\\)，输出为 \\((N, C, H_{out}, W_{out})\\)，其中:\n\\[\\begin{split}H_{out} = H_{in} + \\text{padding_top} + \\text{padding_bottom}\n\\\\W_{out} = W_{in} + \\text{padding_left} + \\text{padding_right}\\end{split}\\]",
        "return_value": "",
        "parameters": "padding(int, tuple): 填充的大小",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> m = nn.ReflectionPad2d((1, 1, 2, 0))\n>>> input = jt.arange(9,dtype='float32').reshape(1, 1, 3, 3)\n>>> input\njt.Var([[[[0. 1. 2.]\n        [3. 4. 5.]\n        [6. 7. 8.]]]], dtype=float32)\n>>> m(input)\njt.Var([[[[7. 6. 7. 8. 7.]\n        [4. 3. 4. 5. 4.]\n        [1. 0. 1. 2. 1.]\n        [4. 3. 4. 5. 4.]\n        [7. 6. 7. 8. 7.]]]], dtype=float32)"
    },
    {
        "api_name": "jittor.Module.register_backward_hook",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.Module.register_backward_hook",
        "api_signature": "register_backward_hook(func)",
        "api_description": "在此模块的反向传播过程中挂钩输入和输出。\ngrad_input 是本模块原本的梯度输入。grad_output 是梯度输出, 返回值会替代输入的输入梯度。",
        "return_value": "",
        "parameters": "hook(module, grad_input:tuple(jt.Var)\ngrad_output:tuple(jt.Var)) -> tuple(jt.Var) or None",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "jittor.Module.register_forward_hook",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.Module.register_forward_hook",
        "api_signature": "register_forward_hook(func)",
        "api_description": "注册一个前向函数钩子，在Module.execute之后调用。",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "jittor.register_hook",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.register_hook",
        "api_signature": "jittor.register_hook(v, hook)",
        "api_description": "将钩子函数注册到指定变量上。钩子函数将在变量的backward操作结束后被调用，主要用于获取变量的梯度信息。",
        "return_value": "无返回值",
        "parameters": "v (jittor.Var): 需要注册钩子的张量(jittor.Var)\nhook (Callable(jittor.Var, Union{None, jittor.Var})): 用于处理变量梯度的钩子函数，接收Jittor变量作为参数，返回None或Jittor变量。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> x = jt.array([0.0, 0.0])\n>>> y = x * [1, 2]\n>>> y.register_hook(lambda g: g * 2)\n>>> dx = jt.grad(y, x)\n>>> print(dx)\njt.Var([2. 4.], dtype=float32)"
    },
    {
        "api_name": "jittor.Module.register_pre_forward_hook",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.Module.register_pre_forward_hook",
        "api_signature": "register_pre_forward_hook(func)",
        "api_description": "注册一个前向函数钩子，在Module.execute之前调用。\n此函数可以按照如下格式调用:\nhook(module, input_args)",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "jittor_core.Var.reindex",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor_core.Var.reindex",
        "api_signature": "jittor_core.Var.reindex()",
        "api_description": "函数C++定义格式:\njt.Var reindex(jt.Var x,  NanoVector shape,  vector<string>&& indexes,  float64 overflow_value=0,  vector<string>&& overflow_conditions={}, vector<VarHolder*>&& extras={})\n根据特定的索引规则将一个张量中的元素映射到另一个张量中。",
        "return_value": "output (jittor.Var): 输出映射之后的jittor张量.",
        "parameters": "x (jittor.Var): 输入的 jittor 变量.\nshape (Tuple[int]): 输出的形状, 一个整数元组。\nindexes (List[str]): 由C++样式整数表达式构成的数组, 它的长度应该与x的维数相同。\noverflow_value (float， 可选): 溢出值。默认值：0\noverflow_conditions (List[str]): 由C++样式布尔表达式构成的数组, 它的长度可以不同。默认值：空字典，即{}。\nextras (List[jittor.Var]): 用于索引的额外变量。默认值：空字典，即{}。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> # 使用 reindex 操作实现的卷积操作\n>>> def conv(x, w):\n>>>     N, H, W, C = x.shape\n>>>     Kh, Kw, _C, Kc = w.shape\n>>>     assert C == _C\n>>>     xx = jt.reindex(x,shape= [N, H - Kh + 1, W - Kw + 1, Kh, Kw, C, Kc], indexes=[\n>>>         'i0',  # Nid\n>>>         'i1+i3',  # Hid+Khid\n>>>         'i2+i4',  # Wid+KWid\n>>>         'i5',  # Cid\n>>>     ])\n>>>     ww = w.broadcast_var(xx)\n>>>     yy = xx * ww\n>>>     y = yy.sum([3, 4, 5])  # Kh, Kw, C\n>>>     return y, yy"
    },
    {
        "api_name": "jittor_core.Var.reindex_reduce",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor_core.Var.reindex_reduce",
        "api_signature": "jittor_core.Var.reindex_reduce()",
        "api_description": "函数C++定义格式:\njt.Var reindex_reduce(jt.Var y,  String op,  NanoVector shape,  vector<string>&& indexes,  vector<string>&& overflow_conditions={}, vector<VarHolder*>&& extras={})\nreindex_reduce算子是一种many-to-one的映射算子。\n它的运行机制和下面的python伪代码一致:\n# input is y, output is x\nn = len(y.shape)-1\nm = len(shape)-1\nk = len(overflow_conditions)-1\nx = np.zeros(shape, y.dtype)\nx[:] = initial_value(op)\nfor i0 in range(y.shape[0]): # 1-st loop\nfor i1 in range(y.shape[1]): # 2-nd loop\n...... # many loops\nfor in in range(y.shape[n]) # n+1 -th loop\n# indexes[i] is a c++ style integer expression consisting of i0,i1,...,in\nxi0,xi1,...,xim = indexes[0],indexes[1],...,indexes[m]\nif not is_overflow(xi0,xi1,...,xim):\nx[xi0,xi1,...,xim] = op(x[xi0,xi1,...,xim], y[i0,i1,...,in])\n# is_overflow is defined as following\ndef is_overflow(xi0,xi1,...,xim):\nreturn (\nxi0 < 0 || xi0 >= shape[0] ||\nxi1 < 0 || xi1 >= shape[1] ||\n......\nxim < 0 || xim >= shape[m] ||\n# overflow_conditions[i] is a c++ style boolean expression consisting of i0,i1,...,in\noverflow_conditions[0] ||\noverflow_conditions[1] ||\n......\noverflow_conditions[k]\n)\n参数:\ny: 输入Jittor变量\nop:  一个代表reduce操作类型的str。\nshape:  输出形状。\nindexes: C++风格的数组表达式。它的长度需要和输出形状一致。一些内建变量如下所示:\nXDIM, xshape0, ..., xshapem, xstride0, ..., xstridem\nYDIM, yshape0, ..., yshapen, ystride0, ..., ystriden\ni0, i1, ..., in\n@e0(...), @e1(...) for extras input index\ne0p, e1p , ... for extras input pointer\noverflow_conditions: C++风格的数组表达式，用于限制什么时候进行映射操作。它长度随意，内建变量同上。\nextras:  用于索引的额外变量，注意需要用 List 形式传入。\n示例代码:\n使用此算子实现pool操作:\ndef pool(x, size, op):\nN,H,W,C = x.shape\nh = (H+size-1)//size\nw = (W+size-1)//size\nreturn x.reindex_reduce(op, [N,h,w,C], [\n\"i0\", # Nid\nf\"i1/{size}\", # Hid\nf\"i2/{size}\", # Wid\n\"i3\", # Cid\n])",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "jittor.nn.relu",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.nn.html#jittor.nn.relu",
        "api_signature": "jittor.nn.relu(x)",
        "api_description": "该函数为Jittor的ReLU激活函数（修正线性单元），在神经网络中应用广泛。ReLU函数在输入值 x > 0 时，返回 x；当输入值 x <= 0 时，返回 0，即 \\(\\text{ReLU}(x) = \\max(0,x)\\)\n参数：\nx (Var) : 输入的Var张量\n返回值：Var: 输入的张量x应用ReLU激活的结果\n代码示例：>>> import jittor as jt\n>>> from jittor import nn\n>>> x = jt.array([0.5, -0.5, -0.7])\n>>> nn.relu(x)\njt.Var([0.5 0.  0. ], dtype=float32)",
        "return_value": "Var: 输入的张量x应用ReLU激活的结果",
        "parameters": "x (Var) : 输入的Var张量",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> from jittor import nn\n>>> x = jt.array([0.5, -0.5, -0.7])\n>>> nn.relu(x)\njt.Var([0.5 0.  0. ], dtype=float32)"
    },
    {
        "api_name": "jittor.nn.relu6",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.nn.html#jittor.nn.relu6",
        "api_signature": "jittor.nn.relu6(x)",
        "api_description": "该函数为Jittor的ReLU6激活函数，这是一个元素级别（element-wise）函数。与ReLU函数不同的是，输入值 x >= 6 时，不直接返回 x，而是返回6:\n\\[\\text{ReLU6}(x) = \\min(\\max(0,x), 6)\\]\n参数：\nx(Var): 输入的Var张量\n返回值：Var张量。张量x应用ReLU6激活的结果，每个元素的值在0和6之间\n代码示例：>>> import jittor as jt\n>>> from jittor import nn\n>>> x = jt.array([0.5, 6.5, -0.7])\n>>> nn.relu6(x)\njt.Var([0.5 6.  0. ], dtype=float32)",
        "return_value": "Var张量。张量x应用ReLU6激活的结果，每个元素的值在0和6之间",
        "parameters": "x(Var): 输入的Var张量",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> from jittor import nn\n>>> x = jt.array([0.5, 6.5, -0.7])\n>>> nn.relu6(x)\njt.Var([0.5 6.  0. ], dtype=float32)"
    },
    {
        "api_name": "jittor.init.relu_invariant_gauss",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.init.html#jittor.init.relu_invariant_gauss",
        "api_signature": "jittor.init.relu_invariant_gauss(shape, dtype='float32', mode='fan_in')",
        "api_description": "返回由 relu_invariant_gauss 初始化的 Var。",
        "return_value": "由 relu_invariant_gauss 初始化的 Jittor Var",
        "parameters": "shape (int or Tuple[int]): 输出Var的形状\ndtype (str): 输出Var的 dtype ，默认 float32\nmode (str): 模式选择，应为 fan_in 或 fan_out。选择 'fan_in' 保留正向传递中权重方差的大小。选择 'fan_out' 保留反向传递中的大小。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> from jittor import init\n>>> from jittor import nn\n>>> a = init.relu_invariant_gauss((2,2))\n>>> print(a)\njt.Var([[ 0.30814755 -0.1328245 ]\n                [-0.10410424 -0.01558159]], dtype=float32)"
    },
    {
        "api_name": "jittor.init.relu_invariant_gauss_",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.init.html#jittor.init.relu_invariant_gauss_",
        "api_signature": "jittor.init.relu_invariant_gauss_(var, mode='fan_in')",
        "api_description": "用随机的 relu 不变高斯初始化 Var。",
        "return_value": "由随机 relu 不变高斯初始化的Var",
        "parameters": "var (Var): 要用随机 relu 不变高斯初始化的Var\nmode (str): 模式选择，应为 fan_in 或 fan_out。选择 'fan_in' 保留正向传递中权重方差的大小。选择 'fan_out' 保留反向传递中的大小。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> from jittor import init\n>>> from jittor import nn\n>>> linear = nn.Linear(2,2)\n>>> init.relu_invariant_gauss_(linear.weight)\n>>> print(linear.weight)\njt.Var([[ 0.74033755 -0.74033755]\n                [-0.74033755  0.74033755]], dtype=float32)\n>>> linear.weight.relu_invariant_gauss_() # 这样也可以"
    },
    {
        "api_name": "jittor.weightnorm.WeightNorm.remove",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.weightnorm.html#jittor.weightnorm.WeightNorm.remove",
        "api_signature": "remove(module: Module)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "jittor.Module.remove_backward_hook",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.Module.remove_backward_hook",
        "api_signature": "remove_backward_hook()",
        "api_description": "删除反向传播的输入和输出hook",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "jittor.Module.remove_forward_hook",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.Module.remove_forward_hook",
        "api_signature": "remove_forward_hook()",
        "api_description": "移除当前的前向钩子。",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "jittor.Module.remove_pre_forward_hook",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.Module.remove_pre_forward_hook",
        "api_signature": "remove_pre_forward_hook()",
        "api_description": "移除当前的预前向钩子。",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "jittor.weightnorm.remove_weight_norm",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.weightnorm.html#jittor.weightnorm.remove_weight_norm",
        "api_signature": "jittor.weightnorm.remove_weight_norm(module, name: str = 'weight')",
        "api_description": "移除模块的权重归一化。该函数通过检查模块是否有权重归一化相关的属性 __fhook2__ 来判断是否对该模块进行过权重归一化操作。如果存在，则删除该属性，用于消除对模块的权重归一化操作。\n参数:\nmodule (Module): 需要移除权重归一化的模块\nname (str, 可选): 权重属性的名称. 默认值： ‘weight’\n返回值:若模块存在权重归一化属性，返回删除权重归一化属性后的模块。如果模块中没有找到指定的权重归一化属性，会抛出ValueError。\n代码示例：>>> from jittor import weightnorm\n>>> from jittor import nn\n>>> model = nn.Linear(20, 40)\n>>> model = weightnorm.weight_norm(model, 'weight', -1)\n>>> hasattr(model,\"__fhook2__\")\nTrue\n>>> model = weightnorm.remove_weight_norm(model, 'weight')\n>>> hasattr(model,\"__fhook2__\")\nFalse",
        "return_value": "若模块存在权重归一化属性，返回删除权重归一化属性后的模块。如果模块中没有找到指定的权重归一化属性，会抛出ValueError。",
        "parameters": "module (Module): 需要移除权重归一化的模块\nname (str, 可选): 权重属性的名称. 默认值： ‘weight’",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> from jittor import weightnorm\n>>> from jittor import nn\n>>> model = nn.Linear(20, 40)\n>>> model = weightnorm.weight_norm(model, 'weight', -1)\n>>> hasattr(model,\"__fhook2__\")\nTrue\n>>> model = weightnorm.remove_weight_norm(model, 'weight')\n>>> hasattr(model,\"__fhook2__\")\nFalse"
    },
    {
        "api_name": "jittor.einops.repeat",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.einops.html#jittor.einops.repeat",
        "api_signature": "jittor.einops.repeat(tensor: Tensor, pattern: str, **axes_lengths)",
        "api_description": "以任意组合的方式重新排序和重复元素。该操作包括 repeat、tile 和 broadcast 函数的功能。\n参数:\ntensor (Union[Var, List[Var]]): 支持的任何库（例如 numpy.ndarray, jittor.Var）的张量，或相同类型和形状的张量列表。\npattern (str): 重排模式的字符串描述。\naxes_lengths: 维度的额外规格说明。\n返回值:\n返回与输入相同类型的张量。如果可能，返回指向原始张量的视图。\n代码示例:>>> from jittor import einops\n>>> import numpy as np\n# 灰度图像（30x40）\n>>> image = np.random.randn(30, 40)\n# 转换为 RGB 格式\n>>> einops.repeat(image, 'h w -> h w c', c=3).shape\n(30, 40, 3)\n# 沿高度轴重复 2 次\n>>> einops.repeat(image, 'h w -> (repeat h) w', repeat=2).shape\n(60, 40)\n# 沿高度和宽度分别重复 2 次和 3 次\n>>> einops.repeat(image, 'h w -> (h2 h) (w3 w)', h2=2, w3=3).shape\n(60, 120)\n# 放大图像 2 倍\n>>> einops.repeat(image, 'h w -> (h h2) (w w2)', h2=2, w2=2).shape\n(60, 80)\n# 缩小后放大图像\n>>> downsampled = einops.reduce(image, '(h h2) (w w2) -> h w', 'mean', h2=2, w2=2)\n>>> einops.repeat(downsampled, 'h w -> (h h2) (w w2)', h2=2, w2=2).shape\n(30, 40)",
        "return_value": "返回与输入相同类型的张量。如果可能，返回指向原始张量的视图。",
        "parameters": "tensor (Union[Var, List[Var]]): 支持的任何库（例如 numpy.ndarray, jittor.Var）的张量，或相同类型和形状的张量列表。\npattern (str): 重排模式的字符串描述。\naxes_lengths: 维度的额外规格说明。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> from jittor import einops \n>>> import numpy as np\n# 灰度图像（30x40）\n>>> image = np.random.randn(30, 40)\n# 转换为 RGB 格式\n>>> einops.repeat(image, 'h w -> h w c', c=3).shape\n(30, 40, 3)\n# 沿高度轴重复 2 次\n>>> einops.repeat(image, 'h w -> (repeat h) w', repeat=2).shape\n(60, 40)\n# 沿高度和宽度分别重复 2 次和 3 次\n>>> einops.repeat(image, 'h w -> (h2 h) (w3 w)', h2=2, w3=3).shape\n(60, 120)\n# 放大图像 2 倍\n>>> einops.repeat(image, 'h w -> (h h2) (w w2)', h2=2, w2=2).shape\n(60, 80)\n# 缩小后放大图像\n>>> downsampled = einops.reduce(image, '(h h2) (w w2) -> h w', 'mean', h2=2, w2=2)\n>>> einops.repeat(downsampled, 'h w -> (h h2) (w w2)', h2=2, w2=2).shape\n(30, 40)"
    },
    {
        "api_name": "jittor.misc.repeat",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.misc.repeat",
        "api_signature": "jittor.misc.repeat(x, *shape)",
        "api_description": "在指定维度上重复张量。\n参数:\nx (Var): 输入张量\nshape (tuple[int]): 重复的次数，可以是一个整数，也可以是一个元组。如果是一个整数，则表示在所有维度上重复相同次数；如果是一个元组，则表示在每个维度上重复的次数。\n代码示例:\n>>> x = jt.array([1, 2, 3])\n>>> x.repeat(4, 2)\njt.Var([[1 2 3 1 2 3]\n[1 2 3 1 2 3]\n[1 2 3 1 2 3]\n[1 2 3 1 2 3]], dtype=int32)\n>>> x.repeat(4, 2, 1).size()\n[4, 2, 3,]",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "jittor.misc.repeat_interleave",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.misc.repeat_interleave",
        "api_signature": "jittor.misc.repeat_interleave(x, repeats, dim=None)",
        "api_description": "将张量在某个维度上每个元素重复多次",
        "return_value": "x 在 dim 维度每个元素重复 repeat 次得到的向量，形状 \\((n_0, \\ldots, n_{\\mathtt{dim}} \\times \\mathtt{repeats}, \\ldots, n_k)\\)",
        "parameters": "x (Var): 被重复的张量，形状 \\((n_0, \\ldots, n_{\\mathtt{dim}}, \\ldots, n_k)\\)\nrepeats (int): 每个元素重复的次数\ndim (int): 在哪个维度上重复，负数表示从后往前数，默认为将向量扁平化后重复",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "jittor.nn.ReplicationPad2d",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.nn.html#jittor.nn.ReplicationPad2d",
        "api_signature": "jittor.nn.ReplicationPad2d(padding)",
        "api_description": "使用输入边界的复制来填充输入张量。\n输入为 \\((N, C, H_{in}, W_{in})\\)，输出为 \\((N, C, H_{out}, W_{out})\\)，其中:\n\\[\\begin{split}H_{out} = H_{in} + \\text{padding_top} + \\text{padding_bottom}\n\\\\W_{out} = W_{in} + \\text{padding_left} + \\text{padding_right}\\end{split}\\]",
        "return_value": "",
        "parameters": "padding(int, tuple): 填充的大小",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> m = nn.ReplicationPad2d((1, 1, 2, 0))\n>>> input = jt.arange(9,dtype='float32').reshape(1, 1, 3, 3)\n>>> input\njt.Var([[[[0. 1. 2.]\n        [3. 4. 5.]\n        [6. 7. 8.]]]], dtype=float32)\n>>> m(input)\njt.Var([[[[0. 0. 1. 2. 2.]\n        [0. 0. 1. 2. 2.]\n        [0. 0. 1. 2. 2.]\n        [3. 3. 4. 5. 5.]\n        [6. 6. 7. 8. 8.]]]], dtype=float32)"
    },
    {
        "api_name": "jittor.Module.requires_grad_",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.Module.requires_grad_",
        "api_signature": "requires_grad_(requires_grad=True)",
        "api_description": "为所有参数和子模块设置 requires_grad。",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "jittor.models.res2net101",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.models.html#jittor.models.res2net101",
        "api_signature": "jittor.models.res2net101(pretrained=False, **kwargs)",
        "api_description": "构建一个Res2Net101_26w_4s模型\nRes2Net101_26w_4s源自论文 Res2Net: A New Multi-scale Backbone Architecture, 是ResNet模型的一种变体。\n参数:\npretrained (bool, optional): 如果为 True, 返回一个在ImageNet上预训练的模型。默认为 False。\n**kwargs (dict, optional): 用于传递额外的关键字参数。\n返回值:\n返回构建好的Res2Net101_26w_4s模型实例。如果 pretrained 为 True, 则返回在ImageNet上预训练的模型。\n代码示例:>>> import jittor as jt\n>>> from jittor.models.res2net import *\n>>> net = res2net101_26w_4s(pretrained=True)\n>>> x = jt.rand(1, 3, 224, 224)\n>>> y = net(x)\n>>> y.shape\n[1, 1000]",
        "return_value": "返回构建好的Res2Net101_26w_4s模型实例。如果 pretrained 为 True, 则返回在ImageNet上预训练的模型。",
        "parameters": "pretrained (bool, optional): 如果为 True, 返回一个在ImageNet上预训练的模型。默认为 False。\n**kwargs (dict, optional): 用于传递额外的关键字参数。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> from jittor.models.res2net import *\n>>> net = res2net101_26w_4s(pretrained=True)\n>>> x = jt.rand(1, 3, 224, 224)\n>>> y = net(x)\n>>> y.shape\n[1, 1000]"
    },
    {
        "api_name": "jittor.models.res2net50",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.models.html#jittor.models.res2net50",
        "api_signature": "jittor.models.res2net50(pretrained=False, **kwargs)",
        "api_description": "构建一个Res2Net50模型\nRes2Net50源自论文 Res2Net: A New Multi-scale Backbone Architecture, 是ResNet模型的一种变体。\n参数:\npretrained (bool, optional): 如果为 True, 返回一个在ImageNet上预训练的模型。默认为 False。\n**kwargs (dict, optional): 用于传递额外的关键字参数。\n返回值:\n返回构建好的Res2Net模型实例。如果 pretrained 为 True, 则返回在ImageNet上预训练的模型。\n代码示例:>>> import jittor as jt\n>>> from jittor.models.res2net import *\n>>> net = res2net50(pretrained=True)\n>>> x = jt.rand(1, 3, 224, 224)\n>>> y = net(x)\n>>> y.shape\n[1, 1000]",
        "return_value": "返回构建好的Res2Net模型实例。如果 pretrained 为 True, 则返回在ImageNet上预训练的模型。",
        "parameters": "pretrained (bool, optional): 如果为 True, 返回一个在ImageNet上预训练的模型。默认为 False。\n**kwargs (dict, optional): 用于传递额外的关键字参数。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> from jittor.models.res2net import *\n>>> net = res2net50(pretrained=True)\n>>> x = jt.rand(1, 3, 224, 224)\n>>> y = net(x)\n>>> y.shape\n[1, 1000]"
    },
    {
        "api_name": "jittor.attention.MultiheadAttention.reset_parameters",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.attention.html#jittor.attention.MultiheadAttention.reset_parameters",
        "api_signature": "reset_parameters()",
        "api_description": "初始化参数\n代码示例:>>> multihead_attn = jt.attention.MultiheadAttention(embed_dim, num_heads)\n>>> multihead_attn.reset_parameters()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> multihead_attn = jt.attention.MultiheadAttention(embed_dim, num_heads)\n>>> multihead_attn.reset_parameters()"
    },
    {
        "api_name": "jittor.reshape",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.reshape",
        "api_signature": "jittor.reshape(x, *shape)",
        "api_description": "函数C++定义格式:\njt.Var reshape(jt.Var x,  NanoVector shape)\n将输入的张量根据指定的形状进行重塑，并返回这个新的张量。保持元素的数据和数量不变。其中一维可以是-1，在这种情况下，它是根据其余的维度和输入的元素数量推断出来的。",
        "return_value": "重塑过后的张量（jt.Var）",
        "parameters": "x(jt.Var): 输入的张量。\nshape(Tuple[int]): 输出的形状,一个整数列表。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> jt.randint(0, 10, shape=(12,))\n    jt.Var([4 0 8 4 6 3 1 8 1 1 2 2], dtype=int32)\n>>> jt.reshape(a, (3, 4))\n    jt.Var([[4 0 8 4]\n        [6 3 1 8]\n        [1 1 2 2]], dtype=int32)\n>>> jt.reshape(a, (-1, 6))\njt.Var([[4 0 8 4 6 3]\n        [1 8 1 1 2 2]], dtype=int32)"
    },
    {
        "api_name": "jittor.nn.resize",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.nn.html#jittor.nn.resize",
        "api_signature": "jittor.nn.resize(img, size, mode='nearest', align_corners=False, tf_mode=False)",
        "api_description": "根据设定的模式(mode)对给定的图像进行大小调整。",
        "return_value": "output(Var): 调整后的图像张量，形状为 \\((N, C, size[0], size[1])\\)",
        "parameters": "img (Var): 输入图像张量，形状为 \\((N, C, H, W)\\)\nsize (Union[int, Tuple[int, int]]): 输出图像的大小，可以是整数或者整数元组\nmode (str): 插值模式，可选 ‘nearest’ (默认), ‘bicubic’, ‘area’,’bilinear’\nalign_corners (bool): 默认为False, 如果设置为 True，输入和输出张量通过其角像素的中心点对齐，保留角像素处的值。如果设置为 False，输入和输出张量通过其角像素的角点对齐，插值使用边缘值填充来处理边界外的值。\ntf_mode (bool): 默认为False",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> x = jt.randn(4,3,32,32)\n>>> output_size = (64, 64)\n>>> jt.nn.resize(x, output_size).shape\n[4, 3, 64, 64]"
    },
    {
        "api_name": "jittor.transform.resize",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.transform.html#jittor.transform.resize",
        "api_signature": "jittor.transform.resize(img, size, interpolation=2)",
        "api_description": "调整图像的大小。",
        "return_value": "PIL.Image.Image: 调整后的图像",
        "parameters": "img (PIL.Image.Image): 输入图像。\nsize (list): 调整后的大小，格式为 [高, 宽]\ninterpolation (int，optional): 调整大小的插值方法类型。默认值: PIL.Image.BILINEAR",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> import numpy as np\n>>> from PIL import Image\n>>> data = np.random.rand(3,4)\n>>> img = Image.fromarray(data, 'L')                              \n>>> img.size\n(3, 4)\n>>> jt.transform.resize(img, [2,3]).size  \n(3, 2)"
    },
    {
        "api_name": "jittor.nn.Resize",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.nn.html#jittor.nn.Resize",
        "api_signature": "jittor.nn.Resize(size, mode='nearest', align_corners=False)",
        "api_description": "对输入张量执行resize函数。",
        "return_value": "",
        "parameters": "size(tuple): 输出尺寸\nmode(str): 插值模式，可选值为：nearest、linear、bilinear,默认为nearest\nalign_corners(bool): 是否对齐角点，默认为False",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> m = nn.Resize((16,16))\n>>> input = jt.rand(2,3,32,32)\n>>> m(input).shape\n[2, 3, 16, 16]"
    },
    {
        "api_name": "jittor.transform.Resize",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.transform.html#jittor.transform.Resize",
        "api_signature": "jittor.transform.Resize(size, mode=2)",
        "api_description": "调整图像尺寸的类。",
        "return_value": "",
        "parameters": "size (int或tuple): 想要调整到的目标尺寸。\nmode (int, 可选): 调整尺寸时采用的插值方式。默认为 Image.BILINEAR，即双线性插值。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> from jittor import transform\n>>> resize_transform = transform.Resize(224)\n>>> img_ = resize_transform(img)"
    },
    {
        "api_name": "jittor.models.Resnet101",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.models.html#jittor.models.Resnet101",
        "api_signature": "jittor.models.Resnet101(pretrained=False, **kwargs)",
        "api_description": "构建ResNet101模型\nResNet101模型源自论文 Deep Residual Learning for Image Recognition, 其通过添加shortcut来改善梯度回传, 允许更深层次的网络有效训练。\n参数:\npretrained (bool): 表示是否加载预训练的ResNet101模型。默认为 False。如果设为 True, 函数会自动下载并加载预训练的ResNet101模型。\n**kwargs: 可变参数, 允许用户传递额外的、自定义的参数给 _resnet 函数。\n返回值:\n返回一个ResNet101模型实例。如果 pretrained=True, 则返回的模型将加载预训练权重；否则, 返回一个未经训练的ResNet101模型。\n代码示例:>>> import jittor as jt\n>>> from jittor.models.resnet import *\n>>> net = ResNet101(pretrained=True)\n>>> x = jt.rand(1, 3, 224, 224)\n>>> y = net(x)\n>>> y.shape\n[1, 1000]",
        "return_value": "返回一个ResNet101模型实例。如果 pretrained=True, 则返回的模型将加载预训练权重；否则, 返回一个未经训练的ResNet101模型。",
        "parameters": "pretrained (bool): 表示是否加载预训练的ResNet101模型。默认为 False。如果设为 True, 函数会自动下载并加载预训练的ResNet101模型。\n**kwargs: 可变参数, 允许用户传递额外的、自定义的参数给 _resnet 函数。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> from jittor.models.resnet import *\n>>> net = ResNet101(pretrained=True)\n>>> x = jt.rand(1, 3, 224, 224)\n>>> y = net(x)\n>>> y.shape\n[1, 1000]"
    },
    {
        "api_name": "jittor.models.resnet101",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.models.html#jittor.models.resnet101",
        "api_signature": "jittor.models.resnet101(pretrained=False, **kwargs)",
        "api_description": "构建ResNet101模型\nResNet101模型源自论文 Deep Residual Learning for Image Recognition, 其通过添加shortcut来改善梯度回传, 允许更深层次的网络有效训练。\n参数:\npretrained (bool): 表示是否加载预训练的ResNet101模型。默认为 False。如果设为 True, 函数会自动下载并加载预训练的ResNet101模型。\n**kwargs: 可变参数, 允许用户传递额外的、自定义的参数给 _resnet 函数。\n返回值:\n返回一个ResNet101模型实例。如果 pretrained=True, 则返回的模型将加载预训练权重；否则, 返回一个未经训练的ResNet101模型。\n代码示例:>>> import jittor as jt\n>>> from jittor.models.resnet import *\n>>> net = ResNet101(pretrained=True)\n>>> x = jt.rand(1, 3, 224, 224)\n>>> y = net(x)\n>>> y.shape\n[1, 1000]",
        "return_value": "返回一个ResNet101模型实例。如果 pretrained=True, 则返回的模型将加载预训练权重；否则, 返回一个未经训练的ResNet101模型。",
        "parameters": "pretrained (bool): 表示是否加载预训练的ResNet101模型。默认为 False。如果设为 True, 函数会自动下载并加载预训练的ResNet101模型。\n**kwargs: 可变参数, 允许用户传递额外的、自定义的参数给 _resnet 函数。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> from jittor.models.resnet import *\n>>> net = ResNet101(pretrained=True)\n>>> x = jt.rand(1, 3, 224, 224)\n>>> y = net(x)\n>>> y.shape\n[1, 1000]"
    },
    {
        "api_name": "jittor.models.Resnet152",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.models.html#jittor.models.Resnet152",
        "api_signature": "jittor.models.Resnet152(pretrained=False, **kwargs)",
        "api_description": "构建ResNet152模型\nResNet152模型源自论文 Deep Residual Learning for Image Recognition, 其通过添加shortcut来改善梯度回传, 允许更深层次的网络有效训练。\n参数:\npretrained (bool): 表示是否加载预训练的ResNet152模型。默认为 False。如果设为 True, 函数会自动下载并加载预训练的ResNet152模型。\n**kwargs: 可变参数, 允许用户传递额外的、自定义的参数给 _resnet 函数。\n返回值:\n返回一个ResNet152模型实例。如果 pretrained=True, 则返回的模型将加载预训练权重；否则, 返回一个未经训练的ResNet152模型。\n代码示例:>>> import jittor as jt\n>>> from jittor.models.resnet import *\n>>> net = ResNet152(pretrained=True)\n>>> x = jt.rand(1, 3, 224, 224)\n>>> y = net(x)\n>>> y.shape\n[1, 1000]",
        "return_value": "返回一个ResNet152模型实例。如果 pretrained=True, 则返回的模型将加载预训练权重；否则, 返回一个未经训练的ResNet152模型。",
        "parameters": "pretrained (bool): 表示是否加载预训练的ResNet152模型。默认为 False。如果设为 True, 函数会自动下载并加载预训练的ResNet152模型。\n**kwargs: 可变参数, 允许用户传递额外的、自定义的参数给 _resnet 函数。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> from jittor.models.resnet import *\n>>> net = ResNet152(pretrained=True)\n>>> x = jt.rand(1, 3, 224, 224)\n>>> y = net(x)\n>>> y.shape\n[1, 1000]"
    },
    {
        "api_name": "jittor.models.resnet152",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.models.html#jittor.models.resnet152",
        "api_signature": "jittor.models.resnet152(pretrained=False, **kwargs)",
        "api_description": "构建ResNet152模型\nResNet152模型源自论文 Deep Residual Learning for Image Recognition, 其通过添加shortcut来改善梯度回传, 允许更深层次的网络有效训练。\n参数:\npretrained (bool): 表示是否加载预训练的ResNet152模型。默认为 False。如果设为 True, 函数会自动下载并加载预训练的ResNet152模型。\n**kwargs: 可变参数, 允许用户传递额外的、自定义的参数给 _resnet 函数。\n返回值:\n返回一个ResNet152模型实例。如果 pretrained=True, 则返回的模型将加载预训练权重；否则, 返回一个未经训练的ResNet152模型。\n代码示例:>>> import jittor as jt\n>>> from jittor.models.resnet import *\n>>> net = ResNet152(pretrained=True)\n>>> x = jt.rand(1, 3, 224, 224)\n>>> y = net(x)\n>>> y.shape\n[1, 1000]",
        "return_value": "返回一个ResNet152模型实例。如果 pretrained=True, 则返回的模型将加载预训练权重；否则, 返回一个未经训练的ResNet152模型。",
        "parameters": "pretrained (bool): 表示是否加载预训练的ResNet152模型。默认为 False。如果设为 True, 函数会自动下载并加载预训练的ResNet152模型。\n**kwargs: 可变参数, 允许用户传递额外的、自定义的参数给 _resnet 函数。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> from jittor.models.resnet import *\n>>> net = ResNet152(pretrained=True)\n>>> x = jt.rand(1, 3, 224, 224)\n>>> y = net(x)\n>>> y.shape\n[1, 1000]"
    },
    {
        "api_name": "jittor.models.Resnet18",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.models.html#jittor.models.Resnet18",
        "api_signature": "jittor.models.Resnet18(pretrained=False, **kwargs)",
        "api_description": "构建Resnet18模型\nResNet18模型源自论文 Deep Residual Learning for Image Recognition, 其通过添加shortcut来改善梯度回传, 允许更深层次的网络有效训练。\n参数:\npretrained (bool): 表示是否加载预训练的ResNet18模型。默认为 False。如果设为 True, 函数会自动下载并加载预训练的ResNet18模型。\n**kwargs: 可变参数, 允许用户传递额外的、自定义的参数给 _resnet 函数。\n返回值:\n返回一个ResNet18模型实例。如果 pretrained=True, 则返回的模型将加载预训练权重；否则, 返回一个未经训练的ResNet18模型。\n代码示例:>>> import jittor as jt\n>>> from jittor.models.resnet import *\n>>> net = Resnet18(pretrained=True)\n>>> x = jt.rand(1, 3, 224, 224)\n>>> y = net(x)\n>>> y.shape\n[1, 1000]",
        "return_value": "返回一个ResNet18模型实例。如果 pretrained=True, 则返回的模型将加载预训练权重；否则, 返回一个未经训练的ResNet18模型。",
        "parameters": "pretrained (bool): 表示是否加载预训练的ResNet18模型。默认为 False。如果设为 True, 函数会自动下载并加载预训练的ResNet18模型。\n**kwargs: 可变参数, 允许用户传递额外的、自定义的参数给 _resnet 函数。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> from jittor.models.resnet import *\n>>> net = Resnet18(pretrained=True)\n>>> x = jt.rand(1, 3, 224, 224)\n>>> y = net(x)\n>>> y.shape\n[1, 1000]"
    },
    {
        "api_name": "jittor.models.resnet18",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.models.html#jittor.models.resnet18",
        "api_signature": "jittor.models.resnet18(pretrained=False, **kwargs)",
        "api_description": "构建Resnet18模型\nResNet18模型源自论文 Deep Residual Learning for Image Recognition, 其通过添加shortcut来改善梯度回传, 允许更深层次的网络有效训练。\n参数:\npretrained (bool): 表示是否加载预训练的ResNet18模型。默认为 False。如果设为 True, 函数会自动下载并加载预训练的ResNet18模型。\n**kwargs: 可变参数, 允许用户传递额外的、自定义的参数给 _resnet 函数。\n返回值:\n返回一个ResNet18模型实例。如果 pretrained=True, 则返回的模型将加载预训练权重；否则, 返回一个未经训练的ResNet18模型。\n代码示例:>>> import jittor as jt\n>>> from jittor.models.resnet import *\n>>> net = Resnet18(pretrained=True)\n>>> x = jt.rand(1, 3, 224, 224)\n>>> y = net(x)\n>>> y.shape\n[1, 1000]",
        "return_value": "返回一个ResNet18模型实例。如果 pretrained=True, 则返回的模型将加载预训练权重；否则, 返回一个未经训练的ResNet18模型。",
        "parameters": "pretrained (bool): 表示是否加载预训练的ResNet18模型。默认为 False。如果设为 True, 函数会自动下载并加载预训练的ResNet18模型。\n**kwargs: 可变参数, 允许用户传递额外的、自定义的参数给 _resnet 函数。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> from jittor.models.resnet import *\n>>> net = Resnet18(pretrained=True)\n>>> x = jt.rand(1, 3, 224, 224)\n>>> y = net(x)\n>>> y.shape\n[1, 1000]"
    },
    {
        "api_name": "jittor.models.Resnet26",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.models.html#jittor.models.Resnet26",
        "api_signature": "jittor.models.Resnet26(pretrained=False, **kwargs)",
        "api_description": "构建ResNet26模型\nResNet26模型源自论文 Deep Residual Learning for Image Recognition, 其通过添加shortcut来改善梯度回传, 允许更深层次的网络有效训练。\n参数:\npretrained (bool): 表示是否加载预训练的ResNet26模型。默认为 False。如果设为 True, 函数会自动下载并加载预训练的ResNet26模型。\n**kwargs: 可变参数, 允许用户传递额外的、自定义的参数给 _resnet 函数。\n返回值:\n返回一个ResNet26模型实例。如果 pretrained=True, 则返回的模型将加载预训练权重；否则, 返回一个未经训练的ResNet26模型。\n代码示例:>>> import jittor as jt\n>>> from jittor.models.resnet import *\n>>> net = ResNet26(pretrained=True)\n>>> x = jt.rand(1, 3, 224, 224)\n>>> y = net(x)\n>>> y.shape\n[1, 1000]",
        "return_value": "返回一个ResNet26模型实例。如果 pretrained=True, 则返回的模型将加载预训练权重；否则, 返回一个未经训练的ResNet26模型。",
        "parameters": "pretrained (bool): 表示是否加载预训练的ResNet26模型。默认为 False。如果设为 True, 函数会自动下载并加载预训练的ResNet26模型。\n**kwargs: 可变参数, 允许用户传递额外的、自定义的参数给 _resnet 函数。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> from jittor.models.resnet import *\n>>> net = ResNet26(pretrained=True)\n>>> x = jt.rand(1, 3, 224, 224)\n>>> y = net(x)\n>>> y.shape\n[1, 1000]"
    },
    {
        "api_name": "jittor.models.resnet26",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.models.html#jittor.models.resnet26",
        "api_signature": "jittor.models.resnet26(pretrained=False, **kwargs)",
        "api_description": "构建ResNet26模型\nResNet26模型源自论文 Deep Residual Learning for Image Recognition, 其通过添加shortcut来改善梯度回传, 允许更深层次的网络有效训练。\n参数:\npretrained (bool): 表示是否加载预训练的ResNet26模型。默认为 False。如果设为 True, 函数会自动下载并加载预训练的ResNet26模型。\n**kwargs: 可变参数, 允许用户传递额外的、自定义的参数给 _resnet 函数。\n返回值:\n返回一个ResNet26模型实例。如果 pretrained=True, 则返回的模型将加载预训练权重；否则, 返回一个未经训练的ResNet26模型。\n代码示例:>>> import jittor as jt\n>>> from jittor.models.resnet import *\n>>> net = ResNet26(pretrained=True)\n>>> x = jt.rand(1, 3, 224, 224)\n>>> y = net(x)\n>>> y.shape\n[1, 1000]",
        "return_value": "返回一个ResNet26模型实例。如果 pretrained=True, 则返回的模型将加载预训练权重；否则, 返回一个未经训练的ResNet26模型。",
        "parameters": "pretrained (bool): 表示是否加载预训练的ResNet26模型。默认为 False。如果设为 True, 函数会自动下载并加载预训练的ResNet26模型。\n**kwargs: 可变参数, 允许用户传递额外的、自定义的参数给 _resnet 函数。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> from jittor.models.resnet import *\n>>> net = ResNet26(pretrained=True)\n>>> x = jt.rand(1, 3, 224, 224)\n>>> y = net(x)\n>>> y.shape\n[1, 1000]"
    },
    {
        "api_name": "jittor.models.Resnet34",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.models.html#jittor.models.Resnet34",
        "api_signature": "jittor.models.Resnet34(pretrained=False, **kwargs)",
        "api_description": "构建Resnet34模型\nResNet34模型源自论文 Deep Residual Learning for Image Recognition, 其通过添加shortcut来改善梯度回传, 允许更深层次的网络有效训练。\n参数:\npretrained (bool): 表示是否加载预训练的ResNet34模型。默认为 False。如果设为 True, 函数会自动下载并加载预训练的ResNet34模型。\n**kwargs: 可变参数, 允许用户传递额外的、自定义的参数给 _resnet 函数。\n返回值:\n返回一个ResNet34模型实例。如果 pretrained=True, 则返回的模型将加载预训练权重；否则, 返回一个未经训练的ResNet34模型。\n代码示例:>>> import jittor as jt\n>>> from jittor.models.resnet import *\n>>> net = Resnet34(pretrained=True)\n>>> x = jt.rand(1, 3, 224, 224)\n>>> y = net(x)\n>>> y.shape\n[1, 1000]",
        "return_value": "返回一个ResNet34模型实例。如果 pretrained=True, 则返回的模型将加载预训练权重；否则, 返回一个未经训练的ResNet34模型。",
        "parameters": "pretrained (bool): 表示是否加载预训练的ResNet34模型。默认为 False。如果设为 True, 函数会自动下载并加载预训练的ResNet34模型。\n**kwargs: 可变参数, 允许用户传递额外的、自定义的参数给 _resnet 函数。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> from jittor.models.resnet import *\n>>> net = Resnet34(pretrained=True)\n>>> x = jt.rand(1, 3, 224, 224)\n>>> y = net(x)\n>>> y.shape\n[1, 1000]"
    },
    {
        "api_name": "jittor.models.resnet34",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.models.html#jittor.models.resnet34",
        "api_signature": "jittor.models.resnet34(pretrained=False, **kwargs)",
        "api_description": "构建Resnet34模型\nResNet34模型源自论文 Deep Residual Learning for Image Recognition, 其通过添加shortcut来改善梯度回传, 允许更深层次的网络有效训练。\n参数:\npretrained (bool): 表示是否加载预训练的ResNet34模型。默认为 False。如果设为 True, 函数会自动下载并加载预训练的ResNet34模型。\n**kwargs: 可变参数, 允许用户传递额外的、自定义的参数给 _resnet 函数。\n返回值:\n返回一个ResNet34模型实例。如果 pretrained=True, 则返回的模型将加载预训练权重；否则, 返回一个未经训练的ResNet34模型。\n代码示例:>>> import jittor as jt\n>>> from jittor.models.resnet import *\n>>> net = Resnet34(pretrained=True)\n>>> x = jt.rand(1, 3, 224, 224)\n>>> y = net(x)\n>>> y.shape\n[1, 1000]",
        "return_value": "返回一个ResNet34模型实例。如果 pretrained=True, 则返回的模型将加载预训练权重；否则, 返回一个未经训练的ResNet34模型。",
        "parameters": "pretrained (bool): 表示是否加载预训练的ResNet34模型。默认为 False。如果设为 True, 函数会自动下载并加载预训练的ResNet34模型。\n**kwargs: 可变参数, 允许用户传递额外的、自定义的参数给 _resnet 函数。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> from jittor.models.resnet import *\n>>> net = Resnet34(pretrained=True)\n>>> x = jt.rand(1, 3, 224, 224)\n>>> y = net(x)\n>>> y.shape\n[1, 1000]"
    },
    {
        "api_name": "jittor.models.Resnet38",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.models.html#jittor.models.Resnet38",
        "api_signature": "jittor.models.Resnet38(pretrained=False, **kwargs)",
        "api_description": "构建ResNet38模型\nResNet38模型源自论文 Deep Residual Learning for Image Recognition, 其通过添加shortcut来改善梯度回传, 允许更深层次的网络有效训练。\n参数:\npretrained (bool): 表示是否加载预训练的ResNet38模型。默认为 False。如果设为 True, 函数会自动下载并加载预训练的ResNet38模型。\n**kwargs: 可变参数, 允许用户传递额外的、自定义的参数给 _resnet 函数。\n返回值:\n返回一个ResNet38模型实例。如果 pretrained=True, 则返回的模型将加载预训练权重；否则, 返回一个未经训练的ResNet38模型。\n代码示例:>>> import jittor as jt\n>>> from jittor.models.resnet import *\n>>> net = ResNet38(pretrained=True)\n>>> x = jt.rand(1, 3, 224, 224)\n>>> y = net(x)\n>>> y.shape\n[1, 1000]",
        "return_value": "返回一个ResNet38模型实例。如果 pretrained=True, 则返回的模型将加载预训练权重；否则, 返回一个未经训练的ResNet38模型。",
        "parameters": "pretrained (bool): 表示是否加载预训练的ResNet38模型。默认为 False。如果设为 True, 函数会自动下载并加载预训练的ResNet38模型。\n**kwargs: 可变参数, 允许用户传递额外的、自定义的参数给 _resnet 函数。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> from jittor.models.resnet import *\n>>> net = ResNet38(pretrained=True)\n>>> x = jt.rand(1, 3, 224, 224)\n>>> y = net(x)\n>>> y.shape\n[1, 1000]"
    },
    {
        "api_name": "jittor.models.resnet38",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.models.html#jittor.models.resnet38",
        "api_signature": "jittor.models.resnet38(pretrained=False, **kwargs)",
        "api_description": "构建ResNet38模型\nResNet38模型源自论文 Deep Residual Learning for Image Recognition, 其通过添加shortcut来改善梯度回传, 允许更深层次的网络有效训练。\n参数:\npretrained (bool): 表示是否加载预训练的ResNet38模型。默认为 False。如果设为 True, 函数会自动下载并加载预训练的ResNet38模型。\n**kwargs: 可变参数, 允许用户传递额外的、自定义的参数给 _resnet 函数。\n返回值:\n返回一个ResNet38模型实例。如果 pretrained=True, 则返回的模型将加载预训练权重；否则, 返回一个未经训练的ResNet38模型。\n代码示例:>>> import jittor as jt\n>>> from jittor.models.resnet import *\n>>> net = ResNet38(pretrained=True)\n>>> x = jt.rand(1, 3, 224, 224)\n>>> y = net(x)\n>>> y.shape\n[1, 1000]",
        "return_value": "返回一个ResNet38模型实例。如果 pretrained=True, 则返回的模型将加载预训练权重；否则, 返回一个未经训练的ResNet38模型。",
        "parameters": "pretrained (bool): 表示是否加载预训练的ResNet38模型。默认为 False。如果设为 True, 函数会自动下载并加载预训练的ResNet38模型。\n**kwargs: 可变参数, 允许用户传递额外的、自定义的参数给 _resnet 函数。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> from jittor.models.resnet import *\n>>> net = ResNet38(pretrained=True)\n>>> x = jt.rand(1, 3, 224, 224)\n>>> y = net(x)\n>>> y.shape\n[1, 1000]"
    },
    {
        "api_name": "jittor.models.Resnet50",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.models.html#jittor.models.Resnet50",
        "api_signature": "jittor.models.Resnet50(pretrained=False, **kwargs)",
        "api_description": "构建Resnet50模型\nResNet50模型源自论文 Deep Residual Learning for Image Recognition, 其通过添加shortcut来改善梯度回传, 允许更深层次的网络有效训练。\n参数:\npretrained (bool): 表示是否加载预训练的ResNet50模型。默认为 False。如果设为 True, 函数会自动下载并加载预训练的ResNet50模型。\n**kwargs: 可变参数, 允许用户传递额外的、自定义的参数给 _resnet 函数。\n返回值:\n返回一个ResNet50模型实例。如果 pretrained=True, 则返回的模型将加载预训练权重；否则, 返回一个未经训练的ResNet50模型。\n代码示例:>>> import jittor as jt\n>>> from jittor.models.resnet import *\n>>> net = Resnet50(pretrained=True)\n>>> x = jt.rand(1, 3, 224, 224)\n>>> y = net(x)\n>>> y.shape\n[1, 1000]",
        "return_value": "返回一个ResNet50模型实例。如果 pretrained=True, 则返回的模型将加载预训练权重；否则, 返回一个未经训练的ResNet50模型。",
        "parameters": "pretrained (bool): 表示是否加载预训练的ResNet50模型。默认为 False。如果设为 True, 函数会自动下载并加载预训练的ResNet50模型。\n**kwargs: 可变参数, 允许用户传递额外的、自定义的参数给 _resnet 函数。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> from jittor.models.resnet import *\n>>> net = Resnet50(pretrained=True)\n>>> x = jt.rand(1, 3, 224, 224)\n>>> y = net(x)\n>>> y.shape\n[1, 1000]"
    },
    {
        "api_name": "jittor.models.resnet50",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.models.html#jittor.models.resnet50",
        "api_signature": "jittor.models.resnet50(pretrained=False, **kwargs)",
        "api_description": "构建Resnet50模型\nResNet50模型源自论文 Deep Residual Learning for Image Recognition, 其通过添加shortcut来改善梯度回传, 允许更深层次的网络有效训练。\n参数:\npretrained (bool): 表示是否加载预训练的ResNet50模型。默认为 False。如果设为 True, 函数会自动下载并加载预训练的ResNet50模型。\n**kwargs: 可变参数, 允许用户传递额外的、自定义的参数给 _resnet 函数。\n返回值:\n返回一个ResNet50模型实例。如果 pretrained=True, 则返回的模型将加载预训练权重；否则, 返回一个未经训练的ResNet50模型。\n代码示例:>>> import jittor as jt\n>>> from jittor.models.resnet import *\n>>> net = Resnet50(pretrained=True)\n>>> x = jt.rand(1, 3, 224, 224)\n>>> y = net(x)\n>>> y.shape\n[1, 1000]",
        "return_value": "返回一个ResNet50模型实例。如果 pretrained=True, 则返回的模型将加载预训练权重；否则, 返回一个未经训练的ResNet50模型。",
        "parameters": "pretrained (bool): 表示是否加载预训练的ResNet50模型。默认为 False。如果设为 True, 函数会自动下载并加载预训练的ResNet50模型。\n**kwargs: 可变参数, 允许用户传递额外的、自定义的参数给 _resnet 函数。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> from jittor.models.resnet import *\n>>> net = Resnet50(pretrained=True)\n>>> x = jt.rand(1, 3, 224, 224)\n>>> y = net(x)\n>>> y.shape\n[1, 1000]"
    },
    {
        "api_name": "jittor.models.ResNet",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.models.html#jittor.models.ResNet",
        "api_signature": "jittor.models.ResNet(block, layers, num_classes=1000, zero_init_residual=False, groups=1, width_per_group=64, replace_stride_with_dilation=None, norm_layer=None)",
        "api_description": "ResNet源自论文 Deep Residual Learning for Image Recognition, 其通过添加shortcut来改善梯度回传, 允许更深层次的网络有效训练。\n参数:\nblock (nn.Module): 残差块的类, 用于建立 ResNet 的不同层。 block 参数通常是一个实现了特定类型残差块结构的类, 比如 ‘BasicBlock’ 或 ‘Bottleneck’。\nlayers (list[int]): 每个 ResNet 层中残差块的数量, 是一个由4个元素组成的列表\nnum_classes (int, optional): 分类任务中的类别数。默认值: 1000\nzero_init_residual (bool, optional): 是否使用零初始化残差块。默认值: False\ngroups (int, optional): 分组卷积的组数量。1意味着不使用分组卷积。默认值: 1\nwidth_per_group (int, optional): 每个分组的宽度。默认值: 64\nreplace_stride_with_dilation (list[bool], optional): 是否用空洞卷积替换某些层的步长。若不为None则应为包含三个布尔值的列表。默认值: None\nnorm_layer (nn.Module, optional): 使用的标准化层, 默认使用 ‘nn.BatchNorm’。默认值: None\n属性:\nconv1 (nn.Conv): ResNet 的第一个卷积层。\nbn1 (nn.BatchNorm): ResNet 的第一个标准化层。\nrelu (nn.Relu): Relu激活函数。\nmaxpool (nn.Pool): 最大池化层。\nlayer1 (nn.Sequential): ResNet 的第一层。\nlayer2 (nn.Sequential): ResNet 的第二层。\nlayer3 (nn.Sequential): ResNet 的第三层。\nlayer4 (nn.Sequential): ResNet 的第四层。\navgpool (nn.AdaptiveAvgPool2d): 自适应平均池化层。\nfc (nn.Linear): 全连接层。\n代码示例:  >>> import jittor as jt\n>>> from jittor.models.resnet import Bottleneck, ResNet\n>>> resnet = ResNet(Bottleneck, [3,4,6,3])\n>>> print(resnet)\nResNet(\nconv1: Conv(3, 64, (7, 7), (2, 2), (3, 3), (1, 1), 1, None, None, Kw=None, fan=None, i=None, bound=None)\n...)",
        "return_value": "",
        "parameters": "block (nn.Module): 残差块的类, 用于建立 ResNet 的不同层。 block 参数通常是一个实现了特定类型残差块结构的类, 比如 ‘BasicBlock’ 或 ‘Bottleneck’。\nlayers (list[int]): 每个 ResNet 层中残差块的数量, 是一个由4个元素组成的列表\nnum_classes (int, optional): 分类任务中的类别数。默认值: 1000\nzero_init_residual (bool, optional): 是否使用零初始化残差块。默认值: False\ngroups (int, optional): 分组卷积的组数量。1意味着不使用分组卷积。默认值: 1\nwidth_per_group (int, optional): 每个分组的宽度。默认值: 64\nreplace_stride_with_dilation (list[bool], optional): 是否用空洞卷积替换某些层的步长。若不为None则应为包含三个布尔值的列表。默认值: None\nnorm_layer (nn.Module, optional): 使用的标准化层, 默认使用 ‘nn.BatchNorm’。默认值: None",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> from jittor.models.resnet import Bottleneck, ResNet\n>>> resnet = ResNet(Bottleneck, [3,4,6,3])\n>>> print(resnet)\nResNet(\n    conv1: Conv(3, 64, (7, 7), (2, 2), (3, 3), (1, 1), 1, None, None, Kw=None, fan=None, i=None, bound=None)\n    ...)"
    },
    {
        "api_name": "jittor.models.Resnext101_32x8d",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.models.html#jittor.models.Resnext101_32x8d",
        "api_signature": "jittor.models.Resnext101_32x8d(pretrained=False, **kwargs)",
        "api_description": "构建一个Resnext101_32x8d模型\nResnext源自论文 Aggregated Residual Transformations for Deep Neural Networks。\n参数:\npretrained (bool, optional): 表示是否预加载预训练模型。默认为 False。\n返回值:\n返回构建好的Resnext101_32x8d模型实例。如果 pretrained 为 True, 则返回在ImageNet上预训练的模型。\n代码示例:>>> import jittor as jt\n>>> from jittor.models.resnet import *\n>>> net = Resnext101_32x8d(pretrained=False)\n>>> x = jt.rand(1, 3, 224, 224)\n>>> y = net(x)\n>>> y.shape\n[1, 1000]",
        "return_value": "返回构建好的Resnext101_32x8d模型实例。如果 pretrained 为 True, 则返回在ImageNet上预训练的模型。",
        "parameters": "pretrained (bool, optional): 表示是否预加载预训练模型。默认为 False。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> from jittor.models.resnet import *\n>>> net = Resnext101_32x8d(pretrained=False)\n>>> x = jt.rand(1, 3, 224, 224)\n>>> y = net(x)\n>>> y.shape\n[1, 1000]"
    },
    {
        "api_name": "jittor.models.resnext101_32x8d",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.models.html#jittor.models.resnext101_32x8d",
        "api_signature": "jittor.models.resnext101_32x8d(pretrained=False, **kwargs)",
        "api_description": "构建一个Resnext101_32x8d模型\nResnext源自论文 Aggregated Residual Transformations for Deep Neural Networks。\n参数:\npretrained (bool, optional): 表示是否预加载预训练模型。默认为 False。\n返回值:\n返回构建好的Resnext101_32x8d模型实例。如果 pretrained 为 True, 则返回在ImageNet上预训练的模型。\n代码示例:>>> import jittor as jt\n>>> from jittor.models.resnet import *\n>>> net = Resnext101_32x8d(pretrained=False)\n>>> x = jt.rand(1, 3, 224, 224)\n>>> y = net(x)\n>>> y.shape\n[1, 1000]",
        "return_value": "返回构建好的Resnext101_32x8d模型实例。如果 pretrained 为 True, 则返回在ImageNet上预训练的模型。",
        "parameters": "pretrained (bool, optional): 表示是否预加载预训练模型。默认为 False。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> from jittor.models.resnet import *\n>>> net = Resnext101_32x8d(pretrained=False)\n>>> x = jt.rand(1, 3, 224, 224)\n>>> y = net(x)\n>>> y.shape\n[1, 1000]"
    },
    {
        "api_name": "jittor.models.Resnext50_32x4d",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.models.html#jittor.models.Resnext50_32x4d",
        "api_signature": "jittor.models.Resnext50_32x4d(pretrained=False, **kwargs)",
        "api_description": "构建一个Resnext50_32x4d模型\nResnext源自论文 Aggregated Residual Transformations for Deep Neural Networks。\n参数:\npretrained (bool, optional): 表示是否预加载预训练模型。默认为 False。\n返回值:\n返回构建好的Resnext50_32x4d模型实例。如果 pretrained 为 True, 则返回在ImageNet上预训练的模型。\n代码示例:>>> import jittor as jt\n>>> from jittor.models.resnet import *\n>>> net = Resnext50_32x4d(pretrained=False)\n>>> x = jt.rand(1, 3, 224, 224)\n>>> y = net(x)\n>>> y.shape\n[1, 1000]",
        "return_value": "返回构建好的Resnext50_32x4d模型实例。如果 pretrained 为 True, 则返回在ImageNet上预训练的模型。",
        "parameters": "pretrained (bool, optional): 表示是否预加载预训练模型。默认为 False。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> from jittor.models.resnet import *\n>>> net = Resnext50_32x4d(pretrained=False)\n>>> x = jt.rand(1, 3, 224, 224)\n>>> y = net(x)\n>>> y.shape\n[1, 1000]"
    },
    {
        "api_name": "jittor.models.resnext50_32x4d",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.models.html#jittor.models.resnext50_32x4d",
        "api_signature": "jittor.models.resnext50_32x4d(pretrained=False, **kwargs)",
        "api_description": "构建一个Resnext50_32x4d模型\nResnext源自论文 Aggregated Residual Transformations for Deep Neural Networks。\n参数:\npretrained (bool, optional): 表示是否预加载预训练模型。默认为 False。\n返回值:\n返回构建好的Resnext50_32x4d模型实例。如果 pretrained 为 True, 则返回在ImageNet上预训练的模型。\n代码示例:>>> import jittor as jt\n>>> from jittor.models.resnet import *\n>>> net = Resnext50_32x4d(pretrained=False)\n>>> x = jt.rand(1, 3, 224, 224)\n>>> y = net(x)\n>>> y.shape\n[1, 1000]",
        "return_value": "返回构建好的Resnext50_32x4d模型实例。如果 pretrained 为 True, 则返回在ImageNet上预训练的模型。",
        "parameters": "pretrained (bool, optional): 表示是否预加载预训练模型。默认为 False。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> from jittor.models.resnet import *\n>>> net = Resnext50_32x4d(pretrained=False)\n>>> x = jt.rand(1, 3, 224, 224)\n>>> y = net(x)\n>>> y.shape\n[1, 1000]"
    },
    {
        "api_name": "jittor_core.Var.right_shift",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor_core.Var.right_shift",
        "api_signature": "jittor_core.Var.right_shift()",
        "api_description": "函数C++定义格式:\njt.Var right_shift(jt.Var x, jt.Var y)\n对两个张量中对应元素计算右移移位运算，也可以使用 >> 运算符调用",
        "return_value": "形状与 x 和 y 相同的张量，其元素为 x 和 y 对应索引位置的元素计算右移操作的结果数",
        "parameters": "x (Var): 被右移的张量，元素的数据类型是 int32 或 int64\ny (Var): 右移的位数张量，元素的数据类型是 int32 或 int64 ，形状与 x 相同",
        "input_shape": "",
        "notes": "支持使用 jt.right_shift() 进行调用",
        "code_example": ">>> x = jt.array([123, 345, -456])\n>>> y = jt.array([3, 4, 5])\n>>> x.right_shift(y)\njt.Var([ 15  21 -15], dtype=int32)\n>>> x >> y\njt.Var([ 15  21 -15], dtype=int32)"
    },
    {
        "api_name": "jittor.optim.RMSprop",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.optim.html#jittor.optim.RMSprop",
        "api_signature": "jittor.optim.RMSprop(params, lr=0.01, eps=1e-08, alpha=0.99)",
        "api_description": "RMSprop 优化器类，用于在深度学习模型训练过程中更新模型参数以最小化损失函数，继承自 Optimizer 类。\nRMSprop 是一种自适应学习率方法，为每个参数设置单独的学习率。它的公式包括累积平方梯度和参数更新的计算：\n\\[\\begin{split}v(w, t) &= \\alpha \\cdot v(w, t-1) + (1 - \\alpha) g(w, t)^2  \\\\\nw &= w - \\frac{lr \\cdot g(w, t)}{\\sqrt{v(w, t)} + \\varepsilon}\\end{split}\\]\n其中，\\(g(w, t)\\) 为 \\(t\\) 时刻参数 \\(w\\) 上的梯度，\\(v\\) 为累积平方梯度， \\(\\alpha\\) 为平滑常数， \\(\\varepsilon\\) 为防止零除项。",
        "return_value": "",
        "parameters": "params (list): 被优化的模型的参数。\nlr (float): 学习率，默认值：1e-2\neps (float): 添加到分母的术语以避免零除。默认值： 1e-8\nalpha (float): 平滑常数。 默认值： 0.99",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n... model = jt.nn.Linear(10, 2)\n... loss_fn = jt.nn.CrossEntropyLoss()\n... optimizer = jt.optim.RMSprop(model.parameters(), lr=0.1)\n... x = jt.randn([5, 10])\n... y_true = jt.array([0, 1, 0, 1, 1])\n... y_pred = model(x)\n... loss = loss_fn(y_pred, y_true)\n... optimizer.step(loss)"
    },
    {
        "api_name": "jittor.nn.RNNBase",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.nn.html#jittor.nn.RNNBase",
        "api_signature": "jittor.nn.RNNBase(mode: str, input_size: int, hidden_size: int, num_layers: int = 1, bias: bool = True, batch_first: bool = False, dropout: float = 0, bidirectional: bool = False, proj_size: int = 0, nonlinearity: str | None = None)",
        "api_description": "RNN模块的基类（RNN, LSTM, GRU）。\n实现了RNN、LSTM和GRU类共有的RNN方面，比如模块初始化以及用于参数存储管理的工具方法。",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "jittor.nn.RNNCell",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.nn.html#jittor.nn.RNNCell",
        "api_signature": "jittor.nn.RNNCell(input_size, hidden_size, bias=True, nonlinearity='tanh')",
        "api_description": "一个带有tanh或ReLU非线性的Elman RNN单元。\n\\[h' = \\tanh(W_{ih} x + b_{ih}  +  W_{hh} h + b_{hh})\\]\n如果 nonlinearity='relu' ，则使用ReLU非线性。\n参数:\ninput_size(int): 输入的特征维度\nhidden_size(int): 隐藏层的特征维度\nbias(bool): 如果为 False ，则模型不会使用偏置权重。默认值: True\nnonlinearity(str): 指定非线性激活函数，可选 'tanh' 和 'relu' 。默认值: 'tanh'\n代码示例:>>> rnn = nn.RNNCell(10, 20)\n>>> input = jt.randn(6, 3, 10)\n>>> hx = jt.randn(3, 20)\n>>> output = []\n>>> for i in range(6):\nhx = rnn(input[i], hx)\noutput.append(hx)",
        "return_value": "",
        "parameters": "input_size(int): 输入的特征维度\nhidden_size(int): 隐藏层的特征维度\nbias(bool): 如果为 False ，则模型不会使用偏置权重。默认值: True\nnonlinearity(str): 指定非线性激活函数，可选 'tanh' 和 'relu' 。默认值: 'tanh'",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> rnn = nn.RNNCell(10, 20)\n>>> input = jt.randn(6, 3, 10)\n>>> hx = jt.randn(3, 20)\n>>> output = []\n>>> for i in range(6):\n        hx = rnn(input[i], hx)\n        output.append(hx)"
    },
    {
        "api_name": "jittor.nn.RNN",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.nn.html#jittor.nn.RNN",
        "api_signature": "jittor.nn.RNN(input_size: int, hidden_size: int, num_layers: int = 1, nonlinearity: str = 'tanh', bias: bool = True, batch_first: bool = False, dropout: float = 0, bidirectional: bool = False)",
        "api_description": "将带有tanh或ReLU非线性激活函数的多层Elman RNN应用于输入序列。\n对于输入序列中的每个元素，每一层计算以下函数：\n\\[h_t = \\tanh(W_{ih} x_t + b_{ih} + W_{hh} h_{(t-1)} + b_{hh})\\]\n其中， \\(h_t\\) 是时间t的隐藏状态， \\(x_t\\) 是时间t的输入， \\(h_{(t-1)}\\) 是时间t-1的隐藏状态或者是时间0的初始隐藏状态。如果非线性激活函数选择为’relu’，则使用ReLU代替tanh。\n参数：\ninput_size(int)：输入x的特征数\nhidden_size(int): 隐藏状态h的特征数\nnum_layers(int): RNN层数。默认值：1\nnonlinearity(str)：非线性激活函数。可以选择tanh或relu。默认值：’tanh’\nbias(bool)：如果为False，则层不会使用偏置b_ih和b_hh。默认值：True\nbatch_first(bool)：如果为True，则输入和输出张量的形状为（batch，seq，feature）。默认值：False\ndropout(float)：如果非零，则除了最后一层外，将在每个RNN层的输出上应用丢弃，使用dropout概率。默认值：0\nbidirectional(bool)：如果为True，则RNN层将是双向的，且输出将是双向隐状态的拼接。默认值：False\n代码示例：>>> rnn = nn.RNN(10, 20, 2)\n>>> input = jt.randn(5, 3, 10)\n>>> h0 = jt.randn(2, 3, 20)\n>>> output, hn = rnn(input, h0)",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> rnn = nn.RNN(10, 20, 2)\n>>> input = jt.randn(5, 3, 10)\n>>> h0 = jt.randn(2, 3, 20)\n>>> output, hn = rnn(input, h0)"
    },
    {
        "api_name": "jittor.misc.roll",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.misc.roll",
        "api_signature": "jittor.misc.roll(x, shifts, dims=None)",
        "api_description": "将给定的张量在指定维度上滚动，或者说循环移动 shifts 位。",
        "return_value": "Var，循环移位后的张量",
        "parameters": "x (Var): 输入的张量\nshifts (tuple[int]): 滚动的偏移量\ndims (tuple[int], 可选): 滚动的维度，默认为最后一维",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> x = jt.array([1, 2, 3, 4, 5, 6, 7, 8]).view(4, 2)\n>>> jt.roll(x, 1, 0)\njt.Var([[7 8]\n        [1 2]\n        [3 4]\n        [5 6]], dtype=int32)\n>>> jt.roll(x, -2, 0)\njt.Var([[5 6]\n        [7 8]\n        [1 2]\n        [3 4]], dtype=int32)"
    },
    {
        "api_name": "jittor_core.Var.round",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor_core.Var.round",
        "api_signature": "jittor_core.Var.round()",
        "api_description": "函数C++定义格式:\njt.Var round(jt.Var x)\n创建一个张量, 其将 x 中的每一个数值转换为离其最近的整数（四舍五入）。",
        "return_value": "返回一个新的张量, 其数值是 x 中对应位置取最近整数的值",
        "parameters": "x (Var): Var类型的张量",
        "input_shape": "",
        "notes": "支持使用 jt.round() 进行调用",
        "code_example": ">>> x = jt.randn(5)\n>>> x\njt.Var([ 1.1675875  -0.36471805  2.0246403   1.1496693  -0.09650089], dtype=float32)\n>>> x.round()\njt.Var([ 1. -0.  2.  1. -0.], dtype=float32)"
    },
    {
        "api_name": "jittor_core.Var.round_int",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor_core.Var.round_int",
        "api_signature": "jittor_core.Var.round_int()",
        "api_description": "函数C++定义格式:\njt.Var round_int(jt.Var x)\n创建一个张量, 其将 x 中的每一个数值转换为离其最近的整数（四舍五入）, 数据类型为 int32 。",
        "return_value": "返回一个新的张量, 其数值是 x 中对应位置取最近整数的值, 数据类型为 int32",
        "parameters": "x (Var): Var类型的张量",
        "input_shape": "",
        "notes": "支持使用 jt.round_int() 进行调用",
        "code_example": ">>> x = jt.randn(5)\n>>> x\njt.Var([ 2.00422    -2.081075   -0.7260027   0.46558696 -0.1570169 ], dtype=float32)\n>>> x.round_int()\njt.Var([ 2 -2 -1  0  0], dtype=int32)"
    },
    {
        "api_name": "jittor.misc.rsqrt",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.misc.rsqrt",
        "api_signature": "jittor.misc.rsqrt(x)",
        "api_description": "计算并返回 Jittor 变量的平方根的倒数。\n此函数对给定的 Jittor 变量执行平方根的倒数运算。它等效于 \\(\\displaystyle \\frac{1}{\\sqrt x}\\)。",
        "return_value": "Var: 输入变量的平方根的倒数。返回结果的形状和数据类型与输入参数一致。",
        "parameters": "x (Var): 输入的 Jittor 变量。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> var = jt.array([4, 9, 16])\n>>> result = rsqrt(var)\n>>> print(result)\n    [0.5, 0.33333333, 0.25]  # 分别是 2, 3, 4 的平方根的倒数"
    },
    {
        "api_name": "jittor_core.Var.safe_clip",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor_core.Var.safe_clip",
        "api_signature": "jittor_core.Var.safe_clip()",
        "api_description": "函数C++定义格式:\njt.Var safe_clip(jt.Var x,  float64 left=-1e300,  float64 right=1e300)\n对一个值在一个区间[left, right]内进行安全剪裁,并保持梯度通过。当输入值在剪裁区间之外时,其梯度保持不变。",
        "return_value": "剪裁后的值(Var)。",
        "parameters": "x(Var):输入值。\nleft(float):剪裁的最小值。默认值：float64\nright(float):剪裁的最大值。默认值：float64",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> x = jt.Var([-1,0.75,2])\n>>> jt.safe_clip(x, 0.5, 1)\n[0.5,0.75,1.]"
    },
    {
        "api_name": "jittor.misc.safe_log",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.misc.safe_log",
        "api_signature": "jittor.misc.safe_log(x)",
        "api_description": "计算输入矩阵 x 的对数值。\n\\[y_i=\\ln\\left(x_i\\right)\\]\n该函数在执行对数运算之前，使用 jt.safe_clip 将 x 的值限制在一个安全范围内 (1e-30, 1e+30)，以避免因取对数时出现零或极大值造成计算错误。",
        "return_value": "Var，x 中元素的对数值，与 x 的形状相同。",
        "parameters": "x (Var): 输入变量",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> x = jt.array([0, 0.1, 1.0, 10.0, 100.0, -123])\n>>> jt.safe_log(x)\njt.Var([-69.07755  -2.3025851   0.  2.3025851   4.6051702   -69.07755], dtype=float32)"
    },
    {
        "api_name": "jittor.safepickle",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.safepickle",
        "api_signature": "jittor.safepickle(obj, path)",
        "api_description": "该函数使用pickle模块将指定对象保存至给定路径。",
        "return_value": "无返回值。",
        "parameters": "obj(any):需要被保存的对象。\npath(str): 一个字符串，用于指定对象保存的文件路径。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> a = [1, 2, 3]\n>>> jt.safepickle(a, '/path/to/save')"
    },
    {
        "api_name": "jittor.safeunpickle",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.safeunpickle",
        "api_signature": "jittor.safeunpickle(path)",
        "api_description": "该函数用于安全地反序列化pickle文件。",
        "return_value": "反序列化的对象(obj)。",
        "parameters": "path (str): pickle文件的路径。",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "jittor.dataset.Sampler",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.dataset.html#jittor.dataset.Sampler",
        "api_signature": "jittor.dataset.Sampler(dataset)",
        "api_description": "所有采样器的基类。每个采样器子类都必须提供一个 __iter__() 方法, 提供对数据集元素的索引或索引列表(批次)进行迭代的方法, 以及一个返回返回迭代器长度的 __len__() 方法。\n参数:\ndataset (jittor.dataset.Dataset): 需要采样的数据集。\n属性:\ndataset (jittor.dataset.Dataset): 被采样的数据集。\n代码示例:  >>> from jittor.dataset import Dataset\n>>> from jittor.dataset import Sampler\n>>> class MyDataset(Dataset):\n...     def __len__(self):\n...         return 100\n...     def __getitem__(self, index):\n...         return index\n>>> class MySampler(Sampler):\n...     def __iter__(self):\n...         return iter(range(len(self.dataset)))\n>>> dataset = MyDataset()\n>>> sampler = MySampler(dataset)\n>>> list(iter(sampler))\n[0, 1, 2, ..., 99]",
        "return_value": "",
        "parameters": "dataset (jittor.dataset.Dataset): 需要采样的数据集。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> from jittor.dataset import Dataset\n>>> from jittor.dataset import Sampler\n>>> class MyDataset(Dataset):\n...     def __len__(self):\n...         return 100\n...     def __getitem__(self, index):\n...         return index\n>>> class MySampler(Sampler):\n...     def __iter__(self):\n...         return iter(range(len(self.dataset)))\n>>> dataset = MyDataset()\n>>> sampler = MySampler(dataset)\n>>> list(iter(sampler))\n[0, 1, 2, ..., 99]"
    },
    {
        "api_name": "jittor.Module.save",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.Module.save",
        "api_signature": "save(path: str)",
        "api_description": "将参数保存到文件中。\n示例代码:\n>>> class Net(nn.Module):\n>>> ...\n>>> net = Net()\n>>> net.save('net.pkl')\n>>> net.load('net.pkl')",
        "return_value": "",
        "parameters": "path (str) – path to save.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "jittor.save",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.save",
        "api_signature": "jittor.save(params_dict, path: str)",
        "api_description": "将指定的参数 params_dict 保存到指定的路径 path 。",
        "return_value": "None。",
        "parameters": "params_dict (list or dictionary) : 待保存的参数。\npath (str) : 保存的文件路径。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> params_dict = {\"weight\": np.array([1, 2, 3]), \"bias\": np.array([0.1, 0.2, 0.3])}\n>>> jt.save(params_dict, \"params.p\")"
    },
    {
        "api_name": "jittor.misc.save_image",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.misc.save_image",
        "api_signature": "jittor.misc.save_image(x, filepath, nrow: int = 8, padding: int = 2, normalize: bool = False, range=None, scale_each=False, pad_value=0, format=None)",
        "api_description": "将多个图片按网格状拼成一个图片后保存到 filepath 指定的文件",
        "return_value": "",
        "parameters": "x (Var): 图片数据，形状 \\((n, c, h, w)\\)\nfilepath (str): 保存目标，文件名或打开的文件对象\nnrow (int): 每行多少个图片，最终拼成的网格形状是 \\((n / \\text{nrow}, \\text{nrow})\\) 。默认值：8\npadding (int): 图片间的间隔。默认值：2\nnormalize (bool): 是否将图片值线性映射到 \\([0, 1]\\) 。默认值：False\nrange (None, tuple[float, float]): 映射到 \\([0, 1]\\) 的范围，或者不指定则使用图片数据中的最小最大值。默认值：None\nscale_each (bool): 不支持，必须为 False。默认值：False\npad_value (float): 图片间填充的值。默认值：0\nformat：保存的文件格式，默认由文件名扩展名确定",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "jittor.misc.scatter",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.misc.scatter",
        "api_signature": "jittor.misc.scatter(x: Var, dim: int, index: Var, src: Var, reduce='void')",
        "api_description": "根据指定的索引 index 将 src 中的元素散布到输入数组 x 的 dim 维度，返回修改后的 x 数组。\n当 x 是3维数组时，该函数的效果可表述为:\nself[index[i][j][k]][j][k] = src[i][j][k]  # if dim == 0\nself[i][index[i][j][k]][k] = src[i][j][k]  # if dim == 1\nself[i][j][index[i][j][k]] = src[i][j][k]  # if dim == 2",
        "return_value": "Var，修改后的 x 数组，形状与其一致",
        "parameters": "x (Var): 输入数组\ndim (int): 指定的维度，用于指定沿哪个轴进行索引\nindex (Var): 用于散布元素的索引，可以为空，或与 src 的维数相同。当索引为空时，该操作返回的是原始的 x 数组，即没有做任何修改。\nsrc (Var): 需要散布的源元素\nreduce (str，可选): 确定如何执行散布操作。此参数可以是 add 或 multiply。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> src = jt.arange(1, 11).reshape((2, 5))\n>>> index = jt.array([[0, 1, 2, 0]])\n>>> x = jt.zeros((3, 5), dtype=src.dtype)\n>>> jt.scatter(x, 0, index, src)\njt.Var([[1 0 0 4 0]\n        [0 2 0 0 0]\n        [0 0 3 0 0]], dtype=int32)"
    },
    {
        "api_name": "jittor.misc.searchsorted",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.misc.searchsorted",
        "api_signature": "jittor.misc.searchsorted(sorted, values, right=False)",
        "api_description": "对有序张量 sorted 的最后一个维度进行搜索以查找 values 中每个值应该插入到（插入后仍保持数组有序）的索引位置。",
        "return_value": "返回一个形状与 values 相同的 Var ，数组中的每个值表示对应的 values 在 sorted 中的索引位置。",
        "parameters": "sorted (Var): 输入的有顺序的数组\nvalues (Var): 要进行插入的值\nright (bool): 默认为 False。\n\n如果为 False，则返回的索引 i 满足 sorted[i-1] < value <= sorted[i]；如果为 True，则返回的索引 i 满足 sorted[i-1] <= value < sorted[i]。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> sorted = jt.array([[1, 3, 5, 7, 9], [2, 4, 6, 8, 10]])\n>>> values = jt.array([[3, 6, 9], [3, 6, 9]])\n>>> ret = jt.searchsorted(sorted, values)\n>>> assert (ret == [[1, 3, 4], [1, 2, 4]]).all(), ret\n>>> ret = jt.searchsorted(sorted, values, right=True)\n>>> assert (ret == [[2, 3, 5], [1, 3, 4]]).all(), ret\n>>> sorted_1d = jt.array([1, 3, 5, 7, 9])\n>>> ret = jt.searchsorted(sorted_1d, values)\n>>> assert (ret == [[1, 3, 4], [1, 3, 4]]).all(), ret"
    },
    {
        "api_name": "jittor.dataset.SequentialSampler",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.dataset.html#jittor.dataset.SequentialSampler",
        "api_signature": "jittor.dataset.SequentialSampler(dataset)",
        "api_description": "按顺序对元素进行采样, 采样顺序始终保持不变。\n参数:\ndataset (jittor.dataset.Dataset): 需要采样的数据集。\n属性:\ndataset (jittor.dataset.Dataset): 被采样的数据集。\n代码示例:  >>> import jittor as jt\n>>> from jittor.dataset import Dataset\n>>> from jittor.dataset import SequentialSampler\n>>>\n>>> class MyDataset(Dataset):\n>>>      def __len__(self):\n>>>          return 5\n>>>\n>>>      def __getitem__(self, index):\n>>>          return index\n>>>\n>>> dataset = MyDataset()\n>>> sampler = SequentialSampler(dataset)\n>>> list(iter(sampler))\n[0, 1, 2, 3, 4]",
        "return_value": "",
        "parameters": "dataset (jittor.dataset.Dataset): 需要采样的数据集。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> from jittor.dataset import Dataset\n>>> from jittor.dataset import SequentialSampler\n>>>\n>>> class MyDataset(Dataset):\n>>>      def __len__(self):\n>>>          return 5\n>>>\n>>>      def __getitem__(self, index):\n>>>          return index\n>>> \n>>> dataset = MyDataset()\n>>> sampler = SequentialSampler(dataset)\n>>> list(iter(sampler))\n[0, 1, 2, 3, 4]"
    },
    {
        "api_name": "jittor.nn.Sequential",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.nn.html#jittor.nn.Sequential",
        "api_signature": "jittor.nn.Sequential(*args)",
        "api_description": "顺序容器。\n模块将按照它们在构造函数中传递的顺序被添加。Sequential的execute()方法接受任何输入，并将其转发给它包含的第一个模块。然后它将每个后续模块的输出“链式”地连接到输入，最终返回最后一个模块的输出。Sequential相比于手动调用一系列模块的优势在于，它允许将整个容器视为单个模块，这样对Sequential进行的转换将应用于它存储的每个模块（每个都是Sequential的注册子模块）。",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> model = nn.Sequential(\n  nn.Conv2d(1,20,5),\n  nn.ReLU(),\n  nn.Conv2d(20,64,5),\n  nn.ReLU()\n)\n>>> model = nn.Sequential(OrderedDict([\n  ('conv1', nn.Conv2d(1,20,5)),\n  ('relu1', nn.ReLU()),\n  ('conv2', nn.Conv2d(20,64,5)),\n  ('relu2', nn.ReLU())\n]))\n>>> model = nn.Sequential()\n>>> model.append(nn.Conv2d(1,20,5))\n>>> model.append(nn.ReLU())"
    },
    {
        "api_name": "jittor.dataset.Dataset.set_attrs",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.dataset.html#jittor.dataset.Dataset.set_attrs",
        "api_signature": "set_attrs(**kw)",
        "api_description": "设置数据集的属性。\n参数:\n**kw (dict): 数据集的属性字典。\n返回:\nDataset: 返回设置了属性的数据集对象\n代码示例:  >>> dataset = YourDataset().set_attrs(batch_size=256, shuffle=True)",
        "return_value": "",
        "parameters": "**kw (dict): 数据集的属性字典。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> dataset = YourDataset().set_attrs(batch_size=256, shuffle=True)"
    },
    {
        "api_name": "jittor.misc.set_global_seed",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.misc.set_global_seed",
        "api_signature": "jittor.misc.set_global_seed(seed, different_seed_for_mpi=True)",
        "api_description": "设置Python，numpy和jittor的随机数生成器的种子。\n这个函数同步地设置了这三个随机数生成器的种子，将它们设置为同一个值，以保证在使用这些生成器时获得一致的随机数。\nJittor还确保了jittor.dataset.Dataset的每一个工作进程都持有一个不同的种子，同时也保证了使用mpi的每一个进程都持有一个不同的种子。这是通过如下的公式实现的:\n(global_seed ^ (worker_id * 1167)) ^ 1234 + jt.rank * 2591",
        "return_value": "",
        "parameters": "seed (int): 指定要设置的随机数种子的值，无默认值\ndifferent_seed_for_mpi (bool): 是否为每一个使用mpi的进程设置一个不同的种子，默认为True。",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "jittor.contrib.setitem",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.contrib.html#jittor.contrib.setitem",
        "api_signature": "jittor.contrib.setitem(x, slices, value)",
        "api_description": "对数组 x 进行切片赋值。函数的目标是将一个数组的切片赋予一个特定的值。首先通过 slice, var, index 函数处理切片信息，然后创建一个与目标切片相同形状的广播值。然后将广播值累加到目标切片，并将得到的结果赋值回原数组。",
        "return_value": "赋值后的数组(Var)。",
        "parameters": "x (Var): 原始数组。\nslices (int, slice object 或者 tuple): 对数组的切片信息。如果是 tuple，其长度需要和数组x的维度一致。\nvalue (int, float 或者 Var): 要赋给数组切片的值。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> from jittor.contrib import setitem\n>>> import jittor as jt\n>>> x = jt.array([0, 1, 2, 3, 4])\n>>> setitem(x, slice(1, 4), 9)\njt.Var([0 9 9 9 4], dtype=int32)"
    },
    {
        "api_name": "jittor_core.Var.setitem",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor_core.Var.setitem",
        "api_signature": "jittor_core.Var.setitem()",
        "api_description": "函数C++定义格式:\njt.Var setitem(jt.Var x,  VarSlices&& slices, jt.Var y,  String op=ns_void)\n将 Jittor 变量的某一个切片设置为另一个 Jittor 变量的值。在操作中的 x 和 y 可看作两个 tensor ， op 决定了如何计算这两个 tensor。",
        "return_value": "更新后的变量(Var)",
        "parameters": "x(Var): 变量实例。\nslices(slice): 描述要被剪切的切片信息, 类型为切片。\ny(Var): 用于赋值给x的某个切片。\nop(str, optional): 操作类型。默认值：\"void\"",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> x = jt.array([[1,2],[3,4]])\n>>> y = jt.array([0.1,0.2])\n>>> jt.setitem(x, slice(1, 2), y)\n[[1,2],[0.1,0.2]]"
    },
    {
        "api_name": "jittor.optim.SGD",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.optim.html#jittor.optim.SGD",
        "api_signature": "jittor.optim.SGD(params, lr, momentum=0, weight_decay=0, dampening=0, nesterov=False)",
        "api_description": "随机梯度下降算法优化器的实现，包括学习率衰减、动量、权重衰减等功能。在 step 方法中，根据参数的梯度和当前的动量更新参数值，也支持添加新的参数组，并能够计算带有动量的更新值。\n该算法使用 SGD 进行更新，其中加入了动量项、权重衰减项，并可以选择是否使用 Nesterov 加速。如果使用动量项，则更新过程会考虑历史的梯度信息，更新公式如下:\n\\[\\begin{split}v &= \\text{momentum} \\times v + dp \\times (1 - \\text{dampening}) \\\\\ndp &= p \\times \\text{weight_decay} + g\\end{split}\\]\n如果使用 Nesterov 动量, 则更新过程会提前按历史方向进行一步预测，这样在某些问题上会获得更好的训练效果，更新公式如下:\n\\[p = p - (dp + \\text{momentum} * v) * lr\\]\n否则，更新公式为：\n\\[p = p - v \\times lr\\]\n以上 \\(p,g,v\\) 分别表示参数值、梯度以及参数更新量。",
        "return_value": "",
        "parameters": "params(list): 待优化的参数或者已经定义好的参数组\nlr(float): 学习率，用于控制参数更新的步长。\nmomentum(float, 可选): 动量因子。默认值： 0\nweight_decay(float, 可选): 权重衰减系数。默认值： 0\ndampening(float, 可选): 动量的抑制因子。默认值： 0\nnesterov(bool, 可选): 是否采用 Nesterov 动量。默认值：False",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> model = jt.nn.Linear(10, 2)\n>>> loss_fn = jt.nn.CrossEntropyLoss()\n>>> optimizer = jt.optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=0.001)\n>>> x = jt.randn([5, 10])\n>>> y_true = jt.array([0, 1, 0, 1, 1])\n>>> y_pred = model(x)\n>>> loss = loss_fn(y_pred, y_true)\n>>> optimizer.step(loss)"
    },
    {
        "api_name": "jittor.models.shufflenet_v2_x0_5",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.models.html#jittor.models.shufflenet_v2_x0_5",
        "api_signature": "jittor.models.shufflenet_v2_x0_5(pretrained=False)",
        "api_description": "构建一个shufflenet_v2_x0_5模型\nShuffleNetV2源自论文 ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design, 其核心思想在于统筹通道分割与通道打乱操作, 减少计算复杂度同时保持模型性能。\n参数:\npretrained (bool, optional): 表示是否预加载预训练模型。默认为 False。\n返回值:\n返回构建好的shufflenet_v2_x0_5模型实例。如果 pretrained 为 True, 则返回在ImageNet上预训练的模型。\n代码示例:>>> import jittor as jt\n>>> from jittor.models.shufflenetv2 import *\n>>> net = shufflenet_v2_x0_5(pretrained=False)\n>>> x = jt.rand(1, 3, 224, 224)\n>>> y = net(x)\n>>> y.shape\n[1, 1000]",
        "return_value": "返回构建好的shufflenet_v2_x0_5模型实例。如果 pretrained 为 True, 则返回在ImageNet上预训练的模型。",
        "parameters": "pretrained (bool, optional): 表示是否预加载预训练模型。默认为 False。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> from jittor.models.shufflenetv2 import *\n>>> net = shufflenet_v2_x0_5(pretrained=False)\n>>> x = jt.rand(1, 3, 224, 224)\n>>> y = net(x)\n>>> y.shape\n[1, 1000]"
    },
    {
        "api_name": "jittor.models.shufflenet_v2_x1_0",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.models.html#jittor.models.shufflenet_v2_x1_0",
        "api_signature": "jittor.models.shufflenet_v2_x1_0(pretrained=False)",
        "api_description": "构建一个shufflenet_v2_x1_0模型\nShuffleNetV2源自论文 ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design, 其核心思想在于统筹通道分割与通道打乱操作, 减少计算复杂度同时保持模型性能。\n参数:\npretrained (bool, optional): 表示是否预加载预训练模型。默认为 False。\n返回值:\n返回构建好的shufflenet_v2_x1_0模型实例。如果 pretrained 为 True, 则返回在ImageNet上预训练的模型。\n代码示例:>>> import jittor as jt\n>>> from jittor.models.shufflenetv2 import *\n>>> net = shufflenet_v2_x1_0(pretrained=False)\n>>> x = jt.rand(1, 3, 224, 224)\n>>> y = net(x)\n>>> y.shape\n[1, 1000]",
        "return_value": "返回构建好的shufflenet_v2_x1_0模型实例。如果 pretrained 为 True, 则返回在ImageNet上预训练的模型。",
        "parameters": "pretrained (bool, optional): 表示是否预加载预训练模型。默认为 False。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> from jittor.models.shufflenetv2 import *\n>>> net = shufflenet_v2_x1_0(pretrained=False)\n>>> x = jt.rand(1, 3, 224, 224)\n>>> y = net(x)\n>>> y.shape\n[1, 1000]"
    },
    {
        "api_name": "jittor.models.shufflenet_v2_x1_5",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.models.html#jittor.models.shufflenet_v2_x1_5",
        "api_signature": "jittor.models.shufflenet_v2_x1_5(pretrained=False)",
        "api_description": "构建一个shufflenet_v2_x1_5模型\nShuffleNetV2源自论文 ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design, 其核心思想在于统筹通道分割与通道打乱操作, 减少计算复杂度同时保持模型性能。\n参数:\npretrained (bool, optional): 表示是否预加载预训练模型。默认为 False。\n返回值:\n返回构建好的shufflenet_v2_x1_5模型实例。如果 pretrained 为 True, 则返回在ImageNet上预训练的模型。\n代码示例:>>> import jittor as jt\n>>> from jittor.models.shufflenetv2 import *\n>>> net = shufflenet_v2_x1_5(pretrained=False)\n>>> x = jt.rand(1, 3, 224, 224)\n>>> y = net(x)\n>>> y.shape\n[1, 1000]",
        "return_value": "返回构建好的shufflenet_v2_x1_5模型实例。如果 pretrained 为 True, 则返回在ImageNet上预训练的模型。",
        "parameters": "pretrained (bool, optional): 表示是否预加载预训练模型。默认为 False。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> from jittor.models.shufflenetv2 import *\n>>> net = shufflenet_v2_x1_5(pretrained=False)\n>>> x = jt.rand(1, 3, 224, 224)\n>>> y = net(x)\n>>> y.shape\n[1, 1000]"
    },
    {
        "api_name": "jittor.models.shufflenet_v2_x2_0",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.models.html#jittor.models.shufflenet_v2_x2_0",
        "api_signature": "jittor.models.shufflenet_v2_x2_0(pretrained=False)",
        "api_description": "构建一个shufflenet_v2_x2_0模型\nShuffleNetV2源自论文 ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design, 其核心思想在于统筹通道分割与通道打乱操作, 减少计算复杂度同时保持模型性能。\n参数:\npretrained (bool, optional): 表示是否预加载预训练模型。默认为 False。\n返回值:\n返回构建好的shufflenet_v2_x2_0模型实例。如果 pretrained 为 True, 则返回在ImageNet上预训练的模型。\n代码示例:>>> import jittor as jt\n>>> from jittor.models.shufflenetv2 import *\n>>> net = shufflenet_v2_x2_0(pretrained=False)\n>>> x = jt.rand(1, 3, 224, 224)\n>>> y = net(x)\n>>> y.shape\n[1, 1000]",
        "return_value": "返回构建好的shufflenet_v2_x2_0模型实例。如果 pretrained 为 True, 则返回在ImageNet上预训练的模型。",
        "parameters": "pretrained (bool, optional): 表示是否预加载预训练模型。默认为 False。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> from jittor.models.shufflenetv2 import *\n>>> net = shufflenet_v2_x2_0(pretrained=False)\n>>> x = jt.rand(1, 3, 224, 224)\n>>> y = net(x)\n>>> y.shape\n[1, 1000]"
    },
    {
        "api_name": "jittor.models.ShuffleNetV2",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.models.html#jittor.models.ShuffleNetV2",
        "api_signature": "jittor.models.ShuffleNetV2(stages_repeats, stages_out_channels, num_classes=1000, inverted_residual=<class 'jittor.models.shufflenetv2.InvertedResidual'>)",
        "api_description": "ShuffleNetV2源自论文 ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design, 其核心思想在于统筹通道分割与通道打乱操作, 减少计算复杂度同时保持模型性能。\n参数:\nstages_repeats (list[int]): 每个阶段中InvertedResidual模块的重复次数, 长度应为3。\nstages_out_channels (list[int]): 每个阶段最终输出通道数, 长度应为5。\nnum_classes (int, optional): 分类的类别数, 默认为1000。\ninverted_residual (nn.Module, optional): 用于构建每个阶段的反转残差模块, 默认为InvertedResidual类。默认值: None\n属性:\nstages_repeats (list[int]): 每个阶段中InvertedResidual模块的重复次数。\nstages_out_channels (list[int]): 每个阶段最终输出通道数。\nnum_classes (int): 分类的类别数。\ninverted_residual (nn.Module): 用于构建每个阶段的反转残差模块。\n代码示例:  >>> import jittor as jt\n>>> from jittor.models.shufflenetv2 import ShuffleNetV2\n>>> stages_repeats = [4, 8, 4]\n>>> stages_out_channels = [24, 48, 96, 192, 1024]\n>>> net = ShuffleNetV2(stages_repeats, stages_out_channels, num_classes=1000)\n>>> input = jt.randn(10, 3, 256, 256)  # 假设是一个形状为[批次大小, 通道数, 高度, 宽度]的张量\n>>> net(input).shape\n[10,1000,]",
        "return_value": "",
        "parameters": "stages_repeats (list[int]): 每个阶段中InvertedResidual模块的重复次数, 长度应为3。\nstages_out_channels (list[int]): 每个阶段最终输出通道数, 长度应为5。\nnum_classes (int, optional): 分类的类别数, 默认为1000。\ninverted_residual (nn.Module, optional): 用于构建每个阶段的反转残差模块, 默认为InvertedResidual类。默认值: None",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> from jittor.models.shufflenetv2 import ShuffleNetV2\n>>> stages_repeats = [4, 8, 4]\n>>> stages_out_channels = [24, 48, 96, 192, 1024]\n>>> net = ShuffleNetV2(stages_repeats, stages_out_channels, num_classes=1000)\n>>> input = jt.randn(10, 3, 256, 256)  # 假设是一个形状为[批次大小, 通道数, 高度, 宽度]的张量\n>>> net(input).shape\n[10,1000,]"
    },
    {
        "api_name": "jittor_core.Var.sigmoid",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor_core.Var.sigmoid",
        "api_signature": "jittor_core.Var.sigmoid()",
        "api_description": "函数C++定义格式:\njt.Var sigmoid(jt.Var x)\n创建一个张量, 其将输入变量 x 中的每一个数值进行 sigmoid 变换。\n\\[out_i =  \\frac{1}{1 + e^{-x_i}}\\]",
        "return_value": "返回一个新的张量, 其数值是 x 中对应位置进行 sigmoid 变换后的值",
        "parameters": "x (Var): Var类型的张量",
        "input_shape": "",
        "notes": "sigmoid 变换的定义域是实数集，值域是(0, 1)\n支持使用 jt.sigmoid() 进行调用",
        "code_example": ">>> x = jt.randn(5) \n>>> x\njt.Var([ 2.7442768  -0.6135011  -0.09157802 -2.3600576   0.98465353], dtype=float32)\n>>> x.sigmoid()\njt.Var([0.93958926 0.35126096 0.47712144 0.08626965 0.72803056], dtype=float32)"
    },
    {
        "api_name": "jittor.sigmoid_",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.sigmoid_",
        "api_signature": "jittor.sigmoid_(x)",
        "api_description": "原地版本的 sigmoid 函数。这个函数会把输入的 Jittor 变量直接进行替换。\nsigmoid 函数的数学形式为:\n\\[\\sigma(x) = \\frac{1}{1 + e^{-x}}\\]",
        "return_value": "应用了 \\(sigmoid\\) 函数后的 Jittor 变量(Var)。",
        "parameters": "x (Var): 输入的 Jittor 变量。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> a = jt.array([1.0, 2.0, 3.0])\n>>> b = sigmoid_(a)\njt.Var([0.7310586  0.880797   0.95257413], dtype=float32)"
    },
    {
        "api_name": "jittor.nn.Sigmoid",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.nn.html#jittor.nn.Sigmoid",
        "api_signature": null,
        "api_description": "该类用于对张量中的每个元素进行 Sigmoid 激活函数运算，将值映射到 (0, 1) 区间内，公式如下：\n\\[\\text{Sigmoid}(x) = \\sigma(x) = \\frac{1}{1 + \\exp(-x)}\\]",
        "return_value": "",
        "parameters": "",
        "input_shape": "Input: \\((*)\\)，其中 * 表示任意数量的额外维度\nOutput: \\((*)\\)，维度和输入相同",
        "notes": "",
        "code_example": ">>> m = nn.Sigmoid()\n>>> input = jt.randn(5)\n>>> input\njt.Var([ 0.00915895  1.5580896   1.5417911  -2.0431511  -1.6694698 ], dtype=float32)\n>>> output = m(input)\n>>> output\njt.Var([0.5022897  0.8260791  0.8237249  0.11474625 0.15849487], dtype=float32)"
    },
    {
        "api_name": "jittor.nn.sign",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.nn.html#jittor.nn.sign",
        "api_signature": "jittor.nn.sign(x: Var)",
        "api_description": "这是一个元素级别（element-wise）函数。对输入变量x对应索引的元素应用符号函数。具体来说：当元素的值大于0时，返回1；当元素的值等于0时，返回0；当元素的值小于0时，返回-1，即：\n\\[\\begin{split}sign(x) =\n\\begin{cases}\n1, & \\text {if } x > 0 \\\\\n0, & \\text {if } x = 0 \\\\\n-1, & \\text {if } x < 0\n\\end{cases}\\end{split}\\]\n参数：\nx(Var): 输入的Var张量\n返回值：Var张量。张量x应用符号函数的结果\n代码示例：>>> import jittor as jt\n>>> from jittor import nn\n>>> x = jt.array([0, 6.5, -0.7])\n>>> nn.sign(x)\njt.Var([ 0.  1. -1.], dtype=float32)",
        "return_value": "Var张量。张量x应用符号函数的结果",
        "parameters": "x(Var): 输入的Var张量",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> from jittor import nn\n>>> x = jt.array([0, 6.5, -0.7])\n>>> nn.sign(x)\njt.Var([ 0.  1. -1.], dtype=float32)"
    },
    {
        "api_name": "jittor.nn.silu",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.nn.html#jittor.nn.silu",
        "api_signature": "jittor.nn.silu(x)",
        "api_description": "该函数为Jittor的SILU(Sigmoid线性单元)激活函数。这是一个元素级别（element-wise）函数。具体来说，其计算过程如下：\n\\[\\text{SILU}(x) = x\\ *\\ \\text{Sigmoid}(x)\\]",
        "return_value": "Var张量:张量x应用SILU激活的结果",
        "parameters": "x(Var): 输入的Var张量",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> from jittor import nn\n>>> x = jt.array([0, 6.5, -0.7])\n>>> nn.silu(x)\njt.Var([ 0.          6.490242   -0.23226856], dtype=float32)"
    },
    {
        "api_name": "jittor.distributions.simple_presum",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.distributions.html#jittor.distributions.simple_presum",
        "api_signature": "jittor.distributions.simple_presum(x)",
        "api_description": "对输入的 data 进行简单的前缀和计算。公式如下：\n\\[y[i_0, ..., i_{n-1}, j] = \\sum_{k=0}^{j} x[i_0, ..., i_{n-1}, k]\\]\n其中， \\(n\\) 为 \\(x\\) 的维度数。",
        "return_value": "\\(x\\) 的 shape 向后扩展一位的结果，数据类型与 \\(x\\) 相同",
        "parameters": "x (Var): 输入的 data ，可以是任意维度。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> jt.flags.use_cuda = 1\n>>> x = jt.array([1.0, 2.0, 3.0, 4.0])\n>>> print(simple_presum(x))\njt.Var([0. 1. 3. 6. 10.])"
    },
    {
        "api_name": "jittor_core.Var.sin",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor_core.Var.sin",
        "api_signature": "jittor_core.Var.sin()",
        "api_description": "函数C++定义格式:\njt.Var sin(jt.Var x)\n创建一个张量, 其将 x 中的每一个元素进行正弦运算。\n\\[y_i = \\sin(x_i)\\]",
        "return_value": "返回一个新的张量, 其数值是 x 中对应位置进行正弦运算的结果",
        "parameters": "x (Var): Var类型的张量",
        "input_shape": "",
        "notes": "正弦函数的定义域是实数集，值域是 [-1, 1]\n支持使用 jt.sin() 进行调用",
        "code_example": ">>> x = jt.randn(5)\n>>> x\njt.Var([-1.6485653  -0.10077649 -0.7546536  -0.02543143  1.0830703 ], dtype=float32)\n>>> x.sin()\njt.Var([-0.9969775  -0.10060599 -0.68503636 -0.02542868  0.88340074], dtype=float32)"
    },
    {
        "api_name": "jittor.single_process_scope",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.single_process_scope",
        "api_signature": "jittor.single_process_scope(rank=0)",
        "api_description": "为指定的单个进程建立独立的作用域。",
        "return_value": "无返回值",
        "parameters": "rank (int): 对应进程的的编号。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> @jt.single_process_scope(rank=0)\n>>> def xxx():\n>>> ..."
    },
    {
        "api_name": "jittor_core.Var.sinh",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor_core.Var.sinh",
        "api_signature": "jittor_core.Var.sinh()",
        "api_description": "函数C++定义格式:\njt.Var sinh(jt.Var x)\n创建一个张量, 其将 x 中的每一个数值进行双曲正弦运算。\n\\[y_i = \\sinh(x_i) = \\frac{e^x - e^{-x}}{2}\\]\n参数：\nx (Var): Var类型的张量\n代码示例：>>> x = jt.randn(5)\n>>> x\njt.Var([ 0.50530916 -1.0814334   0.0039318   1.1737247  -1.4199417 ], dtype=float32)\n>>> jt.sin(x)\njt.Var([ 0.5270894  -1.3048973   0.00393181  1.4624014  -1.9475756 ], dtype=float32)\n返回值：返回一个新的张量, 其数值是 x 中对应位置进行双曲正弦运算的结果\n注意事项：\n正弦函数的定义域和值域是实数集\n支持使用 x.sinh() 进行调用",
        "return_value": "返回一个新的张量, 其数值是 x 中对应位置进行双曲正弦运算的结果",
        "parameters": "x (Var): Var类型的张量",
        "input_shape": "",
        "notes": "正弦函数的定义域和值域是实数集\n支持使用 x.sinh() 进行调用",
        "code_example": ">>> x = jt.randn(5)\n>>> x\njt.Var([ 0.50530916 -1.0814334   0.0039318   1.1737247  -1.4199417 ], dtype=float32)\n>>> jt.sin(x)\njt.Var([ 0.5270894  -1.3048973   0.00393181  1.4624014  -1.9475756 ], dtype=float32)"
    },
    {
        "api_name": "jittor.size",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.size",
        "api_signature": "jittor.size(v, dim=None)",
        "api_description": "返回该张量的大小。 如果未指定dim，则返回值是该张量的大小。 如果指定了dim，则返回一个该维度的大小。",
        "return_value": "output{int, tuple}: 如果dim为None，返回v的总大小；否则返回v在维度dim上的大小。",
        "parameters": "v (jt.Var): 输入的张量。\ndim (int， 可选): 要查询大小的维度。默认值：None",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> a = jt.random((2,3))\n>>> jt.size(a)\n6\n>>> jt.size(a, 0)\n2\n>>> jt.size(a, 1)\n3"
    },
    {
        "api_name": "jittor.nn.skip_init",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.nn.html#jittor.nn.skip_init",
        "api_signature": "jittor.nn.skip_init(module_cls, *args, **kw)",
        "api_description": "跳过初始化模块类的函数。这个函数主要用来直接返回需要实例化的模块类，而不对其进行初始化，可用于在实例化模块类后进行特殊的初始化设置。\n参数:\nmodule_cls（Var）：需要实例化的模块类\n*args（tuple）：传递给模块类实例化的非关键字参数\n**kw （dict）：传递给模块类实例化的关键字参数\n返回值:Var: 模块类的实例\n代码示例：>>> nn.skip_init(nn.Linear, 20, 30)\n# 等同于：\n>>> nn.Linear(20, 30)",
        "return_value": "Var: 模块类的实例",
        "parameters": "module_cls（Var）：需要实例化的模块类\n*args（tuple）：传递给模块类实例化的非关键字参数\n**kw （dict）：传递给模块类实例化的关键字参数",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> nn.skip_init(nn.Linear, 20, 30)\n# 等同于：\n>>> nn.Linear(20, 30)"
    },
    {
        "api_name": "jittor.dataset.SkipFirstBatchesSampler",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.dataset.html#jittor.dataset.SkipFirstBatchesSampler",
        "api_signature": "jittor.dataset.SkipFirstBatchesSampler(sampler, num_skip_batches)",
        "api_description": "在数据采样过程中, 跳过前N个批次的数据采样器。\n参数:\nsampler (jittor.dataset.Sampler): 被包装的原始采样器。被包装的采样器 sampler 必须是 Sampler 的实例或其子类之一。\nnum_skip_batches (int): 要跳过的批次数量\n属性:\nsampler (jittor.dataset.Sampler): 被包装的原始采样器。\nnum_skip_batches (int): 要跳过的批次数量。\n代码示例:  >>> # 假设有一个已经创建好的采样器\n>>> original_sampler = ...\n>>> # 创建一个新的采样器实例, 跳过开始的2个批次\n>>> skip_sampler = SkipFirstBatchesSampler(original_sampler, num_skip_batches=2)\n>>> for batch in skip_sampler:\n>>>     # 这里开始迭代的批次是原采样器的第三个批次",
        "return_value": "",
        "parameters": "sampler (jittor.dataset.Sampler): 被包装的原始采样器。被包装的采样器 sampler 必须是 Sampler 的实例或其子类之一。\nnum_skip_batches (int): 要跳过的批次数量",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> # 假设有一个已经创建好的采样器\n>>> original_sampler = ...\n>>> # 创建一个新的采样器实例, 跳过开始的2个批次\n>>> skip_sampler = SkipFirstBatchesSampler(original_sampler, num_skip_batches=2)\n>>> for batch in skip_sampler:\n>>>     # 这里开始迭代的批次是原采样器的第三个批次"
    },
    {
        "api_name": "jittor.contrib.slice_var_index",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.contrib.html#jittor.contrib.slice_var_index",
        "api_signature": "jittor.contrib.slice_var_index(x, slices)",
        "api_description": "对于给定的变量 x 和切片 slices ，执行切片操作。切片操作根据数组的索引范围、步长等信息来获取数组的一部分。该函数主要用于实现切片操作，可以将 slices 中的切片应用到 x 上，返回一个新的张量。",
        "return_value": "out_shape: 输出张量形状的列表(list)。\nout_index: 输出张量索引的列表(list)。\n0: 一个常数 0，表示无额外输出。\n[]: 空列表(list)，表示无额外输出。\nextras: 其中包含需要执行额外操作的切片(list)。",
        "parameters": "输入的张量(Var)\n\nslices( tuple, list, numpy.ndarray,Var ):\n切片可以是 int, slice, bool 等类型，具体由输入数据决定。\n如果 slices 不是元组，那么将 slices 转换为元组。\n如果 len(slices)==1 且 slices[0] 的 dtype = bool，则将 slices[0] 重定位。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> from jittor.contrib import slice_var_index\n>>> x = jt.array([[1, 2, 3], [4, 5, 6]]) \n>>> slices = (0, slice(1, None, 2)) \n>>> out_shape, out_index, _, __, ___ = slice_var_index(x, slices) \n>>> print(out_shape)\n[1, 1]\n>>> print(out_index)\n['0', '1+i1*2']"
    },
    {
        "api_name": "jittor.linalg.slogdet",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.linalg.html#jittor.linalg.slogdet",
        "api_signature": "jittor.linalg.slogdet(x)",
        "api_description": "计算输入矩阵的行列式的符号与对数值。其采用 LU 分解来计算行列式的符号和自然对数值：\n\\[det(a) = sign * exp(logdet)\\]\n其中, sign 表示行列式的符号,  logdet 表示行列式的自然对数值。",
        "return_value": "返回两个张量, 第一个张量代表行列式的符号, 第二个张量代表行列式的对数值。",
        "parameters": "x (Var): (..., M, M) 维张量, 表示需要求行列式的矩阵。其中,  \\(M\\) 代表矩阵的行列数。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> x = jt.array([[1.0,2.0],[3.0,4.0]])\n>>> sign, logdet = jt.linalg.slogdet(x)\n>>> print(f\"Sign: {sign.data}, LogDet: {logdet.data}\")\nSign: [-1.], LogDet: [0.6931472]"
    },
    {
        "api_name": "jittor.nn.smooth_l1_loss",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.nn.html#jittor.nn.smooth_l1_loss",
        "api_signature": "jittor.nn.smooth_l1_loss(y_true, y_pred, reduction='mean')",
        "api_description": "计算给定输出和目标之间的Smooth-L1损失：\n\\[\\begin{split}L_n =\n\\begin{cases}\n\\frac{0.5(x_n - y_n)^2}{\\beta}, & \\text{if } |x_n - y_n| < \\beta \\\\\n|x_n - y_n| - 0.5 \\cdot \\beta, & \\text{otherwise}\n\\end{cases}\\end{split}\\]",
        "return_value": "Var: 一个标量张量，表示Smooth-L1损失。",
        "parameters": "y_true (Var): 真实值，通常为[N, 4]的形状，但是也可以是其他形状。\ny_pred (Var): 预测值，通常为[N, 4]的形状，但是也可以是其他形状。\nreduction (str, optional): 计算损失的方式，其值只能在[‘mean’, ‘sum’, ‘none’]中选择。默认值： 'mean'",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> from jittor import nn\n>>> output = jt.array([1.0, 1.0, 1.0])\n>>> target = jt.array([0.5, 0.6, -2.0])\n>>> nn.smooth_l1_loss(target, output) \njt.Var([0.9016667], dtype=float32)"
    },
    {
        "api_name": "jittor.nn.softmax",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.nn.html#jittor.nn.softmax",
        "api_signature": "jittor.nn.softmax(x, dim=None, log=False)",
        "api_description": "此函数接收一个输入张量x，并在指定的维度上应用softmax函数。如果参数 log 为True，则应用log_softmax。具体公式如下：\n给定输入 \\(x\\) ， softmax函数可以计算为：\n\\[\\text{softmax}(x_i) = \\frac{\\exp(x_i)}{\\sum_j \\exp(x_j)}\\]\n当参数 log 为True时，log_softmax函数可以计算为：\n\\[\\text{log_softmax}(x_i) = \\log(\\frac{\\exp(x_i)}{\\sum_j \\exp(x_j)})\\]\n参数:\nx (Var): 输入张量，可以是任意维度。\ndim (int, tuple(int), optional): 指定softmax操作的维度。不指定时，操作会应用于所有的元素上。默认值：None\nlog (bool, optional): 如果设为True，则应用log_softmax操作，否则应用普通的softmax操作。默认值：False\n返回值:Var: 与输入 x 形状相同的张量，其各元素 \\(y_i\\) 等于输入x在dim维度上的softmax或者log_softmax的结果。\n代码示例：>>> import jittor as jt\n>>> from jittor import nn\n>>> x = jt.array([0.8, 0.2, 0.5])\n>>> nn.softmax(x)\njt.Var([0.4367518  0.23969446 0.32355368], dtype=float32)",
        "return_value": "Var: 与输入 x 形状相同的张量，其各元素 \\(y_i\\) 等于输入x在dim维度上的softmax或者log_softmax的结果。",
        "parameters": "x (Var): 输入张量，可以是任意维度。\ndim (int, tuple(int), optional): 指定softmax操作的维度。不指定时，操作会应用于所有的元素上。默认值：None\nlog (bool, optional): 如果设为True，则应用log_softmax操作，否则应用普通的softmax操作。默认值：False",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> from jittor import nn\n>>> x = jt.array([0.8, 0.2, 0.5])\n>>> nn.softmax(x)\njt.Var([0.4367518  0.23969446 0.32355368], dtype=float32)"
    },
    {
        "api_name": "jittor.nn.softplus",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.nn.html#jittor.nn.softplus",
        "api_signature": "jittor.nn.softplus(x, beta=1.0, threshold=20.0)",
        "api_description": "Softplus函数实现。Softplus函数是一种平滑函数，常用于构建深度神经网络的非线性变化层，对x的过大过小值进行了阈值剪裁以提高数值稳定性。具体的数学形式如下:\n\\[y = \\frac{1}{\\beta} * \\log(1 + \\exp(\\beta * x)) + \\max(0, x - \\frac{\\text{threshold}}{\\beta})\\]",
        "return_value": "Var: x应用Softplus函数之后的结果张量，与输入x形状相同",
        "parameters": "x (Var) : 输入张量\nbeta (float, optional) : 控制函数曲线的剧烈程度，默认值: 1.0\nthreshold (float, optional): 阈值，当beta * x大于threshold时，会对x进行截断，默认值: 20.0",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> x = jt.array([1.0, 2.0, 3.0])\n>>> jt.nn.softplus(x)\njt.Var([1.3132616 2.126928  3.0485873], dtype=float32)"
    },
    {
        "api_name": "jittor.nn.Softplus",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.nn.html#jittor.nn.Softplus",
        "api_signature": "jittor.nn.Softplus(beta=1, threshold=20)",
        "api_description": "逐元素应用softplus函数：\\(\\text{Softplus}(x) = \\frac{1}{\\beta} * \\log(1 + \\exp(\\beta * x))\\)。\nSoftplus 函数是 ReLU 函数的平滑近似，可确保机器输出始终为正。当输入与 β 的乘积超过一定阈值时，为了数值稳定性，会转换为线性函数。这样做可避免大数的指数运算，保持计算的稳定性。\n参数:\nbeta(float): 公式中的beta,默认值为1\nthreshold(float): 超过此值的部分将转换为线性函数。默认值为20\n代码示例:>>> m = nn.Softplus()\n>>> input = jt.rand(2)\n>>> output = m(input)\n>>> output\njt.Var([0.9192298 1.0103612], dtype=float32)",
        "return_value": "",
        "parameters": "beta(float): 公式中的beta,默认值为1\nthreshold(float): 超过此值的部分将转换为线性函数。默认值为20",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> m = nn.Softplus()\n>>> input = jt.rand(2)\n>>> output = m(input)\n>>> output\njt.Var([0.9192298 1.0103612], dtype=float32)"
    },
    {
        "api_name": "jittor.linalg.solve",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.linalg.html#jittor.linalg.solve",
        "api_signature": "jittor.linalg.solve(a, b)",
        "api_description": "求解线性矩阵方程 \\(Ax = B\\) , 通过以下公式实现：\n\\[x = A^{-1}B\\]\n其中 A 是一个非奇异矩阵, B 是一个向量或者矩阵。函数的正向传播将通过Numpy的linalg.solve实现,\n反向传播将通过计算梯度公式实现。函数的反向传播在 A 或 B 中存在0的情况下可能会得到不正确的结果。",
        "return_value": "线性方程组的解向量(Var)。",
        "parameters": "a(Var): 线性方程组的系数矩阵 \\(A\\)。\nb(Var): 线性方程组的常数向量 \\(B\\)。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> a = jt.array([[1., 2.], [3., 4.]]) \n>>> b = jt.array([5., 6.])\n>>> from jittor.linalg import solve\n>>> print(solve(a, b))\njt.Var([-4.   4.5], dtype=float32)"
    },
    {
        "api_name": "jittor.misc.sort",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.misc.sort",
        "api_signature": "jittor.misc.sort(input, dim=-1, descending=False, stable=False)",
        "api_description": "对输入张量 input 沿着指定维按升序非稳定排序。如果不给定 dim ，则默认为输入的最后一维。如果指定参数 descending 为 True ，则按降序排序。如果指定参数``stable`` 为 True ，则为稳定排序。",
        "return_value": "返回元组 (value, index) ， index 为原始输入中的下标。",
        "parameters": "input (Var): 要对比的张量\ndim (int, 可选): 沿着此维排序，默认为最后一维\ndescending (bool, 可选): 布尔值，控制升降排序，默认为升序\nstable(bool, 可选): 布尔值，控制稳定排序，默认非稳定排序",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> x = jt.random((3, 4))\njt.Var([[ 0.25843996 0.6806731 0.48188502 -0.8385996 ]\n        [-0.97904104 -1.0399561 -0.97162217 3.834338 ]\n        [-0.6144879 -0.97539455 -0.81440794 -0.12566419]], dtype=float32)\n>>> value, index = jt.sort(x)\n>>> value\njt.Var([[-0.8385996 0.25843996 0.48188502 0.6806731 ]\n        [-1.0399561 -0.97904104 -0.97162217 3.834338 ]\n        [-0.97539455 -0.81440794 -0.6144879 -0.12566419]], dtype=float32)\n>>> index\njt.Var([[3 0 2 1]\n        [1 0 2 3]\n        [1 2 0 3]], dtype=int32)"
    },
    {
        "api_name": "jittor.sparse.sparse_array",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.sparse.html#jittor.sparse.sparse_array",
        "api_signature": "jittor.sparse.sparse_array(indices, values, shape)",
        "api_description": "将给出的稀疏数组的索引、值和形状用于构建一个Jittor稀疏张量。\n对于任何给定的索引，其对应的值不应该为零。\n以下为数学表示，其中 \\((i_1, i_2, ..., i_N)\\) 是indices的某一行，  \\((j)\\) 是该行在indices中的索引：\n\\[SparseVar[i_1, i_2, ..., i_N] = values[j]\\]",
        "return_value": "jt.sparse.SparseVar: 一个Jittor的稀疏张量",
        "parameters": "indices (Var): 稀疏数组的索引，必须是二维整型的Jittor变量。其每一行是代表非零值在最终结果中的位置\nvalues (Var): 非零值组成的一维浮点型Jittor变量。\nshape (NanoVector): 稀疏张量的形状。长度必须和indices中的列数相同。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> indices = jt.array([[0, 0], [1, 2]])\n>>> values = jt.array([1, 2])\n>>> shape = jt.NanoVector([3, 4])\n>>> jt.sparse.sparse_array(indices, values, shape).shape\n[3,4,]"
    },
    {
        "api_name": "jittor.sparse.SparseVar",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.sparse.html#jittor.sparse.SparseVar",
        "api_signature": "jittor.sparse.SparseVar(indices, values, shape)",
        "api_description": "用于存储稀疏的 Var。",
        "return_value": "",
        "parameters": "indices (jt.Var): 索引值，表示稀疏 Var 中非零元素的位置。\nvalues (jt.Var): 数值，表示稀疏 Var 中非零元素的值。\nshape (jt.NanoVector): 表示稀疏 Var 的形状。",
        "input_shape": "indices: \\((D, N)\\)，其中 \\(D\\) 为稀疏 Var 维数，\\(N\\) 为非零元素索引个数。\nvalues: \\((N,)\\)，其中 \\(N\\) 为非零元素索引个数。\nshape: 长度为 \\(D\\) 的 NanoVector，其中 \\(D\\) 为稀疏 Var 维数。",
        "notes": "",
        "code_example": ">>> from jittor.sparse import SparseVar\n>>> indices = jt.array([[0, 1, 2], [1, 2, 3]])\n>>> values = jt.array([2, 3, 3])\n>>> shape = jt.NanoVector([5, 5,])\n>>> a = SparseVar(indices, values, shape)\n>>> a.to_dense()\njt.Var([[0 2 0 0 0]\n        [0 0 3 0 0]\n        [0 0 0 3 0]\n        [0 0 0 0 0]\n        [0 0 0 0 0]], dtype=int32)"
    },
    {
        "api_name": "jittor.misc.split",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.misc.split",
        "api_signature": "jittor.misc.split(d, split_size, dim=0)",
        "api_description": "将张量分割成多个块。每个块都是原始张量的一个视图。\n如果 split_size 是整型，那么张量将被平均分割成等大小的块（如果可能的话）。如果给定维度 dim 上的张量大小不能被 split_size 整除，那么最后一个块的大小将会小于其他块。\n如果 split_size 是列表，那么张量将被分割成 len(split_size) 块，每个块在 dim 维度上的大小将按照 split_size 列表指定。",
        "return_value": "分割后的张量块，以元组的形式返回: Tuple[Var, ...]",
        "parameters": "d (Var): 需要被分割的张量 Var, 任意形状。\nsplit_size (int, list(int)): 单个块的大小，或者每个块的大小的列表。注意如果这里传入列表的话，应当确保列表的元素之和等于输入张量在给定分割维度上的长度 d.shape[dim]。\ndim (int): 需要分割的张量的维度。默认值为 0。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> a = jt.arange(10).reshape(5, 2))\n>>> jt.split(a, 2)\n(jt.Var([[0 1]\n        [2 3]], dtype=int32), \njt.Var([[4 5]\n        [6 7]], dtype=int32), \njt.Var([[8 9]], dtype=int32))\n>>> jt.split(a, [3, 1, 1])\n(jt.Var([[0 1]\n        [2 3]\n        [4 5]], dtype=int32), \njt.Var([[6 7]], dtype=int32), \njt.Var([[8 9]], dtype=int32))"
    },
    {
        "api_name": "jittor.sparse.spmm",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.sparse.html#jittor.sparse.spmm",
        "api_signature": "jittor.sparse.spmm(spase_x, y)",
        "api_description": "稀疏矩阵和密集矩阵的乘法操作。此函数先将稀疏矩阵转化为密集矩阵，然后使用Jittor的矩阵乘法操作将两个矩阵相乘。此函数需要等长的rows和columns，即输入的两个矩阵可以执行矩阵乘法操作。",
        "return_value": "jt.Var:  结果矩阵。它是一个N行和P列的密集矩阵",
        "parameters": "spase_x (jittor.sparse.SparseVar): 2维稀疏矩阵。假设有N行和M列\ny (Var): 2维密集矩阵。假设有M行和P列",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> indices = jt.array([[0, 0], [1, 2]])\n>>> values = jt.array([1, 2])\n>>> shape = jt.NanoVector([3, 4])\n>>> sparse_mat = jt.sparse.sparse_array(indices, values, shape)\n[3,4,]\n>>> y = jt.randn(4,5) \n>>> jt.sparse.spmm(sparse_mat, y).shape \n[3,5,]"
    },
    {
        "api_name": "jittor.sqr",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.sqr",
        "api_signature": "jittor.sqr(x)",
        "api_description": "创建一个张量，其将 x 中的每一个数值进行平方运算。\n\\[out_i = \\sqr{input_i}\\]",
        "return_value": "返回一个新的张量, 其数值是 x 中对应位置进行平方运算后的值",
        "parameters": "x (Var): Var类型的张量",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> x = jt.randn(5)\n>>> x\njt.Var([-0.29049814  0.3832289  -0.6810643  -0.65432096  0.39759803], dtype=float32)\n>>> jt.sqr(x)\njt.Var([0.08438917 0.14686438 0.4638486  0.4281359  0.1580842 ], dtype=float32)"
    },
    {
        "api_name": "jittor_core.Var.sqrt",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor_core.Var.sqrt",
        "api_signature": "jittor_core.Var.sqrt()",
        "api_description": "函数C++定义格式:\njt.Var sqrt(jt.Var x)\n创建一个张量，其将 x 中的每一个数值进行平方根运算。\n\\[out_i = \\sqrt{input_i}\\]",
        "return_value": "返回一个新的张量, 其数值是 x 中对应位置进行平方根运算后的值",
        "parameters": "x (Var): Var类型的张量",
        "input_shape": "",
        "notes": "如果输入元素是负数，返回 nan\n支持使用 jt.sqrt() 进行调用",
        "code_example": ">>> x = jt.randn(5)\n>>> x\njt.Var([ 1.1675875  -0.36471805  2.0246403   1.1496693  -0.09650089], dtype=float32)\n>>> x.sqrt()\njt.Var([1.0805496       nan 1.4228985 1.0722263       nan], dtype=float32)"
    },
    {
        "api_name": "jittor.sqrt_",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.sqrt_",
        "api_signature": "jittor.sqrt_(x)",
        "api_description": "对给定的输入变量 x 进行开原地开平方，并将结果赋值给原变量。使用算法\n\\[\\sqrt{x}\\]\n对输入的每一个元素进行开平方。",
        "return_value": "经过开平方处理的变量(Var)。",
        "parameters": "x (Var): 需要进行开平方的变量。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> a = jt.array([4., 9.])\n>>> jt.sqrt_(a)\njt.Var([2., 3.], dtype=float32)"
    },
    {
        "api_name": "jittor.squeeze",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.squeeze",
        "api_signature": "jittor.squeeze(x, dim=None)",
        "api_description": "创建一个新张量，移除 输入变量``x`` 张量中所有长度为1的维度。如果指定维度 dim, 则只移除该维度（如果其长度为1）。",
        "return_value": "返回一个新的张量（Var），移除指定维度长度为1的维度。",
        "parameters": "x (Var): Var类型的张量\ndim (int, optional): 需要删除的维度，默认为 None 表示所有维度。",
        "input_shape": "",
        "notes": "如果 dim 传入了长度不为1的维度，或者多个维度的元组，会引起异常",
        "code_example": ">>> x = jt.zeros(2, 1, 2, 1, 2)\n>>> x.size()\n[2,1,2,1,2,]\n>>> y1 = jt.squeeze(x)\n>>> y1.size()\n[2,2,2,]\n>>> y2 = jt.squeeze(x, 1)\n>>> y2.size()\n[2,2,1,2,]"
    },
    {
        "api_name": "jittor.models.squeezenet1_0",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.models.html#jittor.models.squeezenet1_0",
        "api_signature": "jittor.models.squeezenet1_0(pretrained=False, **kwargs)",
        "api_description": "构建一个squeezenet1_0模型\nSqueezenet源自论文 SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <0.5MB model size。\n参数:\npretrained (bool, optional): 表示是否预加载预训练模型。默认为 False。\nkwargs: 其他optional参数。\n返回值:\n返回构建好的squeezenet1_0模型实例。如果 pretrained 为 True, 则返回在ImageNet上预训练的模型。\n代码示例:>>> import jittor as jt\n>>> from jittor.models.squeezenet import *\n>>> net = squeezenet1_0(pretrained=False)\n>>> x = jt.rand(1, 3, 224, 224)\n>>> y = net(x)\n>>> y.shape\n[1, 1000]",
        "return_value": "返回构建好的squeezenet1_0模型实例。如果 pretrained 为 True, 则返回在ImageNet上预训练的模型。",
        "parameters": "pretrained (bool, optional): 表示是否预加载预训练模型。默认为 False。\nkwargs: 其他optional参数。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> from jittor.models.squeezenet import *\n>>> net = squeezenet1_0(pretrained=False)\n>>> x = jt.rand(1, 3, 224, 224)\n>>> y = net(x)\n>>> y.shape\n[1, 1000]"
    },
    {
        "api_name": "jittor.models.squeezenet1_1",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.models.html#jittor.models.squeezenet1_1",
        "api_signature": "jittor.models.squeezenet1_1(pretrained=False, **kwargs)",
        "api_description": "构建一个squeezenet1_1模型\nSqueezenet源自论文 SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <0.5MB model size。\n参数:\npretrained (bool, optional): 表示是否预加载预训练模型。默认为 False。\nkwargs: 其他optional参数。\n返回值:\n返回构建好的squeezenet1_1模型实例。如果 pretrained 为 True, 则返回在ImageNet上预训练的模型。\n代码示例:>>> import jittor as jt\n>>> from jittor.models.squeezenet import *\n>>> net = squeezenet1_1(pretrained=False)\n>>> x = jt.rand(1, 3, 224, 224)\n>>> y = net(x)\n>>> y.shape\n[1, 1000]",
        "return_value": "返回构建好的squeezenet1_1模型实例。如果 pretrained 为 True, 则返回在ImageNet上预训练的模型。",
        "parameters": "pretrained (bool, optional): 表示是否预加载预训练模型。默认为 False。\nkwargs: 其他optional参数。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> from jittor.models.squeezenet import *\n>>> net = squeezenet1_1(pretrained=False)\n>>> x = jt.rand(1, 3, 224, 224)\n>>> y = net(x)\n>>> y.shape\n[1, 1000]"
    },
    {
        "api_name": "jittor.models.SqueezeNet",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.models.html#jittor.models.SqueezeNet",
        "api_signature": "jittor.models.SqueezeNet(version='1_0', num_classes=1000)",
        "api_description": "Squeezenet源自论文 SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <0.5MB model size。\n注意:\n当 version 为 ‘1_0’ 时, 模型结构特定为:Conv1 -> Relu -> Pool -> Fire(96, 16, 64, 64) -> … -> Fire(512, 64, 256, 256)\n当 version 为 ‘1_1’ 时, 模型结构为:Conv1 -> Relu -> Pool -> Fire(64, 16, 64, 64) -> … -> Fire(512, 64, 256, 256)\n最终, 模型通过一个分类器进行类别判断, 分类器包含一个Dropout层、一个1x1卷积层和一个平均池化层。\n参数:\nversion (str, optional): SqueezeNet的版本, 可以是’1_0’或’1_1’, 分别对应不同的网络结构配置。版本’1_0’使用7x7的卷积核进行第一层卷积, 版本’1_1’则使用更小的3x3卷积核以进一步减小模型大小。默认值: ‘1_0’。\nnum_classes (int, optional): 模型最后输出的类别数。默认值: 1000\n代码示例:  >>> import jittor as jt\n>>> from jittor.models.squeezenet import SqueezeNet\n>>> model = SqueezeNet(version='1_0', num_classes=1000)\n>>> x = jt.rand([10, 3, 224, 224])\n>>> output = model.execute(x)\n>>> print(output.shape)\n[10,1000,]",
        "return_value": "",
        "parameters": "version (str, optional): SqueezeNet的版本, 可以是’1_0’或’1_1’, 分别对应不同的网络结构配置。版本’1_0’使用7x7的卷积核进行第一层卷积, 版本’1_1’则使用更小的3x3卷积核以进一步减小模型大小。默认值: ‘1_0’。\nnum_classes (int, optional): 模型最后输出的类别数。默认值: 1000",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> from jittor.models.squeezenet import SqueezeNet\n>>> model = SqueezeNet(version='1_0', num_classes=1000)\n>>> x = jt.rand([10, 3, 224, 224])\n>>> output = model.execute(x)\n>>> print(output.shape)\n[10,1000,]"
    },
    {
        "api_name": "jittor.misc.stack",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.misc.stack",
        "api_signature": "jittor.misc.stack(x, dim=0)",
        "api_description": "将多个形状相同的张量组合成为一个更高维的张量",
        "return_value": "组合为的张量，形状 \\((n_0, \\ldots, n_{\\mathtt{dim} - 1}, \\mathtt{len(x)}, n_{\\mathtt{dim}}, \\ldots, n_k)\\)",
        "parameters": "x (list[Var]): 被组合的张量，每个形状相同，都是 \\((n_0, \\ldots, n_{\\mathtt{dim}}, \\ldots, n_k)\\)\ndim (int): 第几个维度前插入一个维度。默认值：0",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "jittor_core.Var.start_grad",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor_core.Var.start_grad",
        "api_signature": "jittor_core.Var.start_grad()",
        "api_description": "函数C++定义格式:\njt.Var start_grad()\n开启当前变量的梯度记录。",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "jittor.std",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.std",
        "api_signature": "jittor.std(x)",
        "api_description": "计算给定张量 x 的标准差。\n标准差的计算公式如下:\n\\[\\sigma=\\sqrt{\\sum_{i=0}^{N-1}\\left(x_i-\\bar{x}\\right)^2}\\]\n其中 \\(\\bar{x}\\) 为 x 的均值, \\(N\\) 为 x 中元素的数量，  \\(x_i\\) 为 x 中的第i个元素",
        "return_value": "输入张量 x 的标准差,类型为jittor.Var。",
        "parameters": "x (jt.Var) : 输入的jittor张量。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> x = jt.array([1.0, 2.0, 3.0, 4.0])\n>>> jt.std(x)\njt.Var([1.2909944], dtype=float32)"
    },
    {
        "api_name": "jittor.lr_scheduler.StepLR",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.optim.html#jittor.lr_scheduler.StepLR",
        "api_signature": "jittor.lr_scheduler.StepLR(optimizer, step_size, gamma=0.1, last_epoch=-1)",
        "api_description": "实现了对学习率的周期性衰减调度，每达设定的步数时，对学习率进行一次衰减。",
        "return_value": "",
        "parameters": "optimizer (Optimizer) : 使用的优化器。\nstep_size (int) : 设定的步数，即每过多少轮后，进行一次衰减。\ngamma (float, 可选) : 学习率衰减的系数, 默认为 0.1\nlast_epoch (int, 可选) : 上个周期（epoch）的编号，即初始的步数。默认为 -1 ，表示从第一个周期开始调整。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> scheduler = StepLR(optimizer, step_size=30, gamma=0.1)\n>>> for epoch in range(100):\n...     train(...)\n...     validate(...)\n...     scheduler.step()"
    },
    {
        "api_name": "jittor_core.Var.stop_fuse",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor_core.Var.stop_fuse",
        "api_signature": "jittor_core.Var.stop_fuse()",
        "api_description": "函数C++定义格式:\ninline jt.Var stop_fuse()\n停止算子的fusion，注意这可能会影响性能。",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "jittor_core.Var.stop_grad",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor_core.Var.stop_grad",
        "api_signature": "jittor_core.Var.stop_grad()",
        "api_description": "函数C++定义格式:\ninline jt.Var stop_grad()\n关闭当前变量的梯度记录。",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "jittor_core.Var.sub",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor_core.Var.sub",
        "api_signature": "jittor_core.Var.sub()",
        "api_description": "函数C++定义格式:\njt.Var subtract(jt.Var x, jt.Var y)\n对两个张量中对应元素计算求差，也可以使用 - 运算符调用",
        "return_value": "x 和 y 每对对应元素相减结果，形状与 x 和 y 相同",
        "parameters": "x (Var): 输入数据\ny (Var): 输入数据，形状与 x 相同",
        "input_shape": "",
        "notes": "jt.sub() 和 jt.subtract() 是等价的。",
        "code_example": ">>> x = jt.array([1, 2, 4])\n>>> y = jt.array([3, 4, 5])\n>>> x.subtract(y)\njt.Var([-2 -2 -1], dtype=int32)\n>>> x - y\njt.Var([-2 -2 -1], dtype=int32)"
    },
    {
        "api_name": "jittor.dataset.SubsetRandomSampler",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.dataset.html#jittor.dataset.SubsetRandomSampler",
        "api_signature": "jittor.dataset.SubsetRandomSampler(dataset, indice)",
        "api_description": "从给定的索引列表中随机抽取元素。\n参数:\ndataset (jittor.dataset.Dataset): 需要采样的数据集。\nindices (tuple[int]): 用于采样的索引范围, 包括开始索引和结束索引, 形如(start_index, end_index)。索引范围`indices`是左闭右开区间, 即包括开始索引但不包括结束索引。开始索引需大于等于0, 结束索引需小于数据集的长度且大于开始索引。如果索引范围不满足要求, 则抛出AssertionError。\n代码示例:  >>> import jittor as jt\n>>> from jittor.dataset import Dataset\n>>> from jittor.dataset import SubsetRandomSampler\n>>> class MyDataset(Dataset):\n>>>      def __len__(self):\n>>>          return 10\n>>>\n>>>      def __getitem__(self, index):\n>>>          return index\n>>>\n>>>\n>>> dataset = MyDataset()\n>>> sampler = SubsetRandomSampler(dataset, (3, 7))\n>>> list(iter(sampler))\n[4, 3, 5, 6]",
        "return_value": "",
        "parameters": "dataset (jittor.dataset.Dataset): 需要采样的数据集。\nindices (tuple[int]): 用于采样的索引范围, 包括开始索引和结束索引, 形如(start_index, end_index)。索引范围`indices`是左闭右开区间, 即包括开始索引但不包括结束索引。开始索引需大于等于0, 结束索引需小于数据集的长度且大于开始索引。如果索引范围不满足要求, 则抛出AssertionError。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> from jittor.dataset import Dataset\n>>> from jittor.dataset import SubsetRandomSampler\n>>> class MyDataset(Dataset):\n>>>      def __len__(self):\n>>>          return 10\n>>>\n>>>      def __getitem__(self, index):\n>>>          return index\n>>>\n>>>\n>>> dataset = MyDataset()\n>>> sampler = SubsetRandomSampler(dataset, (3, 7))\n>>> list(iter(sampler))\n[4, 3, 5, 6]"
    },
    {
        "api_name": "jittor_core.Var.subtract",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor_core.Var.subtract",
        "api_signature": "jittor_core.Var.subtract()",
        "api_description": "函数C++定义格式:\njt.Var subtract(jt.Var x, jt.Var y)\n对两个张量中对应元素计算求差，也可以使用 - 运算符调用",
        "return_value": "x 和 y 每对对应元素相减结果，形状与 x 和 y 相同",
        "parameters": "x (Var): 输入数据\ny (Var): 输入数据，形状与 x 相同",
        "input_shape": "",
        "notes": "jt.sub() 和 jt.subtract() 是等价的。",
        "code_example": ">>> x = jt.array([1, 2, 4])\n>>> y = jt.array([3, 4, 5])\n>>> x.subtract(y)\njt.Var([-2 -2 -1], dtype=int32)\n>>> x - y\njt.Var([-2 -2 -1], dtype=int32)"
    },
    {
        "api_name": "jittor_core.Var.sum",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor_core.Var.sum",
        "api_signature": "jittor_core.Var.sum()",
        "api_description": "函数C++定义格式:\njt.Var reduce_add(jt.Var x,  int dim,  bool keepdims=false)\n对张量的 dims 这些维度求和",
        "return_value": "求和的结果。如果 keepdims 为 True 则形状为 x 将 dims 里给出的维度的长度变成 1，否则是 x 的形状去掉 dims 给出的维度",
        "parameters": "x (Var): 输入数据\ndim (tuple[int]): 在哪些维度求和。默认是全部元素的最大值。默认值： ()\nkeepdims (bool): 是否在结果中保留 dim 维度。默认值：False",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> x = jt.randint(0, 10, shape=(2, 3))\n>>> x\njt.Var([[2 7 9]\n        [2 6 3]], dtype=int32)\n>>> x.sum()\njt.Var([29], dtype=int32)\n>>> x.sum((0, 1))\njt.Var([29], dtype=int32)"
    },
    {
        "api_name": "jittor.linalg.svd",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.linalg.html#jittor.linalg.svd",
        "api_signature": "jittor.linalg.svd(x)",
        "api_description": "计算输入矩阵的奇异值分解。计算方式遵循公式： \\(x\\) =  \\(u\\) \\(s\\) \\(v\\) , 其中 \\(u\\) 、 \\(s\\) 、 \\(v\\) 分别是输入矩阵的左奇异向量、奇异值和右奇异向量。",
        "return_value": "返回奇异值分解后的各部分, u,s,v(ndarray)",
        "parameters": "x (ndarray, jittor.Var): 输入的待分解矩阵。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> from jittor.linalg import svd\n>>> X = jt.Var([[1., 0., 0., 0., 2.], [0., 0., 3., 0., 0.], [0., 0., 0., 0., 0.], [0., 2., 0., 0., 0.]])\n>>> u, s, v = svd(X)\n>>> print(u)\njt.Var([[-0.  1.  0.  0.]\n        [ 0.  0.  1.  0.]\n        [-1.  0.  0.  0.]\n        [ 0.  0.  0.  1.]], dtype=float32)"
    },
    {
        "api_name": "jittor_core.Var.swap",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor_core.Var.swap",
        "api_signature": "jittor_core.Var.swap()",
        "api_description": "函数C++定义格式:\ninline jt.Var swap(jt.Var v)\n和另外一个Var交换数据内容。",
        "return_value": "就地交换，返回交换后 x 的值。",
        "parameters": "v (Var): 另一个Var",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> x, y = jt.randn(5), jt.randn(5)\n>>> x\njt.Var([-1.9465057  -0.17227498  0.08347224 -1.2592555   0.1071676 ], dtype=float32)\n>>> y\njt.Var([ 0.27618277 -0.8328144  -0.33380997  0.03772787  0.89321905], dtype=float32)\n>>> x.swap(y)\njt.Var([ 0.27618277 -0.8328144  -0.33380997  0.03772787  0.89321905], dtype=float32)\n>>> x\njt.Var([ 0.27618277 -0.8328144  -0.33380997  0.03772787  0.89321905], dtype=float32)\n>>> y\njt.Var([-1.9465057  -0.17227498  0.08347224 -1.2592555   0.1071676 ], dtype=float32)"
    },
    {
        "api_name": "jittor.misc.t",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.misc.t",
        "api_signature": "jittor.misc.t(x)",
        "api_description": "转置张量的最后两个维度",
        "return_value": "转置后的张量，形状 \\((n_0, n_1, \\ldots, n_{k - 3}, n_{k - 1}, n_{k - 2})\\)",
        "parameters": "x (Var): 被转置的张量，形状 \\((n_0, n_1, \\ldots, n_{k - 3}, n_{k - 2}, n_{k - 1})\\)",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "jittor_core.Var.tan",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor_core.Var.tan",
        "api_signature": "jittor_core.Var.tan()",
        "api_description": "函数C++定义格式:\njt.Var tan(jt.Var x)\n创建一个张量, 其将 x 中的每一个数值进行正切运算，即正弦值除以余弦值。\n\\[y_i = \\tan(x_i) = \\frac{\\sin(x)}{\\cos(x)}\\]",
        "return_value": "返回一个新的张量, 其数值是 x 中对应位置进行正切运算的结果",
        "parameters": "x (Var): Var类型的张量",
        "input_shape": "",
        "notes": "该函数的值域为实数集，定义域为\n\n\n\\[x \\in \\mathbb{R} \\setminus \\left\\{ \\left(2k+1\\right)\\frac{\\pi}{2} \\mid k \\in \\mathbb{Z} \\right\\}\\]\n\n支持使用 jt.tan() 进行调用",
        "code_example": ">>> x = jt.randn(5)\n>>> x\njt.Var([-0.1146331  -0.22897322 -1.1813502   0.7146521  -0.02658333], dtype=float32)\n>>> x.tan()\njt.Var([-0.11513788 -0.23306055 -2.4366024   0.8676503  -0.02658959], dtype=float32)"
    },
    {
        "api_name": "jittor_core.Var.tanh",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor_core.Var.tanh",
        "api_signature": "jittor_core.Var.tanh()",
        "api_description": "函数C++定义格式:\njt.Var tanh(jt.Var x)\n创建一个张量, 其将 x 中的每一个数值进行双曲正切运算。\n\\[\\tanh(x) = \\frac{{e^{x} - e^{-x}}}{{e^{x} + e^{-x}}}\\]",
        "return_value": "返回一个新的张量, 其数值是 x 中对应位置进行双曲正切运算的结果",
        "parameters": "x (Var): Var类型的张量",
        "input_shape": "",
        "notes": "双曲正切运算的定义域为所有实数，值域为 (-1, 1)\n支持使用 jt.tanh() 进行调用",
        "code_example": ">>> x = jt.randn(5) \n>>> x\njt.Var([ 0.561267    1.7155168  -0.53718305  1.6382825   0.5129911 ], dtype=float32)\n>>> x.tanh()\njt.Var([ 0.508917    0.93732095 -0.4908527   0.9272321   0.47227246], dtype=float32)"
    },
    {
        "api_name": "jittor.nn.Tanh",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.nn.html#jittor.nn.Tanh",
        "api_signature": null,
        "api_description": "该类用于对张量中的每个元素进行 Tanh 激活函数运算，公式如下：\n\\[\\text{Tanh}(x) = \\tanh(x) = \\frac{\\exp(x) - \\exp(-x)} {\\exp(x) + \\exp(-x)}\\]",
        "return_value": "",
        "parameters": "",
        "input_shape": "Input: \\((*)\\)，其中 * 表示任意数量的额外维度\nOutput: \\((*)\\)，维度和输入相同",
        "notes": "",
        "code_example": ">>> m = nn.Tanh()\n>>> input = jt.randn(5)\n>>> input\njt.Var([ 1.7310377  -3.0513763   0.79816824  1.806881    0.9393758 ], dtype=float32)\n>>> output = m(input)\n>>> output\njt.Var([ 0.93917847 -0.99553657  0.66301143  0.94751394  0.7349353 ], dtype=float32)"
    },
    {
        "api_name": "jittor_core.Var.tape",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor_core.Var.tape",
        "api_signature": "jittor_core.Var.tape()",
        "api_description": "函数C++定义格式:\njt.Var tape(jt.Var x)\n此函数用于计算Jittor变量的tape。",
        "return_value": "输入变量的tape。",
        "parameters": "x(Var): 需要计算tape的Jittor变量。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> jt.set_seed(2)\n>>> a = jt.random([2,3]) \n>>> tape_a = a.tape()\njt.Var([[0.4359949  0.18508208 0.02592623]\n[0.93154085 0.5496625  0.9477306 ]], dtype=float32)"
    },
    {
        "api_name": "jittor.transform.TenCrop",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.transform.html#jittor.transform.TenCrop",
        "api_signature": "jittor.transform.TenCrop(size, vertical_flip=False)",
        "api_description": "将给定图像裁剪成四个角落和中心部分，以及这些部分的翻转版本（默认为水平翻转，可选择垂直翻转），共十张图片。",
        "return_value": "",
        "parameters": "size (sequence, int): 裁剪的期望输出大小。如果size是一个整数而不是像(h, w)这样的序列，会进行一个(size, size)的正方形裁剪\nvertical_flip (bool, 可选): 若为True，则使用垂直翻转。否则使用水平翻转。默认值: False",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> from jittor import transform\n>>> import numpy as np\n>>> from PIL import Image\n>>> data = np.random.rand(200, 200, 3)\n>>> img = Image.fromarray(data, 'RGB')\n>>> ten_crop = transform.TenCrop(16)\n>>> imgs = ten_crop(img)\n>>> imgs[0].size, imgs[1].size, imgs[2].size, imgs[8].size, imgs[9].size \n((16, 16), (16, 16), (16, 16), (16, 16), (16, 16))"
    },
    {
        "api_name": "jittor.dataset.TensorDataset",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.dataset.html#jittor.dataset.TensorDataset",
        "alias": "VarDataset"
    },
    {
        "api_name": "jittor.dataset.Dataset.terminate",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.dataset.html#jittor.dataset.Dataset.terminate",
        "api_signature": "terminate()",
        "api_description": "终止数据集的工作进程。\n代码示例:  >>> dataset.terminate()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> dataset.terminate()"
    },
    {
        "api_name": "jittor_core.Var.ternary",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor_core.Var.ternary",
        "api_signature": "jittor_core.Var.ternary()",
        "api_description": "函数C++定义格式:\njt.Var ternary(jt.Var cond, jt.Var x, jt.Var y)\n根据条件变量cond的值，从变量x或y中选择一个作为输出。当cond为真时，返回x的值；否则，返回y的值。这个功能类似于C/C++中的三元运算符。",
        "return_value": "output (jittor.Var): 返回由cond决定的，x或y中的一个Jittor张量。",
        "parameters": "cond (jittor.Var): 一个布尔类型的Jittor张量，用于决定返回x还是y。\nx (jittor.Var): 当cond为真时返回的Jittor张量。\ny (jittor.Var): 当cond为假时返回的Jittor张量。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> cond = jt.Var(False)\n>>> x = jt.Var(2)\n>>> y = jt.Var(1)\n>>> z = jt.ternary(cond, x, y)\n>>> print(z)\njt.Var([1], dtype=int32)"
    },
    {
        "api_name": "jittor.misc.to",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.misc.to",
        "api_signature": "jittor.misc.to(x, *args, **kargs)",
        "api_description": "将 Jittor变量 x 转换为指定类型或将其移动到指定计算设备上。\n这个函数可以对Jittor变量进行类型转换(如，将整数类型转换为浮点数类型)，还可以把数据在CPU和GPU设备之间移动(假设你的系统有GPU设备，且已经安装好了CUDA)。",
        "return_value": "Var: 一个新的 Jittor 变量，已移至指定设备或转换为新的数据类型。如果未指定设备或数据类型，则返回 ‘x’ 的克隆。",
        "parameters": "NanoString 或可调用对象：在这种情况下，x 将被转换为指定的数据类型。\nstr: 指定设备的字符串（’cuda’ 或 ‘cpu’）。如果字符串中包含 ‘cuda’，Jittor 将使用 CUDA（GPU）进行未来的操作。如果字符串中包含 ‘cpu’，Jittor 将使用 CPU。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> var = jt.array([1, 2, 3])\n>>> var_cuda = to(var, 'cuda') # 移动到 GPU\n>>> var_float = to(var, jt.float32) # 转换为 float32"
    },
    {
        "api_name": "jittor.to_bool",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.to_bool",
        "api_signature": "jittor.to_bool(v)",
        "api_description": "将输入变量v中元素转化为布尔值.",
        "return_value": "output (bool): 输出转化之后的布尔值。",
        "parameters": "v (jittor.Var): 输入只能是单个元素的Var, 其中的元素必须是布尔值(bool)或者是整数(int)。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> print(jt.to_bool(jt.Var([1])))\nTrue\n>>> print(jt.to_bool(jt.Var(0)))\nFalse\n>>> print(jt.to_bool(jt.Var(False)))\nFalse\n>>> print(jt.to_bool(jt.Var(9)))\nTrue"
    },
    {
        "api_name": "jittor.to_float",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.to_float",
        "api_signature": "jittor.to_float(v)",
        "api_description": "将输入变量v中元素转化为浮点数类型.",
        "return_value": "output (float): 输出转化之后的浮点数。",
        "parameters": "v (jittor.Var): 输入只能是单个元素的张量(jittor.Var)。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> print(jt.to_float(jt.Var(1)))\n1.0\n>>> print(jt.to_float(jt.Var([[True]])))\n1.0"
    },
    {
        "api_name": "jittor.to_int",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.to_int",
        "api_signature": "jittor.to_int(v)",
        "api_description": "将v中元素转化为整数.",
        "return_value": "output (int): 输出转化之后的整数。",
        "parameters": "v (jittor.Var): 输入只能是单个元素的Var。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> print(jt.to_int(jt.Var(True)))\n1\n>>> print(jt.to_int(jt.Var([[1.6]])))\n1"
    },
    {
        "api_name": "jittor.dataset.Dataset.to_jittor",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.dataset.html#jittor.dataset.Dataset.to_jittor",
        "api_signature": "to_jittor(batch)",
        "api_description": "",
        "return_value": "jt.Var: 一个包含输入变量的列表的 Jittor 变量。",
        "parameters": "batch (list): 需要转换的输入变量的列表。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> batch = [np.array([1,2,3]), np.array([4,5,6])]\n>>> to_jittor(batch)\n>>> [jt.Var([1, 2, 3]), jt.Var([4, 5, 6])]"
    },
    {
        "api_name": "jittor.transform.to_pil_image",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.transform.html#jittor.transform.to_pil_image",
        "api_signature": "jittor.transform.to_pil_image(pic, mode=None)",
        "api_description": "将一个jt.Var或np.ndarray转换为PIL Image对象",
        "return_value": "PIL Image: 转换后的PIL Image",
        "parameters": "pic (Var, numpy.ndarray): 需要被转换为PIL Image的图像，应为HWC格式\nmode (PIL.Image mode, optional): 输入数据的颜色空间和像素深度。默认值：None",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> import numpy as np\n>>> from PIL import Image\n>>> data = np.random.rand(200,200, 3)\n>>> img = jt.transform.to_pil_image(data) \n>>> img.show()"
    },
    {
        "api_name": "jittor.transform.to_tensor",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.transform.html#jittor.transform.to_tensor",
        "api_signature": "jittor.transform.to_tensor(pic)",
        "api_description": "将Image.Image对象转换为格式为CHW的np.array",
        "return_value": "经格式转换、规范化等操作后的np.array对象",
        "parameters": "pic (PIL.Image.Image): 输入图像",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> import numpy as np\n>>> from PIL import Image\n>>> data = np.random.rand(200, 200, 3)\n>>> img = Image.fromarray(data, 'RGB')   \n>>> arr = jt.transform.to_tensor(img) \n>>> type(arr), arr.shape\n(<class 'numpy.ndarray'>, (3, 200, 200))"
    },
    {
        "api_name": "jittor.misc.tolist",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.misc.tolist",
        "api_signature": "jittor.misc.tolist(x)",
        "api_description": "将输入的 Jittor Var 类型变量转化为 Python 的 list 类型。",
        "return_value": "转化后的 Python 列表类型 (list)",
        "parameters": "x (Var): 任意形状的输入张量 Var。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> a = jt.array([1,2,3])\n>>> jt.tolist(a)\n[1, 2, 3]"
    },
    {
        "api_name": "jittor.transform.ToPILImage",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.transform.html#jittor.transform.ToPILImage",
        "api_signature": "jittor.transform.ToPILImage(mode=None)",
        "api_description": "将张量或ndarray转换为PIL图像。该类将一个C x H x W 的张量或者一个H x W x C 型的numpy ndarray转换成 PIL Image ，同时根据mode调整value的范围。",
        "return_value": "",
        "parameters": "pic (Var, numpy.ndarray): 将被转换成PIL image 的 HWC 格式的 image。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> from jittor import transform\n>>> transform = transform.ToPILImage('RGB')\n>>> img_ = transform(img_var)"
    },
    {
        "api_name": "jittor.misc.topk",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.misc.topk",
        "api_signature": "jittor.misc.topk(input, k, dim=None, largest=True, sorted=True)",
        "api_description": "在指定维度上保留最大/最小的 k 个元素\n该函数实现了在输入 Var input 的指定维度上找出最大（或最小）的 k 个元素，并返回这些元素的值以及其在原始 Var 中的索引。返回的 Var 的大小与 input 一致，除了在指定的维度上大小为 k",
        "return_value": "Tuple[Var], 其中包含 input 在指定维度上仅保留最大/最小 k 个元素后的张量 (Var)，以及这些元素在 input 中的索引 (Var)。",
        "parameters": "input (Var): 输入张量 Var, 任意形状\nk (int):  指定寻找最大/最小元素个数\ndim (int): 指定在哪个维度上寻找最大/最小元素，默认为最后一个维度\nlargest (bool): 为 True 时，返回最大的 k 个元素；为 False 时，返回最小的 k 个元素。默认为 True\nsorted (bool): 为 True 时，返回的 k 个元素按照从大到小的顺序排列；为 False 时，返回的 k 个元素顺序是未指定的。默认为 True",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> var = jt.array([[1, 2, 3, 4],[5, 6, 7, 8],[9, 10, 11, 12]])\n>>> jt.topk(var, 2, dim=0)\n(jt.Var([[ 9 10 11 12]\n        [ 5  6  7  8]], dtype=int32), \njt.Var([[2 2 2 2]\n        [1 1 1 1]], dtype=int32))\n>>> jt.topk(var, 2, dim=1)\n(jt.Var([[ 4  3]\n        [ 8  7]\n        [12 11]], dtype=int32), \njt.Var([[3 2]\n        [3 2]\n        [3 2]], dtype=int32))"
    },
    {
        "api_name": "jittor.transform.ToTensor",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.transform.html#jittor.transform.ToTensor",
        "api_signature": null,
        "api_description": "将 PIL Image 或 numpy.ndarray 格式的图像转换为 Tensor。\nToTensor 类的实例是一个可调用对象，它可以将 PIL 库中的图像或者 numpy.ndarray 类型的数组转换为 Jittor 的 Tensor 格式。此操作通常用于数据预处理阶段。转换后的 Tensor 数据格式为 C x H x W，并且数据范围被归一化到 [0, 1]。",
        "return_value": "Var: 转换后的 Tensor，形状为 C x H x W，范围 [0.0, 1.0]。",
        "parameters": "pic (PIL.Image, numpy.ndarray): 需要转换为 Tensor 的图像对象。如果是 PIL 图像，则应为 L, LA, RGB, RGBA, P, PA, I, I;16, I;16L, I;16B, I;16P, F 等模式之一。如果是 numpy.ndarray，则应为形状为 H x W x C，数据类型为 np.uint8 的数组。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> from jittor import transform\n>>> import numpy as np\n>>> from PIL import Image\n>>> data = np.random.rand(200, 200, 3)\n>>> img = Image.fromarray(data, 'RGB')\n>>> to_tensor = transform.ToTensor()\n>>> tensor = to_tensor(img)\n>>> print(tensor.dtype, tensor.shape)\nfloat32 (3, 200, 200)"
    },
    {
        "api_name": "jittor.Module.train",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.Module.train",
        "api_signature": "train()",
        "api_description": "将模块设置为评估training模式。",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "jittor.transpose",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.transpose",
        "api_signature": "jittor.transpose(x, *dim)",
        "api_description": "函数C++定义格式:\njt.Var transpose(jt.Var x,  NanoVector axes=NanoVector())\n返回一个张量，其数据与原始张量 x 相同，其指定的维度已经根据 dim 参数中的指示进行了交换。\n参数：\nx (Var): Var类型的张量\naxes (int…): 整数序列，表示需要交换的维度, 默认为空，表示对所有轴进行转置",
        "return_value": "返回改变维度的张量(Var).",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> x = jt.randn((3,4))\n>>> x\njt.Var([[ 1.7929314   0.8375565   0.13141492  1.4283884 ]\n        [ 0.0627789   1.4741095  -0.3331003   0.34370992]\n        [ 0.405448    1.6529875   0.19640142 -0.09192007]], dtype=float32)\n>>> jt.transpose(x)\njt.Var([[ 1.7929314   0.0627789   0.405448  ]\n        [ 0.8375565   1.4741095   1.6529875 ]\n        [ 0.13141492 -0.3331003   0.19640142]\n        [ 1.4283884   0.34370992 -0.09192007]], dtype=float32)"
    },
    {
        "api_name": "jittor.misc.tril",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.misc.tril",
        "api_signature": "jittor.misc.tril(input: Var, diagonal: int = 0)",
        "api_description": "返回一个矩阵（2-D张量）或批量矩阵输入的下三角部分，结果张量out的其它元素被设为0.",
        "return_value": "下三角矩阵，类型为 Var。",
        "parameters": "input(Var): 输入张量。\ndiagonal(int): 考虑的对角线，默认值为 0。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> a = jt.ones(3, 3)\n>>> b = jt.tril(a)\n>>> assert jt.all_equal(b, [[1,0,0],[1,1,0],[1,1,1]])\n>>> b = jt.tril(a, diagonal=1)\n>>> assert jt.all_equal(b, [[1,1,0],[1,1,1],[1,1,1]])\n>>> b = jt.tril(a, diagonal=-1)\n>>> assert jt.all_equal(b, [[0,0,0],[1,0,0],[1,1,0]])"
    },
    {
        "api_name": "jittor.misc.triu",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.misc.triu",
        "api_signature": "jittor.misc.triu(input: Var, diagonal: int = 0)",
        "api_description": "返回输入的矩阵 (即2-D张量) 或者批量矩阵的上三角部分，其他元素被设置为0。",
        "return_value": "返回上三角的张量，类型为 Var .",
        "parameters": "input(Var): 输入张量。\ndiagonal(int): 考虑的对角线，默认值为 0。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> a = jt.ones(3, 3)\n>>> b = jt.triu(a)\n>>> assert jt.all_equal(b, [[1,1,1],[0,1,1],[0,0,1]])\n>>> b = jt.triu(a, diagonal=1)\n>>> assert jt.all_equal(b, [[0,1,1],[0,0,1],[0,0,0]])\n>>> b = jt.triu(a, diagonal=-1)\n>>> assert jt.all_equal(b, [[1,1,1],[1,1,1],[0,1,1]])"
    },
    {
        "api_name": "jittor.misc.triu_",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.misc.triu_",
        "api_signature": "jittor.misc.triu_(x, diagonal=0)",
        "api_description": "返回输入二维矩阵或批量矩阵输入 x 的上三角部分，其余部分为 0。注意输入矩阵可以不是方阵。\n对角线的偏移量可以用 diagonal 确定，diagonal = 0 代表主对角线， diagonal > 0 为沿右上方移动， diagonal < 0 为沿左下方移动。",
        "return_value": "基于输入的上三角矩阵, Var",
        "parameters": "x (Var): 输入的 Var， 维度需要大于等于 2\ndiagonal (int): 对角偏移量，默认为 0。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> x = jt.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=\"float32\")\n>>> jt.triu_(x, 0)\njt.Var([[1. 2. 3.]\n        [0. 5. 6.]\n        [0. 0. 9.]], dtype=float32)\n>>> jt.triu_(x, 1)\njt.Var([[0. 2. 3.]\n        [0. 0. 6.]\n        [0. 0. 0.]], dtype=float32)\n>>> jt.triu_(x, -1)\njt.Var([[1. 2. 3.]\n        [4. 5. 6.]\n        [0. 8. 9.]], dtype=float32)"
    },
    {
        "api_name": "jittor.init.trunc_normal_",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.init.html#jittor.init.trunc_normal_",
        "api_signature": "jittor.init.trunc_normal_(var: Var, mean: float = 0.0, std: float = 1.0, a: float = -2.0, b: float = 2.0)",
        "api_description": "将输入的 Var 用一个截断正态分布中的值填充。\n具体而言，trunc_normal 函数采样的方法如下：\n首先，根据指定的均值 mean 和标准差 std ，从正态分布中采样一个随机值 \\(x\\) 。\n然后，对采样的随机值 \\(x\\) 进行截断操作，将其限制在指定的范围 [a, b] 内。具体而言，如果  \\(x\\) 小于下界 a ，则将它替换为 a ；如果x大于上界 b，则将它替换为 b。最后，将截断后的值作为初始化值赋给张量 var 的对应元素。",
        "return_value": "使用截断正态分布填充的输入变量(Var)",
        "parameters": "var (Var): 一个 n 维的变量\nmean (float，可选): 正态分布的均值，默认值：0.0\nstd (float，可选): 正态分布的标准差，默认值：1.0\na (float，可选): 最小截断值，默认值：-2.0\nb (float，可选): 最大截断值，默认值： 2.0",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> from jittor import init\n>>> from jittor import nn\n>>> linear = nn.Linear(2,2)\n>>> init.trunc_normal_(linear.weight, std=.02)\n>>> linear.weight"
    },
    {
        "api_name": "jittor.type_as",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.type_as",
        "api_signature": "jittor.type_as(a, b)",
        "api_description": "将张量 a 的数据类型修改为与张量 b 的数据类型相同。",
        "return_value": "a 中数据类型已经修改为张量 b 的数据类型的新张量。类型为jittor.Var。",
        "parameters": "a (jt.Var) : 输入的源张量，需要修改其数据类型的张量。\nb (jt.Var) : 输入的目标张量，将会根据此张量的数据类型修改a的数据类型。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> a = jt.float32([1,2,3])\n>>> b = jt.int32([4,5,6])\n>>> a = jt.type_as(a, b)\n>>> print(a)\njt.Var([1 2 3], dtype=int32)"
    },
    {
        "api_name": "jittor_core.Var.uint16",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor_core.Var.uint16",
        "api_signature": "jittor_core.Var.uint16()",
        "api_description": "函数C++定义格式:\njt.Var uint16_(jt.Var x)\n创建并返回一个 x 的张量副本，并将其类型转换为 uint16 ，数据范围为 [0, 65535]",
        "return_value": "返回一个新的张量, 数据类型为 uint16",
        "parameters": "x (Var): Var类型的张量",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> x = jt.randn((2,2))\n>>> x\njt.Var([[-0.7970826   0.99872327]\n        [-0.543484   -2.133479  ]], dtype=float32)\n>>> x.uint16()\njt.Var([[    0     0]\n        [    0 65534]], dtype=uint16)"
    },
    {
        "api_name": "jittor_core.Var.uint32",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor_core.Var.uint32",
        "api_signature": "jittor_core.Var.uint32()",
        "api_description": "函数C++定义格式:\njt.Var uint32_(jt.Var x)\n创建并返回一个 x 的张量副本，并将其类型转换为 uint32 ，数据范围为 [0, 4294967295]",
        "return_value": "返回一个新的张量, 数据类型为 uint32",
        "parameters": "x (Var): Var类型的张量",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> x = jt.randn((2,2))\n>>> x\njt.Var([[-0.7970826   0.99872327]\n        [-0.543484   -2.133479  ]], dtype=float32)\n>>> x.uint32()\njt.Var([[         0          0]\n        [         0 4294967294]], dtype=uint32)"
    },
    {
        "api_name": "jittor_core.Var.uint64",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor_core.Var.uint64",
        "api_signature": "jittor_core.Var.uint64()",
        "api_description": "函数C++定义格式:\njt.Var uint64_(jt.Var x)\n创建并返回一个 x 的张量副本，并将其类型转换为 uint64 ，数据范围为 [0, 18446744073709551615]",
        "return_value": "返回一个新的张量, 数据类型为 uint64",
        "parameters": "x (Var): Var类型的张量",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> x = jt.randn((2,2))\n>>> x\njt.Var([[-0.7970826   0.99872327]\n        [-0.543484   -2.133479  ]], dtype=float32)\n>>> x.uint64()\njt.Var([[                   0                    0]\n        [                   0 18446744073709551614]], dtype=uint64)"
    },
    {
        "api_name": "jittor_core.Var.uint8",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor_core.Var.uint8",
        "api_signature": "jittor_core.Var.uint8()",
        "api_description": "函数C++定义格式:\njt.Var uint8_(jt.Var x)\n创建并返回一个 x 的张量副本，并将其类型转换为 uint8 ，数据范围为 [0, 255]",
        "return_value": "返回一个新的张量, 数据类型为 uint8",
        "parameters": "x (Var): Var类型的张量",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> x = jt.randn((2,2))\n>>> x\njt.Var([[-0.7970826   0.99872327]\n        [-0.543484   -2.133479  ]], dtype=float32)\n>>> x.uint8()\njt.Var([[  0   0]\n        [  0 254]], dtype=uint8)"
    },
    {
        "api_name": "jittor_core.Var.unary",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor_core.Var.unary",
        "api_signature": "jittor_core.Var.unary()",
        "api_description": "函数C++定义格式:\njt.Var unary(jt.Var x,  String op)\n函数的目的是在输入张量上应用一个一元运算。具体的一元运算是由字符串参数`op`指定的。",
        "return_value": "结果张量(Var)。运算 op 被应用到输入张量 x 的每一个元素上，结果是一个新的张量，与输入张量有相同的形状。",
        "parameters": "x(Var): 输入的张量。\nop(str):一元运算的字符串。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> x = jt.array([1.0, -1.0, 3.0, -3.0])\n>>> x.unary('abs')\n[1., 1., 3., 3.]"
    },
    {
        "api_name": "jittor.misc.unbind",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.misc.unbind",
        "api_signature": "jittor.misc.unbind(x, dim=0)",
        "api_description": "解除var的一个维度。返回沿着给定维度的所有切片的元组，这个维度会从输出的各个切片中移除。",
        "return_value": "返回一个 List ，包含了沿着给定维度的所有切片的元组。",
        "parameters": "x (Var): 需要解除维度的var\ndim (int): 需要移除的维度，默认值为 0",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> a = jt.random((2,3))\n>>> b = jt.unbind(a, 0)\n>>> c = jt.unbind(a, 1)\n>>> a,b,c\njt.Var([[0.2874082  0.46987164 0.13281713]\n        [0.2576157  0.7470389  0.48285943]], dtype=float32)\n[jt.Var([0.2874082  0.46987164 0.13281713], dtype=float32), jt.Var([0.2576157  0.7470389  0.48285943],  dtype=float32)]\n[jt.Var([0.2874082 0.2576157], dtype=float32), jt.Var([0.46987164 0.7470389 ], dtype=float32), jt.Var([0.13281713 0.48285943], dtype=float32)]"
    },
    {
        "api_name": "jittor.nn.unfold",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.nn.html#jittor.nn.unfold",
        "api_signature": "jittor.nn.unfold(X, kernel_size, dilation=1, padding=0, stride=1)",
        "api_description": "将输入的4维张量按照规定的滑窗大小、步幅等参数展开。\n考虑一个形状为(N,C,∗) 的批量输入张量，其中 N 是批次维度，C 是通道维度，而 ∗ 代表任意的空间维度。这个操作将输入的空间维度内的每个滑动大小为 kernel_size  的块展平成一个列（即，最后一个维度），形成一个形状为 (N, \\(C \\times \\prod(\\text{kernel_size})\\), L) 的3-D输出张量，其中 \\(C \\times \\prod(\\text{kernel_size})\\) 是每个块内的总值数（一个块有 \\(\\prod(\\text{kernel_size})\\) 个空间位置，每个位置包含一个 C 个通道的值），L 是这样的块的总数。\n\\[L = \\prod_{i=0}^{d-1} \\left\\lfloor\\frac{\\text{input_size}[i] + 2 \\times \\text{padding}[i] - \\text{dilation}[i]\n\\times (\\text{kernel_size}[i] - 1) - 1}{\\text{stride}[i]} + 1\\right\\rfloor\\]",
        "return_value": "output(Var): 展开后的张量",
        "parameters": "X (Var): 输入的4维张量，形状为 \\((N, C, H, W)\\)。\nkernel_size (Union[int, tuple[int]]): 滑窗的大小，可以是单个整数或者是一个长度为2的tuple，分别表示滑窗的高和宽。\ndilation (Union[int, tuple[int]], optional): 滑窗元素之间的间距，可以是单个整数或者是一个长度为2的tuple，分别表示滑窗的高和宽。默认为1。\npadding (Union[int, tuple[int]], optional): 输入在高和宽维度上的padding大小，可以是单个整数或者是一个长度为2的tuple，分别表示高和宽维度上的padding大小。默认为0。\nstride (Union[int, tuple[int]], optional): 滑窗的步幅大小，可以是单个整数或者是一个长度为2的tuple，分别表示滑窗的高和宽。默认为1。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> input = jt.rand(2,5,3,4)\n>>> jt.nn.unfold(input, (2,3)).shape\n[2, 30, 4]"
    },
    {
        "api_name": "jittor.init.uniform",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.init.html#jittor.init.uniform",
        "api_signature": "jittor.init.uniform(shape, dtype='float32', low=0, high=1)",
        "api_description": "创建一个张量，其形状由可变参数 shape 决定， 其数局类型由 dtype 决定， 其数值满足如下均匀分布：\n\\[out_i \\sim \\mathcal{U}(\\text{low}, \\text{high})\\]",
        "return_value": "返回一个数值符合均匀分布的张量",
        "parameters": "shape (Tuple[int]): 整型序列，定义了输出的形状\ndtype (var.dtype, 可选): 数据类型，默认为 float32\nlow (int or Var, 可选): 均匀分布的下界， 默认值为 0\nhigh (int or Var, 可选): 均匀分布的上界，默认值为 1",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> init.uniform((3,2))\njt.Var([[0.82488173 0.7875814 ]\n        [0.8802127  0.7350983 ]\n        [0.79734576 0.9903714 ]], dtype=float32)\n>>> init.uniform((2,2), 'float32', 0, 10)\njt.Var([[0.35382926 6.8104105 ]\n        [2.5638878  1.2676293 ]], dtype=float32)"
    },
    {
        "api_name": "jittor.init.uniform_",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.init.html#jittor.init.uniform_",
        "api_signature": "jittor.init.uniform_(var, low=0, high=1)",
        "api_description": "原地修改张量 var ，将其数值填充满足如下均匀分布的随机数：\n\\[out_i \\sim \\mathcal{U}(\\text{low}, \\text{high})\\]",
        "return_value": "就地修改张量 var ，返回一个数值符合均匀分布的张量",
        "parameters": "var (Var): Var 类型的张量\nlow (int or Var, 可选): 均匀分布的下界， 默认值为 0\nhigh (int or Var, 可选): 均匀分布的上界，默认值为 1",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> x = jt.ones((3,2))\n>>> x\njt.Var([[1. 1.]\n        [1. 1.]\n        [1. 1.]], dtype=float32)\n>>> init.uniform_(x)\n>>> x\njt.Var([[0.82488173 0.7875814 ]\n        [0.8802127  0.7350983 ]\n        [0.79734576 0.9903714 ]], dtype=float32)"
    },
    {
        "api_name": "jittor.distributions.Uniform",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.distributions.html#jittor.distributions.Uniform",
        "api_signature": "jittor.distributions.Uniform(low, high)",
        "api_description": "生成指定范围内的均匀分布的随机数。如果需要产生一定形状（shape）的均匀分布样本，或者计算在指定位置的对数概率和熵，可以分别使用 sample 和 log_prob ， entropy 三个方法。",
        "return_value": "",
        "parameters": "low (float): 均匀分布的下界。\nhigh (float): 均匀分布的上界，必须大于参数 low 。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> unif = Uniform(1, 3)\n>>> unif.entropy()\njt.Var([0.6931472], dtype=float32)"
    },
    {
        "api_name": "jittor.misc.unique",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.misc.unique",
        "api_signature": "jittor.misc.unique(input: Var, return_inverse: bool = False, return_counts: bool = False, dim: int | None = None)",
        "api_description": "返回张量中去重后的元素",
        "return_value": "如果 return_inverse 未指定，则返回 Var 为去重结果\n否则，如果 return_inverse 和 return_counts 指定，则返回 tuple[Var, Var, Var] 为去重结果、下标、出现次数\n否则，仅 return_inverse 指定，则返回 tuple[Var, Var] 为去重结果、下标",
        "parameters": "input (Var): 被去重的张量\nreturn_inverse (bool): 是否返回 input 每个元素对应去重结果中的不重复元素下标。默认值：False\nreturn_counts (bool): 是否返回去重结果中每个不重复元素在 input 里出现次数，必须同时指定 return_inverse=True 才有效。默认值：False\ndim (int): 去重的维度，如果为 None 则将输入扁平化后再去重。默认值：None",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> jt.unique(jt.array([1, 3, 2, 3]))\njt.Var([1 2 3], dtype=int32)\n>>> jt.unique(jt.array([1, 3, 2, 3, 2]), return_inverse=True, return_counts=True)\n(jt.Var([1 2 3], dtype=int32), jt.Var([0 2 1 2 1], dtype=int32), jt.Var([1 2 2], dtype=int32))\n>>> jt.unique(jt.array([[1, 3], [2, 3]]), return_inverse=True)\n(jt.Var([1 2 3], dtype=int32), jt.Var([[0 2]\n                                       [1 2]], dtype=int32))\n>>> jt.unique(jt.array([[1, 3], [1, 3]]), dim=0)\njt.Var([[1 3]], dtype=int32)"
    },
    {
        "api_name": "jittor.unsqueeze",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.unsqueeze",
        "api_signature": "jittor.unsqueeze(x, dim)",
        "api_description": "创建一个新张量，其数据与原始张量 x 相同，其形状在指定的 dim 维度上增加了一个大小为 1 的新维度。",
        "return_value": "返回一个新的张量（Var），其形状在指定的 dim 维度上增加了一个大小为 1 的新维度。其他维度保持不变。",
        "parameters": "x (Var): Var类型的张量\ndim (int): 插入单维度的索引值，该值的范围需要满足 [-x.dim() - 1, x.dim() + 1) 。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> x = jt.array([1,2,3,4])\njt.Var([1 2 3 4], dtype=int32)\n>>> jt.unsqueeze(x, 0)\njt.Var([[1 2 3 4]], dtype=int32)\n>>> jt.unsqueeze(x, 1)\njt.Var([[1]\n        [2]\n        [3]\n        [4]], dtype=int32)"
    },
    {
        "api_name": "jittor_core.Var.update",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor_core.Var.update",
        "api_signature": "jittor_core.Var.update()",
        "api_description": "函数C++定义格式:\njt.Var update(jt.Var v)\n更新参数和全局变量。和assign不同，它会停止原始变量和分配变量之间的梯度，并在后台更新。",
        "return_value": "Var: 返回自身，但实际上已经原地复制完毕",
        "parameters": "v (Var): 另一个Var",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> x, y = jt.randn(5), jt.randn(5)\n>>> x\njt.Var([-1.3378737   0.8863208   1.9383168   0.35481802  1.4916613 ], dtype=float32)\n>>> y\njt.Var([ 0.59842813  1.281807    0.05155467 -0.74704844 -1.0341489 ], dtype=float32)\n>>> x.update(y)\njt.Var([ 0.59842813  1.281807    0.05155467 -0.74704844 -1.0341489 ], dtype=float32)\n>>> x\njt.Var([ 0.59842813  1.281807    0.05155467 -0.74704844 -1.0341489 ], dtype=float32)"
    },
    {
        "api_name": "jittor.nn.upsample",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.nn.html#jittor.nn.upsample",
        "api_signature": "jittor.nn.upsample(img, size, mode='nearest', align_corners=False, tf_mode=False)",
        "api_description": "根据设定的模式(mode)对给定的图像进行大小调整。",
        "return_value": "output(Var): 调整后的图像张量，形状为 \\((N, C, size[0], size[1])\\)",
        "parameters": "img (Var): 输入图像张量，形状为 \\((N, C, H, W)\\)\nsize (Union[int, Tuple[int, int]]): 输出图像的大小，可以是整数或者整数元组\nmode (str): 插值模式，可选 ‘nearest’ (默认), ‘bicubic’, ‘area’,’bilinear’\nalign_corners (bool): 默认为False, 如果设置为 True，输入和输出张量通过其角像素的中心点对齐，保留角像素处的值。如果设置为 False，输入和输出张量通过其角像素的角点对齐，插值使用边缘值填充来处理边界外的值。\ntf_mode (bool): 默认为False",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> x = jt.randn(4,3,32,32)\n>>> output_size = (64, 64)\n>>> jt.nn.upsample(x, output_size).shape\n[4, 3, 64, 64]"
    },
    {
        "api_name": "jittor.nn.Upsample",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.nn.html#jittor.nn.Upsample",
        "api_signature": "jittor.nn.Upsample(scale_factor=None, mode='nearest')",
        "api_description": "上采样模块，用于将输入的张量在空间维度（宽和高）上进行上采样\n输入张量的形状为 \\((N, C, H_{in}, W_{in})\\)，输出张量的形状为 \\((N, C, H_{out}, W_{out})\\)，其中：\n\\[\\begin{split}H_{out} = \\lfloor H_{in} \\times \\text{scale_factor[0]} \\rfloor \\\\\nW_{out} = \\lfloor W_{in} \\times \\text{scale_factor[1]} \\rfloor\\end{split}\\]",
        "return_value": "",
        "parameters": "scale_factor (float, tuple): 上采样的尺度因子\nmode (str): 上采样的模式，可选值为: ‘nearest’ | ‘linear’ | ‘area’",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> m = nn.Upsample((1.1,1.1))\n>>> input = jt.rand(2,3,32,32)\n>>> m(input).shape\n[2, 3, 35, 35]"
    },
    {
        "api_name": "jittor.nn.UpsamplingBilinear2d",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.nn.html#jittor.nn.UpsamplingBilinear2d",
        "api_signature": "jittor.nn.UpsamplingBilinear2d(scale_factor=None)",
        "api_description": "对由多个输入通道组成的输入信号应用2D双线性上采样。\n输入张量的形状为 \\((N, C, H_{in}, W_{in})\\)，输出张量的形状为 \\((N, C, H_{out}, W_{out})\\)，其中：\n\\[\\begin{split}H_{out} = \\lfloor H_{in} \\times \\text{scale_factor[0]} \\rfloor \\\\\nW_{out} = \\lfloor W_{in} \\times \\text{scale_factor[1]} \\rfloor\\end{split}\\]",
        "return_value": "",
        "parameters": "scale_factor (float, tuple): 上采样的尺度因子",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> m = nn.UpsamplingBilinear2d((1.1,1.1))\n>>> input = jt.rand(2,3,32,32)\n>>> m(input).shape\n[2, 3, 35, 35]"
    },
    {
        "api_name": "jittor.nn.UpsamplingNearest2d",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.nn.html#jittor.nn.UpsamplingNearest2d",
        "api_signature": "jittor.nn.UpsamplingNearest2d(scale_factor=None)",
        "api_description": "对由多个输入通道组成的输入信号应用2D最近邻上采样。\n输入张量的形状为 \\((N, C, H_{in}, W_{in})\\)，输出张量的形状为 \\((N, C, H_{out}, W_{out})\\)，其中：\n\\[\\begin{split}H_{out} = \\lfloor H_{in} \\times \\text{scale_factor[0]} \\rfloor \\\\\nW_{out} = \\lfloor W_{in} \\times \\text{scale_factor[1]} \\rfloor\\end{split}\\]",
        "return_value": "",
        "parameters": "scale_factor (float, tuple): 上采样的尺度因子",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> m = nn.UpsamplingNearest2d((1.1,1.1))\n>>> input = jt.rand(2,3,32,32)\n>>> m(input).shape\n[2, 3, 35, 35]"
    },
    {
        "api_name": "jittor.var",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.var",
        "api_signature": "jittor.var(x, dim=None, dims=None, unbiased=False, keepdims=False)",
        "api_description": "返回样本对指定维度的方差。如果``unbiased``为True，则使用贝塞尔修正。",
        "return_value": "返回计算得到的方差,类型为jittor.Var。",
        "parameters": "x (jt.Var) : 输入的jittor变量。\ndim (int) : 计算方差的维度。如果dim和dims都为None，则将计算整个张量的方差。默认值: None\ndims (tuple of int) : 计算方差的维度。如果dim和dims都为None，则将计算整个张量的方差。默认值: None\nunbiased (bool) : 如果为True，则使用贝塞尔修正。\nkeepdims (bool) : 用于指定计算的输出是否保持原始张量的维度。如果 keepdims=True，输出形状与输入形状相同。比如，输入的张量形状是 (3, 4) ，方差是对第一维（dim=0）求的，则输出的形状还是 (3, 4)。如果 keepdims=False，则输出的形状就是 (4,)。默认值: False 。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> a = jt.rand(3)\n>>> a\njt.Var([0.79613626 0.29322362 0.19785859], dtype=float32)\n>>> a.var()\njt.Var([0.06888353], dtype=float32)\n>>> a.var(unbiased=True)\njt.Var([0.10332529], dtype=float32)"
    },
    {
        "api_name": "jittor.dataset.VarDataset",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.dataset.html#jittor.dataset.VarDataset",
        "api_signature": "jittor.dataset.VarDataset(*args)",
        "api_description": "使用 Var 对象直接创建数据集的类, TensorDataset 是 VarDataset 的别名。这个类允许用户直接从 Jittor 变量中创建数据集, 而无需对数据执行任何预处理。数据集中的每个元素都是根据相应的索引从给定的 Jittor 变量中提取的。所有输入变量的第一个维度长度必须相等, 否则创建 VarDataset 时会触发错误。\n参数:\n*args (jt.Var): 一个或多个 Jittor 变量。所有变量的长度必须相同, 且变量的维度数可以是任意的。这些变量将会并行被索引, 以创建数据集中的条目。\n代码示例:  >>> import jittor as jt\n>>> from jittor.dataset import VarDataset\n>>> x = jt.array([1,2,3])\n>>> y = jt.array([4,5,6])\n>>> z = jt.array([7,8,9])\n>>> dataset = VarDataset(x, y, z)\n>>> dataset.set_attrs(batch_size=1)\n>>> for a, b, c in dataset:\n>>>     print(a, b, c)\n>>> #  1, 4, 7\n>>> #  2, 5, 8\n>>> #  3, 6, 9",
        "return_value": "",
        "parameters": "*args (jt.Var): 一个或多个 Jittor 变量。所有变量的长度必须相同, 且变量的维度数可以是任意的。这些变量将会并行被索引, 以创建数据集中的条目。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> from jittor.dataset import VarDataset\n>>> x = jt.array([1,2,3])\n>>> y = jt.array([4,5,6])\n>>> z = jt.array([7,8,9])\n>>> dataset = VarDataset(x, y, z)\n>>> dataset.set_attrs(batch_size=1)\n>>> for a, b, c in dataset:\n>>>     print(a, b, c)\n>>> #  1, 4, 7\n>>> #  2, 5, 8\n>>> #  3, 6, 9"
    },
    {
        "api_name": "jittor.transform.vflip",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.transform.html#jittor.transform.vflip",
        "api_signature": "jittor.transform.vflip(img)",
        "api_description": "对给定的图像进行垂直翻转。",
        "return_value": "PIL.Image.Image: 垂直翻转后的图像",
        "parameters": "img (PIL.Image.Image): 输入图像",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> import numpy as np\n>>> from PIL import Image\n>>> data = np.random.rand(200,200)\n>>> img = Image.fromarray(data, 'L')       \n>>> img.size\n(200, 200)\n>>> img_vflipped = jt.transform.vflip(img) # 垂直翻转"
    },
    {
        "api_name": "jittor.models.vgg11",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.models.html#jittor.models.vgg11",
        "api_signature": "jittor.models.vgg11(pretrained=False, **kwargs)",
        "api_description": "构建VGG11模型\nVGG模型源自 Very Deep Convolutional Networks for Large-Scale Image Recognition。\n参数:\npretrained (bool): 表示是否加载预训练的VGG11模型。默认为 False。如果设为 True, 函数会自动下载并加载预训练的VGG11模型。\n**kwargs: 可变参数, 允许用户传递额外的、自定义的参数给 _vgg 函数。\n代码示例:>>> import jittor as jt\n>>> from jittor.models.vgg import *\n>>> net = vgg11(pretrained=True)\n>>> x = jt.rand(1, 3, 224, 224)\n>>> y = net(x)\n>>> y.shape\n[1,1000,]\n返回值:\n返回一个VGG11模型实例。如果 pretrained=True, 则返回的模型将加载预训练权重；否则, 返回一个未经训练的VGG11模型。",
        "return_value": "返回一个VGG11模型实例。如果 pretrained=True, 则返回的模型将加载预训练权重；否则, 返回一个未经训练的VGG11模型。",
        "parameters": "pretrained (bool): 表示是否加载预训练的VGG11模型。默认为 False。如果设为 True, 函数会自动下载并加载预训练的VGG11模型。\n**kwargs: 可变参数, 允许用户传递额外的、自定义的参数给 _vgg 函数。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> from jittor.models.vgg import *\n>>> net = vgg11(pretrained=True)\n>>> x = jt.rand(1, 3, 224, 224)\n>>> y = net(x)\n>>> y.shape\n[1,1000,]"
    },
    {
        "api_name": "jittor.models.vgg11_bn",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.models.html#jittor.models.vgg11_bn",
        "api_signature": "jittor.models.vgg11_bn(pretrained=False, **kwargs)",
        "api_description": "构建带有批量归一化的VGG11模型\nVGG模型源自 Very Deep Convolutional Networks for Large-Scale Image Recognition。\n参数:\npretrained (bool): 表示是否加载预训练的VGG11模型。默认为 False。如果设为 True, 函数会自动下载并加载预训练的VGG11模型。\n**kwargs: 可变参数, 允许用户传递额外的、自定义的参数给 _vgg 函数。\n返回值:\n返回一个VGG11_BN模型实例。如果 pretrained=True, 则返回的模型将加载预训练权重；否则, 返回一个未经训练的VGG11_BN模型。\n代码示例:>>> import jittor as jt\n>>> from jittor.models.vgg import *\n>>> net = vgg11_bn(pretrained=True)\n>>> x = jt.rand(1, 3, 224, 224)\n>>> y = net(x)\n>>> y.shape\n[1,1000,]",
        "return_value": "返回一个VGG11_BN模型实例。如果 pretrained=True, 则返回的模型将加载预训练权重；否则, 返回一个未经训练的VGG11_BN模型。",
        "parameters": "pretrained (bool): 表示是否加载预训练的VGG11模型。默认为 False。如果设为 True, 函数会自动下载并加载预训练的VGG11模型。\n**kwargs: 可变参数, 允许用户传递额外的、自定义的参数给 _vgg 函数。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> from jittor.models.vgg import *\n>>> net = vgg11_bn(pretrained=True)\n>>> x = jt.rand(1, 3, 224, 224)\n>>> y = net(x)\n>>> y.shape\n[1,1000,]"
    },
    {
        "api_name": "jittor.models.vgg13",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.models.html#jittor.models.vgg13",
        "api_signature": "jittor.models.vgg13(pretrained=False, **kwargs)",
        "api_description": "构建并返回VGG13模型\nVGG模型源自 Very Deep Convolutional Networks for Large-Scale Image Recognition。\n参数:\npretrained (bool): 表示是否加载预训练的VGG13模型。默认为 False。如果设为 True, 函数会自动下载并加载预训练的VGG13模型。\n**kwargs: 可变参数, 允许用户传递额外的、自定义的参数给 _vgg 函数。\n返回值:\n返回一个VGG13模型实例。如果 pretrained=True, 则返回的模型将加载预训练权重；否则, 返回一个未经训练的VGG13模型。\n代码示例:>>> import jittor as jt\n>>> from jittor.models.vgg import *\n>>> net = vgg13(pretrained=True)\n>>> x = jt.rand(1, 3, 224, 224)\n>>> y = net(x)\n>>> y.shape\n[1,1000,]",
        "return_value": "返回一个VGG13模型实例。如果 pretrained=True, 则返回的模型将加载预训练权重；否则, 返回一个未经训练的VGG13模型。",
        "parameters": "pretrained (bool): 表示是否加载预训练的VGG13模型。默认为 False。如果设为 True, 函数会自动下载并加载预训练的VGG13模型。\n**kwargs: 可变参数, 允许用户传递额外的、自定义的参数给 _vgg 函数。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> from jittor.models.vgg import *\n>>> net = vgg13(pretrained=True)\n>>> x = jt.rand(1, 3, 224, 224)\n>>> y = net(x)\n>>> y.shape\n[1,1000,]"
    },
    {
        "api_name": "jittor.models.vgg13_bn",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.models.html#jittor.models.vgg13_bn",
        "api_signature": "jittor.models.vgg13_bn(pretrained=False, **kwargs)",
        "api_description": "构建带有批量归一化的VGG13模型\nVGG模型源自 Very Deep Convolutional Networks for Large-Scale Image Recognition。\n参数:\npretrained (bool): 表示是否加载预训练的VGG13模型。默认为 False。如果设为 True, 函数会自动下载并加载预训练的VGG13模型。\n**kwargs: 可变参数, 允许用户传递额外的、自定义的参数给 _vgg 函数。\n返回值:\n返回一个VGG13_BN模型实例。如果 pretrained=True, 则返回的模型将加载预训练权重；否则, 返回一个未经训练的VGG13_BN模型。\n代码示例:>>> import jittor as jt\n>>> from jittor.models.vgg import *\n>>> net = vgg13_bn(pretrained=True)\n>>> x = jt.rand(1, 3, 224, 224)\n>>> y = net(x)\n>>> y.shape\n[1,1000,]",
        "return_value": "返回一个VGG13_BN模型实例。如果 pretrained=True, 则返回的模型将加载预训练权重；否则, 返回一个未经训练的VGG13_BN模型。",
        "parameters": "pretrained (bool): 表示是否加载预训练的VGG13模型。默认为 False。如果设为 True, 函数会自动下载并加载预训练的VGG13模型。\n**kwargs: 可变参数, 允许用户传递额外的、自定义的参数给 _vgg 函数。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> from jittor.models.vgg import *\n>>> net = vgg13_bn(pretrained=True)\n>>> x = jt.rand(1, 3, 224, 224)\n>>> y = net(x)\n>>> y.shape\n[1,1000,]"
    },
    {
        "api_name": "jittor.models.vgg16",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.models.html#jittor.models.vgg16",
        "api_signature": "jittor.models.vgg16(pretrained=False, **kwargs)",
        "api_description": "构建VGG16模型\nVGG模型源自 Very Deep Convolutional Networks for Large-Scale Image Recognition。\n参数:\npretrained (bool): 表示是否加载预训练的VGG16模型。默认为 False。如果设为 True, 函数会自动下载并加载预训练的VGG16模型。\n**kwargs: 可变参数, 允许用户传递额外的、自定义的参数给 _vgg 函数。\n返回值:\n返回一个VGG16模型实例。如果 pretrained=True, 则返回的模型将加载预训练权重；否则, 返回一个未经训练的VGG16模型。\n代码示例:>>> import jittor as jt\n>>> from jittor.models.vgg import *\n>>> net = vgg16(pretrained=True)\n>>> x = jt.rand(1, 3, 224, 224)\n>>> y = net(x)\n>>> y.shape\n[1,1000,]",
        "return_value": "返回一个VGG16模型实例。如果 pretrained=True, 则返回的模型将加载预训练权重；否则, 返回一个未经训练的VGG16模型。",
        "parameters": "pretrained (bool): 表示是否加载预训练的VGG16模型。默认为 False。如果设为 True, 函数会自动下载并加载预训练的VGG16模型。\n**kwargs: 可变参数, 允许用户传递额外的、自定义的参数给 _vgg 函数。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> from jittor.models.vgg import *\n>>> net = vgg16(pretrained=True)\n>>> x = jt.rand(1, 3, 224, 224)\n>>> y = net(x)\n>>> y.shape\n[1,1000,]"
    },
    {
        "api_name": "jittor.models.vgg16_bn",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.models.html#jittor.models.vgg16_bn",
        "api_signature": "jittor.models.vgg16_bn(pretrained=False, **kwargs)",
        "api_description": "构建带有批量归一化的VGG16模型\nVGG模型源自 Very Deep Convolutional Networks for Large-Scale Image Recognition。\n参数:\npretrained (bool): 表示是否加载预训练的VGG16模型。默认为 False。如果设为 True, 函数会自动下载并加载预训练的VGG16模型。\n**kwargs: 可变参数, 允许用户传递额外的、自定义的参数给 _vgg 函数。\n返回值:\n返回一个VGG16_BN模型实例。如果 pretrained=True, 则返回的模型将加载预训练权重；否则, 返回一个未经训练的VGG16_BN模型。\n代码示例:>>> import jittor as jt\n>>> from jittor.models.vgg import *\n>>> net = vgg16_bn(pretrained=True)\n>>> x = jt.rand(1, 3, 224, 224)\n>>> y = net(x)\n>>> y.shape\n[1,1000,]",
        "return_value": "返回一个VGG16_BN模型实例。如果 pretrained=True, 则返回的模型将加载预训练权重；否则, 返回一个未经训练的VGG16_BN模型。",
        "parameters": "pretrained (bool): 表示是否加载预训练的VGG16模型。默认为 False。如果设为 True, 函数会自动下载并加载预训练的VGG16模型。\n**kwargs: 可变参数, 允许用户传递额外的、自定义的参数给 _vgg 函数。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> from jittor.models.vgg import *\n>>> net = vgg16_bn(pretrained=True)\n>>> x = jt.rand(1, 3, 224, 224)\n>>> y = net(x)\n>>> y.shape\n[1,1000,]"
    },
    {
        "api_name": "jittor.models.vgg19",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.models.html#jittor.models.vgg19",
        "api_signature": "jittor.models.vgg19(pretrained=False, **kwargs)",
        "api_description": "构建VGG19模型\nVGG模型源自 Very Deep Convolutional Networks for Large-Scale Image Recognition。\n参数:\npretrained (bool): 表示是否加载预训练的VGG19模型。默认为 False。如果设为 True, 函数会自动下载并加载预训练的VGG19模型。\n**kwargs: 可变参数, 允许用户传递额外的、自定义的参数给 _vgg 函数。\n返回值:\n返回一个VGG19模型实例。如果 pretrained=True, 则返回的模型将加载预训练权重；否则, 返回一个未经训练的VGG19模型。\n代码示例:>>> import jittor as jt\n>>> from jittor.models.vgg import *\n>>> net = vgg19(pretrained=True)\n>>> x = jt.rand(1, 3, 224, 224)\n>>> y = net(x)\n>>> y.shape\n[1,1000,]",
        "return_value": "返回一个VGG19模型实例。如果 pretrained=True, 则返回的模型将加载预训练权重；否则, 返回一个未经训练的VGG19模型。",
        "parameters": "pretrained (bool): 表示是否加载预训练的VGG19模型。默认为 False。如果设为 True, 函数会自动下载并加载预训练的VGG19模型。\n**kwargs: 可变参数, 允许用户传递额外的、自定义的参数给 _vgg 函数。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> from jittor.models.vgg import *\n>>> net = vgg19(pretrained=True)\n>>> x = jt.rand(1, 3, 224, 224)\n>>> y = net(x)\n>>> y.shape\n[1,1000,]"
    },
    {
        "api_name": "jittor.models.vgg19_bn",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.models.html#jittor.models.vgg19_bn",
        "api_signature": "jittor.models.vgg19_bn(pretrained=False, **kwargs)",
        "api_description": "构建带有批量归一化的VGG19模型\nVGG模型源自 Very Deep Convolutional Networks for Large-Scale Image Recognition。\n参数:\npretrained (bool): 表示是否加载预训练的VGG19模型。默认为 False。如果设为 True, 函数会自动下载并加载预训练的VGG19模型。\n**kwargs: 可变参数, 允许用户传递额外的、自定义的参数给 _vgg 函数。\n返回值:\n返回一个VGG19_BN模型实例。如果 pretrained=True, 则返回的模型将加载预训练权重；否则, 返回一个未经训练的VGG19_BN模型。\n代码示例:>>> import jittor as jt\n>>> from jittor.models.vgg import *\n>>> net = vgg19_bn(pretrained=True)\n>>> x = jt.rand(1, 3, 224, 224)\n>>> y = net(x)\n>>> y.shape\n[1,1000,]",
        "return_value": "返回一个VGG19_BN模型实例。如果 pretrained=True, 则返回的模型将加载预训练权重；否则, 返回一个未经训练的VGG19_BN模型。",
        "parameters": "pretrained (bool): 表示是否加载预训练的VGG19模型。默认为 False。如果设为 True, 函数会自动下载并加载预训练的VGG19模型。\n**kwargs: 可变参数, 允许用户传递额外的、自定义的参数给 _vgg 函数。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> from jittor.models.vgg import *\n>>> net = vgg19_bn(pretrained=True)\n>>> x = jt.rand(1, 3, 224, 224)\n>>> y = net(x)\n>>> y.shape\n[1,1000,]"
    },
    {
        "api_name": "jittor.models.VGG",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.models.html#jittor.models.VGG",
        "api_signature": "jittor.models.VGG(features, num_classes=1000, init_weights=True)",
        "api_description": "VGG模型源自 Very Deep Convolutional Networks for Large-Scale Image Recognition。\n参数:\nfeatures (nn.Module): 用来构建VGG中的卷积层的模块\nnum_classes (int, optional): 分类的类别数量。默认值: 1000\ndrop_last (bool, optional): 是否初始化权重。默认值: True\n属性:\nfeatures (nn.Module): VGG卷积层部分。\navgpool (nn.AdaptiveAvgPool2d): 自适应平均池化层, 输出固定为 7x7\nclassifier (nn.Sequential): 分类器部分, 包括三个完全连接层和激活/丢失层\n代码示例:  >>> import jittor as jt\n>>> from jittor.models.vgg import VGG, vgg16_bn\n>>> vgg_model = VGG(features=vgg16_bn().features, num_classes=1000, init_weights=True)\n>>> input_tensor = jt.randn(1, 3, 224, 224)\n>>> output = vgg_model(input_tensor)",
        "return_value": "",
        "parameters": "features (nn.Module): 用来构建VGG中的卷积层的模块\nnum_classes (int, optional): 分类的类别数量。默认值: 1000\ndrop_last (bool, optional): 是否初始化权重。默认值: True",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> from jittor.models.vgg import VGG, vgg16_bn\n>>> vgg_model = VGG(features=vgg16_bn().features, num_classes=1000, init_weights=True)\n>>> input_tensor = jt.randn(1, 3, 224, 224)\n>>> output = vgg_model(input_tensor)"
    },
    {
        "api_name": "jittor.view",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.view",
        "api_signature": "jittor.view(x, *shape)",
        "api_description": "函数C++定义格式:\njt.Var reshape(jt.Var x,  NanoVector shape)\n将输入的张量根据指定的形状进行重塑，并返回这个新的张量。保持元素的数据和数量不变。其中一维可以是-1，在这种情况下，它是根据其余的维度和输入的元素数量推断出来的。",
        "return_value": "重塑过后的张量（jt.Var）",
        "parameters": "x(jt.Var): 输入的张量。\nshape(Tuple[int]): 输出的形状,一个整数列表。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> jt.randint(0, 10, shape=(12,))\n    jt.Var([4 0 8 4 6 3 1 8 1 1 2 2], dtype=int32)\n>>> jt.reshape(a, (3, 4))\n    jt.Var([[4 0 8 4]\n        [6 3 1 8]\n        [1 1 2 2]], dtype=int32)\n>>> jt.reshape(a, (-1, 6))\njt.Var([[4 0 8 4 6 3]\n        [1 8 1 1 2 2]], dtype=int32)"
    },
    {
        "api_name": "jittor.misc.view_as",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.misc.view_as",
        "api_signature": "jittor.misc.view_as(x, y)",
        "api_description": "以 y 的形状对 x 进行 reshape。\n也就是说，对 x 中的元素进行重排，使得 x 与 y 的形状相同。注意，x 中的元素数量必须与 y 中的元素数量相同，否则报错。",
        "return_value": "重塑后的 Var, 数据来自 x, 形状和 y 相同。",
        "parameters": "x (Var): 需要重塑的输入张量 Var。\ny (Var): 作为参照形状的的输入张量 Var。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> x = jt.arange(4)\n>>> y = jt.array([[1,2],[3,4]])\n>>> jt.view_as(x, y)\njt.Var([[0 1]\n        [2 3]], dtype=int32)"
    },
    {
        "api_name": "jittor.dataset.VOC",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.dataset.html#jittor.dataset.VOC",
        "api_signature": "jittor.dataset.VOC(data_root='/home/zwy/.cache/jittor/dataset/voc/', split='train')",
        "api_description": "Pascal VOC 数据集\n参数:\ndata_root (str): 数据集的根目录\nsplit (str, optional): 选择数据集的子集, ‘train’表示训练集, ‘val’表示验证集。默认值: ‘train’\n属性:\ndata_root (str): 数据集的根目录\nsplit (str): 数据集的子集\nimage_root (str): 图像文件夹的路径\nlabel_root (str): 标签文件夹的路径\ndata_list_path (str): 数据列表文件的路径\nimage_path (list of str): 图像文件的路径列表\nlabel_path (list of str): 标签文件的路径列表\n代码示例:  >>> from jittor.dataset.voc import VOC\n>>> train_loader = VOC(data_root='path/to/VOC').set_attrs(batch_size=16, shuffle=True)\n>>> for i, (imgs, target) in enumerate(train_loader):\n>>>     # 处理图像和标签",
        "return_value": "",
        "parameters": "data_root (str): 数据集的根目录\nsplit (str, optional): 选择数据集的子集, ‘train’表示训练集, ‘val’表示验证集。默认值: ‘train’",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> from jittor.dataset.voc import VOC\n>>> train_loader = VOC(data_root='path/to/VOC').set_attrs(batch_size=16, shuffle=True)\n>>> for i, (imgs, target) in enumerate(train_loader):\n>>>     # 处理图像和标签"
    },
    {
        "api_name": "jittor.vtos",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.vtos",
        "api_signature": "jittor.vtos(v)",
        "api_description": "将给定的输入向量v转换为字符串。",
        "return_value": "output (str): 转换后的字符串。",
        "parameters": "v (jittor.Var): 输入的需要转换的张量。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> v = jt.array([[1, 2, 3], [1, 2, 3], [1, 2, 3]])\n>>> print(jt.vtos(v))  # '1 2 3'\njt.Var([[1 2 3]\n        [1 2 3]\n        [1 2 3]], dtype=int32)"
    },
    {
        "api_name": "jittor.weightnorm.weight_norm",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.weightnorm.html#jittor.weightnorm.weight_norm",
        "api_signature": "jittor.weightnorm.weight_norm(module, name, dim)",
        "api_description": "对模块增加一个权重归一化操作。通过WeightNorm在权重矩阵每一个切片（在维度dim上）进行 \\(L_2\\) 范数归一化，数学公式描述如下：设权重矩阵为W，经过归一化后的权重矩阵为W’，则有 \\(W' = \\frac{W}{||W||_2}\\)\n参数:\nmodule: 输入的模型，类型为模型对象\nname (Jittor.Var): 指定的参数的名称\ndim (int): 进行权重归一化的维度\n返回值:处理后的模型对象。\n代码示例：>>> import jittor as jt\n>>> from jittor import weightnorm\n>>> class jt_module(jt.nn.Module):\n>>>         def __init__(self, weight):\n>>>             super().__init__()\n>>>             self.linear = jt.array(weight)\n>>>\n>>>         def execute(self, x):\n>>>             return jt.matmul(self.linear, x)\n>>>\n>>> jm = jt_module(weight)\n>>> weightnorm.weight_norm(jm, 'linear', -1)",
        "return_value": "处理后的模型对象。",
        "parameters": "module: 输入的模型，类型为模型对象\nname (Jittor.Var): 指定的参数的名称\ndim (int): 进行权重归一化的维度",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> from jittor import weightnorm\n>>> class jt_module(jt.nn.Module):\n>>>         def __init__(self, weight):\n>>>             super().__init__()\n>>>             self.linear = jt.array(weight)\n>>> \n>>>         def execute(self, x):\n>>>             return jt.matmul(self.linear, x)\n>>> \n>>> jm = jt_module(weight)\n>>> weightnorm.weight_norm(jm, 'linear', -1)"
    },
    {
        "api_name": "jittor.weightnorm.WeightNorm",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.weightnorm.html#jittor.weightnorm.WeightNorm",
        "api_signature": "jittor.weightnorm.WeightNorm(name: str, dim: int)",
        "api_description": "权重标准化的类。权重标准化是一种标准化技术，可以使模型的训练过程更稳定。它通过将权重参数的幅值和方向进行解耦，使得网络更容易优化。它可以用来对某个Module添加或者移除权重标准化模块。\n参数:\nname (str): 要标准化的权重变量名称；\ndim (int): 对权重进行标准化的维度。-1表示对最后一维进行权重标准化。\n形状:使用该类，可以对一个神经网络层的某个张量作权重归一化。归一化对张量的形状没有影响。\n代码示例:  >>> import jittor as jt\n>>> from jittor import weightnorm\n>>> wn = weightnorm.WeightNorm(\"weight\", -1)\n>>> linear_layer = jt.nn.Linear(3,4)\n>>> wn.apply(linear_layer, \"weight\", -1) # 对linear_layer的weight变量作归一化\n<jittor.weightnorm.WeightNorm object at 0x0000012FB9C3C400>\n>>> hasattr(linear_layer, 'weight_g')\nTrue\n>>> wn.remove(linear_layer)\n>>> hasattr(linear_layer, 'weight_g')\nFalse",
        "return_value": "",
        "parameters": "name (str): 要标准化的权重变量名称；\ndim (int): 对权重进行标准化的维度。-1表示对最后一维进行权重标准化。",
        "input_shape": "使用该类，可以对一个神经网络层的某个张量作权重归一化。归一化对张量的形状没有影响。",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> from jittor import weightnorm\n>>> wn = weightnorm.WeightNorm(\"weight\", -1)\n>>> linear_layer = jt.nn.Linear(3,4)\n>>> wn.apply(linear_layer, \"weight\", -1) # 对linear_layer的weight变量作归一化\n<jittor.weightnorm.WeightNorm object at 0x0000012FB9C3C400>\n>>> hasattr(linear_layer, 'weight_g')\nTrue\n>>> wn.remove(linear_layer)\n>>> hasattr(linear_layer, 'weight_g')\nFalse"
    },
    {
        "api_name": "jittor_core.Var.where",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor_core.Var.where",
        "api_signature": "jittor_core.Var.where()",
        "api_description": "函数C++定义格式:\njt.Var where_(jt.Var cond, jt.Var x, jt.Var y)\n对张量中元素根据计算条件 cond 进行选择。",
        "return_value": "形状与输入都一样，每个值如果对应 cond 中的值为非零或 True 则返回 x 中对应的元素，否则返回 y 中对应的元素",
        "parameters": "cond (Var): 选择条件\nx (Var): 条件我非零值或 True 时选择的数据，形状与 cond 一样\ny (Var): 条件为零或 False 时选择的数据，形状与 cond 一样",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> cond = jt.array([1, 0, 1])\n>>> x = jt.array([3, 4, 5])\n>>> y = jt.array([9, 8, 7])\n>>> jt.where(cond, x, y)\njt.Var([3 8 5], dtype=int32)"
    },
    {
        "api_name": "jittor.models.Wide_resnet101_2",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.models.html#jittor.models.Wide_resnet101_2",
        "api_signature": "jittor.models.Wide_resnet101_2(pretrained=False, **kwargs)",
        "api_description": "构建一个Wide_resnet101_2模型\nWide_resnet源自论文 Wide Residual Networks, 是Resnet的一个变种, 其主要改进是增加了网络的宽度, 即增加了每层的通道数, 从而提高了网络的性能。\n参数:\npretrained (bool, optional): 表示是否预加载预训练模型。默认为 False。\n返回值:\n返回构建好的Wide_resnet101_2模型实例。如果 pretrained 为 True, 则返回在ImageNet上预训练的模型。\n代码示例:>>> import jittor as jt\n>>> from jittor.models.resnet import *\n>>> net = Wide_resnet101_2(pretrained=False)\n>>> x = jt.rand(1, 3, 224, 224)\n>>> y = net(x)\n>>> y.shape\n[1, 1000]",
        "return_value": "返回构建好的Wide_resnet101_2模型实例。如果 pretrained 为 True, 则返回在ImageNet上预训练的模型。",
        "parameters": "pretrained (bool, optional): 表示是否预加载预训练模型。默认为 False。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> from jittor.models.resnet import *\n>>> net = Wide_resnet101_2(pretrained=False)\n>>> x = jt.rand(1, 3, 224, 224)\n>>> y = net(x)\n>>> y.shape\n[1, 1000]"
    },
    {
        "api_name": "jittor.models.wide_resnet101_2",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.models.html#jittor.models.wide_resnet101_2",
        "api_signature": "jittor.models.wide_resnet101_2(pretrained=False, **kwargs)",
        "api_description": "构建一个Wide_resnet101_2模型\nWide_resnet源自论文 Wide Residual Networks, 是Resnet的一个变种, 其主要改进是增加了网络的宽度, 即增加了每层的通道数, 从而提高了网络的性能。\n参数:\npretrained (bool, optional): 表示是否预加载预训练模型。默认为 False。\n返回值:\n返回构建好的Wide_resnet101_2模型实例。如果 pretrained 为 True, 则返回在ImageNet上预训练的模型。\n代码示例:>>> import jittor as jt\n>>> from jittor.models.resnet import *\n>>> net = Wide_resnet101_2(pretrained=False)\n>>> x = jt.rand(1, 3, 224, 224)\n>>> y = net(x)\n>>> y.shape\n[1, 1000]",
        "return_value": "返回构建好的Wide_resnet101_2模型实例。如果 pretrained 为 True, 则返回在ImageNet上预训练的模型。",
        "parameters": "pretrained (bool, optional): 表示是否预加载预训练模型。默认为 False。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> from jittor.models.resnet import *\n>>> net = Wide_resnet101_2(pretrained=False)\n>>> x = jt.rand(1, 3, 224, 224)\n>>> y = net(x)\n>>> y.shape\n[1, 1000]"
    },
    {
        "api_name": "jittor.models.Wide_resnet50_2",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.models.html#jittor.models.Wide_resnet50_2",
        "api_signature": "jittor.models.Wide_resnet50_2(pretrained=False, **kwargs)",
        "api_description": "构建一个Wide_resnet50_2模型\nWide_resnet源自论文 Wide Residual Networks, 是Resnet的一个变种, 其主要改进是增加了网络的宽度, 即增加了每层的通道数, 从而提高了网络的性能。\n参数:\npretrained (bool, optional): 表示是否预加载预训练模型。默认为 False。\n返回值:\n返回构建好的Wide_resnet50_2模型实例。如果 pretrained 为 True, 则返回在ImageNet上预训练的模型。\n代码示例:>>> import jittor as jt\n>>> from jittor.models.resnet import *\n>>> net = Wide_resnet50_2(pretrained=False)\n>>> x = jt.rand(1, 3, 224, 224)\n>>> y = net(x)\n>>> y.shape\n[1, 1000]",
        "return_value": "返回构建好的Wide_resnet50_2模型实例。如果 pretrained 为 True, 则返回在ImageNet上预训练的模型。",
        "parameters": "pretrained (bool, optional): 表示是否预加载预训练模型。默认为 False。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> from jittor.models.resnet import *\n>>> net = Wide_resnet50_2(pretrained=False)\n>>> x = jt.rand(1, 3, 224, 224)\n>>> y = net(x)\n>>> y.shape\n[1, 1000]"
    },
    {
        "api_name": "jittor.models.wide_resnet50_2",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.models.html#jittor.models.wide_resnet50_2",
        "api_signature": "jittor.models.wide_resnet50_2(pretrained=False, **kwargs)",
        "api_description": "构建一个Wide_resnet50_2模型\nWide_resnet源自论文 Wide Residual Networks, 是Resnet的一个变种, 其主要改进是增加了网络的宽度, 即增加了每层的通道数, 从而提高了网络的性能。\n参数:\npretrained (bool, optional): 表示是否预加载预训练模型。默认为 False。\n返回值:\n返回构建好的Wide_resnet50_2模型实例。如果 pretrained 为 True, 则返回在ImageNet上预训练的模型。\n代码示例:>>> import jittor as jt\n>>> from jittor.models.resnet import *\n>>> net = Wide_resnet50_2(pretrained=False)\n>>> x = jt.rand(1, 3, 224, 224)\n>>> y = net(x)\n>>> y.shape\n[1, 1000]",
        "return_value": "返回构建好的Wide_resnet50_2模型实例。如果 pretrained 为 True, 则返回在ImageNet上预训练的模型。",
        "parameters": "pretrained (bool, optional): 表示是否预加载预训练模型。默认为 False。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import jittor as jt\n>>> from jittor.models.resnet import *\n>>> net = Wide_resnet50_2(pretrained=False)\n>>> x = jt.rand(1, 3, 224, 224)\n>>> y = net(x)\n>>> y.shape\n[1, 1000]"
    },
    {
        "api_name": "jittor.init.xavier_gauss",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.init.html#jittor.init.xavier_gauss",
        "api_signature": "jittor.init.xavier_gauss(shape, dtype='float32', gain=1.0)",
        "api_description": "返回由 xavier_gauss 初始化的 Var。结果 Var 的值将从 \\(\\mathcal N(-a, a)\\) 中采样，其中\n\\[\\text{std} = \\text{gain} \\times \\sqrt{\\frac{2}{\\text{fan_in} + \\text{fan_out}}}\\]",
        "return_value": "由 xavier_gauss 初始化的 Var",
        "parameters": "shape (int or Tuple[int]): 输出Var的形状\ndtype (str): 输出 Var 的 dtype ，默认 float32\ngain (float): 可选的缩放因子。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> from jittor import init\n>>> from jittor import nn\n>>> linear = nn.Linear(2,2)\n>>> init.xavier_gauss_(linear.weight, init.calculate_gain('relu'))\n>>> print(linear.weight)\njt.Var([[ 0.27429324 -0.15574329]\n                [-0.15574329 -0.27429324]], dtype=float32)\n>>> linear.weight.xavier_gauss_() # This is ok too\njt.Var([[ 0.27429324 -0.15574329]\n                [-0.15574329 -0.27429324]], dtype=float32)"
    },
    {
        "api_name": "jittor.init.xavier_gauss_",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.init.html#jittor.init.xavier_gauss_",
        "api_signature": "jittor.init.xavier_gauss_(var, gain=1.0)",
        "api_description": "返回由 xavier_gauss 初始化的 Var。结果 Var 的值将从 \\(\\mathcal N(-a, a)\\) 中采样，其中\n\\[\\text{std} = \\text{gain} \\times \\sqrt{\\frac{2}{\\text{fan_in} + \\text{fan_out}}}\\]",
        "return_value": "由 xavier_gauss 初始化的 Var",
        "parameters": "var (Var): 通过 xavier_guass 随机初始化的变量\ngain (float): 可选的缩放因子。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> from jittor import init\n>>> from jittor import nn\n>>> linear = nn.Linear(2,2)\n>>> init.xavier_gauss_(linear.weight, init.calculate_gain('relu'))\n>>> print(linear.weight)\njt.Var([[ 0.27429324 -0.15574329]\n                [-0.15574329 -0.27429324]], dtype=float32)\n>>> linear.weight.xavier_gauss_() # This is ok too\njt.Var([[ 0.27429324 -0.15574329]\n                [-0.15574329 -0.27429324]], dtype=float32)"
    },
    {
        "api_name": "jittor.init.xavier_uniform",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.init.html#jittor.init.xavier_uniform",
        "api_signature": "jittor.init.xavier_uniform(shape, dtype='float32', gain=1.0)",
        "api_description": "返回由 xavier_uniform 初始化的 Var。结果 Var 的值将从 \\(\\mathcal U(-a, a)\\) 中采样，其中\n\\[a = \\text{gain} \\times \\sqrt{\\frac{6}{\\text{fan_in} + \\text{fan_out}}}\\]",
        "return_value": "初始化后的 Var",
        "parameters": "shape (int or Tuple[int]): 输出Var的形状\ndtype (str): 输出 Var 的 dtype ，默认 float32\ngain (float): 可选的缩放因子。",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> from jittor import init\n>>> from jittor import nn\n>>> init.xavier_uniform((2,2), gain=init.calculate_gain('relu'))"
    },
    {
        "api_name": "jittor.init.xavier_uniform_",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.init.html#jittor.init.xavier_uniform_",
        "api_signature": "jittor.init.xavier_uniform_(var, gain=1.0)",
        "api_description": "返回由 xavier_uniform 初始化的 Var。结果 Var 的值将从 \\(\\mathcal U(-a, a)\\) 中采样，其中\n\\[a = \\text{gain} \\times \\sqrt{\\frac{6}{\\text{fan_in} + \\text{fan_out}}}\\]",
        "return_value": "返回初始化后的 var。",
        "parameters": "var (Var): 通过 xavier_uniform 随机初始化的变量\ngain (float): 可选的缩放因子。",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "jittor.init.zero",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.init.html#jittor.init.zero",
        "api_signature": "jittor.init.zero(shape, dtype='float32')",
        "api_description": "返回一个全为 0 的张量(Var)，形状由可变参数 shape 定义，\n数据类型由 dtype 定义。如果不给定 dtype , 默认类型为 float32。",
        "return_value": "全为 0 的张量(Var)",
        "parameters": "shape (Tuple[int]): 整数序列，定义了输出的形状\ndtype (var.dtype，可选): 数据类型，默认为 float32",
        "input_shape": "",
        "notes": "该函数和 jt.zeros() 用处一致",
        "code_example": ">>> init.zero((3,2), dtype='int32')\njt.Var([[0 0]\n        [0 0]\n        [0 0]], dtype=int32)"
    },
    {
        "api_name": "jittor.init.zero_",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.init.html#jittor.init.zero_",
        "api_signature": "jittor.init.zero_(var)",
        "api_description": "原地修改张量 var ，将其修改为数值均为0的张量。",
        "return_value": "全为 0 的张量(Var)",
        "parameters": "var (Var): 输入的 Var",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> x = jt.randn((3,2))\n>>> x\njt.Var([[-1.1730903   0.21458259]\n        [ 1.0399616   0.07660236]\n        [-1.8453276  -0.95629567]], dtype=float32)\n>>> init.zero_(x)\n>>> x\njt.Var([[0. 0.]\n        [0. 0.]\n        [0. 0.]], dtype=float32)"
    },
    {
        "api_name": "jittor.nn.ZeroPad2d",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.nn.html#jittor.nn.ZeroPad2d",
        "api_signature": "jittor.nn.ZeroPad2d(padding)",
        "api_description": "使用零来填充输入张量的边界。\n输入为 \\((N, C, H_{in}, W_{in})\\)，输出为 \\((N, C, H_{out}, W_{out})\\)，其中:\n\\[\\begin{split}H_{out} = H_{in} + \\text{padding_top} + \\text{padding_bottom}\n\\\\W_{out} = W_{in} + \\text{padding_left} + \\text{padding_right}\\end{split}\\]",
        "return_value": "",
        "parameters": "padding(int, tuple): 填充的大小",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> m = nn.ZeroPad2d((1, 1, 2, 0))\n>>> input = jt.randn(1, 1, 3, 3)\n>>> input\njt.Var([[[[ 0.18378055 -0.60490954 -0.68662244]\n        [-0.42572546 -1.4829487  -0.6552902 ]\n        [-0.92770797  0.2502182  -0.10983822]]]], dtype=float32)\n>>> m(input)\njt.Var([[[[ 0.          0.          0.          0.          0.        ]\n        [ 0.          0.          0.          0.          0.        ]\n        [ 0.          0.18378055 -0.60490954 -0.68662244  0.        ]\n        [ 0.         -0.42572546 -1.4829487  -0.6552902   0.        ]\n        [ 0.         -0.92770797  0.2502182  -0.10983822  0.        ]]]], dtype=float32)"
    },
    {
        "api_name": "jittor.zeros",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.zeros",
        "api_signature": "jittor.zeros(*shape, dtype='float32')",
        "api_description": "返回一个全为0的张量(Var)，形状由可变参数 size 定义，数据类型由 dtype 定义。如果不给定 dtype , 默认类型为 float32。",
        "return_value": "元素全为0的张量(Var)",
        "parameters": "size (int…): 整数序列，定义了输出的形状\ndtype (var.dtype, optional): 数据类型，默认为 float32",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> jt.zeros((3,2), dtype='int32')\njt.Var([[0 0]\n        [0 0]\n        [0 0]], dtype=int32)\n>>> jt.ones_like(x)\njt.Var([1. 1. 1.], dtype=float32)"
    },
    {
        "api_name": "jittor.zeros_like",
        "api_url": "https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/jittor.html#jittor.zeros_like",
        "api_signature": "jittor.zeros_like(x, dtype=None)",
        "api_description": "创建一个全为0的张量(Var)，其中张量的形状由张量 x 的形状定义，数据类型默认为 None ,即张量 x 的数据类型。",
        "return_value": "返回一个值全为0，形状大小为和 x 一致的张量(Var)",
        "parameters": "x (Var): Var类型的张量\ndtype (var.dtype, optional): 数据类型，默认为 None",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> jt.array([1, 2, 3]).float()\njt.Var([1. 2. 3.], dtype=float32)\n>>> jt.zeros_like(x)\njt.Var([0. 0. 0.], dtype=float32)"
    }
]