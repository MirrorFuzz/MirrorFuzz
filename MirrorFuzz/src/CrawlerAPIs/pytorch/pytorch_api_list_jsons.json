[
    {
        "api_name": "torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.PowerSGDState.__getstate__",
        "api_url": "https://pytorch.org/docs/stable/ddp_comm_hooks.html#torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.PowerSGDState.__getstate__",
        "api_signature": "__getstate__()",
        "api_description": "Return a Dict[str, Any] which will be pickled and saved.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.Graph.__init__",
        "api_url": "https://pytorch.org/docs/stable/fx.html#torch.fx.Graph.__init__",
        "api_signature": "__init__(owning_module=None, tracer_cls=None, tracer_extras=None)",
        "api_description": "Construct an empty Graph.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.GraphModule.__init__",
        "api_url": "https://pytorch.org/docs/stable/fx.html#torch.fx.GraphModule.__init__",
        "api_signature": "__init__(root, graph, class_name='GraphModule')",
        "api_description": "Construct a GraphModule.",
        "return_value": "",
        "parameters": "root (Union[torch.nn.Module, Dict[str, Any]) – root can either be an nn.Module instance or a Dict mapping strings to any attribute type.\nIn the case that root is a Module, any references to Module-based objects (via qualified\nname) in the Graph’s Nodes’ target field will be copied over from the respective place\nwithin root’s Module hierarchy into the GraphModule’s module hierarchy.\nIn the case that root is a dict, the qualified name found in a Node’s target will be\nlooked up directly in the dict’s keys. The object mapped to by the Dict will be copied\nover into the appropriate place within the GraphModule’s module hierarchy.\ngraph (Graph) – graph contains the nodes this GraphModule should use for code generation\nclass_name (str) – name denotes the name of this GraphModule for debugging purposes. If it’s unset, all\nerror messages will report as originating from GraphModule. It may be helpful to set this\nto root’s original name or a name that makes sense within the context of your transform.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.monitor.Event.__init__",
        "api_url": "https://pytorch.org/docs/stable/monitor.html#torch.monitor.Event.__init__",
        "api_signature": "__init__(self: torch._C._monitor.Event, name: str, timestamp: datetime.datetime, data: dict[str, data_value_t])",
        "api_description": "Constructs the Event.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.monitor.Stat.__init__",
        "api_url": "https://pytorch.org/docs/stable/monitor.html#torch.monitor.Stat.__init__",
        "api_signature": "__init__(self: torch._C._monitor.Stat, name: str, aggregations: list[torch._C._monitor.Aggregation], window_size: datetime.timedelta, max_samples: int = 9223372036854775807)",
        "api_description": "Constructs the Stat.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.monitor.TensorboardEventHandler.__init__",
        "api_url": "https://pytorch.org/docs/stable/monitor.html#torch.monitor.TensorboardEventHandler.__init__",
        "api_signature": "__init__(writer)",
        "api_description": "Constructs the TensorboardEventHandler.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.package.PackageExporter.__init__",
        "api_url": "https://pytorch.org/docs/stable/package.html#torch.package.PackageExporter.__init__",
        "api_signature": "__init__(f, importer=<torch.package.importer._SysImporter object>, debug=False)",
        "api_description": "Create an exporter.",
        "return_value": "",
        "parameters": "f (Union[str, Path, BinaryIO]) – The location to export to. Can be a  string/Path object containing a filename\nor a binary I/O object.\nimporter (Union[Importer, Sequence[Importer]]) – If a single Importer is passed, use that to search for modules.\nIf a sequence of importers are passed, an OrderedImporter will be constructed out of them.\ndebug (bool) – If set to True, add path of broken modules to PackagingErrors.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.package.PackageImporter.__init__",
        "api_url": "https://pytorch.org/docs/stable/package.html#torch.package.PackageImporter.__init__",
        "api_signature": "__init__(file_or_buffer, module_allowed=<function PackageImporter.<lambda>>)",
        "api_description": "Open file_or_buffer for importing. This checks that the imported package only requires modules\nallowed by module_allowed",
        "return_value": "",
        "parameters": "a string, or an os.PathLike object containing a filename.\nmodule_allowed (Callable[[str], bool], optional) – A method to determine if a externally provided module\nshould be allowed. Can be used to ensure packages loaded do not depend on modules that the server\ndoes not support. Defaults to allowing anything.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.tensorboard.writer.SummaryWriter.__init__",
        "api_url": "https://pytorch.org/docs/stable/tensorboard.html#torch.utils.tensorboard.writer.SummaryWriter.__init__",
        "api_signature": "__init__(log_dir=None, comment='', purge_step=None, max_queue=10, flush_secs=120, filename_suffix='')",
        "api_description": "Create a SummaryWriter that will write out events and summaries to the event file.",
        "return_value": "",
        "parameters": "log_dir (str) – Save directory location. Default is\nruns/CURRENT_DATETIME_HOSTNAME, which changes after each run.\nUse hierarchical folder structure to compare\nbetween runs easily. e.g. pass in ‘runs/exp1’, ‘runs/exp2’, etc.\nfor each new experiment to compare across them.\ncomment (str) – Comment log_dir suffix appended to the default\nlog_dir. If log_dir is assigned, this argument has no effect.\npurge_step (int) – When logging crashes at step T+XT+XT+X and restarts at step TTT,\nany events whose global_step larger or equal to TTT will be\npurged and hidden from TensorBoard.\nNote that crashed and resumed experiments should have the same log_dir.\nmax_queue (int) – Size of the queue for pending events and\nsummaries before one of the ‘add’ calls forces a flush to disk.\nDefault is ten items.\nflush_secs (int) – How often, in seconds, to flush the\npending events and summaries to disk. Default is every two minutes.\nfilename_suffix (str) – Suffix added to all event filenames in\nthe log_dir directory. More details on filename construction in\ntensorboard.summary.writer.event_file_writer.EventFileWriter.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.PowerSGDState.__setstate__",
        "api_url": "https://pytorch.org/docs/stable/ddp_comm_hooks.html#torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.PowerSGDState.__setstate__",
        "api_signature": "__setstate__(state)",
        "api_description": "Take a provided state and set to this PowerSGDState instance.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch._assert",
        "api_url": "https://pytorch.org/docs/stable/generated/torch._assert.html#torch._assert",
        "api_signature": "torch._assert(condition, message)",
        "api_description": "A wrapper around Python’s assert which is symbolically traceable.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.agent.server.SimpleElasticAgent._assign_worker_ranks",
        "api_url": "https://pytorch.org/docs/stable/elastic/agent.html#torch.distributed.elastic.agent.server.SimpleElasticAgent._assign_worker_ranks",
        "api_signature": "_assign_worker_ranks(store, group_rank, group_world_size, spec)",
        "api_description": "Determine proper ranks for worker processes.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda.jiterator._create_jit_fn",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.cuda.jiterator._create_jit_fn.html#torch.cuda.jiterator._create_jit_fn",
        "api_signature": "torch.cuda.jiterator._create_jit_fn(code_string, **kwargs)",
        "api_description": "Create a jiterator-generated cuda kernel for an elementwise op.",
        "return_value": "",
        "parameters": "code_string (str) – CUDA code string to be compiled by jiterator. The entry functor must return by value.\nkwargs (Dict, optional) – Keyword arguments for generated function",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda.jiterator._create_multi_output_jit_fn",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.cuda.jiterator._create_multi_output_jit_fn.html#torch.cuda.jiterator._create_multi_output_jit_fn",
        "api_signature": "torch.cuda.jiterator._create_multi_output_jit_fn(code_string, num_outputs, **kwargs)",
        "api_description": "Create a jiterator-generated cuda kernel for an elementwise op that supports returning one or more outputs.",
        "return_value": "",
        "parameters": "code_string (str) – CUDA code string to be compiled by jiterator. The entry functor must return value by reference.\nnum_outputs (int) – number of outputs return by the kernel\nkwargs (Dict, optional) – Keyword arguments for generated function",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda.memory._dump_snapshot",
        "api_url": "https://pytorch.org/docs/stable/torch_cuda_memory.html#torch.cuda.memory._dump_snapshot",
        "api_signature": "torch.cuda.memory._dump_snapshot(filename='dump_snapshot.pickle')",
        "api_description": "Save a pickled version of the torch.memory._snapshot() dictionary to a file.",
        "return_value": "",
        "parameters": "filename (str, optional) – Name of the file to create. Defaults to “dump_snapshot.pickle”.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.agent.server.SimpleElasticAgent._exit_barrier",
        "api_url": "https://pytorch.org/docs/stable/elastic/agent.html#torch.distributed.elastic.agent.server.SimpleElasticAgent._exit_barrier",
        "api_signature": "_exit_barrier()",
        "api_description": "Define a barrier that keeps the agent process alive until all workers finish.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch._foreach_abs",
        "api_url": "https://pytorch.org/docs/stable/generated/torch._foreach_abs.html#torch._foreach_abs",
        "api_signature": "torch._foreach_abs(self: List[Tensor])",
        "api_description": "Apply torch.abs() to each Tensor of the input list.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch._foreach_abs_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch._foreach_abs_.html#torch._foreach_abs_",
        "api_signature": "torch._foreach_abs_(self: List[Tensor])",
        "api_description": "Apply torch.abs() to each Tensor of the input list.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch._foreach_acos",
        "api_url": "https://pytorch.org/docs/stable/generated/torch._foreach_acos.html#torch._foreach_acos",
        "api_signature": "torch._foreach_acos(self: List[Tensor])",
        "api_description": "Apply torch.acos() to each Tensor of the input list.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch._foreach_acos_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch._foreach_acos_.html#torch._foreach_acos_",
        "api_signature": "torch._foreach_acos_(self: List[Tensor])",
        "api_description": "Apply torch.acos() to each Tensor of the input list.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch._foreach_asin",
        "api_url": "https://pytorch.org/docs/stable/generated/torch._foreach_asin.html#torch._foreach_asin",
        "api_signature": "torch._foreach_asin(self: List[Tensor])",
        "api_description": "Apply torch.asin() to each Tensor of the input list.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch._foreach_asin_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch._foreach_asin_.html#torch._foreach_asin_",
        "api_signature": "torch._foreach_asin_(self: List[Tensor])",
        "api_description": "Apply torch.asin() to each Tensor of the input list.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch._foreach_atan",
        "api_url": "https://pytorch.org/docs/stable/generated/torch._foreach_atan.html#torch._foreach_atan",
        "api_signature": "torch._foreach_atan(self: List[Tensor])",
        "api_description": "Apply torch.atan() to each Tensor of the input list.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch._foreach_atan_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch._foreach_atan_.html#torch._foreach_atan_",
        "api_signature": "torch._foreach_atan_(self: List[Tensor])",
        "api_description": "Apply torch.atan() to each Tensor of the input list.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch._foreach_ceil",
        "api_url": "https://pytorch.org/docs/stable/generated/torch._foreach_ceil.html#torch._foreach_ceil",
        "api_signature": "torch._foreach_ceil(self: List[Tensor])",
        "api_description": "Apply torch.ceil() to each Tensor of the input list.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch._foreach_ceil_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch._foreach_ceil_.html#torch._foreach_ceil_",
        "api_signature": "torch._foreach_ceil_(self: List[Tensor])",
        "api_description": "Apply torch.ceil() to each Tensor of the input list.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch._foreach_cos",
        "api_url": "https://pytorch.org/docs/stable/generated/torch._foreach_cos.html#torch._foreach_cos",
        "api_signature": "torch._foreach_cos(self: List[Tensor])",
        "api_description": "Apply torch.cos() to each Tensor of the input list.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch._foreach_cos_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch._foreach_cos_.html#torch._foreach_cos_",
        "api_signature": "torch._foreach_cos_(self: List[Tensor])",
        "api_description": "Apply torch.cos() to each Tensor of the input list.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch._foreach_cosh",
        "api_url": "https://pytorch.org/docs/stable/generated/torch._foreach_cosh.html#torch._foreach_cosh",
        "api_signature": "torch._foreach_cosh(self: List[Tensor])",
        "api_description": "Apply torch.cosh() to each Tensor of the input list.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch._foreach_cosh_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch._foreach_cosh_.html#torch._foreach_cosh_",
        "api_signature": "torch._foreach_cosh_(self: List[Tensor])",
        "api_description": "Apply torch.cosh() to each Tensor of the input list.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch._foreach_erf",
        "api_url": "https://pytorch.org/docs/stable/generated/torch._foreach_erf.html#torch._foreach_erf",
        "api_signature": "torch._foreach_erf(self: List[Tensor])",
        "api_description": "Apply torch.erf() to each Tensor of the input list.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch._foreach_erf_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch._foreach_erf_.html#torch._foreach_erf_",
        "api_signature": "torch._foreach_erf_(self: List[Tensor])",
        "api_description": "Apply torch.erf() to each Tensor of the input list.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch._foreach_erfc",
        "api_url": "https://pytorch.org/docs/stable/generated/torch._foreach_erfc.html#torch._foreach_erfc",
        "api_signature": "torch._foreach_erfc(self: List[Tensor])",
        "api_description": "Apply torch.erfc() to each Tensor of the input list.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch._foreach_erfc_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch._foreach_erfc_.html#torch._foreach_erfc_",
        "api_signature": "torch._foreach_erfc_(self: List[Tensor])",
        "api_description": "Apply torch.erfc() to each Tensor of the input list.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch._foreach_exp",
        "api_url": "https://pytorch.org/docs/stable/generated/torch._foreach_exp.html#torch._foreach_exp",
        "api_signature": "torch._foreach_exp(self: List[Tensor])",
        "api_description": "Apply torch.exp() to each Tensor of the input list.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch._foreach_exp_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch._foreach_exp_.html#torch._foreach_exp_",
        "api_signature": "torch._foreach_exp_(self: List[Tensor])",
        "api_description": "Apply torch.exp() to each Tensor of the input list.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch._foreach_expm1",
        "api_url": "https://pytorch.org/docs/stable/generated/torch._foreach_expm1.html#torch._foreach_expm1",
        "api_signature": "torch._foreach_expm1(self: List[Tensor])",
        "api_description": "Apply torch.expm1() to each Tensor of the input list.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch._foreach_expm1_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch._foreach_expm1_.html#torch._foreach_expm1_",
        "api_signature": "torch._foreach_expm1_(self: List[Tensor])",
        "api_description": "Apply torch.expm1() to each Tensor of the input list.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch._foreach_floor",
        "api_url": "https://pytorch.org/docs/stable/generated/torch._foreach_floor.html#torch._foreach_floor",
        "api_signature": "torch._foreach_floor(self: List[Tensor])",
        "api_description": "Apply torch.floor() to each Tensor of the input list.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch._foreach_floor_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch._foreach_floor_.html#torch._foreach_floor_",
        "api_signature": "torch._foreach_floor_(self: List[Tensor])",
        "api_description": "Apply torch.floor() to each Tensor of the input list.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch._foreach_frac",
        "api_url": "https://pytorch.org/docs/stable/generated/torch._foreach_frac.html#torch._foreach_frac",
        "api_signature": "torch._foreach_frac(self: List[Tensor])",
        "api_description": "Apply torch.frac() to each Tensor of the input list.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch._foreach_frac_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch._foreach_frac_.html#torch._foreach_frac_",
        "api_signature": "torch._foreach_frac_(self: List[Tensor])",
        "api_description": "Apply torch.frac() to each Tensor of the input list.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch._foreach_lgamma",
        "api_url": "https://pytorch.org/docs/stable/generated/torch._foreach_lgamma.html#torch._foreach_lgamma",
        "api_signature": "torch._foreach_lgamma(self: List[Tensor])",
        "api_description": "Apply torch.lgamma() to each Tensor of the input list.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch._foreach_lgamma_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch._foreach_lgamma_.html#torch._foreach_lgamma_",
        "api_signature": "torch._foreach_lgamma_(self: List[Tensor])",
        "api_description": "Apply torch.lgamma() to each Tensor of the input list.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch._foreach_log",
        "api_url": "https://pytorch.org/docs/stable/generated/torch._foreach_log.html#torch._foreach_log",
        "api_signature": "torch._foreach_log(self: List[Tensor])",
        "api_description": "Apply torch.log() to each Tensor of the input list.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch._foreach_log10",
        "api_url": "https://pytorch.org/docs/stable/generated/torch._foreach_log10.html#torch._foreach_log10",
        "api_signature": "torch._foreach_log10(self: List[Tensor])",
        "api_description": "Apply torch.log10() to each Tensor of the input list.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch._foreach_log10_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch._foreach_log10_.html#torch._foreach_log10_",
        "api_signature": "torch._foreach_log10_(self: List[Tensor])",
        "api_description": "Apply torch.log10() to each Tensor of the input list.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch._foreach_log1p",
        "api_url": "https://pytorch.org/docs/stable/generated/torch._foreach_log1p.html#torch._foreach_log1p",
        "api_signature": "torch._foreach_log1p(self: List[Tensor])",
        "api_description": "Apply torch.log1p() to each Tensor of the input list.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch._foreach_log1p_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch._foreach_log1p_.html#torch._foreach_log1p_",
        "api_signature": "torch._foreach_log1p_(self: List[Tensor])",
        "api_description": "Apply torch.log1p() to each Tensor of the input list.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch._foreach_log2",
        "api_url": "https://pytorch.org/docs/stable/generated/torch._foreach_log2.html#torch._foreach_log2",
        "api_signature": "torch._foreach_log2(self: List[Tensor])",
        "api_description": "Apply torch.log2() to each Tensor of the input list.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch._foreach_log2_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch._foreach_log2_.html#torch._foreach_log2_",
        "api_signature": "torch._foreach_log2_(self: List[Tensor])",
        "api_description": "Apply torch.log2() to each Tensor of the input list.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch._foreach_log_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch._foreach_log_.html#torch._foreach_log_",
        "api_signature": "torch._foreach_log_(self: List[Tensor])",
        "api_description": "Apply torch.log() to each Tensor of the input list.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch._foreach_neg",
        "api_url": "https://pytorch.org/docs/stable/generated/torch._foreach_neg.html#torch._foreach_neg",
        "api_signature": "torch._foreach_neg(self: List[Tensor])",
        "api_description": "Apply torch.neg() to each Tensor of the input list.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch._foreach_neg_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch._foreach_neg_.html#torch._foreach_neg_",
        "api_signature": "torch._foreach_neg_(self: List[Tensor])",
        "api_description": "Apply torch.neg() to each Tensor of the input list.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch._foreach_reciprocal",
        "api_url": "https://pytorch.org/docs/stable/generated/torch._foreach_reciprocal.html#torch._foreach_reciprocal",
        "api_signature": "torch._foreach_reciprocal(self: List[Tensor])",
        "api_description": "Apply torch.reciprocal() to each Tensor of the input list.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch._foreach_reciprocal_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch._foreach_reciprocal_.html#torch._foreach_reciprocal_",
        "api_signature": "torch._foreach_reciprocal_(self: List[Tensor])",
        "api_description": "Apply torch.reciprocal() to each Tensor of the input list.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch._foreach_round",
        "api_url": "https://pytorch.org/docs/stable/generated/torch._foreach_round.html#torch._foreach_round",
        "api_signature": "torch._foreach_round(self: List[Tensor])",
        "api_description": "Apply torch.round() to each Tensor of the input list.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch._foreach_round_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch._foreach_round_.html#torch._foreach_round_",
        "api_signature": "torch._foreach_round_(self: List[Tensor])",
        "api_description": "Apply torch.round() to each Tensor of the input list.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch._foreach_sigmoid",
        "api_url": "https://pytorch.org/docs/stable/generated/torch._foreach_sigmoid.html#torch._foreach_sigmoid",
        "api_signature": "torch._foreach_sigmoid(self: List[Tensor])",
        "api_description": "Apply torch.sigmoid() to each Tensor of the input list.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch._foreach_sigmoid_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch._foreach_sigmoid_.html#torch._foreach_sigmoid_",
        "api_signature": "torch._foreach_sigmoid_(self: List[Tensor])",
        "api_description": "Apply torch.sigmoid() to each Tensor of the input list.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch._foreach_sin",
        "api_url": "https://pytorch.org/docs/stable/generated/torch._foreach_sin.html#torch._foreach_sin",
        "api_signature": "torch._foreach_sin(self: List[Tensor])",
        "api_description": "Apply torch.sin() to each Tensor of the input list.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch._foreach_sin_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch._foreach_sin_.html#torch._foreach_sin_",
        "api_signature": "torch._foreach_sin_(self: List[Tensor])",
        "api_description": "Apply torch.sin() to each Tensor of the input list.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch._foreach_sinh",
        "api_url": "https://pytorch.org/docs/stable/generated/torch._foreach_sinh.html#torch._foreach_sinh",
        "api_signature": "torch._foreach_sinh(self: List[Tensor])",
        "api_description": "Apply torch.sinh() to each Tensor of the input list.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch._foreach_sinh_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch._foreach_sinh_.html#torch._foreach_sinh_",
        "api_signature": "torch._foreach_sinh_(self: List[Tensor])",
        "api_description": "Apply torch.sinh() to each Tensor of the input list.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch._foreach_sqrt",
        "api_url": "https://pytorch.org/docs/stable/generated/torch._foreach_sqrt.html#torch._foreach_sqrt",
        "api_signature": "torch._foreach_sqrt(self: List[Tensor])",
        "api_description": "Apply torch.sqrt() to each Tensor of the input list.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch._foreach_sqrt_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch._foreach_sqrt_.html#torch._foreach_sqrt_",
        "api_signature": "torch._foreach_sqrt_(self: List[Tensor])",
        "api_description": "Apply torch.sqrt() to each Tensor of the input list.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch._foreach_tan",
        "api_url": "https://pytorch.org/docs/stable/generated/torch._foreach_tan.html#torch._foreach_tan",
        "api_signature": "torch._foreach_tan(self: List[Tensor])",
        "api_description": "Apply torch.tan() to each Tensor of the input list.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch._foreach_tan_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch._foreach_tan_.html#torch._foreach_tan_",
        "api_signature": "torch._foreach_tan_(self: List[Tensor])",
        "api_description": "Apply torch.tan() to each Tensor of the input list.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch._foreach_trunc",
        "api_url": "https://pytorch.org/docs/stable/generated/torch._foreach_trunc.html#torch._foreach_trunc",
        "api_signature": "torch._foreach_trunc(self: List[Tensor])",
        "api_description": "Apply torch.trunc() to each Tensor of the input list.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch._foreach_trunc_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch._foreach_trunc_.html#torch._foreach_trunc_",
        "api_signature": "torch._foreach_trunc_(self: List[Tensor])",
        "api_description": "Apply torch.trunc() to each Tensor of the input list.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch._foreach_zero_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch._foreach_zero_.html#torch._foreach_zero_",
        "api_signature": "torch._foreach_zero_(self: List[Tensor])",
        "api_description": "Apply torch.zero() to each Tensor of the input list.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.agent.server.SimpleElasticAgent._initialize_workers",
        "api_url": "https://pytorch.org/docs/stable/elastic/agent.html#torch.distributed.elastic.agent.server.SimpleElasticAgent._initialize_workers",
        "api_signature": "_initialize_workers(worker_group)",
        "api_description": "Start a fresh set of workers for the worker_group.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.profiler._KinetoProfile",
        "api_url": "https://pytorch.org/docs/stable/profiler.html#torch.profiler._KinetoProfile",
        "api_signature": "torch.profiler._KinetoProfile(*, activities=None, record_shapes=False, profile_memory=False, with_stack=False, with_flops=False, with_modules=False, experimental_config=None, execution_trace_observer=None)",
        "api_description": "Low-level profiler wrap the autograd profile",
        "return_value": "",
        "parameters": "activities (iterable) – list of activity groups (CPU, CUDA) to use in profiling, supported values:\ntorch.profiler.ProfilerActivity.CPU, torch.profiler.ProfilerActivity.CUDA.\nDefault value: ProfilerActivity.CPU and (when available) ProfilerActivity.CUDA.\nrecord_shapes (bool) – save information about operator’s input shapes.\nprofile_memory (bool) – track tensor memory allocation/deallocation (see export_memory_timeline\nfor more details).\nwith_stack (bool) – record source information (file and line number) for the ops.\nwith_flops (bool) – use formula to estimate the FLOPS of specific operators\n(matrix multiplication and 2D convolution).\nwith_modules (bool) – record module hierarchy (including function names)\ncorresponding to the callstack of the op. e.g. If module A’s forward call’s\nmodule B’s forward which contains an aten::add op,\nthen aten::add’s module hierarchy is A.B\nNote that this support exist, at the moment, only for TorchScript models\nand not eager mode models.\nexperimental_config (_ExperimentalConfig) – A set of experimental options\nused by profiler libraries like Kineto. Note, backward compatibility is not guaranteed.\nrepresentation of AI/ML workloads and enable replay benchmarks, simulators, and emulators.\nWhen this argument is included the observer start() and stop() will be called for the\npath (str) – save stacks file to this location;\nmetric (str) – metric to use: “self_cpu_time_total” or “self_cuda_time_total”",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.agent.server.SimpleElasticAgent._monitor_workers",
        "api_url": "https://pytorch.org/docs/stable/elastic/agent.html#torch.distributed.elastic.agent.server.SimpleElasticAgent._monitor_workers",
        "api_signature": "_monitor_workers(worker_group)",
        "api_description": "Check on the workers for the worker_group.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda.memory._record_memory_history",
        "api_url": "https://pytorch.org/docs/stable/torch_cuda_memory.html#torch.cuda.memory._record_memory_history",
        "api_signature": "torch.cuda.memory._record_memory_history(enabled='all', context='all', stacks='all', max_entries=9223372036854775807, device=None)",
        "api_description": "Enable recording of stack traces associated with memory\nallocations, so you can tell what allocated any piece of memory in\ntorch.cuda.memory._snapshot().",
        "return_value": "",
        "parameters": "enabled (Literal[None, \"state\", \"all\"], optional) – None, disable recording memory history.\n“state”, keep information for currenly allocated memory.\n“all”, additionally keep a history of all alloc/free calls.\nDefaults to “all”.\ncontext (Literal[None, \"state\", \"alloc\", \"all\"], optional) – None, Do not record any tracebacks.\n“state”, Record tracebacks for currently allocated memory.\n“alloc”, additionally keep tracebacks for alloc calls.\n“all”, additionally keep tracebacks for free calls.\nDefaults to “all”.\nstacks (Literal[\"python\", \"all\"], optional) – “python”, include Python, TorchScript, and inductor frames in tracebacks\n“all”, additionally include C++ frames\nDefaults to “all”.\nmax_entries (int, optional) – Keep a maximum of max_entries\nalloc/free events in the recorded history recorded.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.agent.server.SimpleElasticAgent._rendezvous",
        "api_url": "https://pytorch.org/docs/stable/elastic/agent.html#torch.distributed.elastic.agent.server.SimpleElasticAgent._rendezvous",
        "api_signature": "_rendezvous(worker_group)",
        "api_description": "Run rendezvous for the workers specified by the worker spec.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.agent.server.SimpleElasticAgent._restart_workers",
        "api_url": "https://pytorch.org/docs/stable/elastic/agent.html#torch.distributed.elastic.agent.server.SimpleElasticAgent._restart_workers",
        "api_signature": "_restart_workers(worker_group)",
        "api_description": "Restart (stops, rendezvous, starts) all local workers in the group.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.agent.server.SimpleElasticAgent._shutdown",
        "api_url": "https://pytorch.org/docs/stable/elastic/agent.html#torch.distributed.elastic.agent.server.SimpleElasticAgent._shutdown",
        "api_signature": "_shutdown(death_sig=Signals.SIGTERM)",
        "api_description": "Clean up any resources that were allocated during the agent’s work.",
        "return_value": "",
        "parameters": "death_sig (Signals) – Signal to send to the child process, SIGTERM is default",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda.memory._snapshot",
        "api_url": "https://pytorch.org/docs/stable/torch_cuda_memory.html#torch.cuda.memory._snapshot",
        "api_signature": "torch.cuda.memory._snapshot(device=None)",
        "api_description": "Save a snapshot of CUDA memory state at the time it was called.",
        "return_value": "The Snapshot dictionary object\n",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.agent.server.SimpleElasticAgent._start_workers",
        "api_url": "https://pytorch.org/docs/stable/elastic/agent.html#torch.distributed.elastic.agent.server.SimpleElasticAgent._start_workers",
        "api_signature": "_start_workers(worker_group)",
        "api_description": "Start worker_group.spec.local_world_size number of workers.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.agent.server.SimpleElasticAgent._stop_workers",
        "api_url": "https://pytorch.org/docs/stable/elastic/agent.html#torch.distributed.elastic.agent.server.SimpleElasticAgent._stop_workers",
        "api_signature": "_stop_workers(worker_group)",
        "api_description": "Stop all workers in the given worker group.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.abs",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.abs.html#torch.abs",
        "api_signature": "torch.abs(input, *, out=None)",
        "api_description": "Computes the absolute value of each element in input.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.abs",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.abs.html#torch.Tensor.abs",
        "api_signature": "Tensor.abs()",
        "api_description": "See torch.abs()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.abs_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.abs_.html#torch.Tensor.abs_",
        "api_signature": "Tensor.abs_()",
        "api_description": "In-place version of abs()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.absolute",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.absolute.html#torch.absolute",
        "api_signature": "torch.absolute(input, *, out=None)",
        "api_description": "Alias for torch.abs()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.absolute",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.absolute.html#torch.Tensor.absolute",
        "api_signature": "Tensor.absolute()",
        "api_description": "Alias for abs()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.absolute_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.absolute_.html#torch.Tensor.absolute_",
        "api_signature": "Tensor.absolute_()",
        "api_description": "In-place version of absolute()\nAlias for abs_()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.transforms.AbsTransform",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.transforms.AbsTransform",
        "api_signature": "torch.distributions.transforms.AbsTransform(cache_size=0)",
        "api_description": "Transform via the mapping y=∣x∣y = |x|y=∣x∣.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.acos",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.acos.html#torch.acos",
        "api_signature": "torch.acos(input, *, out=None)",
        "api_description": "Computes the inverse cosine of each element in input.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.acos",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.acos.html#torch.Tensor.acos",
        "api_signature": "Tensor.acos()",
        "api_description": "See torch.acos()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.acos_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.acos_.html#torch.Tensor.acos_",
        "api_signature": "Tensor.acos_()",
        "api_description": "In-place version of acos()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.acosh",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.acosh.html#torch.acosh",
        "api_signature": "torch.acosh(input, *, out=None)",
        "api_description": "Returns a new tensor with the inverse hyperbolic cosine of the elements of input.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.acosh",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.acosh.html#torch.Tensor.acosh",
        "api_signature": "Tensor.acosh()",
        "api_description": "See torch.acosh()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.acosh_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.acosh_.html#torch.Tensor.acosh_",
        "api_signature": "Tensor.acosh_()",
        "api_description": "In-place version of acosh()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.timer.TimerClient.acquire",
        "api_url": "https://pytorch.org/docs/stable/elastic/timer.html#torch.distributed.elastic.timer.TimerClient.acquire",
        "api_signature": "acquire(scope_id, expiration_time)",
        "api_description": "Acquires a timer for the worker that holds this client object\ngiven the scope_id and expiration_time. Typically registers\nthe timer with the TimerServer.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.Adadelta",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.Adadelta.html#torch.optim.Adadelta",
        "api_signature": "torch.optim.Adadelta(params, lr=1.0, rho=0.9, eps=1e-06, weight_decay=0, foreach=None, *, maximize=False, differentiable=False)",
        "api_description": "Implements Adadelta algorithm.",
        "return_value": "a handle that can be used to remove the added hook by calling\nhandle.remove()\na handle that can be used to remove the added hook by calling\nhandle.remove()\na handle that can be used to remove the added hook by calling\nhandle.remove()\na handle that can be used to remove the added hook by calling\nhandle.remove()\na handle that can be used to remove the added hook by calling\nhandle.remove()\na handle that can be used to remove the added hook by calling\nhandle.remove()\n",
        "parameters": "params (iterable) – iterable of parameters to optimize or dicts defining\nparameter groups\nrho (float, optional) – coefficient used for computing a running average\nof squared gradients (default: 0.9). A higher value of rho will\nresult in a slower average, which can be helpful for preventing\noscillations in the learning process.\neps (float, optional) – term added to the denominator to improve\nnumerical stability (default: 1e-6).\nlr (float, optional) – coefficient that scale delta before it is applied\nto the parameters (default: 1.0)\nweight_decay (float, optional) – weight decay (L2 penalty) (default: 0)\nforeach (bool, optional) – whether foreach implementation of optimizer\nis used. If unspecified by the user (so foreach is None), we will try to use\nforeach over the for-loop implementation on CUDA, since it is usually\nsignificantly more performant. Note that the foreach implementation uses\n~ sizeof(params) more peak memory than the for-loop version due to the intermediates\nbeing a tensorlist vs just one tensor. If memory is prohibitive, batch fewer\nparameters through the optimizer at a time or switch this flag to False (default: None)\nmaximize (bool, optional) – maximize the objective with respect to the\nparams, instead of minimizing (default: False)\ndifferentiable (bool, optional) – whether autograd should\noccur through the optimizer step in training. Otherwise, the step()\nfunction runs in a torch.no_grad() context. Setting to True can impair\nperformance, so leave it False if you don’t intend to run autograd\nthrough this instance (default: False)\nparam_group (dict) – Specifies what Tensors should be optimized along with group\nspecific optimization options.\nstate_dict (dict) – optimizer state. Should be an object returned\nfrom a call to state_dict().\nhook (Callable) – The user defined hook to be registered.\nprepend (bool) – If True, the provided post hook will be fired before\nall the already registered post-hooks on load_state_dict. Otherwise,\nthe provided hook will be fired after all the already registered\npost-hooks. (default: False)\nhook (Callable) – The user defined hook to be registered.\nprepend (bool) – If True, the provided pre hook will be fired before\nall the already registered pre-hooks on load_state_dict. Otherwise,\nthe provided hook will be fired after all the already registered\npre-hooks. (default: False)\nhook (Callable) – The user defined hook to be registered.\nprepend (bool) – If True, the provided post hook will be fired before\nall the already registered post-hooks on state_dict. Otherwise,\nthe provided hook will be fired after all the already registered\npost-hooks. (default: False)\nhook (Callable) – The user defined hook to be registered.\nprepend (bool) – If True, the provided pre hook will be fired before\nall the already registered pre-hooks on state_dict. Otherwise,\nthe provided hook will be fired after all the already registered\npre-hooks. (default: False)\nhook (Callable) – The user defined hook to be registered.\nhook (Callable) – The user defined hook to be registered.\nclosure (Callable, optional) – A closure that reevaluates the model\nand returns the loss.\nset_to_none (bool) – instead of setting to zero, set the grads to None.\nThis will in general have lower memory footprint, and can modestly improve performance.\nHowever, it changes certain behaviors. For example:\n1. When the user tries to access a gradient and perform manual ops on it,\na None attribute or a Tensor full of 0s will behave differently.\n2. If the user requests zero_grad(set_to_none=True) followed by a backward pass, .grads\nare guaranteed to be None for params that did not receive a gradient.\n3. torch.optim optimizers have a different behavior if the gradient is 0 or None\n(in one case it does the step with a gradient of 0 and in the other it skips\nthe step altogether).",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.Adagrad",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.Adagrad.html#torch.optim.Adagrad",
        "api_signature": "torch.optim.Adagrad(params, lr=0.01, lr_decay=0, weight_decay=0, initial_accumulator_value=0, eps=1e-10, foreach=None, *, maximize=False, differentiable=False)",
        "api_description": "Implements Adagrad algorithm.",
        "return_value": "a handle that can be used to remove the added hook by calling\nhandle.remove()\na handle that can be used to remove the added hook by calling\nhandle.remove()\na handle that can be used to remove the added hook by calling\nhandle.remove()\na handle that can be used to remove the added hook by calling\nhandle.remove()\na handle that can be used to remove the added hook by calling\nhandle.remove()\na handle that can be used to remove the added hook by calling\nhandle.remove()\n",
        "parameters": "params (iterable) – iterable of parameters to optimize or dicts defining\nparameter groups\nlr (float, optional) – learning rate (default: 1e-2)\nlr_decay (float, optional) – learning rate decay (default: 0)\nweight_decay (float, optional) – weight decay (L2 penalty) (default: 0)\neps (float, optional) – term added to the denominator to improve\nnumerical stability (default: 1e-10)\nforeach (bool, optional) – whether foreach implementation of optimizer\nis used. If unspecified by the user (so foreach is None), we will try to use\nforeach over the for-loop implementation on CUDA, since it is usually\nsignificantly more performant. Note that the foreach implementation uses\n~ sizeof(params) more peak memory than the for-loop version due to the intermediates\nbeing a tensorlist vs just one tensor. If memory is prohibitive, batch fewer\nparameters through the optimizer at a time or switch this flag to False (default: None)\nmaximize (bool, optional) – maximize the objective with respect to the\nparams, instead of minimizing (default: False)\ndifferentiable (bool, optional) – whether autograd should\noccur through the optimizer step in training. Otherwise, the step()\nfunction runs in a torch.no_grad() context. Setting to True can impair\nperformance, so leave it False if you don’t intend to run autograd\nthrough this instance (default: False)\nparam_group (dict) – Specifies what Tensors should be optimized along with group\nspecific optimization options.\nstate_dict (dict) – optimizer state. Should be an object returned\nfrom a call to state_dict().\nhook (Callable) – The user defined hook to be registered.\nprepend (bool) – If True, the provided post hook will be fired before\nall the already registered post-hooks on load_state_dict. Otherwise,\nthe provided hook will be fired after all the already registered\npost-hooks. (default: False)\nhook (Callable) – The user defined hook to be registered.\nprepend (bool) – If True, the provided pre hook will be fired before\nall the already registered pre-hooks on load_state_dict. Otherwise,\nthe provided hook will be fired after all the already registered\npre-hooks. (default: False)\nhook (Callable) – The user defined hook to be registered.\nprepend (bool) – If True, the provided post hook will be fired before\nall the already registered post-hooks on state_dict. Otherwise,\nthe provided hook will be fired after all the already registered\npost-hooks. (default: False)\nhook (Callable) – The user defined hook to be registered.\nprepend (bool) – If True, the provided pre hook will be fired before\nall the already registered pre-hooks on state_dict. Otherwise,\nthe provided hook will be fired after all the already registered\npre-hooks. (default: False)\nhook (Callable) – The user defined hook to be registered.\nhook (Callable) – The user defined hook to be registered.\nclosure (Callable, optional) – A closure that reevaluates the model\nand returns the loss.\nset_to_none (bool) – instead of setting to zero, set the grads to None.\nThis will in general have lower memory footprint, and can modestly improve performance.\nHowever, it changes certain behaviors. For example:\n1. When the user tries to access a gradient and perform manual ops on it,\na None attribute or a Tensor full of 0s will behave differently.\n2. If the user requests zero_grad(set_to_none=True) followed by a backward pass, .grads\nare guaranteed to be None for params that did not receive a gradient.\n3. torch.optim optimizers have a different behavior if the gradient is 0 or None\n(in one case it does the step with a gradient of 0 and in the other it skips\nthe step altogether).",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.Adam",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam",
        "api_signature": "torch.optim.Adam(params, lr=0.001, betas=(0.9, 0.999)",
        "api_description": "Implements Adam algorithm.",
        "return_value": "a handle that can be used to remove the added hook by calling\nhandle.remove()\na handle that can be used to remove the added hook by calling\nhandle.remove()\na handle that can be used to remove the added hook by calling\nhandle.remove()\na handle that can be used to remove the added hook by calling\nhandle.remove()\na handle that can be used to remove the added hook by calling\nhandle.remove()\na handle that can be used to remove the added hook by calling\nhandle.remove()\n",
        "parameters": "params (iterable) – iterable of parameters to optimize or dicts defining\nparameter groups\nlr (float, Tensor, optional) – learning rate (default: 1e-3). A tensor LR\nis not yet supported for all our implementations. Please use a float\nLR if you are not also specifying fused=True or capturable=True.\nbetas (Tuple[float, float], optional) – coefficients used for computing\nrunning averages of gradient and its square (default: (0.9, 0.999))\neps (float, optional) – term added to the denominator to improve\nnumerical stability (default: 1e-8)\nweight_decay (float, optional) – weight decay (L2 penalty) (default: 0)\namsgrad (bool, optional) – whether to use the AMSGrad variant of this\nalgorithm from the paper On the Convergence of Adam and Beyond\n(default: False)\nforeach (bool, optional) – whether foreach implementation of optimizer\nis used. If unspecified by the user (so foreach is None), we will try to use\nforeach over the for-loop implementation on CUDA, since it is usually\nsignificantly more performant. Note that the foreach implementation uses\n~ sizeof(params) more peak memory than the for-loop version due to the intermediates\nbeing a tensorlist vs just one tensor. If memory is prohibitive, batch fewer\nparameters through the optimizer at a time or switch this flag to False (default: None)\nmaximize (bool, optional) – maximize the objective with respect to the\nparams, instead of minimizing (default: False)\ncapturable (bool, optional) – whether this instance is safe to\ncapture in a CUDA graph. Passing True can impair ungraphed performance,\nso if you don’t intend to graph capture this instance, leave it False\n(default: False)\ndifferentiable (bool, optional) – whether autograd should\noccur through the optimizer step in training. Otherwise, the step()\nfunction runs in a torch.no_grad() context. Setting to True can impair\nperformance, so leave it False if you don’t intend to run autograd\nthrough this instance (default: False)\nfused (bool, optional) – whether the fused implementation (CUDA only) is used.\nCurrently, torch.float64, torch.float32, torch.float16, and torch.bfloat16\nare supported. (default: None)\nparam_group (dict) – Specifies what Tensors should be optimized along with group\nspecific optimization options.\nstate_dict (dict) – optimizer state. Should be an object returned\nfrom a call to state_dict().\nhook (Callable) – The user defined hook to be registered.\nprepend (bool) – If True, the provided post hook will be fired before\nall the already registered post-hooks on load_state_dict. Otherwise,\nthe provided hook will be fired after all the already registered\npost-hooks. (default: False)\nhook (Callable) – The user defined hook to be registered.\nprepend (bool) – If True, the provided pre hook will be fired before\nall the already registered pre-hooks on load_state_dict. Otherwise,\nthe provided hook will be fired after all the already registered\npre-hooks. (default: False)\nhook (Callable) – The user defined hook to be registered.\nprepend (bool) – If True, the provided post hook will be fired before\nall the already registered post-hooks on state_dict. Otherwise,\nthe provided hook will be fired after all the already registered\npost-hooks. (default: False)\nhook (Callable) – The user defined hook to be registered.\nprepend (bool) – If True, the provided pre hook will be fired before\nall the already registered pre-hooks on state_dict. Otherwise,\nthe provided hook will be fired after all the already registered\npre-hooks. (default: False)\nhook (Callable) – The user defined hook to be registered.\nhook (Callable) – The user defined hook to be registered.\nclosure (Callable, optional) – A closure that reevaluates the model\nand returns the loss.\nset_to_none (bool) – instead of setting to zero, set the grads to None.\nThis will in general have lower memory footprint, and can modestly improve performance.\nHowever, it changes certain behaviors. For example:\n1. When the user tries to access a gradient and perform manual ops on it,\na None attribute or a Tensor full of 0s will behave differently.\n2. If the user requests zero_grad(set_to_none=True) followed by a backward pass, .grads\nare guaranteed to be None for params that did not receive a gradient.\n3. torch.optim optimizers have a different behavior if the gradient is 0 or None\n(in one case it does the step with a gradient of 0 and in the other it skips\nthe step altogether).",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.Adamax",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.Adamax.html#torch.optim.Adamax",
        "api_signature": "torch.optim.Adamax(params, lr=0.002, betas=(0.9, 0.999)",
        "api_description": "Implements Adamax algorithm (a variant of Adam based on infinity norm).",
        "return_value": "a handle that can be used to remove the added hook by calling\nhandle.remove()\na handle that can be used to remove the added hook by calling\nhandle.remove()\na handle that can be used to remove the added hook by calling\nhandle.remove()\na handle that can be used to remove the added hook by calling\nhandle.remove()\na handle that can be used to remove the added hook by calling\nhandle.remove()\na handle that can be used to remove the added hook by calling\nhandle.remove()\n",
        "parameters": "params (iterable) – iterable of parameters to optimize or dicts defining\nparameter groups\nlr (float, optional) – learning rate (default: 2e-3)\nbetas (Tuple[float, float], optional) – coefficients used for computing\nrunning averages of gradient and its square\neps (float, optional) – term added to the denominator to improve\nnumerical stability (default: 1e-8)\nweight_decay (float, optional) – weight decay (L2 penalty) (default: 0)\nforeach (bool, optional) – whether foreach implementation of optimizer\nis used. If unspecified by the user (so foreach is None), we will try to use\nforeach over the for-loop implementation on CUDA, since it is usually\nsignificantly more performant. Note that the foreach implementation uses\n~ sizeof(params) more peak memory than the for-loop version due to the intermediates\nbeing a tensorlist vs just one tensor. If memory is prohibitive, batch fewer\nparameters through the optimizer at a time or switch this flag to False (default: None)\nmaximize (bool, optional) – maximize the objective with respect to the\nparams, instead of minimizing (default: False)\ndifferentiable (bool, optional) – whether autograd should\noccur through the optimizer step in training. Otherwise, the step()\nfunction runs in a torch.no_grad() context. Setting to True can impair\nperformance, so leave it False if you don’t intend to run autograd\nthrough this instance (default: False)\ncapturable (bool, optional) – whether this instance is safe to\ncapture in a CUDA graph. Passing True can impair ungraphed performance,\nso if you don’t intend to graph capture this instance, leave it False\n(default: False)\nparam_group (dict) – Specifies what Tensors should be optimized along with group\nspecific optimization options.\nstate_dict (dict) – optimizer state. Should be an object returned\nfrom a call to state_dict().\nhook (Callable) – The user defined hook to be registered.\nprepend (bool) – If True, the provided post hook will be fired before\nall the already registered post-hooks on load_state_dict. Otherwise,\nthe provided hook will be fired after all the already registered\npost-hooks. (default: False)\nhook (Callable) – The user defined hook to be registered.\nprepend (bool) – If True, the provided pre hook will be fired before\nall the already registered pre-hooks on load_state_dict. Otherwise,\nthe provided hook will be fired after all the already registered\npre-hooks. (default: False)\nhook (Callable) – The user defined hook to be registered.\nprepend (bool) – If True, the provided post hook will be fired before\nall the already registered post-hooks on state_dict. Otherwise,\nthe provided hook will be fired after all the already registered\npost-hooks. (default: False)\nhook (Callable) – The user defined hook to be registered.\nprepend (bool) – If True, the provided pre hook will be fired before\nall the already registered pre-hooks on state_dict. Otherwise,\nthe provided hook will be fired after all the already registered\npre-hooks. (default: False)\nhook (Callable) – The user defined hook to be registered.\nhook (Callable) – The user defined hook to be registered.\nclosure (Callable, optional) – A closure that reevaluates the model\nand returns the loss.\nset_to_none (bool) – instead of setting to zero, set the grads to None.\nThis will in general have lower memory footprint, and can modestly improve performance.\nHowever, it changes certain behaviors. For example:\n1. When the user tries to access a gradient and perform manual ops on it,\na None attribute or a Tensor full of 0s will behave differently.\n2. If the user requests zero_grad(set_to_none=True) followed by a backward pass, .grads\nare guaranteed to be None for params that did not receive a gradient.\n3. torch.optim optimizers have a different behavior if the gradient is 0 or None\n(in one case it does the step with a gradient of 0 and in the other it skips\nthe step altogether).",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.AdamW",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html#torch.optim.AdamW",
        "api_signature": "torch.optim.AdamW(params, lr=0.001, betas=(0.9, 0.999)",
        "api_description": "Implements AdamW algorithm.",
        "return_value": "a handle that can be used to remove the added hook by calling\nhandle.remove()\na handle that can be used to remove the added hook by calling\nhandle.remove()\na handle that can be used to remove the added hook by calling\nhandle.remove()\na handle that can be used to remove the added hook by calling\nhandle.remove()\na handle that can be used to remove the added hook by calling\nhandle.remove()\na handle that can be used to remove the added hook by calling\nhandle.remove()\n",
        "parameters": "params (iterable) – iterable of parameters to optimize or dicts defining\nparameter groups\nlr (float, Tensor, optional) – learning rate (default: 1e-3). A tensor LR\nis not yet supported for all our implementations. Please use a float\nLR if you are not also specifying fused=True or capturable=True.\nbetas (Tuple[float, float], optional) – coefficients used for computing\nrunning averages of gradient and its square (default: (0.9, 0.999))\neps (float, optional) – term added to the denominator to improve\nnumerical stability (default: 1e-8)\nweight_decay (float, optional) – weight decay coefficient (default: 1e-2)\namsgrad (bool, optional) – whether to use the AMSGrad variant of this\nalgorithm from the paper On the Convergence of Adam and Beyond\n(default: False)\nmaximize (bool, optional) – maximize the objective with respect to the\nparams, instead of minimizing (default: False)\nforeach (bool, optional) – whether foreach implementation of optimizer\nis used. If unspecified by the user (so foreach is None), we will try to use\nforeach over the for-loop implementation on CUDA, since it is usually\nsignificantly more performant. Note that the foreach implementation uses\n~ sizeof(params) more peak memory than the for-loop version due to the intermediates\nbeing a tensorlist vs just one tensor. If memory is prohibitive, batch fewer\nparameters through the optimizer at a time or switch this flag to False (default: None)\ncapturable (bool, optional) – whether this instance is safe to\ncapture in a CUDA graph. Passing True can impair ungraphed performance,\nso if you don’t intend to graph capture this instance, leave it False\n(default: False)\ndifferentiable (bool, optional) – whether autograd should\noccur through the optimizer step in training. Otherwise, the step()\nfunction runs in a torch.no_grad() context. Setting to True can impair\nperformance, so leave it False if you don’t intend to run autograd\nthrough this instance (default: False)\nfused (bool, optional) – whether the fused implementation (CUDA only) is used.\nCurrently, torch.float64, torch.float32, torch.float16, and torch.bfloat16\nare supported. (default: None)\nparam_group (dict) – Specifies what Tensors should be optimized along with group\nspecific optimization options.\nstate_dict (dict) – optimizer state. Should be an object returned\nfrom a call to state_dict().\nhook (Callable) – The user defined hook to be registered.\nprepend (bool) – If True, the provided post hook will be fired before\nall the already registered post-hooks on load_state_dict. Otherwise,\nthe provided hook will be fired after all the already registered\npost-hooks. (default: False)\nhook (Callable) – The user defined hook to be registered.\nprepend (bool) – If True, the provided pre hook will be fired before\nall the already registered pre-hooks on load_state_dict. Otherwise,\nthe provided hook will be fired after all the already registered\npre-hooks. (default: False)\nhook (Callable) – The user defined hook to be registered.\nprepend (bool) – If True, the provided post hook will be fired before\nall the already registered post-hooks on state_dict. Otherwise,\nthe provided hook will be fired after all the already registered\npost-hooks. (default: False)\nhook (Callable) – The user defined hook to be registered.\nprepend (bool) – If True, the provided pre hook will be fired before\nall the already registered pre-hooks on state_dict. Otherwise,\nthe provided hook will be fired after all the already registered\npre-hooks. (default: False)\nhook (Callable) – The user defined hook to be registered.\nhook (Callable) – The user defined hook to be registered.\nclosure (Callable, optional) – A closure that reevaluates the model\nand returns the loss.\nset_to_none (bool) – instead of setting to zero, set the grads to None.\nThis will in general have lower memory footprint, and can modestly improve performance.\nHowever, it changes certain behaviors. For example:\n1. When the user tries to access a gradient and perform manual ops on it,\na None attribute or a Tensor full of 0s will behave differently.\n2. If the user requests zero_grad(set_to_none=True) followed by a backward pass, .grads\nare guaranteed to be None for params that did not receive a gradient.\n3. torch.optim optimizers have a different behavior if the gradient is 0 or None\n(in one case it does the step with a gradient of 0 and in the other it skips\nthe step altogether).",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.export.unflatten.FlatArgsAdapter.adapt",
        "api_url": "https://pytorch.org/docs/stable/export.html#torch.export.unflatten.FlatArgsAdapter.adapt",
        "api_signature": "adapt(target_spec, input_spec, input_args)",
        "api_description": "NOTE: This adapter may mutate given input_args_with_path.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.onnx.ONNXProgram.adapt_torch_inputs_to_onnx",
        "api_url": "https://pytorch.org/docs/stable/onnx_dynamo.html#torch.onnx.ONNXProgram.adapt_torch_inputs_to_onnx",
        "api_signature": "adapt_torch_inputs_to_onnx(*model_args, model_with_state_dict=None, **model_kwargs)",
        "api_description": "Converts the PyTorch model inputs to exported ONNX model inputs format.",
        "return_value": "A sequence of tensors converted from PyTorch model inputs.\n",
        "parameters": "If not specified, the model used during export is used.\nRequired when enable_fake_mode() is used to extract real initializers as needed by the ONNX graph.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.onnx.ONNXProgram.adapt_torch_outputs_to_onnx",
        "api_url": "https://pytorch.org/docs/stable/onnx_dynamo.html#torch.onnx.ONNXProgram.adapt_torch_outputs_to_onnx",
        "api_signature": "adapt_torch_outputs_to_onnx(model_outputs, model_with_state_dict=None)",
        "api_description": "Converts the PyTorch model outputs to exported ONNX model outputs format.",
        "return_value": "PyTorch model outputs in exported ONNX model outputs format.\n",
        "parameters": "If not specified, the model used during export is used.\nRequired when enable_fake_mode() is used to extract real initializers as needed by the ONNX graph.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.benchmark.Timer.adaptive_autorange",
        "api_url": "https://pytorch.org/docs/stable/benchmark_utils.html#torch.utils.benchmark.Timer.adaptive_autorange",
        "api_signature": "adaptive_autorange(threshold=0.1, *, min_run_time=0.01, max_run_time=10.0, callback=None)",
        "api_description": "Similar to blocked_autorange but also checks for variablility in measurements\nand repeats until iqr/median is smaller than threshold or max_run_time is reached.",
        "return_value": "A Measurement object that contains measured runtimes and\nrepetition counts, and can be used to compute statistics.\n(mean, median, etc.)\n",
        "parameters": "threshold (float) – value of iqr/median threshold for stopping\nmin_run_time (float) – total runtime needed before checking threshold\nmax_run_time (float) – total runtime  for all measurements regardless of threshold",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.functional.adaptive_avg_pool1d",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.functional.adaptive_avg_pool1d.html#torch.nn.functional.adaptive_avg_pool1d",
        "api_signature": "torch.nn.functional.adaptive_avg_pool1d(input, output_size)",
        "api_description": "Applies a 1D adaptive average pooling over an input signal composed of\nseveral input planes.",
        "return_value": "",
        "parameters": "output_size – the target output size (single integer)",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.quantized.functional.adaptive_avg_pool2d",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.nn.quantized.functional.adaptive_avg_pool2d.html#torch.ao.nn.quantized.functional.adaptive_avg_pool2d",
        "api_signature": "torch.ao.nn.quantized.functional.adaptive_avg_pool2d(input, output_size)",
        "api_description": "Applies a 2D adaptive average pooling over a quantized input signal composed\nof several quantized input planes.",
        "return_value": "",
        "parameters": "output_size (None) – the target output size (single integer or\ndouble-integer tuple)",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.functional.adaptive_avg_pool2d",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.functional.adaptive_avg_pool2d.html#torch.nn.functional.adaptive_avg_pool2d",
        "api_signature": "torch.nn.functional.adaptive_avg_pool2d(input, output_size)",
        "api_description": "Apply a 2D adaptive average pooling over an input signal composed of several input planes.",
        "return_value": "",
        "parameters": "output_size (None) – the target output size (single integer or\ndouble-integer tuple)",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.quantized.functional.adaptive_avg_pool3d",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.nn.quantized.functional.adaptive_avg_pool3d.html#torch.ao.nn.quantized.functional.adaptive_avg_pool3d",
        "api_signature": "torch.ao.nn.quantized.functional.adaptive_avg_pool3d(input, output_size)",
        "api_description": "Applies a 3D adaptive average pooling over a quantized input signal composed\nof several quantized input planes.",
        "return_value": "",
        "parameters": "output_size (None) – the target output size (single integer or\ndouble-integer tuple)",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.functional.adaptive_avg_pool3d",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.functional.adaptive_avg_pool3d.html#torch.nn.functional.adaptive_avg_pool3d",
        "api_signature": "torch.nn.functional.adaptive_avg_pool3d(input, output_size)",
        "api_description": "Apply a 3D adaptive average pooling over an input signal composed of several input planes.",
        "return_value": "",
        "parameters": "output_size (None) – the target output size (single integer or\ntriple-integer tuple)",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.functional.adaptive_max_pool1d",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.functional.adaptive_max_pool1d.html#torch.nn.functional.adaptive_max_pool1d",
        "api_signature": "torch.nn.functional.adaptive_max_pool1d(input, output_size, return_indices=False)",
        "api_description": "Applies a 1D adaptive max pooling over an input signal composed of\nseveral input planes.",
        "return_value": "",
        "parameters": "output_size – the target output size (single integer)\nreturn_indices – whether to return pooling indices. Default: False",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.functional.adaptive_max_pool2d",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.functional.adaptive_max_pool2d.html#torch.nn.functional.adaptive_max_pool2d",
        "api_signature": "torch.nn.functional.adaptive_max_pool2d(input, output_size, return_indices=False)",
        "api_description": "Applies a 2D adaptive max pooling over an input signal composed of\nseveral input planes.",
        "return_value": "",
        "parameters": "output_size – the target output size (single integer or\ndouble-integer tuple)\nreturn_indices – whether to return pooling indices. Default: False",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.functional.adaptive_max_pool3d",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.functional.adaptive_max_pool3d.html#torch.nn.functional.adaptive_max_pool3d",
        "api_signature": "torch.nn.functional.adaptive_max_pool3d(input, output_size, return_indices=False)",
        "api_description": "Applies a 3D adaptive max pooling over an input signal composed of\nseveral input planes.",
        "return_value": "",
        "parameters": "output_size – the target output size (single integer or\ntriple-integer tuple)\nreturn_indices – whether to return pooling indices. Default: False",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.AdaptiveAvgPool1d",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.AdaptiveAvgPool1d.html#torch.nn.AdaptiveAvgPool1d",
        "api_signature": "torch.nn.AdaptiveAvgPool1d(output_size)",
        "api_description": "Applies a 1D adaptive average pooling over an input signal composed of several input planes.",
        "return_value": "",
        "parameters": "output_size (Union[int, Tuple[int]]) – the target output size LoutL_{out}Lout​.",
        "input_shape": "\nInput: (N,C,Lin)(N, C, L_{in})(N,C,Lin​) or (C,Lin)(C, L_{in})(C,Lin​).\nOutput: (N,C,Lout)(N, C, L_{out})(N,C,Lout​) or (C,Lout)(C, L_{out})(C,Lout​), where\nLout=output_sizeL_{out}=\\text{output\\_size}Lout​=output_size.\n\n",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.AdaptiveAvgPool2d",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.AdaptiveAvgPool2d.html#torch.nn.AdaptiveAvgPool2d",
        "api_signature": "torch.nn.AdaptiveAvgPool2d(output_size)",
        "api_description": "Applies a 2D adaptive average pooling over an input signal composed of several input planes.",
        "return_value": "",
        "parameters": "output_size (Union[int, None, Tuple[Optional[int], Optional[int]]]) – the target output size of the image of the form H x W.\nCan be a tuple (H, W) or a single H for a square image H x H.\nH and W can be either a int, or None which means the size will\nbe the same as that of the input.",
        "input_shape": "\nInput: (N,C,Hin,Win)(N, C, H_{in}, W_{in})(N,C,Hin​,Win​) or (C,Hin,Win)(C, H_{in}, W_{in})(C,Hin​,Win​).\nOutput: (N,C,S0,S1)(N, C, S_{0}, S_{1})(N,C,S0​,S1​) or (C,S0,S1)(C, S_{0}, S_{1})(C,S0​,S1​), where\nS=output_sizeS=\\text{output\\_size}S=output_size.\n\n",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.AdaptiveAvgPool3d",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.AdaptiveAvgPool3d.html#torch.nn.AdaptiveAvgPool3d",
        "api_signature": "torch.nn.AdaptiveAvgPool3d(output_size)",
        "api_description": "Applies a 3D adaptive average pooling over an input signal composed of several input planes.",
        "return_value": "",
        "parameters": "output_size (Union[int, None, Tuple[Optional[int], Optional[int], Optional[int]]]) – the target output size of the form D x H x W.\nCan be a tuple (D, H, W) or a single number D for a cube D x D x D.\nD, H and W can be either a int, or None which means the size will\nbe the same as that of the input.",
        "input_shape": "\nInput: (N,C,Din,Hin,Win)(N, C, D_{in}, H_{in}, W_{in})(N,C,Din​,Hin​,Win​) or (C,Din,Hin,Win)(C, D_{in}, H_{in}, W_{in})(C,Din​,Hin​,Win​).\nOutput: (N,C,S0,S1,S2)(N, C, S_{0}, S_{1}, S_{2})(N,C,S0​,S1​,S2​) or (C,S0,S1,S2)(C, S_{0}, S_{1}, S_{2})(C,S0​,S1​,S2​),\nwhere S=output_sizeS=\\text{output\\_size}S=output_size.\n\n",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.AdaptiveLogSoftmaxWithLoss",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.AdaptiveLogSoftmaxWithLoss.html#torch.nn.AdaptiveLogSoftmaxWithLoss",
        "api_signature": "torch.nn.AdaptiveLogSoftmaxWithLoss(in_features, n_classes, cutoffs, div_value=4.0, head_bias=False, device=None, dtype=None)",
        "api_description": "Efficient softmax approximation.",
        "return_value": "\noutput is a Tensor of size N containing computed target\nlog probabilities for each example\nloss is a Scalar representing the computed negative\nlog likelihood loss\n\n\nlog-probabilities of for each class ccc\nin range 0<=c<=n_classes0 <= c <= \\texttt{n\\_classes}0<=c<=n_classes, where n_classes\\texttt{n\\_classes}n_classes is a\nparameter passed to AdaptiveLogSoftmaxWithLoss constructor.\na class with the highest probability for each example\n",
        "parameters": "in_features (int) – Number of features in the input tensor\nn_classes (int) – Number of classes in the dataset\ncutoffs (Sequence) – Cutoffs used to assign targets to their buckets\ndiv_value (float, optional) – value used as an exponent to compute sizes\nof the clusters. Default: 4.0\nhead_bias (bool, optional) – If True, adds a bias term to the ‘head’ of the\nadaptive softmax. Default: False\ninput (Tensor) – a minibatch of examples\ninput (Tensor) – a minibatch of examples",
        "input_shape": "\ninput: (N,in_features)(N, \\texttt{in\\_features})(N,in_features) or (in_features)(\\texttt{in\\_features})(in_features)\ntarget: (N)(N)(N) or ()()() where each value satisfies 0<=target[i]<=n_classes0 <= \\texttt{target[i]} <= \\texttt{n\\_classes}0<=target[i]<=n_classes\noutput1: (N)(N)(N) or ()()()\noutput2: Scalar\n\n\nInput: (N,in_features)(N, \\texttt{in\\_features})(N,in_features)\nOutput: (N,n_classes)(N, \\texttt{n\\_classes})(N,n_classes)\n\n\nInput: (N,in_features)(N, \\texttt{in\\_features})(N,in_features)\nOutput: (N)(N)(N)\n\n",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.AdaptiveMaxPool1d",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.AdaptiveMaxPool1d.html#torch.nn.AdaptiveMaxPool1d",
        "api_signature": "torch.nn.AdaptiveMaxPool1d(output_size, return_indices=False)",
        "api_description": "Applies a 1D adaptive max pooling over an input signal composed of several input planes.",
        "return_value": "",
        "parameters": "output_size (Union[int, Tuple[int]]) – the target output size LoutL_{out}Lout​.\nreturn_indices (bool) – if True, will return the indices along with the outputs.\nUseful to pass to nn.MaxUnpool1d. Default: False",
        "input_shape": "\nInput: (N,C,Lin)(N, C, L_{in})(N,C,Lin​) or (C,Lin)(C, L_{in})(C,Lin​).\nOutput: (N,C,Lout)(N, C, L_{out})(N,C,Lout​) or (C,Lout)(C, L_{out})(C,Lout​), where\nLout=output_sizeL_{out}=\\text{output\\_size}Lout​=output_size.\n\n",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.AdaptiveMaxPool2d",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.AdaptiveMaxPool2d.html#torch.nn.AdaptiveMaxPool2d",
        "api_signature": "torch.nn.AdaptiveMaxPool2d(output_size, return_indices=False)",
        "api_description": "Applies a 2D adaptive max pooling over an input signal composed of several input planes.",
        "return_value": "",
        "parameters": "output_size (Union[int, None, Tuple[Optional[int], Optional[int]]]) – the target output size of the image of the form Hout×WoutH_{out} \\times W_{out}Hout​×Wout​.\nCan be a tuple (Hout,Wout)(H_{out}, W_{out})(Hout​,Wout​) or a single HoutH_{out}Hout​ for a\nsquare image Hout×HoutH_{out} \\times H_{out}Hout​×Hout​. HoutH_{out}Hout​ and WoutW_{out}Wout​\ncan be either a int, or None which means the size will be the same as that\nof the input.\nreturn_indices (bool) – if True, will return the indices along with the outputs.\nUseful to pass to nn.MaxUnpool2d. Default: False",
        "input_shape": "\nInput: (N,C,Hin,Win)(N, C, H_{in}, W_{in})(N,C,Hin​,Win​) or (C,Hin,Win)(C, H_{in}, W_{in})(C,Hin​,Win​).\nOutput: (N,C,Hout,Wout)(N, C, H_{out}, W_{out})(N,C,Hout​,Wout​) or (C,Hout,Wout)(C, H_{out}, W_{out})(C,Hout​,Wout​), where\n(Hout,Wout)=output_size(H_{out}, W_{out})=\\text{output\\_size}(Hout​,Wout​)=output_size.\n\n",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.AdaptiveMaxPool3d",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.AdaptiveMaxPool3d.html#torch.nn.AdaptiveMaxPool3d",
        "api_signature": "torch.nn.AdaptiveMaxPool3d(output_size, return_indices=False)",
        "api_description": "Applies a 3D adaptive max pooling over an input signal composed of several input planes.",
        "return_value": "",
        "parameters": "output_size (Union[int, None, Tuple[Optional[int], Optional[int], Optional[int]]]) – the target output size of the image of the form Dout×Hout×WoutD_{out} \\times H_{out} \\times W_{out}Dout​×Hout​×Wout​.\nCan be a tuple (Dout,Hout,Wout)(D_{out}, H_{out}, W_{out})(Dout​,Hout​,Wout​) or a single\nDoutD_{out}Dout​ for a cube Dout×Dout×DoutD_{out} \\times D_{out} \\times D_{out}Dout​×Dout​×Dout​.\nDoutD_{out}Dout​, HoutH_{out}Hout​ and WoutW_{out}Wout​ can be either a\nint, or None which means the size will be the same as that of the input.\nreturn_indices (bool) – if True, will return the indices along with the outputs.\nUseful to pass to nn.MaxUnpool3d. Default: False",
        "input_shape": "\nInput: (N,C,Din,Hin,Win)(N, C, D_{in}, H_{in}, W_{in})(N,C,Din​,Hin​,Win​) or (C,Din,Hin,Win)(C, D_{in}, H_{in}, W_{in})(C,Din​,Hin​,Win​).\nOutput: (N,C,Dout,Hout,Wout)(N, C, D_{out}, H_{out}, W_{out})(N,C,Dout​,Hout​,Wout​) or (C,Dout,Hout,Wout)(C, D_{out}, H_{out}, W_{out})(C,Dout​,Hout​,Wout​),\nwhere (Dout,Hout,Wout)=output_size(D_{out}, H_{out}, W_{out})=\\text{output\\_size}(Dout​,Hout​,Wout​)=output_size.\n\n",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.add",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.add.html#torch.add",
        "api_signature": "torch.add(input, other, *, alpha=1, out=None)",
        "api_description": "Adds other, scaled by alpha, to input.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.\nother (Tensor or Number) – the tensor or number to add to input.\nalpha (Number) – the multiplier for other.\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.Store.add",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#torch.distributed.Store.add",
        "api_signature": "torch.distributed.Store.add(self: torch._C._distributed_c10d.Store, arg0: str, arg1: int)",
        "api_description": "The first call to add for a given key creates a counter associated\nwith key in the store, initialized to amount. Subsequent calls to add\nwith the same key increment the counter by the specified amount.\nCalling add() with a key that has already\nbeen set in the store by set() will result\nin an exception.",
        "return_value": "",
        "parameters": "key (str) – The key in the store whose counter will be incremented.\namount (int) – The quantity by which the counter will be incremented.",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> # Using TCPStore as an example, other store types can also be used\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n>>> store.add(\"first_key\", 1)\n>>> store.add(\"first_key\", 6)\n>>> # Should return 7\n>>> store.get(\"first_key\")\n\n\n"
    },
    {
        "api_name": "torch.ao.ns._numeric_suite.Shadow.add",
        "api_url": "https://pytorch.org/docs/stable/torch.ao.ns._numeric_suite.html#torch.ao.ns._numeric_suite.Shadow.add",
        "api_signature": "add(x, y)",
        "api_description": "Tensor",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.rendezvous.etcd_store.EtcdStore.add",
        "api_url": "https://pytorch.org/docs/stable/elastic/rendezvous.html#torch.distributed.elastic.rendezvous.etcd_store.EtcdStore.add",
        "api_signature": "add(key, num)",
        "api_description": "Atomically increment a value by an integer amount.",
        "return_value": "the new (incremented) value\n",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.symbolic_shapes.DimConstraints.add",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.fx.experimental.symbolic_shapes.DimConstraints.html#torch.fx.experimental.symbolic_shapes.DimConstraints.add",
        "api_signature": "add(expr)",
        "api_description": "Add an expression to the set of constraints.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.monitor.Stat.add",
        "api_url": "https://pytorch.org/docs/stable/monitor.html#torch.monitor.Stat.add",
        "api_signature": "add(self: torch._C._monitor.Stat, v: float)",
        "api_description": "Adds a value to the stat to be aggregated according to the\nconfigured stat type and aggregations.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.add",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.add.html#torch.Tensor.add",
        "api_signature": "Tensor.add(other, *, alpha=1)",
        "api_description": "Add a scalar or tensor to self tensor. If both alpha\nand other are specified, each element of other is scaled by\nalpha before being used.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.add_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.add_.html#torch.Tensor.add_",
        "api_signature": "Tensor.add_(other, *, alpha=1)",
        "api_description": "In-place version of add()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.tensorboard.writer.SummaryWriter.add_audio",
        "api_url": "https://pytorch.org/docs/stable/tensorboard.html#torch.utils.tensorboard.writer.SummaryWriter.add_audio",
        "api_signature": "add_audio(tag, snd_tensor, global_step=None, sample_rate=44100, walltime=None)",
        "api_description": "Add audio data to summary.",
        "return_value": "",
        "parameters": "tag (str) – Data identifier\nsnd_tensor (torch.Tensor) – Sound data\nglobal_step (int) – Global step value to record\nsample_rate (int) – sample rate in Hz\nwalltime (float) – Optional override default walltime (time.time())\nseconds after epoch of event",
        "input_shape": "snd_tensor: (1,L)(1, L)(1,L). The values should lie between [-1, 1].\n",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.tensorboard.writer.SummaryWriter.add_custom_scalars",
        "api_url": "https://pytorch.org/docs/stable/tensorboard.html#torch.utils.tensorboard.writer.SummaryWriter.add_custom_scalars",
        "api_signature": "add_custom_scalars(layout)",
        "api_description": "Create special chart by collecting charts tags in ‘scalars’.",
        "return_value": "",
        "parameters": "layout (dict) – {categoryName: charts}, where charts is also a dictionary\n{chartName: ListOfProperties}. The first element in ListOfProperties is the chart’s type\n(one of Multiline or Margin) and the second element should be a list containing the tags\nyou have used in add_scalar function, which will be collected into the new chart.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.package.PackageExporter.add_dependency",
        "api_url": "https://pytorch.org/docs/stable/package.html#torch.package.PackageExporter.add_dependency",
        "api_signature": "add_dependency(module_name, dependencies=True)",
        "api_description": "Given a module, add it to the dependency graph according to patterns\nspecified by the user.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.futures.Future.add_done_callback",
        "api_url": "https://pytorch.org/docs/stable/futures.html#torch.futures.Future.add_done_callback",
        "api_signature": "add_done_callback(callback)",
        "api_description": "Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed. The callback must take one argument, which is the\nreference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run inline.",
        "return_value": "",
        "parameters": "callback (Future) – a Callable that takes in one argument,\nwhich is the reference to this Future.",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> def callback(fut):\n...     print(\"This will run after the future has finished.\")\n...     print(fut.wait())\n>>> fut = torch.futures.Future()\n>>> fut.add_done_callback(callback)\n>>> fut.set_result(5)\nThis will run after the future has finished.\n5\n\n\n"
    },
    {
        "api_name": "torch.ao.quantization.backend_config.BackendPatternConfig.add_dtype_config",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.quantization.backend_config.BackendPatternConfig.html#torch.ao.quantization.backend_config.BackendPatternConfig.add_dtype_config",
        "api_signature": "add_dtype_config(dtype_config)",
        "api_description": "Add a set of supported data types passed as arguments to quantize ops in the\nreference model spec.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.tensorboard.writer.SummaryWriter.add_embedding",
        "api_url": "https://pytorch.org/docs/stable/tensorboard.html#torch.utils.tensorboard.writer.SummaryWriter.add_embedding",
        "api_signature": "add_embedding(mat, metadata=None, label_img=None, global_step=None, tag='default', metadata_header=None)",
        "api_description": "Add embedding projector data to summary.",
        "return_value": "",
        "parameters": "mat (torch.Tensor or numpy.ndarray) – A matrix which each row is the feature vector of the data point\nmetadata (list) – A list of labels, each element will be convert to string\nlabel_img (torch.Tensor) – Images correspond to each data point\nglobal_step (int) – Global step value to record\ntag (str) – Name for the embedding",
        "input_shape": "mat: (N,D)(N, D)(N,D), where N is number of data and D is feature dimension\nlabel_img: (N,C,H,W)(N, C, H, W)(N,C,H,W)\n",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.symbolic_shapes.DimConstraints.add_equality",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.fx.experimental.symbolic_shapes.DimConstraints.html#torch.fx.experimental.symbolic_shapes.DimConstraints.add_equality",
        "api_signature": "add_equality(source, expr)",
        "api_description": "Add an equality constraint",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.tensorboard.writer.SummaryWriter.add_figure",
        "api_url": "https://pytorch.org/docs/stable/tensorboard.html#torch.utils.tensorboard.writer.SummaryWriter.add_figure",
        "api_signature": "add_figure(tag, figure, global_step=None, close=True, walltime=None)",
        "api_description": "Render matplotlib figure into an image and add it to summary.",
        "return_value": "",
        "parameters": "tag (str) – Data identifier\nfigure (Union[Figure, List[Figure]]) – Figure or a list of figures\nglobal_step (Optional[int]) – Global step value to record\nclose (bool) – Flag to automatically close the figure\nwalltime (Optional[float]) – Optional override default walltime (time.time())\nseconds after epoch of event",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.tensorboard.writer.SummaryWriter.add_graph",
        "api_url": "https://pytorch.org/docs/stable/tensorboard.html#torch.utils.tensorboard.writer.SummaryWriter.add_graph",
        "api_signature": "add_graph(model, input_to_model=None, verbose=False, use_strict_trace=True)",
        "api_description": "Add graph data to summary.",
        "return_value": "",
        "parameters": "model (torch.nn.Module) – Model to draw.\ninput_to_model (torch.Tensor or list of torch.Tensor) – A variable or a tuple of\nvariables to be fed.\nverbose (bool) – Whether to print graph structure in console.\nuse_strict_trace (bool) – Whether to pass keyword argument strict to\ntorch.jit.trace. Pass False when you want the tracer to\nrecord your mutable container types (list, dict)",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.tensorboard.writer.SummaryWriter.add_histogram",
        "api_url": "https://pytorch.org/docs/stable/tensorboard.html#torch.utils.tensorboard.writer.SummaryWriter.add_histogram",
        "api_signature": "add_histogram(tag, values, global_step=None, bins='tensorflow', walltime=None, max_bins=None)",
        "api_description": "Add histogram to summary.",
        "return_value": "",
        "parameters": "tag (str) – Data identifier\nvalues (torch.Tensor, numpy.ndarray, or string/blobname) – Values to build histogram\nglobal_step (int) – Global step value to record\nbins (str) – One of {‘tensorflow’,’auto’, ‘fd’, …}. This determines how the bins are made. You can find\nother options in: https://docs.scipy.org/doc/numpy/reference/generated/numpy.histogram.html\nwalltime (float) – Optional override default walltime (time.time())\nseconds after epoch of event",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.tensorboard.writer.SummaryWriter.add_hparams",
        "api_url": "https://pytorch.org/docs/stable/tensorboard.html#torch.utils.tensorboard.writer.SummaryWriter.add_hparams",
        "api_signature": "add_hparams(hparam_dict, metric_dict, hparam_domain_discrete=None, run_name=None, global_step=None)",
        "api_description": "Add a set of hyperparameters to be compared in TensorBoard.",
        "return_value": "",
        "parameters": "hparam_dict (dict) – Each key-value pair in the dictionary is the\nname of the hyper parameter and it’s corresponding value.\nThe type of the value can be one of bool, string, float,\nint, or None.\nmetric_dict (dict) – Each key-value pair in the dictionary is the\nname of the metric and it’s corresponding value. Note that the key used\nhere should be unique in the tensorboard record. Otherwise the value\nyou added by add_scalar will be displayed in hparam plugin. In most\ncases, this is unwanted.\nhparam_domain_discrete – (Optional[Dict[str, List[Any]]]) A dictionary that\ncontains names of the hyperparameters and all discrete values they can hold\nrun_name (str) – Name of the run, to be included as part of the logdir.\nIf unspecified, will use current timestamp.\nglobal_step (int) – Global step value to record",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.tensorboard.writer.SummaryWriter.add_image",
        "api_url": "https://pytorch.org/docs/stable/tensorboard.html#torch.utils.tensorboard.writer.SummaryWriter.add_image",
        "api_signature": "add_image(tag, img_tensor, global_step=None, walltime=None, dataformats='CHW')",
        "api_description": "Add image data to summary.",
        "return_value": "",
        "parameters": "tag (str) – Data identifier\nimg_tensor (torch.Tensor, numpy.ndarray, or string/blobname) – Image data\nglobal_step (int) – Global step value to record\nwalltime (float) – Optional override default walltime (time.time())\nseconds after epoch of event\ndataformats (str) – Image data format specification of the form\nCHW, HWC, HW, WH, etc.",
        "input_shape": "img_tensor: Default is (3,H,W)(3, H, W)(3,H,W). You can use torchvision.utils.make_grid() to\nconvert a batch of tensor into 3xHxW format or call add_images and let us do the job.\nTensor with (1,H,W)(1, H, W)(1,H,W), (H,W)(H, W)(H,W), (H,W,3)(H, W, 3)(H,W,3) is also suitable as long as\ncorresponding dataformats argument is passed, e.g. CHW, HWC, HW.\n",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.tensorboard.writer.SummaryWriter.add_images",
        "api_url": "https://pytorch.org/docs/stable/tensorboard.html#torch.utils.tensorboard.writer.SummaryWriter.add_images",
        "api_signature": "add_images(tag, img_tensor, global_step=None, walltime=None, dataformats='NCHW')",
        "api_description": "Add batched image data to summary.",
        "return_value": "",
        "parameters": "tag (str) – Data identifier\nimg_tensor (torch.Tensor, numpy.ndarray, or string/blobname) – Image data\nglobal_step (int) – Global step value to record\nwalltime (float) – Optional override default walltime (time.time())\nseconds after epoch of event\ndataformats (str) – Image data format specification of the form\nNCHW, NHWC, CHW, HWC, HW, WH, etc.",
        "input_shape": "img_tensor: Default is (N,3,H,W)(N, 3, H, W)(N,3,H,W). If dataformats is specified, other shape will be\naccepted. e.g. NCHW or NHWC.\n",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.ns._numeric_suite_fx.add_loggers",
        "api_url": "https://pytorch.org/docs/stable/torch.ao.ns._numeric_suite_fx.html#torch.ao.ns._numeric_suite_fx.add_loggers",
        "api_signature": "torch.ao.ns._numeric_suite_fx.add_loggers(name_a, model_a, name_b, model_b, logger_cls, should_log_inputs=False, base_name_to_sets_of_related_ops=None, unmatchable_types_map=None)",
        "api_description": "Instrument model A and model B with loggers.",
        "return_value": "Returns a tuple of (model_a_with_loggers, model_b_with_loggers).  Modifies both models inplace.\n",
        "parameters": "name_a (str) – string name of model A to use in results\nmodel_a (Module) – model A\nname_b (str) – string name of model B to use in results\nmodel_b (Module) – model B\nlogger_cls (Callable) – class of Logger to use\nbase_name_to_sets_of_related_ops (Optional[Dict[str, Set[Union[Callable, str]]]]) – optional override of subgraph base nodes, subject to change\nunmatchable_types_map (Optional[Dict[str, Set[Union[Callable, str]]]]) – optional override of unmatchable types, subject to change",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.tensorboard.writer.SummaryWriter.add_mesh",
        "api_url": "https://pytorch.org/docs/stable/tensorboard.html#torch.utils.tensorboard.writer.SummaryWriter.add_mesh",
        "api_signature": "add_mesh(tag, vertices, colors=None, faces=None, config_dict=None, global_step=None, walltime=None)",
        "api_description": "Add meshes or 3D point clouds to TensorBoard.",
        "return_value": "",
        "parameters": "tag (str) – Data identifier\nvertices (torch.Tensor) – List of the 3D coordinates of vertices.\ncolors (torch.Tensor) – Colors for each vertex\nfaces (torch.Tensor) – Indices of vertices within each triangle. (Optional)\nconfig_dict – Dictionary with ThreeJS classes names and configuration.\nglobal_step (int) – Global step value to record\nwalltime (float) – Optional override default walltime (time.time())\nseconds after epoch of event",
        "input_shape": "vertices: (B,N,3)(B, N, 3)(B,N,3). (batch, number_of_vertices, channels)\ncolors: (B,N,3)(B, N, 3)(B,N,3). The values should lie in [0, 255] for type uint8 or [0, 1] for type float.\nfaces: (B,N,3)(B, N, 3)(B,N,3). The values should lie in [0, number_of_vertices] for type uint8.\n",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.profiler._KinetoProfile.add_metadata",
        "api_url": "https://pytorch.org/docs/stable/profiler.html#torch.profiler._KinetoProfile.add_metadata",
        "api_signature": "add_metadata(key, value)",
        "api_description": "Adds a user defined metadata with a string key and a string value\ninto the trace file",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.profiler._KinetoProfile.add_metadata_json",
        "api_url": "https://pytorch.org/docs/stable/profiler.html#torch.profiler._KinetoProfile.add_metadata_json",
        "api_signature": "add_metadata_json(key, value)",
        "api_description": "Adds a user defined metadata with a string key and a valid json value\ninto the trace file",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.jit.ScriptModule.add_module",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.jit.ScriptModule.html#torch.jit.ScriptModule.add_module",
        "api_signature": "add_module(name, module)",
        "api_description": "Add a child module to the current module.",
        "return_value": "",
        "parameters": "name (str) – name of the child module. The child module can be\naccessed from this module using the given name\nmodule (Module) – child module to be added to the module.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.Module.add_module",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.add_module",
        "api_signature": "add_module(name, module)",
        "api_description": "Add a child module to the current module.",
        "return_value": "",
        "parameters": "name (str) – name of the child module. The child module can be\naccessed from this module using the given name\nmodule (Module) – child module to be added to the module.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.optim.ZeroRedundancyOptimizer.add_param_group",
        "api_url": "https://pytorch.org/docs/stable/distributed.optim.html#torch.distributed.optim.ZeroRedundancyOptimizer.add_param_group",
        "api_signature": "add_param_group(param_group)",
        "api_description": "Add a parameter group to the Optimizer ‘s param_groups.",
        "return_value": "",
        "parameters": "param_group (dict) – specifies the parameters to be optimized and\ngroup-specific optimization options.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.Adadelta.add_param_group",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.Adadelta.html#torch.optim.Adadelta.add_param_group",
        "api_signature": "add_param_group(param_group)",
        "api_description": "Add a param group to the Optimizer s param_groups.",
        "return_value": "",
        "parameters": "param_group (dict) – Specifies what Tensors should be optimized along with group\nspecific optimization options.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.Adagrad.add_param_group",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.Adagrad.html#torch.optim.Adagrad.add_param_group",
        "api_signature": "add_param_group(param_group)",
        "api_description": "Add a param group to the Optimizer s param_groups.",
        "return_value": "",
        "parameters": "param_group (dict) – Specifies what Tensors should be optimized along with group\nspecific optimization options.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.Adam.add_param_group",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam.add_param_group",
        "api_signature": "add_param_group(param_group)",
        "api_description": "Add a param group to the Optimizer s param_groups.",
        "return_value": "",
        "parameters": "param_group (dict) – Specifies what Tensors should be optimized along with group\nspecific optimization options.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.Adamax.add_param_group",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.Adamax.html#torch.optim.Adamax.add_param_group",
        "api_signature": "add_param_group(param_group)",
        "api_description": "Add a param group to the Optimizer s param_groups.",
        "return_value": "",
        "parameters": "param_group (dict) – Specifies what Tensors should be optimized along with group\nspecific optimization options.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.AdamW.add_param_group",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html#torch.optim.AdamW.add_param_group",
        "api_signature": "add_param_group(param_group)",
        "api_description": "Add a param group to the Optimizer s param_groups.",
        "return_value": "",
        "parameters": "param_group (dict) – Specifies what Tensors should be optimized along with group\nspecific optimization options.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.ASGD.add_param_group",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.ASGD.html#torch.optim.ASGD.add_param_group",
        "api_signature": "add_param_group(param_group)",
        "api_description": "Add a param group to the Optimizer s param_groups.",
        "return_value": "",
        "parameters": "param_group (dict) – Specifies what Tensors should be optimized along with group\nspecific optimization options.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.LBFGS.add_param_group",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.LBFGS.html#torch.optim.LBFGS.add_param_group",
        "api_signature": "add_param_group(param_group)",
        "api_description": "Add a param group to the Optimizer s param_groups.",
        "return_value": "",
        "parameters": "param_group (dict) – Specifies what Tensors should be optimized along with group\nspecific optimization options.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.NAdam.add_param_group",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.NAdam.html#torch.optim.NAdam.add_param_group",
        "api_signature": "add_param_group(param_group)",
        "api_description": "Add a param group to the Optimizer s param_groups.",
        "return_value": "",
        "parameters": "param_group (dict) – Specifies what Tensors should be optimized along with group\nspecific optimization options.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.Optimizer.add_param_group",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.Optimizer.add_param_group.html#torch.optim.Optimizer.add_param_group",
        "api_signature": "Optimizer.add_param_group(param_group)",
        "api_description": "Add a param group to the Optimizer s param_groups.",
        "return_value": "",
        "parameters": "param_group (dict) – Specifies what Tensors should be optimized along with group\nspecific optimization options.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.RAdam.add_param_group",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.RAdam.html#torch.optim.RAdam.add_param_group",
        "api_signature": "add_param_group(param_group)",
        "api_description": "Add a param group to the Optimizer s param_groups.",
        "return_value": "",
        "parameters": "param_group (dict) – Specifies what Tensors should be optimized along with group\nspecific optimization options.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.RMSprop.add_param_group",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.RMSprop.html#torch.optim.RMSprop.add_param_group",
        "api_signature": "add_param_group(param_group)",
        "api_description": "Add a param group to the Optimizer s param_groups.",
        "return_value": "",
        "parameters": "param_group (dict) – Specifies what Tensors should be optimized along with group\nspecific optimization options.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.Rprop.add_param_group",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.Rprop.html#torch.optim.Rprop.add_param_group",
        "api_signature": "add_param_group(param_group)",
        "api_description": "Add a param group to the Optimizer s param_groups.",
        "return_value": "",
        "parameters": "param_group (dict) – Specifies what Tensors should be optimized along with group\nspecific optimization options.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.SGD.add_param_group",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.SGD.html#torch.optim.SGD.add_param_group",
        "api_signature": "add_param_group(param_group)",
        "api_description": "Add a param group to the Optimizer s param_groups.",
        "return_value": "",
        "parameters": "param_group (dict) – Specifies what Tensors should be optimized along with group\nspecific optimization options.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.SparseAdam.add_param_group",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.SparseAdam.html#torch.optim.SparseAdam.add_param_group",
        "api_signature": "add_param_group(param_group)",
        "api_description": "Add a param group to the Optimizer s param_groups.",
        "return_value": "",
        "parameters": "param_group (dict) – Specifies what Tensors should be optimized along with group\nspecific optimization options.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.tensorboard.writer.SummaryWriter.add_pr_curve",
        "api_url": "https://pytorch.org/docs/stable/tensorboard.html#torch.utils.tensorboard.writer.SummaryWriter.add_pr_curve",
        "api_signature": "add_pr_curve(tag, labels, predictions, global_step=None, num_thresholds=127, weights=None, walltime=None)",
        "api_description": "Add precision recall curve.",
        "return_value": "",
        "parameters": "tag (str) – Data identifier\nlabels (torch.Tensor, numpy.ndarray, or string/blobname) – Ground truth data. Binary label for each element.\npredictions (torch.Tensor, numpy.ndarray, or string/blobname) – The probability that an element be classified as true.\nValue should be in [0, 1]\nglobal_step (int) – Global step value to record\nnum_thresholds (int) – Number of thresholds used to draw the curve.\nwalltime (float) – Optional override default walltime (time.time())\nseconds after epoch of event",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.utils.prune.PruningContainer.add_pruning_method",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.PruningContainer.html#torch.nn.utils.prune.PruningContainer.add_pruning_method",
        "api_signature": "add_pruning_method(method)",
        "api_description": "Add a child pruning method to the container.",
        "return_value": "",
        "parameters": "method (subclass of BasePruningMethod) – child pruning method\nto be added to the container.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.add_quant_dequant",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.quantization.add_quant_dequant.html#torch.ao.quantization.add_quant_dequant",
        "api_signature": "torch.ao.quantization.add_quant_dequant(module)",
        "api_description": "Wrap the leaf child module in QuantWrapper if it has a valid qconfig\nNote that this function will modify the children of module inplace and it\ncan return a new module which wraps the input module as well.",
        "return_value": "Either the inplace modified module with submodules wrapped in\nQuantWrapper based on qconfig or a new QuantWrapper module which\nwraps the input module, the latter case only happens when the input\nmodule is a leaf module and we want to quantize it.\n",
        "parameters": "module – input module with qconfig attributes for all the leaf modules\nquantize (that we want to) –",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.ns._numeric_suite.Shadow.add_relu",
        "api_url": "https://pytorch.org/docs/stable/torch.ao.ns._numeric_suite.html#torch.ao.ns._numeric_suite.Shadow.add_relu",
        "api_signature": "add_relu(x, y)",
        "api_description": "Tensor",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.ns._numeric_suite.Shadow.add_scalar",
        "api_url": "https://pytorch.org/docs/stable/torch.ao.ns._numeric_suite.html#torch.ao.ns._numeric_suite.Shadow.add_scalar",
        "api_signature": "add_scalar(x, y)",
        "api_description": "Tensor",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.tensorboard.writer.SummaryWriter.add_scalar",
        "api_url": "https://pytorch.org/docs/stable/tensorboard.html#torch.utils.tensorboard.writer.SummaryWriter.add_scalar",
        "api_signature": "add_scalar(tag, scalar_value, global_step=None, walltime=None, new_style=False, double_precision=False)",
        "api_description": "Add scalar data to summary.",
        "return_value": "",
        "parameters": "tag (str) – Data identifier\nscalar_value (float or string/blobname) – Value to save\nglobal_step (int) – Global step value to record\nwalltime (float) – Optional override default walltime (time.time())\nwith seconds after epoch of event\nnew_style (boolean) – Whether to use new style (tensor field) or old\nstyle (simple_value field). New style could lead to faster data loading.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.tensorboard.writer.SummaryWriter.add_scalars",
        "api_url": "https://pytorch.org/docs/stable/tensorboard.html#torch.utils.tensorboard.writer.SummaryWriter.add_scalars",
        "api_signature": "add_scalars(main_tag, tag_scalar_dict, global_step=None, walltime=None)",
        "api_description": "Add many scalar data to summary.",
        "return_value": "",
        "parameters": "main_tag (str) – The parent name for the tags\ntag_scalar_dict (dict) – Key-value pair storing the tag and corresponding values\nglobal_step (int) – Global step value to record\nwalltime (float) – Optional override default walltime (time.time())\nseconds after epoch of event",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.ns._numeric_suite_fx.add_shadow_loggers",
        "api_url": "https://pytorch.org/docs/stable/torch.ao.ns._numeric_suite_fx.html#torch.ao.ns._numeric_suite_fx.add_shadow_loggers",
        "api_signature": "torch.ao.ns._numeric_suite_fx.add_shadow_loggers(name_a, model_a, name_b, model_b, logger_cls, should_log_inputs=False, base_name_to_sets_of_related_ops=None, node_type_to_io_type_map=None, unmatchable_types_map=None)",
        "api_description": "Instrument model A and model B with shadow loggers.",
        "return_value": "",
        "parameters": "name_a (str) – string name of model A to use in results\nmodel_a (Module) – model A\nname_b (str) – string name of model B to use in results\nmodel_b (Module) – model B\nlogger_cls (Callable) – class of Logger to use\nshould_log_inputs (bool) – whether to log inputs\nbase_name_to_sets_of_related_ops (Optional[Dict[str, Set[Union[Callable, str]]]]) – optional override of subgraph base nodes, subject to change\nunmatchable_types_map (Optional[Dict[str, Set[Union[Callable, str]]]]) – optional override of unmatchable types, subject to change",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.GraphModule.add_submodule",
        "api_url": "https://pytorch.org/docs/stable/fx.html#torch.fx.GraphModule.add_submodule",
        "api_signature": "add_submodule(target, m)",
        "api_description": "Adds the given submodule to self.",
        "return_value": "\nWhether or not the submodule could be inserted. Forthis method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute)\n\n\n\n",
        "parameters": "target (str) – The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.)\nm (Module) – The submodule itself; the actual object we want to\ninstall in the current Module",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.tensorboard.writer.SummaryWriter.add_text",
        "api_url": "https://pytorch.org/docs/stable/tensorboard.html#torch.utils.tensorboard.writer.SummaryWriter.add_text",
        "api_signature": "add_text(tag, text_string, global_step=None, walltime=None)",
        "api_description": "Add text data to summary.",
        "return_value": "",
        "parameters": "tag (str) – Data identifier\ntext_string (str) – String to save\nglobal_step (int) – Global step value to record\nwalltime (float) – Optional override default walltime (time.time())\nseconds after epoch of event",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.tensorboard.writer.SummaryWriter.add_video",
        "api_url": "https://pytorch.org/docs/stable/tensorboard.html#torch.utils.tensorboard.writer.SummaryWriter.add_video",
        "api_signature": "add_video(tag, vid_tensor, global_step=None, fps=4, walltime=None)",
        "api_description": "Add video data to summary.",
        "return_value": "",
        "parameters": "tag (str) – Data identifier\nvid_tensor (torch.Tensor) – Video data\nglobal_step (int) – Global step value to record\nfps (float or int) – Frames per second\nwalltime (float) – Optional override default walltime (time.time())\nseconds after epoch of event",
        "input_shape": "vid_tensor: (N,T,C,H,W)(N, T, C, H, W)(N,T,C,H,W). The values should lie in [0, 255] for type uint8 or [0, 1] for type float.\n",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.addbmm",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.addbmm.html#torch.addbmm",
        "api_signature": "torch.addbmm(input, batch1, batch2, *, beta=1, alpha=1, out=None)",
        "api_description": "Performs a batch matrix-matrix product of matrices stored\nin batch1 and batch2,\nwith a reduced add step (all matrix multiplications get accumulated\nalong the first dimension).\ninput is added to the final result.",
        "return_value": "",
        "parameters": "batch1 (Tensor) – the first batch of matrices to be multiplied\nbatch2 (Tensor) – the second batch of matrices to be multiplied\nbeta (Number, optional) – multiplier for input (β\\betaβ)\ninput (Tensor) – matrix to be added\nalpha (Number, optional) – multiplier for batch1 @ batch2 (α\\alphaα)\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.addbmm",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.addbmm.html#torch.Tensor.addbmm",
        "api_signature": "Tensor.addbmm(batch1, batch2, *, beta=1, alpha=1)",
        "api_description": "See torch.addbmm()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.addbmm_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.addbmm_.html#torch.Tensor.addbmm_",
        "api_signature": "Tensor.addbmm_(batch1, batch2, *, beta=1, alpha=1)",
        "api_description": "In-place version of addbmm()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.addcdiv",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.addcdiv.html#torch.addcdiv",
        "api_signature": "torch.addcdiv(input, tensor1, tensor2, *, value=1, out=None)",
        "api_description": "Performs the element-wise division of tensor1 by tensor2,\nmultiplies the result by the scalar value and adds it to input.",
        "return_value": "",
        "parameters": "input (Tensor) – the tensor to be added\ntensor1 (Tensor) – the numerator tensor\ntensor2 (Tensor) – the denominator tensor\nvalue (Number, optional) – multiplier for tensor1/tensor2\\text{tensor1} / \\text{tensor2}tensor1/tensor2\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.addcdiv",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.addcdiv.html#torch.Tensor.addcdiv",
        "api_signature": "Tensor.addcdiv(tensor1, tensor2, *, value=1)",
        "api_description": "See torch.addcdiv()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.addcdiv_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.addcdiv_.html#torch.Tensor.addcdiv_",
        "api_signature": "Tensor.addcdiv_(tensor1, tensor2, *, value=1)",
        "api_description": "In-place version of addcdiv()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.addcmul",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.addcmul.html#torch.addcmul",
        "api_signature": "torch.addcmul(input, tensor1, tensor2, *, value=1, out=None)",
        "api_description": "Performs the element-wise multiplication of tensor1\nby tensor2, multiplies the result by the scalar value\nand adds it to input.",
        "return_value": "",
        "parameters": "input (Tensor) – the tensor to be added\ntensor1 (Tensor) – the tensor to be multiplied\ntensor2 (Tensor) – the tensor to be multiplied\nvalue (Number, optional) – multiplier for tensor1.∗tensor2tensor1 .* tensor2tensor1.∗tensor2\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.addcmul",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.addcmul.html#torch.Tensor.addcmul",
        "api_signature": "Tensor.addcmul(tensor1, tensor2, *, value=1)",
        "api_description": "See torch.addcmul()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.addcmul_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.addcmul_.html#torch.Tensor.addcmul_",
        "api_signature": "Tensor.addcmul_(tensor1, tensor2, *, value=1)",
        "api_description": "In-place version of addcmul()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.addmm",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.addmm.html#torch.addmm",
        "api_signature": "torch.addmm(input, mat1, mat2, *, beta=1, alpha=1, out=None)",
        "api_description": "Performs a matrix multiplication of the matrices mat1 and mat2.\nThe matrix input is added to the final result.",
        "return_value": "",
        "parameters": "input (Tensor) – matrix to be added\nmat1 (Tensor) – the first matrix to be matrix multiplied\nmat2 (Tensor) – the second matrix to be matrix multiplied\nbeta (Number, optional) – multiplier for input (β\\betaβ)\nalpha (Number, optional) – multiplier for mat1@mat2mat1 @ mat2mat1@mat2 (α\\alphaα)\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.sparse.addmm",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.sparse.addmm.html#torch.sparse.addmm",
        "api_signature": "torch.sparse.addmm(mat, mat1, mat2, *, beta=1., alpha=1.)",
        "api_description": "This function does exact same thing as torch.addmm() in the forward,\nexcept that it supports backward for sparse COO matrix mat1.\nWhen mat1 is a COO tensor it must have sparse_dim = 2.\nWhen inputs are COO tensors, this function also supports backward for both inputs.",
        "return_value": "",
        "parameters": "mat (Tensor) – a dense matrix to be added\nmat1 (Tensor) – a sparse matrix to be multiplied\nmat2 (Tensor) – a dense matrix to be multiplied\nbeta (Number, optional) – multiplier for mat (β\\betaβ)\nalpha (Number, optional) – multiplier for mat1@mat2mat1 @ mat2mat1@mat2 (α\\alphaα)",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.addmm",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.addmm.html#torch.Tensor.addmm",
        "api_signature": "Tensor.addmm(mat1, mat2, *, beta=1, alpha=1)",
        "api_description": "See torch.addmm()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.addmm_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.addmm_.html#torch.Tensor.addmm_",
        "api_signature": "Tensor.addmm_(mat1, mat2, *, beta=1, alpha=1)",
        "api_description": "In-place version of addmm()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.addmv",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.addmv.html#torch.addmv",
        "api_signature": "torch.addmv(input, mat, vec, *, beta=1, alpha=1, out=None)",
        "api_description": "Performs a matrix-vector product of the matrix mat and\nthe vector vec.\nThe vector input is added to the final result.",
        "return_value": "",
        "parameters": "input (Tensor) – vector to be added\nmat (Tensor) – matrix to be matrix multiplied\nvec (Tensor) – vector to be matrix multiplied\nbeta (Number, optional) – multiplier for input (β\\betaβ)\nalpha (Number, optional) – multiplier for mat@vecmat @ vecmat@vec (α\\alphaα)\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.addmv",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.addmv.html#torch.Tensor.addmv",
        "api_signature": "Tensor.addmv(mat, vec, *, beta=1, alpha=1)",
        "api_description": "See torch.addmv()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.addmv_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.addmv_.html#torch.Tensor.addmv_",
        "api_signature": "Tensor.addmv_(mat, vec, *, beta=1, alpha=1)",
        "api_description": "In-place version of addmv()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.addr",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.addr.html#torch.addr",
        "api_signature": "torch.addr(input, vec1, vec2, *, beta=1, alpha=1, out=None)",
        "api_description": "Performs the outer-product of vectors vec1 and vec2\nand adds it to the matrix input.",
        "return_value": "",
        "parameters": "input (Tensor) – matrix to be added\nvec1 (Tensor) – the first vector of the outer product\nvec2 (Tensor) – the second vector of the outer product\nbeta (Number, optional) – multiplier for input (β\\betaβ)\nalpha (Number, optional) – multiplier for vec1⊗vec2\\text{vec1} \\otimes \\text{vec2}vec1⊗vec2 (α\\alphaα)\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.addr",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.addr.html#torch.Tensor.addr",
        "api_signature": "Tensor.addr(vec1, vec2, *, beta=1, alpha=1)",
        "api_description": "See torch.addr()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.addr_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.addr_.html#torch.Tensor.addr_",
        "api_signature": "Tensor.addr_(vec1, vec2, *, beta=1, alpha=1)",
        "api_description": "In-place version of addr()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.adjoint",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.adjoint.html#torch.adjoint",
        "api_signature": "torch.adjoint(Tensor)",
        "api_description": "Returns a view of the tensor conjugated and with the last two dimensions transposed.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> x = torch.arange(4, dtype=torch.float)\n>>> A = torch.complex(x, x).reshape(2, 2)\n>>> A\ntensor([[0.+0.j, 1.+1.j],\n        [2.+2.j, 3.+3.j]])\n>>> A.adjoint()\ntensor([[0.-0.j, 2.-2.j],\n        [1.-1.j, 3.-3.j]])\n>>> (A.adjoint() == A.mH).all()\ntensor(True)\n\n\n"
    },
    {
        "api_name": "torch.Tensor.adjoint",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.adjoint.html#torch.Tensor.adjoint",
        "api_signature": "Tensor.adjoint()",
        "api_description": "Alias for adjoint()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.functional.affine_grid",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.functional.affine_grid.html#torch.nn.functional.affine_grid",
        "api_signature": "torch.nn.functional.affine_grid(theta, size, align_corners=None)",
        "api_description": "Generate 2D or 3D flow field (sampling grid), given a batch of affine matrices theta.",
        "return_value": "output Tensor of size (N×H×W×2N \\times H \\times W \\times 2N×H×W×2)\n",
        "parameters": "theta (Tensor) – input batch of affine matrices with shape\n(N×2×3N \\times 2 \\times 3N×2×3) for 2D or\n(N×3×4N \\times 3 \\times 4N×3×4) for 3D\nsize (torch.Size) – the target output image size.\n(N×C×H×WN \\times C \\times H \\times WN×C×H×W for 2D or\nN×C×D×H×WN \\times C \\times D \\times H \\times WN×C×D×H×W for 3D)\nExample: torch.Size((32, 3, 24, 24))\nalign_corners (bool, optional) – if True, consider -1 and 1\nto refer to the centers of the corner pixels rather than the image corners.\nRefer to grid_sample() for a more complete description.\nA grid generated by affine_grid() should be passed to grid_sample()\nwith the same setting for this option.\nDefault: False",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.transforms.AffineTransform",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.transforms.AffineTransform",
        "api_signature": "torch.distributions.transforms.AffineTransform(loc, scale, event_dim=0, cache_size=0)",
        "api_description": "Transform via the pointwise affine mapping y=loc+scale×xy = \\text{loc} + \\text{scale} \\times xy=loc+scale×x.",
        "return_value": "",
        "parameters": "loc (Tensor or float) – Location parameter.\nscale (Tensor or float) – Scale parameter.\nevent_dim (int) – Optional size of event_shape. This should be zero\nfor univariate random variables, 1 for distributions over vectors,\n2 for distributions over matrices, etc.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.monitor.Aggregation",
        "api_url": "https://pytorch.org/docs/stable/monitor.html#torch.monitor.Aggregation",
        "api_signature": null,
        "api_description": "These are types of aggregations that can be used to accumulate stats.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.special.airy_ai",
        "api_url": "https://pytorch.org/docs/stable/special.html#torch.special.airy_ai",
        "api_signature": "torch.special.airy_ai(input, *, out=None)",
        "api_description": "Airy function Ai(input)\\text{Ai}\\left(\\text{input}\\right)Ai(input).",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.align_as",
        "api_url": "https://pytorch.org/docs/stable/named_tensor.html#torch.Tensor.align_as",
        "api_signature": "align_as(other)",
        "api_description": "Permutes the dimensions of the self tensor to match the dimension order\nin the other tensor, adding size-one dims for any new names.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.align_to",
        "api_url": "https://pytorch.org/docs/stable/named_tensor.html#torch.Tensor.align_to",
        "api_signature": "align_to(*names)",
        "api_description": "Permutes the dimensions of the self tensor to match the order\nspecified in names, adding size-one dims for any new names.",
        "return_value": "",
        "parameters": "names (iterable of str) – The desired dimension ordering of the\noutput tensor. May contain up to one Ellipsis that is expanded\nto all unmentioned dim names of self.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.all",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.all.html#torch.all",
        "api_signature": "torch.all(input)",
        "api_description": "Tests if all elements in input evaluate to True.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.\ndim (int or tuple of ints) – the dimension or dimensions to reduce.\nkeepdim (bool) – whether the output tensor has dim retained or not.\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.all",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.all.html#torch.Tensor.all",
        "api_signature": "Tensor.all(dim=None, keepdim=False)",
        "api_description": "See torch.all()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.all_gather",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#torch.distributed.all_gather",
        "api_signature": "torch.distributed.all_gather(tensor_list, tensor, group=None, async_op=False)",
        "api_description": "Gathers tensors from the whole group in a list.",
        "return_value": "Async work handle, if async_op is set to True.\nNone, if not async_op or if not part of the group\n",
        "parameters": "tensor_list (list[Tensor]) – Output list. It should contain\ncorrectly-sized tensors to be used for output of the collective.\ntensor (Tensor) – Tensor to be broadcast from current process.\ngroup (ProcessGroup, optional) – The process group to work on. If None,\nthe default process group will be used.\nasync_op (bool, optional) – Whether this op should be an async op",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.all_gather_into_tensor",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#torch.distributed.all_gather_into_tensor",
        "api_signature": "torch.distributed.all_gather_into_tensor(output_tensor, input_tensor, group=None, async_op=False)",
        "api_description": "Gather tensors from all ranks and put them in a single output tensor.",
        "return_value": "Async work handle, if async_op is set to True.\nNone, if not async_op or if not part of the group\n",
        "parameters": "output_tensor (Tensor) – Output tensor to accommodate tensor elements\nfrom all ranks. It must be correctly sized to have one of the\nfollowing forms:\n(i) a concatenation of all the input tensors along the primary\ndimension; for definition of “concatenation”, see torch.cat();\n(ii) a stack of all the input tensors along the primary dimension;\nfor definition of “stack”, see torch.stack().\nExamples below may better explain the supported output forms.\ninput_tensor (Tensor) – Tensor to be gathered from current rank.\nDifferent from the all_gather API, the input tensors in this\nAPI must have the same size across all ranks.\ngroup (ProcessGroup, optional) – The process group to work on. If None,\nthe default process group will be used.\nasync_op (bool, optional) – Whether this op should be an async op",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.all_gather_object",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#torch.distributed.all_gather_object",
        "api_signature": "torch.distributed.all_gather_object(object_list, obj, group=None)",
        "api_description": "Gathers picklable objects from the whole group into a list.",
        "return_value": "None. If the calling rank is part of this group, the output of the\ncollective will be populated into the input object_list. If the\ncalling rank is not part of the group, the passed in object_list will\nbe unmodified.\n",
        "parameters": "object_list (list[Any]) – Output list. It should be correctly sized as the\nsize of the group for this collective and will contain the output.\nobj (Any) – Pickable Python object to be broadcast from current process.\ngroup (ProcessGroup, optional) – The process group to work on. If None,\nthe default process group will be used. Default is None.",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> # Note: Process group initialization omitted on each rank.\n>>> import torch.distributed as dist\n>>> # Assumes world_size of 3.\n>>> gather_objects = [\"foo\", 12, {1: 2}] # any picklable object\n>>> output = [None for _ in gather_objects]\n>>> dist.all_gather_object(output, gather_objects[dist.get_rank()])\n>>> output\n['foo', 12, {1: 2}]\n\n\n"
    },
    {
        "api_name": "torch.fx.Node.all_input_nodes",
        "api_url": "https://pytorch.org/docs/stable/fx.html#torch.fx.Node.all_input_nodes",
        "api_signature": null,
        "api_description": "Return all Nodes that are inputs to this Node. This is equivalent to\niterating over args and kwargs and only collecting the values that\nare Nodes.",
        "return_value": "List of Nodes that appear in the args and kwargs of this\nNode, in that order.\n",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.onnx.verification.GraphInfo.all_mismatch_leaf_graph_info",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.onnx.verification.GraphInfo.html#torch.onnx.verification.GraphInfo.all_mismatch_leaf_graph_info",
        "api_signature": "all_mismatch_leaf_graph_info()",
        "api_description": "Return a list of all leaf GraphInfo objects that have mismatch.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.package.PackageExporter.all_paths",
        "api_url": "https://pytorch.org/docs/stable/package.html#torch.package.PackageExporter.all_paths",
        "api_signature": "all_paths(src, dst)",
        "api_description": "that has all paths from src to dst.",
        "return_value": "A dot representation containing all paths from src to dst.\n(https://graphviz.org/doc/info/lang.html)\n",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.all_reduce",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#torch.distributed.all_reduce",
        "api_signature": "torch.distributed.all_reduce(tensor, op=<RedOpType.SUM: 0>, group=None, async_op=False)",
        "api_description": "Reduces the tensor data across all machines in a way that all get the final result.",
        "return_value": "Async work handle, if async_op is set to True.\nNone, if not async_op or if not part of the group\n",
        "parameters": "tensor (Tensor) – Input and output of the collective. The function\noperates in-place.\nop (optional) – One of the values from\ntorch.distributed.ReduceOp\nenum.  Specifies an operation used for element-wise reductions.\ngroup (ProcessGroup, optional) – The process group to work on. If None,\nthe default process group will be used.\nasync_op (bool, optional) – Whether this op should be an async op",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.all_to_all",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#torch.distributed.all_to_all",
        "api_signature": "torch.distributed.all_to_all(output_tensor_list, input_tensor_list, group=None, async_op=False)",
        "api_description": "Scatters list of input tensors to all processes in a group and return gathered list of tensors in output list.",
        "return_value": "Async work handle, if async_op is set to True.\nNone, if not async_op or if not part of the group.\n",
        "parameters": "output_tensor_list (list[Tensor]) – List of tensors to be gathered one\nper rank.\ninput_tensor_list (list[Tensor]) – List of tensors to scatter one per rank.\ngroup (ProcessGroup, optional) – The process group to work on. If None,\nthe default process group will be used.\nasync_op (bool, optional) – Whether this op should be an async op.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.all_to_all_single",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#torch.distributed.all_to_all_single",
        "api_signature": "torch.distributed.all_to_all_single(output, input, output_split_sizes=None, input_split_sizes=None, group=None, async_op=False)",
        "api_description": "Split input tensor and then scatter the split list to all processes in a group.",
        "return_value": "Async work handle, if async_op is set to True.\nNone, if not async_op or if not part of the group.\n",
        "parameters": "output (Tensor) – Gathered concatenated output tensor.\ninput (Tensor) – Input tensor to scatter.\noutput_split_sizes – (list[Int], optional): Output split sizes for dim 0\nif specified None or empty, dim 0 of output tensor must divide\nequally by world_size.\ninput_split_sizes – (list[Int], optional): Input split sizes for dim 0\nif specified None or empty, dim 0 of input tensor must divide\nequally by world_size.\ngroup (ProcessGroup, optional) – The process group to work on. If None,\nthe default process group will be used.\nasync_op (bool, optional) – Whether this op should be an async op.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.allclose",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.allclose.html#torch.allclose",
        "api_signature": "torch.allclose(input, other, rtol=1e-05, atol=1e-08, equal_nan=False)",
        "api_description": "This function checks if input and other satisfy the condition:",
        "return_value": "",
        "parameters": "input (Tensor) – first tensor to compare\nother (Tensor) – second tensor to compare\natol (float, optional) – absolute tolerance. Default: 1e-08\nrtol (float, optional) – relative tolerance. Default: 1e-05\nequal_nan (bool, optional) – if True, then two NaN s will be considered equal. Default: False",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.allclose",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.allclose.html#torch.Tensor.allclose",
        "api_signature": "Tensor.allclose(other, rtol=1e-05, atol=1e-08, equal_nan=False)",
        "api_description": "See torch.allclose()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.backends.cuda.matmul.allow_bf16_reduced_precision_reduction",
        "api_url": "https://pytorch.org/docs/stable/backends.html#torch.backends.cuda.matmul.allow_bf16_reduced_precision_reduction",
        "api_signature": null,
        "api_description": "A bool that controls whether reduced precision reductions are allowed with bf16 GEMMs.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction",
        "api_url": "https://pytorch.org/docs/stable/backends.html#torch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction",
        "api_signature": null,
        "api_description": "A bool that controls whether reduced precision reductions (e.g., with fp16 accumulation type) are allowed with fp16 GEMMs.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.compiler.allow_in_graph",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.compiler.allow_in_graph.html#torch.compiler.allow_in_graph",
        "api_signature": "torch.compiler.allow_in_graph(fn)",
        "api_description": "Customize which functions compilation will include in the generated graph.\nIt bypasses all introspection of the symbolic python code in favor of\ndirectly writing it to the graph.\nIf fn is a list or tuple of callables it recursively applies allow_in_graph()\nto each function and returns a new list or tuple containing the modified functions",
        "return_value": "",
        "parameters": "fn – A callable representing the function to be included in the graph.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.autograd.graph.allow_mutation_on_saved_tensors",
        "api_url": "https://pytorch.org/docs/stable/autograd.html#torch.autograd.graph.allow_mutation_on_saved_tensors",
        "api_signature": null,
        "api_description": "Context manager under which mutating tensors saved for backward is allowed.",
        "return_value": "An _AllowMutationOnSavedContext object storing the state managed by this\ncontext manager. This object can be useful for debugging purposes. The state\nmanaged by the context manager is automatically cleared upon exiting.\n",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.backends.cuda.matmul.allow_tf32",
        "api_url": "https://pytorch.org/docs/stable/backends.html#torch.backends.cuda.matmul.allow_tf32",
        "api_signature": null,
        "api_description": "A bool that controls whether TensorFloat-32 tensor cores may be used in matrix\nmultiplications on Ampere or newer GPUs. See TensorFloat-32 (TF32) on Ampere (and later) devices.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.backends.cudnn.allow_tf32",
        "api_url": "https://pytorch.org/docs/stable/backends.html#torch.backends.cudnn.allow_tf32",
        "api_signature": null,
        "api_description": "A bool that controls where TensorFloat-32 tensor cores may be used in cuDNN\nconvolutions on Ampere or newer GPUs. See TensorFloat-32 (TF32) on Ampere (and later) devices.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.algorithms.ddp_comm_hooks.default_hooks.allreduce_hook",
        "api_url": "https://pytorch.org/docs/stable/ddp_comm_hooks.html#torch.distributed.algorithms.ddp_comm_hooks.default_hooks.allreduce_hook",
        "api_signature": "torch.distributed.algorithms.ddp_comm_hooks.default_hooks.allreduce_hook(process_group, bucket)",
        "api_description": "Call allreduce using GradBucket tensors.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> ddp_model.register_comm_hook(process_group, allreduce_hook)\n\n\n"
    },
    {
        "api_name": "torch.nn.functional.alpha_dropout",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.functional.alpha_dropout.html#torch.nn.functional.alpha_dropout",
        "api_signature": "torch.nn.functional.alpha_dropout(input, p=0.5, training=False, inplace=False)",
        "api_description": "Apply alpha dropout to the input.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.AlphaDropout",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.AlphaDropout.html#torch.nn.AlphaDropout",
        "api_signature": "torch.nn.AlphaDropout(p=0.5, inplace=False)",
        "api_description": "Applies Alpha Dropout over the input.",
        "return_value": "",
        "parameters": "p (float) – probability of an element to be dropped. Default: 0.5\ninplace (bool, optional) – If set to True, will do this operation\nin-place",
        "input_shape": "\nInput: (∗)(*)(∗). Input can be of any shape\nOutput: (∗)(*)(∗). Output is of the same shape as input\n\n",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.amax",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.amax.html#torch.amax",
        "api_signature": "torch.amax(input, dim, keepdim=False, *, out=None)",
        "api_description": "Returns the maximum value of each slice of the input tensor in the given\ndimension(s) dim.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.\ndim (int or tuple of ints) – the dimension or dimensions to reduce.\nkeepdim (bool) – whether the output tensor has dim retained or not.\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.amax",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.amax.html#torch.Tensor.amax",
        "api_signature": "Tensor.amax(dim=None, keepdim=False)",
        "api_description": "See torch.amax()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.amin",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.amin.html#torch.amin",
        "api_signature": "torch.amin(input, dim, keepdim=False, *, out=None)",
        "api_description": "Returns the minimum value of each slice of the input tensor in the given\ndimension(s) dim.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.\ndim (int or tuple of ints) – the dimension or dimensions to reduce.\nkeepdim (bool) – whether the output tensor has dim retained or not.\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.amin",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.amin.html#torch.Tensor.amin",
        "api_signature": "Tensor.amin(dim=None, keepdim=False)",
        "api_description": "See torch.amin()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.aminmax",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.aminmax.html#torch.aminmax",
        "api_signature": "torch.aminmax(input, *, dim=None, keepdim=False, out=None)",
        "api_description": "Computes the minimum and maximum values of the input tensor.",
        "return_value": "A named tuple (min, max) containing the minimum and maximum values.\n",
        "parameters": "input (Tensor) – The input tensor\ndim (Optional[int]) – The dimension along which to compute the values. If None,\ncomputes the values over the entire input tensor.\nDefault is None.\nkeepdim (bool) – If True, the reduced dimensions will be kept in the output\ntensor as dimensions with size 1 for broadcasting, otherwise\nthey will be removed, as if calling (torch.squeeze()).\nDefault is False.\nout (Optional[Tuple[Tensor, Tensor]]) – Optional tensors on which to write the result. Must have the same\nshape and dtype as the expected output.\nDefault is None.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.aminmax",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.aminmax.html#torch.Tensor.aminmax",
        "api_signature": "Tensor.aminmax(*, dim=None, keepdim=False)",
        "api_description": "See torch.aminmax()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.angle",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.angle.html#torch.angle",
        "api_signature": "torch.angle(input, *, out=None)",
        "api_description": "Computes the element-wise angle (in radians) of the given input tensor.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.angle",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.angle.html#torch.Tensor.angle",
        "api_signature": "Tensor.angle()",
        "api_description": "See torch.angle()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.jit.annotate",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.jit.annotate.html#torch.jit.annotate",
        "api_signature": "torch.jit.annotate(the_type, the_value)",
        "api_description": "Use to give type of the_value in TorchScript compiler.",
        "return_value": "the_value is passed back as return value.\n",
        "parameters": "the_type – Python type that should be passed to TorchScript compiler as type hint for the_value\nthe_value – Value or expression to hint type for.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.any",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.any.html#torch.any",
        "api_signature": "torch.any(input)",
        "api_description": "Tests if any element in input evaluates to True.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.\ndim (int or tuple of ints) – the dimension or dimensions to reduce.\nkeepdim (bool) – whether the output tensor has dim retained or not.\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.any",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.any.html#torch.Tensor.any",
        "api_signature": "Tensor.any(dim=None, keepdim=False)",
        "api_description": "See torch.any()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.Node.append",
        "api_url": "https://pytorch.org/docs/stable/fx.html#torch.fx.Node.append",
        "api_signature": "append(x)",
        "api_description": "Insert x after this node in the list of nodes in the graph.\nEquivalent to self.next.prepend(x)",
        "return_value": "",
        "parameters": "x (Node) – The node to put after this node. Must be a member of the same graph.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.ModuleList.append",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.ModuleList.html#torch.nn.ModuleList.append",
        "api_signature": "append(module)",
        "api_description": "Append a given module to the end of the list.",
        "return_value": "",
        "parameters": "module (nn.Module) – module to append",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.ParameterList.append",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.ParameterList.html#torch.nn.ParameterList.append",
        "api_signature": "append(value)",
        "api_description": "Append a given value at the end of the list.",
        "return_value": "",
        "parameters": "value (Any) – value to append",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.Sequential.append",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html#torch.nn.Sequential.append",
        "api_signature": "append(module)",
        "api_description": "Append a given module to the end.",
        "return_value": "",
        "parameters": "module (nn.Module) – module to append",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.autograd.function.BackwardCFunction.apply",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.autograd.function.BackwardCFunction.html#torch.autograd.function.BackwardCFunction.apply",
        "api_signature": "apply(*args)",
        "api_description": "Apply method used when executing this Node during the backward",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.fsdp.FullyShardedDataParallel.apply",
        "api_url": "https://pytorch.org/docs/stable/fsdp.html#torch.distributed.fsdp.FullyShardedDataParallel.apply",
        "api_signature": "apply(fn)",
        "api_description": "Apply fn recursively to every submodule (as returned by .children()) as well as self.",
        "return_value": "self\n",
        "parameters": "fn (Module -> None) – function to be applied to each submodule",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.jit.ScriptModule.apply",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.jit.ScriptModule.html#torch.jit.ScriptModule.apply",
        "api_signature": "apply(fn)",
        "api_description": "Apply fn recursively to every submodule (as returned by .children()) as well as self.",
        "return_value": "self\n",
        "parameters": "fn (Module -> None) – function to be applied to each submodule",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.Module.apply",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.apply",
        "api_signature": "apply(fn)",
        "api_description": "Apply fn recursively to every submodule (as returned by .children()) as well as self.",
        "return_value": "self\n",
        "parameters": "fn (Module -> None) – function to be applied to each submodule",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.utils.prune.BasePruningMethod.apply",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.BasePruningMethod.html#torch.nn.utils.prune.BasePruningMethod.apply",
        "api_signature": "apply(module, name, *args, importance_scores=None, **kwargs)",
        "api_description": "Add pruning on the fly and reparametrization of a tensor.",
        "return_value": "",
        "parameters": "module (nn.Module) – module containing the tensor to prune\nname (str) – parameter name within module on which pruning\nwill act.\nargs – arguments passed on to a subclass of\nBasePruningMethod\nimportance_scores (torch.Tensor) – tensor of importance scores (of\nsame shape as module parameter) used to compute mask for pruning.\nThe values in this tensor indicate the importance of the\ncorresponding elements in the parameter being pruned.\nIf unspecified or None, the parameter will be used in its place.\nkwargs – keyword arguments passed on to a subclass of a\nBasePruningMethod",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.utils.prune.CustomFromMask.apply",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.CustomFromMask.html#torch.nn.utils.prune.CustomFromMask.apply",
        "api_signature": "apply(module, name, mask)",
        "api_description": "Add pruning on the fly and reparametrization of a tensor.",
        "return_value": "",
        "parameters": "module (nn.Module) – module containing the tensor to prune\nname (str) – parameter name within module on which pruning\nwill act.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.utils.prune.Identity.apply",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.Identity.html#torch.nn.utils.prune.Identity.apply",
        "api_signature": "apply(module, name)",
        "api_description": "Add pruning on the fly and reparametrization of a tensor.",
        "return_value": "",
        "parameters": "module (nn.Module) – module containing the tensor to prune\nname (str) – parameter name within module on which pruning\nwill act.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.utils.prune.L1Unstructured.apply",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.L1Unstructured.html#torch.nn.utils.prune.L1Unstructured.apply",
        "api_signature": "apply(module, name, amount, importance_scores=None)",
        "api_description": "Add pruning on the fly and reparametrization of a tensor.",
        "return_value": "",
        "parameters": "module (nn.Module) – module containing the tensor to prune\nname (str) – parameter name within module on which pruning\nwill act.\namount (int or float) – quantity of parameters to prune.\nIf float, should be between 0.0 and 1.0 and represent the\nfraction of parameters to prune. If int, it represents the\nabsolute number of parameters to prune.\nimportance_scores (torch.Tensor) – tensor of importance scores (of same\nshape as module parameter) used to compute mask for pruning.\nThe values in this tensor indicate the importance of the corresponding\nelements in the parameter being pruned.\nIf unspecified or None, the module parameter will be used in its place.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.utils.prune.LnStructured.apply",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.LnStructured.html#torch.nn.utils.prune.LnStructured.apply",
        "api_signature": "apply(module, name, amount, n, dim, importance_scores=None)",
        "api_description": "Add pruning on the fly and reparametrization of a tensor.",
        "return_value": "",
        "parameters": "module (nn.Module) – module containing the tensor to prune\nname (str) – parameter name within module on which pruning\nwill act.\namount (int or float) – quantity of parameters to prune.\nIf float, should be between 0.0 and 1.0 and represent the\nfraction of parameters to prune. If int, it represents the\nabsolute number of parameters to prune.\nn (int, float, inf, -inf, 'fro', 'nuc') – See documentation of valid\nentries for argument p in torch.norm().\ndim (int) – index of the dim along which we define channels to\nprune.\nimportance_scores (torch.Tensor) – tensor of importance scores (of same\nshape as module parameter) used to compute mask for pruning.\nThe values in this tensor indicate the importance of the corresponding\nelements in the parameter being pruned.\nIf unspecified or None, the module parameter will be used in its place.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.utils.prune.PruningContainer.apply",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.PruningContainer.html#torch.nn.utils.prune.PruningContainer.apply",
        "api_signature": "apply(module, name, *args, importance_scores=None, **kwargs)",
        "api_description": "Add pruning on the fly and reparametrization of a tensor.",
        "return_value": "",
        "parameters": "module (nn.Module) – module containing the tensor to prune\nname (str) – parameter name within module on which pruning\nwill act.\nargs – arguments passed on to a subclass of\nBasePruningMethod\nimportance_scores (torch.Tensor) – tensor of importance scores (of\nsame shape as module parameter) used to compute mask for pruning.\nThe values in this tensor indicate the importance of the\ncorresponding elements in the parameter being pruned.\nIf unspecified or None, the parameter will be used in its place.\nkwargs – keyword arguments passed on to a subclass of a\nBasePruningMethod",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.utils.prune.RandomStructured.apply",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.RandomStructured.html#torch.nn.utils.prune.RandomStructured.apply",
        "api_signature": "apply(module, name, amount, dim=-1)",
        "api_description": "Add pruning on the fly and reparametrization of a tensor.",
        "return_value": "",
        "parameters": "module (nn.Module) – module containing the tensor to prune\nname (str) – parameter name within module on which pruning\nwill act.\namount (int or float) – quantity of parameters to prune.\nIf float, should be between 0.0 and 1.0 and represent the\nfraction of parameters to prune. If int, it represents the\nabsolute number of parameters to prune.\ndim (int, optional) – index of the dim along which we define\nchannels to prune. Default: -1.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.utils.prune.RandomUnstructured.apply",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.RandomUnstructured.html#torch.nn.utils.prune.RandomUnstructured.apply",
        "api_signature": "apply(module, name, amount)",
        "api_description": "Add pruning on the fly and reparametrization of a tensor.",
        "return_value": "",
        "parameters": "module (nn.Module) – module containing the tensor to prune\nname (str) – parameter name within module on which pruning\nwill act.\namount (int or float) – quantity of parameters to prune.\nIf float, should be between 0.0 and 1.0 and represent the\nfraction of parameters to prune. If int, it represents the\nabsolute number of parameters to prune.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.apply_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.apply_.html#torch.Tensor.apply_",
        "api_signature": "Tensor.apply_(callable)",
        "api_description": "Applies the function callable to each element in the tensor, replacing\neach element with the value returned by callable.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.autograd.function.BackwardCFunction.apply_jvp",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.autograd.function.BackwardCFunction.html#torch.autograd.function.BackwardCFunction.apply_jvp",
        "api_signature": "apply_jvp(*args)",
        "api_description": "Apply method used when executing forward mode AD during the forward",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.utils.prune.BasePruningMethod.apply_mask",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.BasePruningMethod.html#torch.nn.utils.prune.BasePruningMethod.apply_mask",
        "api_signature": "apply_mask(module)",
        "api_description": "Simply handles the multiplication between the parameter being pruned and the generated mask.",
        "return_value": "pruned version of the input tensor\n",
        "parameters": "module (nn.Module) – module containing the tensor to prune",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.utils.prune.CustomFromMask.apply_mask",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.CustomFromMask.html#torch.nn.utils.prune.CustomFromMask.apply_mask",
        "api_signature": "apply_mask(module)",
        "api_description": "Simply handles the multiplication between the parameter being pruned and the generated mask.",
        "return_value": "pruned version of the input tensor\n",
        "parameters": "module (nn.Module) – module containing the tensor to prune",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.utils.prune.Identity.apply_mask",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.Identity.html#torch.nn.utils.prune.Identity.apply_mask",
        "api_signature": "apply_mask(module)",
        "api_description": "Simply handles the multiplication between the parameter being pruned and the generated mask.",
        "return_value": "pruned version of the input tensor\n",
        "parameters": "module (nn.Module) – module containing the tensor to prune",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.utils.prune.L1Unstructured.apply_mask",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.L1Unstructured.html#torch.nn.utils.prune.L1Unstructured.apply_mask",
        "api_signature": "apply_mask(module)",
        "api_description": "Simply handles the multiplication between the parameter being pruned and the generated mask.",
        "return_value": "pruned version of the input tensor\n",
        "parameters": "module (nn.Module) – module containing the tensor to prune",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.utils.prune.LnStructured.apply_mask",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.LnStructured.html#torch.nn.utils.prune.LnStructured.apply_mask",
        "api_signature": "apply_mask(module)",
        "api_description": "Simply handles the multiplication between the parameter being pruned and the generated mask.",
        "return_value": "pruned version of the input tensor\n",
        "parameters": "module (nn.Module) – module containing the tensor to prune",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.utils.prune.PruningContainer.apply_mask",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.PruningContainer.html#torch.nn.utils.prune.PruningContainer.apply_mask",
        "api_signature": "apply_mask(module)",
        "api_description": "Simply handles the multiplication between the parameter being pruned and the generated mask.",
        "return_value": "pruned version of the input tensor\n",
        "parameters": "module (nn.Module) – module containing the tensor to prune",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.utils.prune.RandomStructured.apply_mask",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.RandomStructured.html#torch.nn.utils.prune.RandomStructured.apply_mask",
        "api_signature": "apply_mask(module)",
        "api_description": "Simply handles the multiplication between the parameter being pruned and the generated mask.",
        "return_value": "pruned version of the input tensor\n",
        "parameters": "module (nn.Module) – module containing the tensor to prune",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.utils.prune.RandomUnstructured.apply_mask",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.RandomUnstructured.html#torch.nn.utils.prune.RandomUnstructured.apply_mask",
        "api_signature": "apply_mask(module)",
        "api_description": "Simply handles the multiplication between the parameter being pruned and the generated mask.",
        "return_value": "pruned version of the input tensor\n",
        "parameters": "module (nn.Module) – module containing the tensor to prune",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.arange",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.arange.html#torch.arange",
        "api_signature": "torch.arange(start=0, end, step=1, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False)",
        "api_description": "Returns a 1-D tensor of size ⌈end−startstep⌉\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil⌈stepend−start​⌉\nwith values from the interval [start, end) taken with common difference\nstep beginning from start.",
        "return_value": "",
        "parameters": "start (Number) – the starting value for the set of points. Default: 0.\nend (Number) – the ending value for the set of points\nstep (Number) – the gap between each pair of adjacent points. Default: 1.\nout (Tensor, optional) – the output tensor.\ndtype (torch.dtype, optional) – the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_dtype()). If dtype is not given, infer the data type from the other input\narguments. If any of start, end, or stop are floating-point, the\ndtype is inferred to be the default dtype, see\nget_default_dtype(). Otherwise, the dtype is inferred to\nbe torch.int64.\nlayout (torch.layout, optional) – the desired layout of returned Tensor.\nDefault: torch.strided.\ndevice (torch.device, optional) – the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_device()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types.\nrequires_grad (bool, optional) – If autograd should record operations on the\nreturned tensor. Default: False.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.arccos",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.arccos.html#torch.arccos",
        "api_signature": "torch.arccos(input, *, out=None)",
        "api_description": "Alias for torch.acos().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.arccos",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.arccos.html#torch.Tensor.arccos",
        "api_signature": "Tensor.arccos()",
        "api_description": "See torch.arccos()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.arccos_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.arccos_.html#torch.Tensor.arccos_",
        "api_signature": "Tensor.arccos_()",
        "api_description": "In-place version of arccos()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.arccosh",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.arccosh.html#torch.arccosh",
        "api_signature": "torch.arccosh(input, *, out=None)",
        "api_description": "Alias for torch.acosh().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.arccosh",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.arccosh.html#torch.Tensor.arccosh",
        "api_signature": "Tensor.arccosh()",
        "api_description": "acosh() -> Tensor",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.arccosh_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.arccosh_.html#torch.Tensor.arccosh_",
        "api_signature": "Tensor.arccosh_()",
        "api_description": "acosh_() -> Tensor",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.arcsin",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.arcsin.html#torch.arcsin",
        "api_signature": "torch.arcsin(input, *, out=None)",
        "api_description": "Alias for torch.asin().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.arcsin",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.arcsin.html#torch.Tensor.arcsin",
        "api_signature": "Tensor.arcsin()",
        "api_description": "See torch.arcsin()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.arcsin_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.arcsin_.html#torch.Tensor.arcsin_",
        "api_signature": "Tensor.arcsin_()",
        "api_description": "In-place version of arcsin()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.arcsinh",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.arcsinh.html#torch.arcsinh",
        "api_signature": "torch.arcsinh(input, *, out=None)",
        "api_description": "Alias for torch.asinh().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.arcsinh",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.arcsinh.html#torch.Tensor.arcsinh",
        "api_signature": "Tensor.arcsinh()",
        "api_description": "See torch.arcsinh()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.arcsinh_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.arcsinh_.html#torch.Tensor.arcsinh_",
        "api_signature": "Tensor.arcsinh_()",
        "api_description": "In-place version of arcsinh()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.arctan",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.arctan.html#torch.arctan",
        "api_signature": "torch.arctan(input, *, out=None)",
        "api_description": "Alias for torch.atan().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.arctan",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.arctan.html#torch.Tensor.arctan",
        "api_signature": "Tensor.arctan()",
        "api_description": "See torch.arctan()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.arctan2",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.arctan2.html#torch.arctan2",
        "api_signature": "torch.arctan2(input, other, *, out=None)",
        "api_description": "Alias for torch.atan2().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.arctan2",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.arctan2.html#torch.Tensor.arctan2",
        "api_signature": "Tensor.arctan2(other)",
        "api_description": "See torch.arctan2()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.arctan2_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.arctan2_.html#torch.Tensor.arctan2_",
        "api_signature": "Tensor.arctan2_()",
        "api_description": "atan2_(other) -> Tensor",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.arctan_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.arctan_.html#torch.Tensor.arctan_",
        "api_signature": "Tensor.arctan_()",
        "api_description": "In-place version of arctan()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.arctanh",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.arctanh.html#torch.arctanh",
        "api_signature": "torch.arctanh(input, *, out=None)",
        "api_description": "Alias for torch.atanh().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.arctanh",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.arctanh.html#torch.Tensor.arctanh",
        "api_signature": "Tensor.arctanh()",
        "api_description": "See torch.arctanh()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.arctanh_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.arctanh_.html#torch.Tensor.arctanh_",
        "api_signature": "Tensor.arctanh_(other)",
        "api_description": "In-place version of arctanh()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.are_deterministic_algorithms_enabled",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.are_deterministic_algorithms_enabled.html#torch.are_deterministic_algorithms_enabled",
        "api_signature": "torch.are_deterministic_algorithms_enabled()",
        "api_description": "Returns True if the global deterministic flag is turned on. Refer to\ntorch.use_deterministic_algorithms() documentation for more details.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.bernoulli.Bernoulli.arg_constraints",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.bernoulli.Bernoulli.arg_constraints",
        "api_signature": "Real()",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.beta.Beta.arg_constraints",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.beta.Beta.arg_constraints",
        "api_signature": "GreaterThan(lower_bound=0.0)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.binomial.Binomial.arg_constraints",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.binomial.Binomial.arg_constraints",
        "api_signature": "Real()",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.categorical.Categorical.arg_constraints",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.categorical.Categorical.arg_constraints",
        "api_signature": "IndependentConstraint(Real()",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.cauchy.Cauchy.arg_constraints",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.cauchy.Cauchy.arg_constraints",
        "api_signature": "Real()",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.chi2.Chi2.arg_constraints",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.chi2.Chi2.arg_constraints",
        "api_signature": "GreaterThan(lower_bound=0.0)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.continuous_bernoulli.ContinuousBernoulli.arg_constraints",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.continuous_bernoulli.ContinuousBernoulli.arg_constraints",
        "api_signature": "Real()",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.dirichlet.Dirichlet.arg_constraints",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.dirichlet.Dirichlet.arg_constraints",
        "api_signature": "IndependentConstraint(GreaterThan(lower_bound=0.0)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.distribution.Distribution.arg_constraints",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.distribution.Distribution.arg_constraints",
        "api_signature": null,
        "api_description": "Returns a dictionary from argument names to\nConstraint objects that\nshould be satisfied by each argument of this distribution. Args that\nare not tensors need not appear in this dict.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.exponential.Exponential.arg_constraints",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.exponential.Exponential.arg_constraints",
        "api_signature": "GreaterThan(lower_bound=0.0)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.fishersnedecor.FisherSnedecor.arg_constraints",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.fishersnedecor.FisherSnedecor.arg_constraints",
        "api_signature": "GreaterThan(lower_bound=0.0)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.gamma.Gamma.arg_constraints",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.gamma.Gamma.arg_constraints",
        "api_signature": "GreaterThan(lower_bound=0.0)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.geometric.Geometric.arg_constraints",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.geometric.Geometric.arg_constraints",
        "api_signature": "Real()",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.gumbel.Gumbel.arg_constraints",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.gumbel.Gumbel.arg_constraints",
        "api_signature": "Real()",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.half_cauchy.HalfCauchy.arg_constraints",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.half_cauchy.HalfCauchy.arg_constraints",
        "api_signature": "GreaterThan(lower_bound=0.0)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.half_normal.HalfNormal.arg_constraints",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.half_normal.HalfNormal.arg_constraints",
        "api_signature": "GreaterThan(lower_bound=0.0)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.independent.Independent.arg_constraints",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.independent.Independent.arg_constraints",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.inverse_gamma.InverseGamma.arg_constraints",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.inverse_gamma.InverseGamma.arg_constraints",
        "api_signature": "GreaterThan(lower_bound=0.0)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.kumaraswamy.Kumaraswamy.arg_constraints",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.kumaraswamy.Kumaraswamy.arg_constraints",
        "api_signature": "GreaterThan(lower_bound=0.0)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.laplace.Laplace.arg_constraints",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.laplace.Laplace.arg_constraints",
        "api_signature": "Real()",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.lkj_cholesky.LKJCholesky.arg_constraints",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.lkj_cholesky.LKJCholesky.arg_constraints",
        "api_signature": "GreaterThan(lower_bound=0.0)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.log_normal.LogNormal.arg_constraints",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.log_normal.LogNormal.arg_constraints",
        "api_signature": "Real()",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.arg_constraints",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.arg_constraints",
        "api_signature": "IndependentConstraint(GreaterThan(lower_bound=0.0)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.mixture_same_family.MixtureSameFamily.arg_constraints",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.mixture_same_family.MixtureSameFamily.arg_constraints",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.multinomial.Multinomial.arg_constraints",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.multinomial.Multinomial.arg_constraints",
        "api_signature": "IndependentConstraint(Real()",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.multivariate_normal.MultivariateNormal.arg_constraints",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.multivariate_normal.MultivariateNormal.arg_constraints",
        "api_signature": "PositiveDefinite()",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.negative_binomial.NegativeBinomial.arg_constraints",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.negative_binomial.NegativeBinomial.arg_constraints",
        "api_signature": "Real()",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.normal.Normal.arg_constraints",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.normal.Normal.arg_constraints",
        "api_signature": "Real()",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.one_hot_categorical.OneHotCategorical.arg_constraints",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.one_hot_categorical.OneHotCategorical.arg_constraints",
        "api_signature": "IndependentConstraint(Real()",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.pareto.Pareto.arg_constraints",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.pareto.Pareto.arg_constraints",
        "api_signature": "GreaterThan(lower_bound=0.0)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.poisson.Poisson.arg_constraints",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.poisson.Poisson.arg_constraints",
        "api_signature": "GreaterThanEq(lower_bound=0.0)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.arg_constraints",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.arg_constraints",
        "api_signature": "Real()",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.relaxed_bernoulli.RelaxedBernoulli.arg_constraints",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.relaxed_bernoulli.RelaxedBernoulli.arg_constraints",
        "api_signature": "Real()",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.arg_constraints",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.arg_constraints",
        "api_signature": "IndependentConstraint(Real()",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.studentT.StudentT.arg_constraints",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.studentT.StudentT.arg_constraints",
        "api_signature": "GreaterThan(lower_bound=0.0)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.transformed_distribution.TransformedDistribution.arg_constraints",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.transformed_distribution.TransformedDistribution.arg_constraints",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.uniform.Uniform.arg_constraints",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.uniform.Uniform.arg_constraints",
        "api_signature": "Dependent()",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.von_mises.VonMises.arg_constraints",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.von_mises.VonMises.arg_constraints",
        "api_signature": "GreaterThan(lower_bound=0.0)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.weibull.Weibull.arg_constraints",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.weibull.Weibull.arg_constraints",
        "api_signature": "GreaterThan(lower_bound=0.0)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.wishart.Wishart.arg_constraints",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.wishart.Wishart.arg_constraints",
        "api_signature": "PositiveDefinite()",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.argmax",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.argmax.html#torch.argmax",
        "api_signature": "torch.argmax(input)",
        "api_description": "Returns the indices of the maximum value of all elements in the input tensor.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.\ninput (Tensor) – the input tensor.\ndim (int) – the dimension to reduce. If None, the argmax of the flattened input is returned.\nkeepdim (bool) – whether the output tensor has dim retained or not.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.argmax",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.argmax.html#torch.Tensor.argmax",
        "api_signature": "Tensor.argmax(dim=None, keepdim=False)",
        "api_description": "See torch.argmax()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.argmin",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.argmin.html#torch.argmin",
        "api_signature": "torch.argmin(input, dim=None, keepdim=False)",
        "api_description": "Returns the indices of the minimum value(s) of the flattened tensor or along a dimension",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.\ndim (int) – the dimension to reduce. If None, the argmin of the flattened input is returned.\nkeepdim (bool) – whether the output tensor has dim retained or not.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.argmin",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.argmin.html#torch.Tensor.argmin",
        "api_signature": "Tensor.argmin(dim=None, keepdim=False)",
        "api_description": "See torch.argmin()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.Node.args",
        "api_url": "https://pytorch.org/docs/stable/fx.html#torch.fx.Node.args",
        "api_signature": null,
        "api_description": "The tuple of arguments to this Node. The interpretation of arguments\ndepends on the node’s opcode. See the Node docstring for more\ninformation.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.argsort",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.argsort.html#torch.argsort",
        "api_signature": "torch.argsort(input, dim=-1, descending=False, stable=False)",
        "api_description": "Returns the indices that sort a tensor along a given dimension in ascending\norder by value.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.\ndim (int, optional) – the dimension to sort along\ndescending (bool, optional) – controls the sorting order (ascending or descending)\nstable (bool, optional) – controls the relative order of equivalent elements",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.argsort",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.argsort.html#torch.Tensor.argsort",
        "api_signature": "Tensor.argsort(dim=-1, descending=False)",
        "api_description": "See torch.argsort()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.argwhere",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.argwhere.html#torch.argwhere",
        "api_signature": "torch.argwhere(input)",
        "api_description": "Returns a tensor containing the indices of all non-zero elements of\ninput.  Each row in the result contains the indices of a non-zero\nelement in input. The result is sorted lexicographically, with\nthe last index changing the fastest (C-style).",
        "return_value": "",
        "parameters": "{input} –",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.argwhere",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.argwhere.html#torch.Tensor.argwhere",
        "api_signature": "Tensor.argwhere()",
        "api_description": "See torch.argwhere()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nested.as_nested_tensor",
        "api_url": "https://pytorch.org/docs/stable/nested.html#torch.nested.as_nested_tensor",
        "api_signature": "torch.nested.as_nested_tensor(tensor_list, dtype=None, device=None, layout=None)",
        "api_description": "Constructs a nested tensor preserving autograd history from tensor_list a list of tensors.",
        "return_value": "",
        "parameters": "tensor_list (List[Tensor]) – a list of tensors with the same ndim\ndtype (torch.dtype, optional) – the desired type of returned nested tensor.\nDefault: if None, same torch.dtype as leftmost tensor in the list.\ndevice (torch.device, optional) – the desired device of returned nested tensor.\nDefault: if None, same torch.device as leftmost tensor in the list\nlayout (torch.layout, optional) – the desired layout of returned nested tensor.\nOnly strided and jagged layouts are supported. Default: if None, the strided layout.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.sparse.as_sparse_gradcheck",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.sparse.as_sparse_gradcheck.html#torch.sparse.as_sparse_gradcheck",
        "api_signature": "torch.sparse.as_sparse_gradcheck(gradcheck)",
        "api_description": "Decorate function, to extend gradcheck for sparse tensors.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.benchmark.CallgrindStats.as_standardized",
        "api_url": "https://pytorch.org/docs/stable/benchmark_utils.html#torch.utils.benchmark.CallgrindStats.as_standardized",
        "api_signature": "as_standardized()",
        "api_description": "Strip library names and some prefixes from function strings.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.as_strided",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.as_strided.html#torch.as_strided",
        "api_signature": "torch.as_strided(input, size, stride, storage_offset=None)",
        "api_description": "Create a view of an existing torch.Tensor input with specified\nsize, stride and storage_offset.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.\nsize (tuple or ints) – the shape of the output tensor\nstride (tuple or ints) – the stride of the output tensor\nstorage_offset (int, optional) – the offset in the underlying storage of the output tensor.\nIf None, the storage_offset of the output tensor will match the input tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.as_strided",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.as_strided.html#torch.Tensor.as_strided",
        "api_signature": "Tensor.as_strided(size, stride, storage_offset=None)",
        "api_description": "See torch.as_strided()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.as_subclass",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.as_subclass.html#torch.Tensor.as_subclass",
        "api_signature": "Tensor.as_subclass(cls)",
        "api_description": "Makes a cls instance with the same data pointer as self. Changes\nin the output mirror changes in self, and the output stays attached\nto the autograd graph. cls must be a subclass of Tensor.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.as_tensor",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.as_tensor.html#torch.as_tensor",
        "api_signature": "torch.as_tensor(data, dtype=None, device=None)",
        "api_description": "Converts data into a tensor, sharing data and preserving autograd\nhistory if possible.",
        "return_value": "",
        "parameters": "data (array_like) – Initial data for the tensor. Can be a list, tuple,\nNumPy ndarray, scalar, and other types.\ndtype (torch.dtype, optional) – the desired data type of returned tensor.\nDefault: if None, infers data type from data.\ndevice (torch.device, optional) – the device of the constructed tensor. If None and data is a tensor\nthen the device of data is used. If None and data is not a tensor then\nthe result tensor is constructed on the current device.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.asarray",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.asarray.html#torch.asarray",
        "api_signature": "torch.asarray(obj, *, dtype=None, device=None, copy=None, requires_grad=False)",
        "api_description": "Converts obj to a tensor.",
        "return_value": "",
        "parameters": "obj (object) – a tensor, NumPy array, DLPack Capsule, object that implements Python’s\nbuffer protocol, scalar, or sequence of scalars.\ndtype (torch.dtype, optional) – the datatype of the returned tensor.\nDefault: None, which causes the datatype of the returned tensor to be\ninferred from obj.\ncopy (bool, optional) – controls whether the returned tensor shares memory with obj.\nDefault: None, which causes the returned tensor to share memory with obj\nwhenever possible. If True then the returned tensor does not share its memory.\nIf False then the returned tensor shares its memory with obj and an\nerror is thrown if it cannot.\ndevice (torch.device, optional) – the device of the returned tensor.\nDefault: None, which causes the device of obj to be used. Or, if\nobj is a Python sequence, the current default device will be used.\nrequires_grad (bool, optional) – whether the returned tensor requires grad.\nDefault: False, which causes the returned tensor not to require a gradient.\nIf True, then the returned tensor will require a gradient, and if obj\nis also a tensor with an autograd history then the returned tensor will have\nthe same history.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.ASGD",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.ASGD.html#torch.optim.ASGD",
        "api_signature": "torch.optim.ASGD(params, lr=0.01, lambd=0.0001, alpha=0.75, t0=1000000.0, weight_decay=0, foreach=None, maximize=False, differentiable=False, capturable=False)",
        "api_description": "Implements Averaged Stochastic Gradient Descent.",
        "return_value": "a handle that can be used to remove the added hook by calling\nhandle.remove()\na handle that can be used to remove the added hook by calling\nhandle.remove()\na handle that can be used to remove the added hook by calling\nhandle.remove()\na handle that can be used to remove the added hook by calling\nhandle.remove()\na handle that can be used to remove the added hook by calling\nhandle.remove()\na handle that can be used to remove the added hook by calling\nhandle.remove()\n",
        "parameters": "params (iterable) – iterable of parameters to optimize or dicts defining\nparameter groups\nlr (float, optional) – learning rate (default: 1e-2)\nlambd (float, optional) – decay term (default: 1e-4)\nalpha (float, optional) – power for eta update (default: 0.75)\nt0 (float, optional) – point at which to start averaging (default: 1e6)\nweight_decay (float, optional) – weight decay (L2 penalty) (default: 0)\nforeach (bool, optional) – whether foreach implementation of optimizer\nis used. If unspecified by the user (so foreach is None), we will try to use\nforeach over the for-loop implementation on CUDA, since it is usually\nsignificantly more performant. Note that the foreach implementation uses\n~ sizeof(params) more peak memory than the for-loop version due to the intermediates\nbeing a tensorlist vs just one tensor. If memory is prohibitive, batch fewer\nparameters through the optimizer at a time or switch this flag to False (default: None)\nmaximize (bool, optional) – maximize the objective with respect to the\nparams, instead of minimizing (default: False)\ndifferentiable (bool, optional) – whether autograd should\noccur through the optimizer step in training. Otherwise, the step()\nfunction runs in a torch.no_grad() context. Setting to True can impair\nperformance, so leave it False if you don’t intend to run autograd\nthrough this instance (default: False)\ncapturable (bool, optional) – whether this instance is safe to\ncapture in a CUDA graph. Passing True can impair ungraphed performance,\nso if you don’t intend to graph capture this instance, leave it False\n(default: False)\nparam_group (dict) – Specifies what Tensors should be optimized along with group\nspecific optimization options.\nstate_dict (dict) – optimizer state. Should be an object returned\nfrom a call to state_dict().\nhook (Callable) – The user defined hook to be registered.\nprepend (bool) – If True, the provided post hook will be fired before\nall the already registered post-hooks on load_state_dict. Otherwise,\nthe provided hook will be fired after all the already registered\npost-hooks. (default: False)\nhook (Callable) – The user defined hook to be registered.\nprepend (bool) – If True, the provided pre hook will be fired before\nall the already registered pre-hooks on load_state_dict. Otherwise,\nthe provided hook will be fired after all the already registered\npre-hooks. (default: False)\nhook (Callable) – The user defined hook to be registered.\nprepend (bool) – If True, the provided post hook will be fired before\nall the already registered post-hooks on state_dict. Otherwise,\nthe provided hook will be fired after all the already registered\npost-hooks. (default: False)\nhook (Callable) – The user defined hook to be registered.\nprepend (bool) – If True, the provided pre hook will be fired before\nall the already registered pre-hooks on state_dict. Otherwise,\nthe provided hook will be fired after all the already registered\npre-hooks. (default: False)\nhook (Callable) – The user defined hook to be registered.\nhook (Callable) – The user defined hook to be registered.\nclosure (Callable, optional) – A closure that reevaluates the model\nand returns the loss.\nset_to_none (bool) – instead of setting to zero, set the grads to None.\nThis will in general have lower memory footprint, and can modestly improve performance.\nHowever, it changes certain behaviors. For example:\n1. When the user tries to access a gradient and perform manual ops on it,\na None attribute or a Tensor full of 0s will behave differently.\n2. If the user requests zero_grad(set_to_none=True) followed by a backward pass, .grads\nare guaranteed to be None for params that did not receive a gradient.\n3. torch.optim optimizers have a different behavior if the gradient is 0 or None\n(in one case it does the step with a gradient of 0 and in the other it skips\nthe step altogether).",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.asin",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.asin.html#torch.asin",
        "api_signature": "torch.asin(input, *, out=None)",
        "api_description": "Returns a new tensor with the arcsine of the elements of input.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.asin",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.asin.html#torch.Tensor.asin",
        "api_signature": "Tensor.asin()",
        "api_description": "See torch.asin()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.asin_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.asin_.html#torch.Tensor.asin_",
        "api_signature": "Tensor.asin_()",
        "api_description": "In-place version of asin()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.asinh",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.asinh.html#torch.asinh",
        "api_signature": "torch.asinh(input, *, out=None)",
        "api_description": "Returns a new tensor with the inverse hyperbolic sine of the elements of input.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.asinh",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.asinh.html#torch.Tensor.asinh",
        "api_signature": "Tensor.asinh()",
        "api_description": "See torch.asinh()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.asinh_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.asinh_.html#torch.Tensor.asinh_",
        "api_signature": "Tensor.asinh_()",
        "api_description": "In-place version of asinh()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.testing.assert_allclose",
        "api_url": "https://pytorch.org/docs/stable/testing.html#torch.testing.assert_allclose",
        "api_signature": "torch.testing.assert_allclose(actual, expected, rtol=None, atol=None, equal_nan=True, msg='')",
        "api_description": "Warning",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.testing.assert_close",
        "api_url": "https://pytorch.org/docs/stable/testing.html#torch.testing.assert_close",
        "api_signature": "torch.testing.assert_close(actual, expected, *, allow_subclasses=True, rtol=None, atol=None, equal_nan=False, check_device=True, check_dtype=True, check_layout=True, check_stride=False, msg=None)",
        "api_description": "Asserts that actual and expected are close.",
        "return_value": "",
        "parameters": "actual (Any) – Actual input.\nexpected (Any) – Expected input.\nallow_subclasses (bool) – If True (default) and except for Python scalars, inputs of directly related types\nare allowed. Otherwise type equality is required.\nrtol (Optional[float]) – Relative tolerance. If specified atol must also be specified. If omitted, default\nvalues based on the dtype are selected with the below table.\natol (Optional[float]) – Absolute tolerance. If specified rtol must also be specified. If omitted, default\nvalues based on the dtype are selected with the below table.\nequal_nan (Union[bool, str]) – If True, two NaN values will be considered equal.\ncheck_device (bool) – If True (default), asserts that corresponding tensors are on the same\ndevice. If this check is disabled, tensors on different\ndevice’s are moved to the CPU before being compared.\ncheck_dtype (bool) – If True (default), asserts that corresponding tensors have the same dtype. If this\ncheck is disabled, tensors with different dtype’s are promoted  to a common dtype (according to\ntorch.promote_types()) before being compared.\ncheck_layout (bool) – If True (default), asserts that corresponding tensors have the same layout. If this\ncheck is disabled, tensors with different layout’s are converted to strided tensors before being\ncompared.\ncheck_stride (bool) – If True and corresponding tensors are strided, asserts that they have the same stride.\nmsg (Optional[Union[str, Callable[[str], str]]]) – Optional error message to use in case a failure occurs during\nthe comparison. Can also passed as callable in which case it will be called with the generated message and\nshould return the new message.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.compiler.assume_constant_result",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.compiler.assume_constant_result.html#torch.compiler.assume_constant_result",
        "api_signature": "torch.compiler.assume_constant_result(fn)",
        "api_description": "This function is used to mark a function fn as having a constant result.\nThis allows the compiler to optimize away your function\nReturns The same function fn",
        "return_value": "",
        "parameters": "fn – The function to be marked as having a constant result.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.rpc.functions.async_execution",
        "api_url": "https://pytorch.org/docs/stable/rpc.html#torch.distributed.rpc.functions.async_execution",
        "api_signature": "torch.distributed.rpc.functions.async_execution(fn)",
        "api_description": "A decorator for a function indicating that the return value of the function\nis guaranteed to be a Future object and this\nfunction can run asynchronously on the RPC callee. More specifically, the\ncallee extracts the Future returned by the wrapped\nfunction and installs subsequent processing steps as a callback to that\nFuture. The installed callback will read the value\nfrom the Future when completed and send the\nvalue back as the RPC response. That also means the returned\nFuture only exists on the callee side and is never\nsent through RPC. This decorator is useful when the wrapped function’s\n(fn) execution needs to pause and resume due to, e.g., containing\nrpc_async() or waiting for other signals.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": "The returned Future object can come from\nrpc_async(),\nthen(), or Future\nconstructor. The example below shows directly using the\nFuture returned by\nthen().\n>>> from torch.distributed import rpc\n>>>\n>>> # omitting setup and shutdown RPC\n>>>\n>>> # On all workers\n>>> @rpc.functions.async_execution\n>>> def async_add_chained(to, x, y, z):\n>>>     # This function runs on \"worker1\" and returns immediately when\n>>>     # the callback is installed through the `then(cb)` API. In the\n>>>     # mean time, the `rpc_async` to \"worker2\" can run concurrently.\n>>>     # When the return value of that `rpc_async` arrives at\n>>>     # \"worker1\", \"worker1\" will run the lambda function accordingly\n>>>     # and set the value for the previously returned `Future`, which\n>>>     # will then trigger RPC to send the result back to \"worker0\".\n>>>     return rpc.rpc_async(to, torch.add, args=(x, y)).then(\n>>>         lambda fut: fut.wait() + z\n>>>     )\n>>>\n>>> # On worker0\n>>> ret = rpc.rpc_sync(\n>>>     \"worker1\",\n>>>     async_add_chained,\n>>>     args=(\"worker2\", torch.ones(2), 1, 1)\n>>> )\n>>> print(ret)  # prints tensor([3., 3.])\n\n\nWhen combined with TorchScript decorators, this decorator must be the\noutmost one.\n>>> from torch import Tensor\n>>> from torch.futures import Future\n>>> from torch.distributed import rpc\n>>>\n>>> # omitting setup and shutdown RPC\n>>>\n>>> # On all workers\n>>> @torch.jit.script\n>>> def script_add(x: Tensor, y: Tensor) -> Tensor:\n>>>     return x + y\n>>>\n>>> @rpc.functions.async_execution\n>>> @torch.jit.script\n>>> def async_add(to: str, x: Tensor, y: Tensor) -> Future[Tensor]:\n>>>     return rpc.rpc_async(to, script_add, (x, y))\n>>>\n>>> # On worker0\n>>> ret = rpc.rpc_sync(\n>>>     \"worker1\",\n>>>     async_add,\n>>>     args=(\"worker2\", torch.ones(2), 1)\n>>> )\n>>> print(ret)  # prints tensor([2., 2.])\n\n\nWhen combined with static or class method, this decorator must be the\ninner one.\n>>> from torch.distributed import rpc\n>>>\n>>> # omitting setup and shutdown RPC\n>>>\n>>> # On all workers\n>>> class AsyncExecutionClass:\n>>>\n>>>     @staticmethod\n>>>     @rpc.functions.async_execution\n>>>     def static_async_add(to, x, y, z):\n>>>         return rpc.rpc_async(to, torch.add, args=(x, y)).then(\n>>>             lambda fut: fut.wait() + z\n>>>         )\n>>>\n>>>     @classmethod\n>>>     @rpc.functions.async_execution\n>>>     def class_async_add(cls, to, x, y, z):\n>>>         ret_fut = torch.futures.Future()\n>>>         rpc.rpc_async(to, torch.add, args=(x, y)).then(\n>>>             lambda fut: ret_fut.set_result(fut.wait() + z)\n>>>         )\n>>>         return ret_fut\n>>>\n>>>     @rpc.functions.async_execution\n>>>     def bound_async_add(self, to, x, y, z):\n>>>         return rpc.rpc_async(to, torch.add, args=(x, y)).then(\n>>>             lambda fut: fut.wait() + z\n>>>         )\n>>>\n>>> # On worker0\n>>> ret = rpc.rpc_sync(\n>>>     \"worker1\",\n>>>     AsyncExecutionClass.static_async_add,\n>>>     args=(\"worker2\", torch.ones(2), 1, 2)\n>>> )\n>>> print(ret)  # prints tensor([4., 4.])\n>>>\n>>> ret = rpc.rpc_sync(\n>>>     \"worker1\",\n>>>     AsyncExecutionClass.class_async_add,\n>>>     args=(\"worker2\", torch.ones(2), 1, 2)\n>>> )\n>>> print(ret)  # prints tensor([4., 4.])\n\n\nThis decorator also works with RRef helpers, i.e., .\ntorch.distributed.rpc.RRef.rpc_sync(),\ntorch.distributed.rpc.RRef.rpc_async(), and\ntorch.distributed.rpc.RRef.remote().\n>>> from torch.distributed import rpc\n>>>\n>>> # reuse the AsyncExecutionClass class above\n>>> rref = rpc.remote(\"worker1\", AsyncExecutionClass)\n>>> ret = rref.rpc_sync().static_async_add(\"worker2\", torch.ones(2), 1, 2)\n>>> print(ret)  # prints tensor([4., 4.])\n>>>\n>>> rref = rpc.remote(\"worker1\", AsyncExecutionClass)\n>>> ret = rref.rpc_async().static_async_add(\"worker2\", torch.ones(2), 1, 2).wait()\n>>> print(ret)  # prints tensor([4., 4.])\n>>>\n>>> rref = rpc.remote(\"worker1\", AsyncExecutionClass)\n>>> ret = rref.remote().static_async_add(\"worker2\", torch.ones(2), 1, 2).to_here()\n>>> print(ret)  # prints tensor([4., 4.])\n\n\n"
    },
    {
        "api_name": "torch.distributed.checkpoint.state_dict_saver.async_save",
        "api_url": "https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict_saver.async_save",
        "api_signature": "torch.distributed.checkpoint.state_dict_saver.async_save(state_dict, *, checkpoint_id=None, storage_writer=None, planner=None, process_group=None)",
        "api_description": "Asynchronous version of save_state_dict. This code first de-stages the state_dict on CPU, and then calls\nsave in a separate thread.",
        "return_value": "A future holding the resultant Metadata object from save.\n",
        "parameters": "state_dict (Dict[str, Any]) – The state_dict to save.\ncheckpoint_id (Union[str, os.PathLike, None]) – The ID of this checkpoint instance. The meaning of the checkpoint_id\ndepends on the storage. It can be a path to a folder or to a file.\nIt can also be a key if the storage is a key-value store.\n(Default: None)\nstorage_writer (Optional[StorageWriter]) – Instance of StorageWriter used to perform writes. If this is not\nspecified, DCP will automatically infer the writer based on the\ncheckpoint_id. If checkpoint_id is also None, an exception will\nbe raised. (Default: None)\nplanner (Optional[SavePlanner]) – Instance of SavePlanner. If this is not specificed, the default\nplanner will be used. (Default: None)\nprocess_group (Optional[ProcessGroup]) – ProcessGroup to be used for cross-rank synchronization.\n(Default: None)",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.atan",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.atan.html#torch.atan",
        "api_signature": "torch.atan(input, *, out=None)",
        "api_description": "Returns a new tensor with the arctangent of the elements of input.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.atan",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.atan.html#torch.Tensor.atan",
        "api_signature": "Tensor.atan()",
        "api_description": "See torch.atan()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.atan2",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.atan2.html#torch.atan2",
        "api_signature": "torch.atan2(input, other, *, out=None)",
        "api_description": "Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi​/otheri​\nwith consideration of the quadrant. Returns a new tensor with the signed angles\nin radians between vector (otheri,inputi)(\\text{other}_{i}, \\text{input}_{i})(otheri​,inputi​)\nand vector (1,0)(1, 0)(1,0). (Note that otheri\\text{other}_{i}otheri​, the second\nparameter, is the x-coordinate, while inputi\\text{input}_{i}inputi​, the first\nparameter, is the y-coordinate.)",
        "return_value": "",
        "parameters": "input (Tensor) – the first input tensor\nother (Tensor) – the second input tensor\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.atan2",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.atan2.html#torch.Tensor.atan2",
        "api_signature": "Tensor.atan2(other)",
        "api_description": "See torch.atan2()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.atan2_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.atan2_.html#torch.Tensor.atan2_",
        "api_signature": "Tensor.atan2_(other)",
        "api_description": "In-place version of atan2()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.atan_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.atan_.html#torch.Tensor.atan_",
        "api_signature": "Tensor.atan_()",
        "api_description": "In-place version of atan()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.atanh",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.atanh.html#torch.atanh",
        "api_signature": "torch.atanh(input, *, out=None)",
        "api_description": "Returns a new tensor with the inverse hyperbolic tangent of the elements of input.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.atanh",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.atanh.html#torch.Tensor.atanh",
        "api_signature": "Tensor.atanh()",
        "api_description": "See torch.atanh()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.atanh_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.atanh_.html#torch.Tensor.atanh_",
        "api_signature": "Tensor.atanh_(other)",
        "api_description": "In-place version of atanh()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.atleast_1d",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.atleast_1d.html#torch.atleast_1d",
        "api_signature": "torch.atleast_1d(*tensors)",
        "api_description": "Returns a 1-dimensional view of each input tensor with zero dimensions.\nInput tensors with one or more dimensions are returned as-is.",
        "return_value": "output (Tensor or tuple of Tensors)\n",
        "parameters": "input (Tensor or list of Tensors) –",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.atleast_2d",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.atleast_2d.html#torch.atleast_2d",
        "api_signature": "torch.atleast_2d(*tensors)",
        "api_description": "Returns a 2-dimensional view of each input tensor with zero dimensions.\nInput tensors with two or more dimensions are returned as-is.",
        "return_value": "output (Tensor or tuple of Tensors)\n",
        "parameters": "input (Tensor or list of Tensors) –",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.atleast_3d",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.atleast_3d.html#torch.atleast_3d",
        "api_signature": "torch.atleast_3d(*tensors)",
        "api_description": "Returns a 3-dimensional view of each input tensor with zero dimensions.\nInput tensors with three or more dimensions are returned as-is.",
        "return_value": "output (Tensor or tuple of Tensors)\n",
        "parameters": "input (Tensor or list of Tensors) –",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.jit.Attribute",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.jit.Attribute.html#torch.jit.Attribute",
        "api_signature": "torch.jit.Attribute(value, type)",
        "api_description": "This method is a pass-through function that returns value, mostly\nused to indicate to the TorchScript compiler that the left-hand side\nexpression is a class instance attribute with type of type. Note that\ntorch.jit.Attribute should only be used in __init__ method of jit.ScriptModule\nsubclasses.",
        "return_value": "Returns value\n",
        "parameters": "value – An initial value to be assigned to attribute.\ntype – A Python type",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.autocast",
        "api_url": "https://pytorch.org/docs/stable/amp.html#torch.autocast",
        "api_signature": "torch.autocast(device_type, dtype=None, enabled=True, cache_enabled=None)",
        "api_description": "Instances of autocast serve as context managers or decorators that\nallow regions of your script to run in mixed precision.",
        "return_value": "",
        "parameters": "device_type (str, required) – Device type to use. Possible values are: ‘cuda’, ‘cpu’, ‘xpu’ and ‘hpu’.\nThe type is the same as the type attribute of a torch.device.\nThus, you may obtain the device type of a tensor using Tensor.device.type.\nenabled (bool, optional) – Whether autocasting should be enabled in the region.\nDefault: True\ndtype (torch_dtype, optional) – Whether to use torch.float16 or torch.bfloat16.\ncache_enabled (bool, optional) – Whether the weight cache inside autocast should be enabled.\nDefault: True",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cpu.amp.autocast",
        "api_url": "https://pytorch.org/docs/stable/amp.html#torch.cpu.amp.autocast",
        "api_signature": "torch.cpu.amp.autocast(enabled=True, dtype=torch.bfloat16, cache_enabled=True)",
        "api_description": "See torch.autocast.\ntorch.cpu.amp.autocast(args...) is equivalent to torch.autocast(\"cpu\", args...)",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda.amp.autocast",
        "api_url": "https://pytorch.org/docs/stable/amp.html#torch.cuda.amp.autocast",
        "api_signature": "torch.cuda.amp.autocast(enabled=True, dtype=torch.float16, cache_enabled=True)",
        "api_description": "See torch.autocast.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.functional.avg_pool1d",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.functional.avg_pool1d.html#torch.nn.functional.avg_pool1d",
        "api_signature": "torch.nn.functional.avg_pool1d(input, kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True)",
        "api_description": "Applies a 1D average pooling over an input signal composed of several\ninput planes.",
        "return_value": "",
        "parameters": "input – input tensor of shape (minibatch,in_channels,iW)(\\text{minibatch} , \\text{in\\_channels} , iW)(minibatch,in_channels,iW)\nkernel_size – the size of the window. Can be a single number or a\ntuple (kW,)\nstride – the stride of the window. Can be a single number or a tuple\n(sW,). Default: kernel_size\npadding – implicit zero paddings on both sides of the input. Can be a\nsingle number or a tuple (padW,). Default: 0\nceil_mode – when True, will use ceil instead of floor to compute the\noutput shape. Default: False\ncount_include_pad – when True, will include the zero-padding in the\naveraging calculation. Default: True",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.quantized.functional.avg_pool2d",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.nn.quantized.functional.avg_pool2d.html#torch.ao.nn.quantized.functional.avg_pool2d",
        "api_signature": "torch.ao.nn.quantized.functional.avg_pool2d(input, kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True, divisor_override=None)",
        "api_description": "Applies 2D average-pooling operation in kH×kWkH \\times kWkH×kW regions by step size\nsH×sWsH \\times sWsH×sW steps. The number of output features is equal to the number of\ninput planes.",
        "return_value": "",
        "parameters": "input – quantized input tensor (minibatch,in_channels,iH,iW)(\\text{minibatch} , \\text{in\\_channels} , iH , iW)(minibatch,in_channels,iH,iW)\nkernel_size – size of the pooling region. Can be a single number or a\ntuple (kH, kW)\nstride – stride of the pooling operation. Can be a single number or a\ntuple (sH, sW). Default: kernel_size\npadding – implicit zero paddings on both sides of the input. Can be a\nsingle number or a tuple (padH, padW). Default: 0\nceil_mode – when True, will use ceil instead of floor in the formula\nto compute the output shape. Default: False\ncount_include_pad – when True, will include the zero-padding in the\naveraging calculation. Default: True\ndivisor_override – if specified, it will be used as divisor, otherwise\nsize of the pooling region will be used. Default: None",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.functional.avg_pool2d",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.functional.avg_pool2d.html#torch.nn.functional.avg_pool2d",
        "api_signature": "torch.nn.functional.avg_pool2d(input, kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True, divisor_override=None)",
        "api_description": "Applies 2D average-pooling operation in kH×kWkH \\times kWkH×kW regions by step size\nsH×sWsH \\times sWsH×sW steps. The number of output features is equal to the number of\ninput planes.",
        "return_value": "",
        "parameters": "input – input tensor (minibatch,in_channels,iH,iW)(\\text{minibatch} , \\text{in\\_channels} , iH , iW)(minibatch,in_channels,iH,iW)\nkernel_size – size of the pooling region. Can be a single number or a\ntuple (kH, kW)\nstride – stride of the pooling operation. Can be a single number or a\ntuple (sH, sW). Default: kernel_size\npadding – implicit zero paddings on both sides of the input. Can be a\nsingle number or a tuple (padH, padW). Default: 0\nceil_mode – when True, will use ceil instead of floor in the formula\nto compute the output shape. Default: False\ncount_include_pad – when True, will include the zero-padding in the\naveraging calculation. Default: True\ndivisor_override – if specified, it will be used as divisor, otherwise\nsize of the pooling region will be used. Default: None",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.quantized.functional.avg_pool3d",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.nn.quantized.functional.avg_pool3d.html#torch.ao.nn.quantized.functional.avg_pool3d",
        "api_signature": "torch.ao.nn.quantized.functional.avg_pool3d(input, kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True, divisor_override=None)",
        "api_description": "Applies 3D average-pooling operation in kD timeskH×kWkD \\ times kH \\times kWkD timeskH×kW regions by step size\nsD×sH×sWsD \\times sH \\times sWsD×sH×sW steps. The number of output features is equal to the number of\ninput planes.",
        "return_value": "",
        "parameters": "input – quantized input tensor (minibatch,in_channels,iH,iW)(\\text{minibatch} , \\text{in\\_channels} , iH , iW)(minibatch,in_channels,iH,iW)\nkernel_size – size of the pooling region. Can be a single number or a\ntuple (kD, kH, kW)\nstride – stride of the pooling operation. Can be a single number or a\ntuple (sD, sH, sW). Default: kernel_size\npadding – implicit zero paddings on both sides of the input. Can be a\nsingle number or a tuple (padD, padH, padW). Default: 0\nceil_mode – when True, will use ceil instead of floor in the formula\nto compute the output shape. Default: False\ncount_include_pad – when True, will include the zero-padding in the\naveraging calculation. Default: True\ndivisor_override – if specified, it will be used as divisor, otherwise\nsize of the pooling region will be used. Default: None",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.functional.avg_pool3d",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.functional.avg_pool3d.html#torch.nn.functional.avg_pool3d",
        "api_signature": "torch.nn.functional.avg_pool3d(input, kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True, divisor_override=None)",
        "api_description": "Applies 3D average-pooling operation in kT×kH×kWkT \\times kH \\times kWkT×kH×kW regions by step\nsize sT×sH×sWsT \\times sH \\times sWsT×sH×sW steps. The number of output features is equal to\n⌊input planessT⌋\\lfloor\\frac{\\text{input planes}}{sT}\\rfloor⌊sTinput planes​⌋.",
        "return_value": "",
        "parameters": "input – input tensor (minibatch,in_channels,iT×iH,iW)(\\text{minibatch} , \\text{in\\_channels} , iT \\times iH , iW)(minibatch,in_channels,iT×iH,iW)\nkernel_size – size of the pooling region. Can be a single number or a\ntuple (kT, kH, kW)\nstride – stride of the pooling operation. Can be a single number or a\ntuple (sT, sH, sW). Default: kernel_size\npadding – implicit zero paddings on both sides of the input. Can be a\nsingle number or a tuple (padT, padH, padW), Default: 0\nceil_mode – when True, will use ceil instead of floor in the formula\nto compute the output shape\ncount_include_pad – when True, will include the zero-padding in the\naveraging calculation\ndivisor_override – if specified, it will be used as divisor, otherwise\nsize of the pooling region will be used. Default: None",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.AvgPool1d",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.AvgPool1d.html#torch.nn.AvgPool1d",
        "api_signature": "torch.nn.AvgPool1d(kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True)",
        "api_description": "Applies a 1D average pooling over an input signal composed of several input planes.",
        "return_value": "",
        "parameters": "kernel_size (Union[int, Tuple[int]]) – the size of the window\nstride (Union[int, Tuple[int]]) – the stride of the window. Default value is kernel_size\npadding (Union[int, Tuple[int]]) – implicit zero padding to be added on both sides\nceil_mode (bool) – when True, will use ceil instead of floor to compute the output shape\ncount_include_pad (bool) – when True, will include the zero-padding in the averaging calculation",
        "input_shape": "\nInput: (N,C,Lin)(N, C, L_{in})(N,C,Lin​) or (C,Lin)(C, L_{in})(C,Lin​).\nOutput: (N,C,Lout)(N, C, L_{out})(N,C,Lout​) or (C,Lout)(C, L_{out})(C,Lout​), where\n\nLout=⌊Lin+2×padding−kernel_sizestride+1⌋L_{out} = \\left\\lfloor \\frac{L_{in} +\n2 \\times \\text{padding} - \\text{kernel\\_size}}{\\text{stride}} + 1\\right\\rfloor\n\nLout​=⌊strideLin​+2×padding−kernel_size​+1⌋Per the note above, if ceil_mode is True and (Lout−1)×stride≥Lin+padding(L_{out} - 1) \\times \\text{stride} \\geq L_{in}\n+ \\text{padding}(Lout​−1)×stride≥Lin​+padding, we skip the last window as it would start in the right padded region, resulting in\nLoutL_{out}Lout​ being reduced by one.\n\n\n",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.AvgPool2d",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.AvgPool2d.html#torch.nn.AvgPool2d",
        "api_signature": "torch.nn.AvgPool2d(kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True, divisor_override=None)",
        "api_description": "Applies a 2D average pooling over an input signal composed of several input planes.",
        "return_value": "",
        "parameters": "kernel_size (Union[int, Tuple[int, int]]) – the size of the window\nstride (Union[int, Tuple[int, int]]) – the stride of the window. Default value is kernel_size\npadding (Union[int, Tuple[int, int]]) – implicit zero padding to be added on both sides\nceil_mode (bool) – when True, will use ceil instead of floor to compute the output shape\ncount_include_pad (bool) – when True, will include the zero-padding in the averaging calculation\ndivisor_override (Optional[int]) – if specified, it will be used as divisor, otherwise size of the pooling region will be used.",
        "input_shape": "\nInput: (N,C,Hin,Win)(N, C, H_{in}, W_{in})(N,C,Hin​,Win​) or (C,Hin,Win)(C, H_{in}, W_{in})(C,Hin​,Win​).\nOutput: (N,C,Hout,Wout)(N, C, H_{out}, W_{out})(N,C,Hout​,Wout​) or (C,Hout,Wout)(C, H_{out}, W_{out})(C,Hout​,Wout​), where\n\nHout=⌊Hin+2×padding[0]−kernel_size[0]stride[0]+1⌋H_{out} = \\left\\lfloor\\frac{H_{in}  + 2 \\times \\text{padding}[0] -\n  \\text{kernel\\_size}[0]}{\\text{stride}[0]} + 1\\right\\rfloor\n\nHout​=⌊stride[0]Hin​+2×padding[0]−kernel_size[0]​+1⌋\nWout=⌊Win+2×padding[1]−kernel_size[1]stride[1]+1⌋W_{out} = \\left\\lfloor\\frac{W_{in}  + 2 \\times \\text{padding}[1] -\n  \\text{kernel\\_size}[1]}{\\text{stride}[1]} + 1\\right\\rfloor\n\nWout​=⌊stride[1]Win​+2×padding[1]−kernel_size[1]​+1⌋Per the note above, if ceil_mode is True and (Hout−1)×stride[0]≥Hin+padding[0](H_{out} - 1)\\times \\text{stride}[0]\\geq H_{in}\n+ \\text{padding}[0](Hout​−1)×stride[0]≥Hin​+padding[0], we skip the last window as it would start in the bottom padded region,\nresulting in HoutH_{out}Hout​ being reduced by one.\nThe same applies for WoutW_{out}Wout​.\n\n\n",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.AvgPool3d",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.AvgPool3d.html#torch.nn.AvgPool3d",
        "api_signature": "torch.nn.AvgPool3d(kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True, divisor_override=None)",
        "api_description": "Applies a 3D average pooling over an input signal composed of several input planes.",
        "return_value": "",
        "parameters": "kernel_size (Union[int, Tuple[int, int, int]]) – the size of the window\nstride (Union[int, Tuple[int, int, int]]) – the stride of the window. Default value is kernel_size\npadding (Union[int, Tuple[int, int, int]]) – implicit zero padding to be added on all three sides\nceil_mode (bool) – when True, will use ceil instead of floor to compute the output shape\ncount_include_pad (bool) – when True, will include the zero-padding in the averaging calculation\ndivisor_override (Optional[int]) – if specified, it will be used as divisor, otherwise kernel_size will be used",
        "input_shape": "\nInput: (N,C,Din,Hin,Win)(N, C, D_{in}, H_{in}, W_{in})(N,C,Din​,Hin​,Win​) or (C,Din,Hin,Win)(C, D_{in}, H_{in}, W_{in})(C,Din​,Hin​,Win​).\nOutput: (N,C,Dout,Hout,Wout)(N, C, D_{out}, H_{out}, W_{out})(N,C,Dout​,Hout​,Wout​) or\n(C,Dout,Hout,Wout)(C, D_{out}, H_{out}, W_{out})(C,Dout​,Hout​,Wout​), where\n\nDout=⌊Din+2×padding[0]−kernel_size[0]stride[0]+1⌋D_{out} = \\left\\lfloor\\frac{D_{in} + 2 \\times \\text{padding}[0] -\n      \\text{kernel\\_size}[0]}{\\text{stride}[0]} + 1\\right\\rfloor\n\nDout​=⌊stride[0]Din​+2×padding[0]−kernel_size[0]​+1⌋\nHout=⌊Hin+2×padding[1]−kernel_size[1]stride[1]+1⌋H_{out} = \\left\\lfloor\\frac{H_{in} + 2 \\times \\text{padding}[1] -\n      \\text{kernel\\_size}[1]}{\\text{stride}[1]} + 1\\right\\rfloor\n\nHout​=⌊stride[1]Hin​+2×padding[1]−kernel_size[1]​+1⌋\nWout=⌊Win+2×padding[2]−kernel_size[2]stride[2]+1⌋W_{out} = \\left\\lfloor\\frac{W_{in} + 2 \\times \\text{padding}[2] -\n      \\text{kernel\\_size}[2]}{\\text{stride}[2]} + 1\\right\\rfloor\n\nWout​=⌊stride[2]Win​+2×padding[2]−kernel_size[2]​+1⌋Per the note above, if ceil_mode is True and (Dout−1)×stride[0]≥Din+padding[0](D_{out} - 1)\\times \\text{stride}[0]\\geq D_{in}\n+ \\text{padding}[0](Dout​−1)×stride[0]≥Din​+padding[0], we skip the last window as it would start in the padded region,\nresulting in DoutD_{out}Dout​ being reduced by one.\nThe same applies for WoutW_{out}Wout​ and HoutH_{out}Hout​.\n\n\n",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.Backend",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#torch.distributed.Backend",
        "api_signature": "torch.distributed.Backend(name)",
        "api_description": "An enum-like class for backends.",
        "return_value": "",
        "parameters": "name (str) – Backend name of the ProcessGroup extension. It\nshould match the one in init_process_group().\nfunc (function) – Function handler that instantiates the backend.\nThe function should be implemented in the backend\nextension and takes four arguments, including\nstore, rank, world_size, and timeout.\nextended_api (bool, optional) – Whether the backend supports extended argument structure.\nDefault: False. If set to True, the backend\nwill get an instance of c10d::DistributedBackendOptions, and\na process group options object as defined by the backend implementation.\ndevice (str or list of str, optional) – device type this backend\nsupports, e.g. “cpu”, “cuda”, etc. If None,\nassuming both “cpu” and “cuda”",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.backend_config.BackendConfig",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.quantization.backend_config.BackendConfig.html#torch.ao.quantization.backend_config.BackendConfig",
        "api_signature": "torch.ao.quantization.backend_config.BackendConfig(name='')",
        "api_description": "Config that defines the set of patterns that can be quantized on a given backend, and how reference\nquantized models can be produced from these patterns.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.backend_config.BackendPatternConfig",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.quantization.backend_config.BackendPatternConfig.html#torch.ao.quantization.backend_config.BackendPatternConfig",
        "api_signature": "torch.ao.quantization.backend_config.BackendPatternConfig(pattern=None)",
        "api_description": "Config object that specifies quantization behavior for a given operator pattern.\nFor a detailed example usage, see BackendConfig.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.rpc.BackendType",
        "api_url": "https://pytorch.org/docs/stable/rpc.html#torch.distributed.rpc.BackendType",
        "api_signature": "torch.distributed.rpc.BackendType(value)",
        "api_description": "An enum class of available backends.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.autograd.backward",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.autograd.backward.html#torch.autograd.backward",
        "api_signature": "torch.autograd.backward(tensors, grad_tensors=None, retain_graph=None, create_graph=False, grad_variables=None, inputs=None)",
        "api_description": "Computes the sum of gradients of given tensors with respect to graph\nleaves.",
        "return_value": "",
        "parameters": "tensors (Sequence[Tensor] or Tensor) – Tensors of which the derivative will be\ncomputed.\ngrad_tensors (Sequence[Tensor or None] or Tensor, optional) – The “vector” in\nthe Jacobian-vector product, usually gradients w.r.t. each element of\ncorresponding tensors. None values can be specified for scalar Tensors or\nones that don’t require grad. If a None value would be acceptable for all\ngrad_tensors, then this argument is optional.\nretain_graph (bool, optional) – If False, the graph used to compute the grad\nwill be freed. Note that in nearly all cases setting this option to True\nis not needed and often can be worked around in a much more efficient\nway. Defaults to the value of create_graph.\ncreate_graph (bool, optional) – If True, graph of the derivative will\nbe constructed, allowing to compute higher order derivative products.\nDefaults to False.\ninputs (Sequence[Tensor] or Tensor or Sequence[GradientEdge], optional) – Inputs w.r.t. which the gradient\nbe will accumulated into .grad. All other Tensors will be ignored. If\nnot provided, the gradient is accumulated into all the leaf Tensors that\nwere used to compute the tensors.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.autograd.backward",
        "api_url": "https://pytorch.org/docs/stable/rpc.html#torch.distributed.autograd.backward",
        "api_signature": "torch.distributed.autograd.backward(context_id: int, roots: List[Tensor], retain_graph=False)",
        "api_description": "Kicks off the distributed backward pass using the provided roots. This\ncurrently implements the FAST mode algorithm which\nassumes all RPC messages sent in the same distributed autograd context\nacross workers would be part of the autograd graph during the backward pass.",
        "return_value": "",
        "parameters": "context_id (int) – The autograd context id for which we should retrieve the gradients.\nroots (list) – Tensors which represent the roots of the autograd\ncomputation. All the tensors should be scalars.\nretain_graph (bool, optional) – If False, the graph used to compute the grad\nwill be freed. Note that in nearly all cases setting this\noption to True is not needed and often can be worked around\nin a much more efficient way. Usually, you need to set this\nto True to run backward multiple times.",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import torch.distributed.autograd as dist_autograd\n>>> with dist_autograd.context() as context_id:\n>>>     pred = model.forward()\n>>>     loss = loss_func(pred, loss)\n>>>     dist_autograd.backward(context_id, loss)\n\n\n"
    },
    {
        "api_name": "torch.autograd.Function.backward",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.autograd.Function.backward.html#torch.autograd.Function.backward",
        "api_signature": "Function.backward(ctx, *grad_outputs)",
        "api_description": "Define a formula for differentiating the operation with backward mode automatic differentiation.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.autograd.function.InplaceFunction.backward",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.autograd.function.InplaceFunction.html#torch.autograd.function.InplaceFunction.backward",
        "api_signature": "backward(ctx, *grad_outputs)",
        "api_description": "Define a formula for differentiating the operation with backward mode automatic differentiation.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.autograd.function.NestedIOFunction.backward",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.autograd.function.NestedIOFunction.html#torch.autograd.function.NestedIOFunction.backward",
        "api_signature": "backward(*gradients)",
        "api_description": "Shared backward utility.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.rpc.PyRRef.backward",
        "api_url": "https://pytorch.org/docs/stable/rpc.html#torch.distributed.rpc.PyRRef.backward",
        "api_signature": "backward(self: torch._C._distributed_rpc.PyRRef, dist_autograd_ctx_id: int = -1, retain_graph: bool = False)",
        "api_description": "Runs the backward pass using the RRef as the root of the\nbackward pass. If dist_autograd_ctx_id is provided,\nwe perform a distributed backward pass using the provided\nctx_id starting from the owner of the RRef. In this case,\nget_gradients() should be\nused to retrieve the gradients. If dist_autograd_ctx_id\nis None, it is assumed that this is a local autograd graph\nand we only perform a local backward pass. In the local case,\nthe node calling this API has to be the owner of the RRef.\nThe value of the RRef is expected to be a scalar Tensor.",
        "return_value": "",
        "parameters": "dist_autograd_ctx_id (int, optional) – The distributed\nautograd context id for which we should retrieve the\ngradients (default: -1).\nretain_graph (bool, optional) – If False, the graph used to\ncompute the grad will be freed. Note that in nearly all\ncases setting this option to True is not needed and\noften can be worked around in a much more efficient way.\nUsually, you need to set this to True to run backward\nmultiple times (default: False).",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import torch.distributed.autograd as dist_autograd\n>>> with dist_autograd.context() as context_id:\n>>>     rref.backward(context_id)\n\n\n"
    },
    {
        "api_name": "torch.Tensor.backward",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.backward.html#torch.Tensor.backward",
        "api_signature": "Tensor.backward(gradient=None, retain_graph=None, create_graph=False, inputs=None)",
        "api_description": "Computes the gradient of current tensor wrt graph leaves.",
        "return_value": "",
        "parameters": "gradient (Tensor or None) – Gradient w.r.t. the\ntensor. If it is a tensor, it will be automatically converted\nto a Tensor that does not require grad unless create_graph is True.\nNone values can be specified for scalar Tensors or ones that\ndon’t require grad. If a None value would be acceptable then\nthis argument is optional.\nretain_graph (bool, optional) – If False, the graph used to compute\nthe grads will be freed. Note that in nearly all cases setting\nthis option to True is not needed and often can be worked around\nin a much more efficient way. Defaults to the value of\ncreate_graph.\ncreate_graph (bool, optional) – If True, graph of the derivative will\nbe constructed, allowing to compute higher order derivative\nproducts. Defaults to False.\ninputs (sequence of Tensor) – Inputs w.r.t. which the gradient will be\naccumulated into .grad. All other Tensors will be ignored. If not\nprovided, the gradient is accumulated into all the leaf Tensors that were\nused to compute the attr::tensors.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.autograd.function.NestedIOFunction.backward_extended",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.autograd.function.NestedIOFunction.html#torch.autograd.function.NestedIOFunction.backward_extended",
        "api_signature": "backward_extended(*grad_output)",
        "api_description": "User defined backward.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.autograd.function.BackwardCFunction",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.autograd.function.BackwardCFunction.html#torch.autograd.function.BackwardCFunction",
        "api_signature": null,
        "api_description": "This class is used for internal autograd work. Do not use.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> class Inplace(Function):\n>>>     @staticmethod\n>>>     def forward(ctx, x):\n>>>         x_npy = x.numpy() # x_npy shares storage with x\n>>>         x_npy += 1\n>>>         ctx.mark_dirty(x)\n>>>         return x\n>>>\n>>>     @staticmethod\n>>>     @once_differentiable\n>>>     def backward(ctx, grad_output):\n>>>         return grad_output\n>>>\n>>> a = torch.tensor(1., requires_grad=True, dtype=torch.double).clone()\n>>> b = a * a\n>>> Inplace.apply(a)  # This would lead to wrong gradients!\n>>>                   # but the engine would not know unless we mark_dirty\n>>> b.backward() # RuntimeError: one of the variables needed for gradient\n>>>              # computation has been modified by an inplace operation\n\n\n>>> class Func(Function):\n>>>     @staticmethod\n>>>     def forward(ctx, x: torch.Tensor, y: torch.Tensor, z: int):\n>>>         w = x * z\n>>>         out = x * y + y * z + w * y\n>>>         ctx.save_for_backward(x, y, w, out)\n>>>         ctx.z = z  # z is not a tensor\n>>>         return out\n>>>\n>>>     @staticmethod\n>>>     @once_differentiable\n>>>     def backward(ctx, grad_out):\n>>>         x, y, w, out = ctx.saved_tensors\n>>>         z = ctx.z\n>>>         gx = grad_out * (y + y * z)\n>>>         gy = grad_out * (x + z + w)\n>>>         gz = None\n>>>         return gx, gy, gz\n>>>\n>>> a = torch.tensor(1., requires_grad=True, dtype=torch.double)\n>>> b = torch.tensor(2., requires_grad=True, dtype=torch.double)\n>>> c = 4\n>>> d = Func.apply(a, b, c)\n\n\n>>> class Func(torch.autograd.Function):\n>>>     @staticmethod\n>>>     def forward(ctx, x: torch.Tensor, y: torch.Tensor, z: int):\n>>>         ctx.save_for_backward(x, y)\n>>>         ctx.save_for_forward(x, y)\n>>>         ctx.z = z\n>>>         return x * y * z\n>>>\n>>>     @staticmethod\n>>>     def jvp(ctx, x_t, y_t, _):\n>>>         x, y = ctx.saved_tensors\n>>>         z = ctx.z\n>>>         return z * (y * x_t + x * y_t)\n>>>\n>>>     @staticmethod\n>>>     def vjp(ctx, grad_out):\n>>>         x, y = ctx.saved_tensors\n>>>         z = ctx.z\n>>>         return z * grad_out * y, z * grad_out * x, None\n>>>\n>>>     a = torch.tensor(1., requires_grad=True, dtype=torch.double)\n>>>     t = torch.tensor(1., dtype=torch.double)\n>>>     b = torch.tensor(2., requires_grad=True, dtype=torch.double)\n>>>     c = 4\n>>>\n>>>     with fwAD.dual_level():\n>>>         a_dual = fwAD.make_dual(a, t)\n>>>         d = Func.apply(a_dual, b, c)\n\n\n>>> class SimpleFunc(Function):\n>>>     @staticmethod\n>>>     def forward(ctx, x):\n>>>         return x.clone(), x.clone()\n>>>\n>>>     @staticmethod\n>>>     @once_differentiable\n>>>     def backward(ctx, g1, g2):\n>>>         return g1 + g2  # No check for None necessary\n>>>\n>>> # We modify SimpleFunc to handle non-materialized grad outputs\n>>> class Func(Function):\n>>>     @staticmethod\n>>>     def forward(ctx, x):\n>>>         ctx.set_materialize_grads(False)\n>>>         ctx.save_for_backward(x)\n>>>         return x.clone(), x.clone()\n>>>\n>>>     @staticmethod\n>>>     @once_differentiable\n>>>     def backward(ctx, g1, g2):\n>>>         x, = ctx.saved_tensors\n>>>         grad_input = torch.zeros_like(x)\n>>>         if g1 is not None:  # We must check for None now\n>>>             grad_input += g1\n>>>         if g2 is not None:\n>>>             grad_input += g2\n>>>         return grad_input\n>>>\n>>> a = torch.tensor(1., requires_grad=True)\n>>> b, _ = Func.apply(a)  # induces g2 to be undefined\n\n\n"
    },
    {
        "api_name": "torch.distributed.fsdp.BackwardPrefetch",
        "api_url": "https://pytorch.org/docs/stable/fsdp.html#torch.distributed.fsdp.BackwardPrefetch",
        "api_signature": "torch.distributed.fsdp.BackwardPrefetch(value)",
        "api_description": "This configures explicit backward prefetching, which improves throughput by\nenabling communication and computation overlap in the backward pass at the\ncost of slightly increased memory usage.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.baddbmm",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.baddbmm.html#torch.baddbmm",
        "api_signature": "torch.baddbmm(input, batch1, batch2, *, beta=1, alpha=1, out=None)",
        "api_description": "Performs a batch matrix-matrix product of matrices in batch1\nand batch2.\ninput is added to the final result.",
        "return_value": "",
        "parameters": "input (Tensor) – the tensor to be added\nbatch1 (Tensor) – the first batch of matrices to be multiplied\nbatch2 (Tensor) – the second batch of matrices to be multiplied\nbeta (Number, optional) – multiplier for input (β\\betaβ)\nalpha (Number, optional) – multiplier for batch1@batch2\\text{batch1} \\mathbin{@} \\text{batch2}batch1@batch2 (α\\alphaα)\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.baddbmm",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.baddbmm.html#torch.Tensor.baddbmm",
        "api_signature": "Tensor.baddbmm(batch1, batch2, *, beta=1, alpha=1)",
        "api_description": "See torch.baddbmm()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.baddbmm_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.baddbmm_.html#torch.Tensor.baddbmm_",
        "api_signature": "Tensor.baddbmm_(batch1, batch2, *, beta=1, alpha=1)",
        "api_description": "In-place version of baddbmm()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.barrier",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#torch.distributed.barrier",
        "api_signature": "torch.distributed.barrier(group=None, async_op=False, device_ids=None)",
        "api_description": "Synchronize all processes.",
        "return_value": "Async work handle, if async_op is set to True.\nNone, if not async_op or if not part of the group\n",
        "parameters": "group (ProcessGroup, optional) – The process group to work on. If None,\nthe default process group will be used.\nasync_op (bool, optional) – Whether this op should be an async op\ndevice_ids ([int], optional) – List of device/GPU ids.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.signal.windows.bartlett",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.signal.windows.bartlett.html#torch.signal.windows.bartlett",
        "api_signature": "torch.signal.windows.bartlett(M, *, sym=True, dtype=None, layout=torch.strided, device=None, requires_grad=False)",
        "api_description": "Computes the Bartlett window.",
        "return_value": "",
        "parameters": "M (int) – the length of the window.\nIn other words, the number of points of the returned window.\nsym (bool, optional) – If False, returns a periodic window suitable for use in spectral analysis.\nIf True, returns a symmetric window suitable for use in filter design. Default: True.\ndtype (torch.dtype, optional) – the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_dtype()).\nlayout (torch.layout, optional) – the desired layout of returned Tensor.\nDefault: torch.strided.\ndevice (torch.device, optional) – the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_device()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types.\nrequires_grad (bool, optional) – If autograd should record operations on the\nreturned tensor. Default: False.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.bartlett_window",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.bartlett_window.html#torch.bartlett_window",
        "api_signature": "torch.bartlett_window(window_length, periodic=True, *, dtype=None, layout=torch.strided, device=None, requires_grad=False)",
        "api_description": "Bartlett window function.",
        "return_value": "A 1-D tensor of size (window_length,)(\\text{window\\_length},)(window_length,) containing the window\n",
        "parameters": "window_length (int) – the size of returned window\nperiodic (bool, optional) – If True, returns a window to be used as periodic\nfunction. If False, return a symmetric window.\ndtype (torch.dtype, optional) – the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_dtype()). Only floating point types are supported.\nlayout (torch.layout, optional) – the desired layout of returned window tensor. Only\ntorch.strided (dense layout) is supported.\ndevice (torch.device, optional) – the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_device()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types.\nrequires_grad (bool, optional) – If autograd should record operations on the\nreturned tensor. Default: False.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.utils.prune.BasePruningMethod",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.BasePruningMethod.html#torch.nn.utils.prune.BasePruningMethod",
        "api_signature": null,
        "api_description": "Abstract base class for creation of new pruning techniques.",
        "return_value": "pruned version of the input tensor\nmask to apply to t, of same dims as t\npruned version of tensor t.\n",
        "parameters": "module (nn.Module) – module containing the tensor to prune\nname (str) – parameter name within module on which pruning\nwill act.\nargs – arguments passed on to a subclass of\nBasePruningMethod\nimportance_scores (torch.Tensor) – tensor of importance scores (of\nsame shape as module parameter) used to compute mask for pruning.\nThe values in this tensor indicate the importance of the\ncorresponding elements in the parameter being pruned.\nIf unspecified or None, the parameter will be used in its place.\nkwargs – keyword arguments passed on to a subclass of a\nBasePruningMethod\nmodule (nn.Module) – module containing the tensor to prune\nt (torch.Tensor) – tensor representing the importance scores of the\nprune. (parameter to) –\ndefault_mask (torch.Tensor) – Base mask from previous pruning\niterations –\nis (that need to be respected after the new mask) –\nt. (applied. Same dims as) –\nt (torch.Tensor) – tensor to prune (of same dimensions as\ndefault_mask).\nimportance_scores (torch.Tensor) – tensor of importance scores (of\nsame shape as t) used to compute mask for pruning t.\nThe values in this tensor indicate the importance of the\ncorresponding elements in the t that is being pruned.\nIf unspecified or None, the tensor t will be used in its place.\ndefault_mask (torch.Tensor, optional) – mask from previous pruning\niteration, if any. To be considered when determining what\nportion of the tensor that pruning should act on. If None,\ndefault to a mask of ones.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.batch_isend_irecv",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#torch.distributed.batch_isend_irecv",
        "api_signature": "torch.distributed.batch_isend_irecv(p2p_op_list)",
        "api_description": "Send or Receive a batch of tensors asynchronously and return a list of requests.",
        "return_value": "A list of distributed request objects returned by calling the corresponding\nop in the op_list.\n",
        "parameters": "p2p_op_list – A list of point-to-point operations(type of each operator is\ntorch.distributed.P2POp). The order of the isend/irecv in the list\nmatters and it needs to match with corresponding isend/irecv on the\nremote end.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.functional.batch_norm",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.functional.batch_norm.html#torch.nn.functional.batch_norm",
        "api_signature": "torch.nn.functional.batch_norm(input, running_mean, running_var, weight=None, bias=None, training=False, momentum=0.1, eps=1e-05)",
        "api_description": "Apply Batch Normalization for each channel across a batch of data.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.distribution.Distribution.batch_shape",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.distribution.Distribution.batch_shape",
        "api_signature": null,
        "api_description": "Returns the shape over which parameters are batched.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.utils.rnn.PackedSequence.batch_sizes",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.PackedSequence.html#torch.nn.utils.rnn.PackedSequence.batch_sizes",
        "api_signature": null,
        "api_description": "Alias for field number 1",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.batched_powerSGD_hook",
        "api_url": "https://pytorch.org/docs/stable/ddp_comm_hooks.html#torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.batched_powerSGD_hook",
        "api_signature": "torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.batched_powerSGD_hook(state, bucket)",
        "api_description": "Implement simplified PowerSGD algorithm.",
        "return_value": "Future handler of the communication, which updates the gradients in place.\n",
        "parameters": "state (PowerSGDState) – State information to configure the compression rate and support error feedback, warm start, etc.\nTo tune the compression configs, mainly need to tune matrix_approximation_rank and start_powerSGD_iter.\nbucket (dist.GradBucket) – Bucket that stores a 1D flattened gradient tensor that batches multiple per-variable tensors.\nNote that since DDP comm hook only supports single process single device mode,\nonly exactly one tensor is stored in this bucket.",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> state = PowerSGDState(process_group=process_group, matrix_approximation_rank=1)\n>>> ddp_model.register_comm_hook(state, batched_powerSGD_hook)\n\n\n"
    },
    {
        "api_name": "torch.nn.BatchNorm1d",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html#torch.nn.BatchNorm1d",
        "api_signature": "torch.nn.BatchNorm1d(num_features, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, device=None, dtype=None)",
        "api_description": "Applies Batch Normalization over a 2D or 3D input.",
        "return_value": "",
        "parameters": "num_features (int) – number of features or channels CCC of the input\neps (float) – a value added to the denominator for numerical stability.\nDefault: 1e-5\nmomentum (float) – the value used for the running_mean and running_var\ncomputation. Can be set to None for cumulative moving average\n(i.e. simple average). Default: 0.1\naffine (bool) – a boolean value that when set to True, this module has\nlearnable affine parameters. Default: True\ntrack_running_stats (bool) – a boolean value that when set to True, this\nmodule tracks the running mean and variance, and when set to False,\nthis module does not track such statistics, and initializes statistics\nbuffers running_mean and running_var as None.\nWhen these buffers are None, this module always uses batch statistics.\nin both training and eval modes. Default: True",
        "input_shape": "\nInput: (N,C)(N, C)(N,C) or (N,C,L)(N, C, L)(N,C,L), where NNN is the batch size,\nCCC is the number of features or channels, and LLL is the sequence length\nOutput: (N,C)(N, C)(N,C) or (N,C,L)(N, C, L)(N,C,L) (same shape as input)\n\n",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.quantized.BatchNorm2d",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.nn.quantized.BatchNorm2d.html#torch.ao.nn.quantized.BatchNorm2d",
        "api_signature": "torch.ao.nn.quantized.BatchNorm2d(num_features, eps=1e-05, momentum=0.1, device=None, dtype=None)",
        "api_description": "This is the quantized version of BatchNorm2d.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.BatchNorm2d",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html#torch.nn.BatchNorm2d",
        "api_signature": "torch.nn.BatchNorm2d(num_features, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, device=None, dtype=None)",
        "api_description": "Applies Batch Normalization over a 4D input.",
        "return_value": "",
        "parameters": "num_features (int) – CCC from an expected input of size\n(N,C,H,W)(N, C, H, W)(N,C,H,W)\neps (float) – a value added to the denominator for numerical stability.\nDefault: 1e-5\nmomentum (float) – the value used for the running_mean and running_var\ncomputation. Can be set to None for cumulative moving average\n(i.e. simple average). Default: 0.1\naffine (bool) – a boolean value that when set to True, this module has\nlearnable affine parameters. Default: True\ntrack_running_stats (bool) – a boolean value that when set to True, this\nmodule tracks the running mean and variance, and when set to False,\nthis module does not track such statistics, and initializes statistics\nbuffers running_mean and running_var as None.\nWhen these buffers are None, this module always uses batch statistics.\nin both training and eval modes. Default: True",
        "input_shape": "\nInput: (N,C,H,W)(N, C, H, W)(N,C,H,W)\nOutput: (N,C,H,W)(N, C, H, W)(N,C,H,W) (same shape as input)\n\n",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.quantized.BatchNorm3d",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.nn.quantized.BatchNorm3d.html#torch.ao.nn.quantized.BatchNorm3d",
        "api_signature": "torch.ao.nn.quantized.BatchNorm3d(num_features, eps=1e-05, momentum=0.1, device=None, dtype=None)",
        "api_description": "This is the quantized version of BatchNorm3d.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.BatchNorm3d",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm3d.html#torch.nn.BatchNorm3d",
        "api_signature": "torch.nn.BatchNorm3d(num_features, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, device=None, dtype=None)",
        "api_description": "Applies Batch Normalization over a 5D input.",
        "return_value": "",
        "parameters": "num_features (int) – CCC from an expected input of size\n(N,C,D,H,W)(N, C, D, H, W)(N,C,D,H,W)\neps (float) – a value added to the denominator for numerical stability.\nDefault: 1e-5\nmomentum (float) – the value used for the running_mean and running_var\ncomputation. Can be set to None for cumulative moving average\n(i.e. simple average). Default: 0.1\naffine (bool) – a boolean value that when set to True, this module has\nlearnable affine parameters. Default: True\ntrack_running_stats (bool) – a boolean value that when set to True, this\nmodule tracks the running mean and variance, and when set to False,\nthis module does not track such statistics, and initializes statistics\nbuffers running_mean and running_var as None.\nWhen these buffers are None, this module always uses batch statistics.\nin both training and eval modes. Default: True",
        "input_shape": "\nInput: (N,C,D,H,W)(N, C, D, H, W)(N,C,D,H,W)\nOutput: (N,C,D,H,W)(N, C, D, H, W)(N,C,D,H,W) (same shape as input)\n\n",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.data.BatchSampler",
        "api_url": "https://pytorch.org/docs/stable/data.html#torch.utils.data.BatchSampler",
        "api_signature": "torch.utils.data.BatchSampler(sampler, batch_size, drop_last)",
        "api_description": "Wraps another sampler to yield a mini-batch of indices.",
        "return_value": "",
        "parameters": "sampler (Sampler or Iterable) – Base sampler. Can be any iterable object\nbatch_size (int) – Size of mini-batch.\ndrop_last (bool) – If True, the sampler will drop the last batch if\nits size would be less than batch_size",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.BCELoss",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html#torch.nn.BCELoss",
        "api_signature": "torch.nn.BCELoss(weight=None, size_average=None, reduce=None, reduction='mean')",
        "api_description": "Creates a criterion that measures the Binary Cross Entropy between the target and\nthe input probabilities:",
        "return_value": "",
        "parameters": "weight (Tensor, optional) – a manual rescaling weight given to the loss\nof each batch element. If given, has to be a Tensor of size nbatch.\nsize_average (bool, optional) – Deprecated (see reduction). By default,\nthe losses are averaged over each loss element in the batch. Note that for\nsome losses, there are multiple elements per sample. If the field size_average\nis set to False, the losses are instead summed for each minibatch. Ignored\nwhen reduce is False. Default: True\nreduce (bool, optional) – Deprecated (see reduction). By default, the\nlosses are averaged or summed over observations for each minibatch depending\non size_average. When reduce is False, returns a loss per\nbatch element instead and ignores size_average. Default: True\nreduction (str, optional) – Specifies the reduction to apply to the output:\n'none' | 'mean' | 'sum'. 'none': no reduction will be applied,\n'mean': the sum of the output will be divided by the number of\nelements in the output, 'sum': the output will be summed. Note: size_average\nand reduce are in the process of being deprecated, and in the meantime,\nspecifying either of those two args will override reduction. Default: 'mean'",
        "input_shape": "\nInput: (∗)(*)(∗), where ∗*∗ means any number of dimensions.\nTarget: (∗)(*)(∗), same shape as the input.\nOutput: scalar. If reduction is 'none', then (∗)(*)(∗), same\nshape as input.\n\n",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.BCEWithLogitsLoss",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html#torch.nn.BCEWithLogitsLoss",
        "api_signature": "torch.nn.BCEWithLogitsLoss(weight=None, size_average=None, reduce=None, reduction='mean', pos_weight=None)",
        "api_description": "This loss combines a Sigmoid layer and the BCELoss in one single\nclass. This version is more numerically stable than using a plain Sigmoid\nfollowed by a BCELoss as, by combining the operations into one layer,\nwe take advantage of the log-sum-exp trick for numerical stability.",
        "return_value": "",
        "parameters": "weight (Tensor, optional) – a manual rescaling weight given to the loss\nof each batch element. If given, has to be a Tensor of size nbatch.\nsize_average (bool, optional) – Deprecated (see reduction). By default,\nthe losses are averaged over each loss element in the batch. Note that for\nsome losses, there are multiple elements per sample. If the field size_average\nis set to False, the losses are instead summed for each minibatch. Ignored\nwhen reduce is False. Default: True\nreduce (bool, optional) – Deprecated (see reduction). By default, the\nlosses are averaged or summed over observations for each minibatch depending\non size_average. When reduce is False, returns a loss per\nbatch element instead and ignores size_average. Default: True\nreduction (str, optional) – Specifies the reduction to apply to the output:\n'none' | 'mean' | 'sum'. 'none': no reduction will be applied,\n'mean': the sum of the output will be divided by the number of\nelements in the output, 'sum': the output will be summed. Note: size_average\nand reduce are in the process of being deprecated, and in the meantime,\nspecifying either of those two args will override reduction. Default: 'mean'\npos_weight (Tensor, optional) – a weight of positive examples to be broadcasted with target.\nMust be a tensor with equal size along the class dimension to the number of classes.\noperations. For a target of size [B, C, H, W] (where B is batch size) pos_weight of\nsize [B, C, H, W] will apply different pos_weights to each element of the batch or\n[C, H, W] the same pos_weights across the batch. To apply the same positive weight\nalong all spacial dimensions for a 2D multi-class target [C, H, W] use: [C, 1, 1].\nDefault: None",
        "input_shape": "\n\nInput: (∗)(*)(∗), where ∗*∗ means any number of dimensions.\nTarget: (∗)(*)(∗), same shape as the input.\nOutput: scalar. If reduction is 'none', then (∗)(*)(∗), same\nshape as input.\n\n\nExamples:\n>>> loss = nn.BCEWithLogitsLoss()\n>>> input = torch.randn(3, requires_grad=True)\n>>> target = torch.empty(3).random_(2)\n>>> output = loss(input, target)\n>>> output.backward()\n\n\n",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.backends.cudnn.benchmark",
        "api_url": "https://pytorch.org/docs/stable/backends.html#torch.backends.cudnn.benchmark",
        "api_signature": null,
        "api_description": "A bool that, if True, causes cuDNN to benchmark multiple convolution algorithms\nand select the fastest.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.backends.cudnn.benchmark_limit",
        "api_url": "https://pytorch.org/docs/stable/backends.html#torch.backends.cudnn.benchmark_limit",
        "api_signature": null,
        "api_description": "A int that specifies the maximum number of cuDNN convolution algorithms to try when\ntorch.backends.cudnn.benchmark is True. Set benchmark_limit to zero to try every\navailable algorithm. Note that this setting only affects convolutions dispatched via the\ncuDNN v8 API.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.bernoulli.Bernoulli",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.bernoulli.Bernoulli",
        "api_signature": "torch.distributions.bernoulli.Bernoulli(probs=None, logits=None, validate_args=None)",
        "api_description": "Bases: ExponentialFamily",
        "return_value": "",
        "parameters": "probs (Number, Tensor) – the probability of sampling 1\nlogits (Number, Tensor) – the log-odds of sampling 1",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.bernoulli",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.bernoulli.html#torch.bernoulli",
        "api_signature": "torch.bernoulli(input, *, generator=None, out=None)",
        "api_description": "Draws binary random numbers (0 or 1) from a Bernoulli distribution.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor of probability values for the Bernoulli distribution\ngenerator (torch.Generator, optional) – a pseudorandom number generator for sampling\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.bernoulli",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.bernoulli.html#torch.Tensor.bernoulli",
        "api_signature": "Tensor.bernoulli(*, generator=None)",
        "api_description": "Returns a result tensor where each result[i]\\texttt{result[i]}result[i] is independently\nsampled from Bernoulli(self[i])\\text{Bernoulli}(\\texttt{self[i]})Bernoulli(self[i]). self must have\nfloating point dtype, and the result will have the same dtype.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.bernoulli_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.bernoulli_.html#torch.Tensor.bernoulli_",
        "api_signature": "Tensor.bernoulli_(p=0.5, *, generator=None)",
        "api_description": "Fills each location of self with an independent sample from\nBernoulli(p)\\text{Bernoulli}(\\texttt{p})Bernoulli(p). self can have integral\ndtype.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.special.bessel_j0",
        "api_url": "https://pytorch.org/docs/stable/special.html#torch.special.bessel_j0",
        "api_signature": "torch.special.bessel_j0(input, *, out=None)",
        "api_description": "Bessel function of the first kind of order 000.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.special.bessel_j1",
        "api_url": "https://pytorch.org/docs/stable/special.html#torch.special.bessel_j1",
        "api_signature": "torch.special.bessel_j1(input, *, out=None)",
        "api_description": "Bessel function of the first kind of order 111.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.beta.Beta",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.beta.Beta",
        "api_signature": "torch.distributions.beta.Beta(concentration1, concentration0, validate_args=None)",
        "api_description": "Bases: ExponentialFamily",
        "return_value": "",
        "parameters": "concentration1 (float or Tensor) – 1st concentration parameter of the distribution\n(often referred to as alpha)\nconcentration0 (float or Tensor) – 2nd concentration parameter of the distribution\n(often referred to as beta)",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.algorithms.ddp_comm_hooks.default_hooks.bf16_compress_hook",
        "api_url": "https://pytorch.org/docs/stable/ddp_comm_hooks.html#torch.distributed.algorithms.ddp_comm_hooks.default_hooks.bf16_compress_hook",
        "api_signature": "torch.distributed.algorithms.ddp_comm_hooks.default_hooks.bf16_compress_hook(process_group, bucket)",
        "api_description": "Warning: This API is experimental, and it requires NCCL version later than 2.9.6.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> ddp_model.register_comm_hook(process_group, bf16_compress_hook)\n\n\n"
    },
    {
        "api_name": "torch.distributed.algorithms.ddp_comm_hooks.default_hooks.bf16_compress_wrapper",
        "api_url": "https://pytorch.org/docs/stable/ddp_comm_hooks.html#torch.distributed.algorithms.ddp_comm_hooks.default_hooks.bf16_compress_wrapper",
        "api_signature": "torch.distributed.algorithms.ddp_comm_hooks.default_hooks.bf16_compress_wrapper(hook)",
        "api_description": "Warning: This API is experimental, and it requires NCCL version later than 2.9.6.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> state = PowerSGDState(process_group=process_group, matrix_approximation_rank=1, start_powerSGD_iter=10)\n>>> ddp_model.register_comm_hook(state, bf16_compress_wrapper(powerSGD_hook))\n\n\n"
    },
    {
        "api_name": "torch.jit.ScriptModule.bfloat16",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.jit.ScriptModule.html#torch.jit.ScriptModule.bfloat16",
        "api_signature": "bfloat16()",
        "api_description": "Casts all floating point parameters and buffers to bfloat16 datatype.",
        "return_value": "self\n",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.Module.bfloat16",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.bfloat16",
        "api_signature": "bfloat16()",
        "api_description": "Casts all floating point parameters and buffers to bfloat16 datatype.",
        "return_value": "self\n",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.bfloat16",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.bfloat16.html#torch.Tensor.bfloat16",
        "api_signature": "Tensor.bfloat16(memory_format=torch.preserve_format)",
        "api_description": "self.bfloat16() is equivalent to self.to(torch.bfloat16). See to().",
        "return_value": "",
        "parameters": "memory_format (torch.memory_format, optional) – the desired memory format of\nreturned Tensor. Default: torch.preserve_format.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.TypedStorage.bfloat16",
        "api_url": "https://pytorch.org/docs/stable/storage.html#torch.TypedStorage.bfloat16",
        "api_signature": "bfloat16()",
        "api_description": "Casts this storage to bfloat16 type.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.UntypedStorage.bfloat16",
        "api_url": "https://pytorch.org/docs/stable/storage.html#torch.UntypedStorage.bfloat16",
        "api_signature": "bfloat16()",
        "api_description": "Casts this storage to bfloat16 type.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.BFloat16Storage",
        "api_url": "https://pytorch.org/docs/stable/storage.html#torch.BFloat16Storage",
        "api_signature": "torch.BFloat16Storage(*args, wrap_storage=None, dtype=None, device=None, _internal=False)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.Bilinear",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.Bilinear.html#torch.nn.Bilinear",
        "api_signature": "torch.nn.Bilinear(in1_features, in2_features, out_features, bias=True, device=None, dtype=None)",
        "api_description": "Applies a bilinear transformation to the incoming data: y=x1TAx2+by = x_1^T A x_2 + by=x1T​Ax2​+b.",
        "return_value": "",
        "parameters": "in1_features (int) – size of each first input sample\nin2_features (int) – size of each second input sample\nout_features (int) – size of each output sample\nbias (bool) – If set to False, the layer will not learn an additive bias.\nDefault: True",
        "input_shape": "\nInput1: (∗,Hin1)(*, H_{in1})(∗,Hin1​) where Hin1=in1_featuresH_{in1}=\\text{in1\\_features}Hin1​=in1_features and\n∗*∗ means any number of additional dimensions including none. All but the last dimension\nof the inputs should be the same.\nInput2: (∗,Hin2)(*, H_{in2})(∗,Hin2​) where Hin2=in2_featuresH_{in2}=\\text{in2\\_features}Hin2​=in2_features.\nOutput: (∗,Hout)(*, H_{out})(∗,Hout​) where Hout=out_featuresH_{out}=\\text{out\\_features}Hout​=out_features\nand all but the last dimension are the same shape as the input.\n\n",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.functional.bilinear",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.functional.bilinear.html#torch.nn.functional.bilinear",
        "api_signature": "torch.nn.functional.bilinear(input1, input2, weight, bias=None)",
        "api_description": "Applies a bilinear transformation to the incoming data:\ny=x1TAx2+by = x_1^T A x_2 + by=x1T​Ax2​+b",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.functional.binary_cross_entropy",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.functional.binary_cross_entropy.html#torch.nn.functional.binary_cross_entropy",
        "api_signature": "torch.nn.functional.binary_cross_entropy(input, target, weight=None, size_average=None, reduce=None, reduction='mean')",
        "api_description": "Measure Binary Cross Entropy between the target and input probabilities.",
        "return_value": "",
        "parameters": "input (Tensor) – Tensor of arbitrary shape as probabilities.\ntarget (Tensor) – Tensor of the same shape as input with values between 0 and 1.\nweight (Tensor, optional) – a manual rescaling weight\nif provided it’s repeated to match input tensor shape\nsize_average (bool, optional) – Deprecated (see reduction). By default,\nthe losses are averaged over each loss element in the batch. Note that for\nsome losses, there multiple elements per sample. If the field size_average\nis set to False, the losses are instead summed for each minibatch. Ignored\nwhen reduce is False. Default: True\nreduce (bool, optional) – Deprecated (see reduction). By default, the\nlosses are averaged or summed over observations for each minibatch depending\non size_average. When reduce is False, returns a loss per\nbatch element instead and ignores size_average. Default: True\nreduction (str, optional) – Specifies the reduction to apply to the output:\n'none' | 'mean' | 'sum'. 'none': no reduction will be applied,\n'mean': the sum of the output will be divided by the number of\nelements in the output, 'sum': the output will be summed. Note: size_average\nand reduce are in the process of being deprecated, and in the meantime,\nspecifying either of those two args will override reduction. Default: 'mean'",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.functional.binary_cross_entropy_with_logits",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.functional.binary_cross_entropy_with_logits.html#torch.nn.functional.binary_cross_entropy_with_logits",
        "api_signature": "torch.nn.functional.binary_cross_entropy_with_logits(input, target, weight=None, size_average=None, reduce=None, reduction='mean', pos_weight=None)",
        "api_description": "Calculate Binary Cross Entropy between target and input logits.",
        "return_value": "",
        "parameters": "input (Tensor) – Tensor of arbitrary shape as unnormalized scores (often referred to as logits).\ntarget (Tensor) – Tensor of the same shape as input with values between 0 and 1\nweight (Tensor, optional) – a manual rescaling weight\nif provided it’s repeated to match input tensor shape\nsize_average (bool, optional) – Deprecated (see reduction). By default,\nthe losses are averaged over each loss element in the batch. Note that for\nsome losses, there multiple elements per sample. If the field size_average\nis set to False, the losses are instead summed for each minibatch. Ignored\nwhen reduce is False. Default: True\nreduce (bool, optional) – Deprecated (see reduction). By default, the\nlosses are averaged or summed over observations for each minibatch depending\non size_average. When reduce is False, returns a loss per\nbatch element instead and ignores size_average. Default: True\nreduction (str, optional) – Specifies the reduction to apply to the output:\n'none' | 'mean' | 'sum'. 'none': no reduction will be applied,\n'mean': the sum of the output will be divided by the number of\nelements in the output, 'sum': the output will be summed. Note: size_average\nand reduce are in the process of being deprecated, and in the meantime,\nspecifying either of those two args will override reduction. Default: 'mean'\npos_weight (Tensor, optional) – a weight of positive examples to be broadcasted with target.\nMust be a tensor with equal size along the class dimension to the number of classes.\noperations. For a target of size [B, C, H, W] (where B is batch size) pos_weight of\nsize [B, C, H, W] will apply different pos_weights to each element of the batch or\n[C, H, W] the same pos_weights across the batch. To apply the same positive weight\nalong all spacial dimensions for a 2D multi-class target [C, H, W] use: [C, 1, 1].\nDefault: None",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.bincount",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.bincount.html#torch.bincount",
        "api_signature": "torch.bincount(input, weights=None, minlength=0)",
        "api_description": "Count the frequency of each value in an array of non-negative ints.",
        "return_value": "a tensor of shape Size([max(input) + 1]) if\ninput is non-empty, else Size(0)\n",
        "parameters": "input (Tensor) – 1-d int tensor\nweights (Tensor) – optional, weight for each value in the input tensor.\nShould be of same size as input tensor.\nminlength (int) – optional, minimum number of bins. Should be non-negative.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.bincount",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.bincount.html#torch.Tensor.bincount",
        "api_signature": "Tensor.bincount(weights=None, minlength=0)",
        "api_description": "See torch.bincount()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.symbolic_shapes.ShapeEnv.bind_symbols",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.fx.experimental.symbolic_shapes.ShapeEnv.html#torch.fx.experimental.symbolic_shapes.ShapeEnv.bind_symbols",
        "api_signature": "bind_symbols(placeholders, args)",
        "api_description": "Given a paired list of placeholders (fake tensors with\nsymbolic sizes) and concrete arguments (regular tensors\nwith real sizes), returns a dictionary mapping each\nsymbol to its real value.  So for example, if you\nhave a placeholder with size (s0, s1), binding\n(2, 4) to it will give you {s0: 2, s1: 4}.  This is\nnot guaranteed to bind ALL symbols in the ShapeEnv;\nwe can’t bind a symbol if it doesn’t occur in any placeholder,\nand symbols that already have replacements won’t get bindings.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.binomial.Binomial",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.binomial.Binomial",
        "api_signature": "torch.distributions.binomial.Binomial(total_count=1, probs=None, logits=None, validate_args=None)",
        "api_description": "Bases: Distribution",
        "return_value": "",
        "parameters": "total_count (int or Tensor) – number of Bernoulli trials\nprobs (Tensor) – Event probabilities\nlogits (Tensor) – Event log-odds",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.bitwise_and",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.bitwise_and.html#torch.bitwise_and",
        "api_signature": "torch.bitwise_and(input, other, *, out=None)",
        "api_description": "Computes the bitwise AND of input and other. The input tensor must be of\nintegral or Boolean types. For bool tensors, it computes the logical AND.",
        "return_value": "",
        "parameters": "input – the first input tensor\nother – the second input tensor\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.bitwise_and",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.bitwise_and.html#torch.Tensor.bitwise_and",
        "api_signature": "Tensor.bitwise_and()",
        "api_description": "See torch.bitwise_and()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.bitwise_and_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.bitwise_and_.html#torch.Tensor.bitwise_and_",
        "api_signature": "Tensor.bitwise_and_()",
        "api_description": "In-place version of bitwise_and()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.bitwise_left_shift",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.bitwise_left_shift.html#torch.bitwise_left_shift",
        "api_signature": "torch.bitwise_left_shift(input, other, *, out=None)",
        "api_description": "Computes the left arithmetic shift of input by other bits.\nThe input tensor must be of integral type. This operator supports\nbroadcasting to a common shape and\ntype promotion.",
        "return_value": "",
        "parameters": "input (Tensor or Scalar) – the first input tensor\nother (Tensor or Scalar) – the second input tensor\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.bitwise_left_shift",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.bitwise_left_shift.html#torch.Tensor.bitwise_left_shift",
        "api_signature": "Tensor.bitwise_left_shift(other)",
        "api_description": "See torch.bitwise_left_shift()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.bitwise_left_shift_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.bitwise_left_shift_.html#torch.Tensor.bitwise_left_shift_",
        "api_signature": "Tensor.bitwise_left_shift_(other)",
        "api_description": "In-place version of bitwise_left_shift()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.bitwise_not",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.bitwise_not.html#torch.bitwise_not",
        "api_signature": "torch.bitwise_not(input, *, out=None)",
        "api_description": "Computes the bitwise NOT of the given input tensor. The input tensor must be of\nintegral or Boolean types. For bool tensors, it computes the logical NOT.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.bitwise_not",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.bitwise_not.html#torch.Tensor.bitwise_not",
        "api_signature": "Tensor.bitwise_not()",
        "api_description": "See torch.bitwise_not()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.bitwise_not_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.bitwise_not_.html#torch.Tensor.bitwise_not_",
        "api_signature": "Tensor.bitwise_not_()",
        "api_description": "In-place version of bitwise_not()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.bitwise_or",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.bitwise_or.html#torch.bitwise_or",
        "api_signature": "torch.bitwise_or(input, other, *, out=None)",
        "api_description": "Computes the bitwise OR of input and other. The input tensor must be of\nintegral or Boolean types. For bool tensors, it computes the logical OR.",
        "return_value": "",
        "parameters": "input – the first input tensor\nother – the second input tensor\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.bitwise_or",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.bitwise_or.html#torch.Tensor.bitwise_or",
        "api_signature": "Tensor.bitwise_or()",
        "api_description": "See torch.bitwise_or()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.bitwise_or_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.bitwise_or_.html#torch.Tensor.bitwise_or_",
        "api_signature": "Tensor.bitwise_or_()",
        "api_description": "In-place version of bitwise_or()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.bitwise_right_shift",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.bitwise_right_shift.html#torch.bitwise_right_shift",
        "api_signature": "torch.bitwise_right_shift(input, other, *, out=None)",
        "api_description": "Computes the right arithmetic shift of input by other bits.\nThe input tensor must be of integral type. This operator supports\nbroadcasting to a common shape and\ntype promotion.\nIn any case, if the value of the right operand is negative or is greater\nor equal to the number of bits in the promoted left operand, the behavior is undefined.",
        "return_value": "",
        "parameters": "input (Tensor or Scalar) – the first input tensor\nother (Tensor or Scalar) – the second input tensor\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.bitwise_right_shift",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.bitwise_right_shift.html#torch.Tensor.bitwise_right_shift",
        "api_signature": "Tensor.bitwise_right_shift(other)",
        "api_description": "See torch.bitwise_right_shift()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.bitwise_right_shift_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.bitwise_right_shift_.html#torch.Tensor.bitwise_right_shift_",
        "api_signature": "Tensor.bitwise_right_shift_(other)",
        "api_description": "In-place version of bitwise_right_shift()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.bitwise_xor",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.bitwise_xor.html#torch.bitwise_xor",
        "api_signature": "torch.bitwise_xor(input, other, *, out=None)",
        "api_description": "Computes the bitwise XOR of input and other. The input tensor must be of\nintegral or Boolean types. For bool tensors, it computes the logical XOR.",
        "return_value": "",
        "parameters": "input – the first input tensor\nother – the second input tensor\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.bitwise_xor",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.bitwise_xor.html#torch.Tensor.bitwise_xor",
        "api_signature": "Tensor.bitwise_xor()",
        "api_description": "See torch.bitwise_xor()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.bitwise_xor_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.bitwise_xor_.html#torch.Tensor.bitwise_xor_",
        "api_signature": "Tensor.bitwise_xor_()",
        "api_description": "In-place version of bitwise_xor()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.signal.windows.blackman",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.signal.windows.blackman.html#torch.signal.windows.blackman",
        "api_signature": "torch.signal.windows.blackman(M, *, sym=True, dtype=None, layout=torch.strided, device=None, requires_grad=False)",
        "api_description": "Computes the Blackman window.",
        "return_value": "",
        "parameters": "M (int) – the length of the window.\nIn other words, the number of points of the returned window.\nsym (bool, optional) – If False, returns a periodic window suitable for use in spectral analysis.\nIf True, returns a symmetric window suitable for use in filter design. Default: True.\ndtype (torch.dtype, optional) – the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_dtype()).\nlayout (torch.layout, optional) – the desired layout of returned Tensor.\nDefault: torch.strided.\ndevice (torch.device, optional) – the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_device()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types.\nrequires_grad (bool, optional) – If autograd should record operations on the\nreturned tensor. Default: False.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.blackman_window",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.blackman_window.html#torch.blackman_window",
        "api_signature": "torch.blackman_window(window_length, periodic=True, *, dtype=None, layout=torch.strided, device=None, requires_grad=False)",
        "api_description": "Blackman window function.",
        "return_value": "A 1-D tensor of size (window_length,)(\\text{window\\_length},)(window_length,) containing the window\n",
        "parameters": "window_length (int) – the size of returned window\nperiodic (bool, optional) – If True, returns a window to be used as periodic\nfunction. If False, return a symmetric window.\ndtype (torch.dtype, optional) – the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_dtype()). Only floating point types are supported.\nlayout (torch.layout, optional) – the desired layout of returned window tensor. Only\ntorch.strided (dense layout) is supported.\ndevice (torch.device, optional) – the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_device()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types.\nrequires_grad (bool, optional) – If autograd should record operations on the\nreturned tensor. Default: False.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.block_diag",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.block_diag.html#torch.block_diag",
        "api_signature": "torch.block_diag(*tensors)",
        "api_description": "Create a block diagonal matrix from provided tensors.",
        "return_value": "A 2 dimensional tensor with all the input tensors arranged in\norder such that their upper left and lower right corners are\ndiagonally adjacent. All other elements are set to 0.\n",
        "parameters": "*tensors – One or more tensors with 0, 1, or 2 dimensions.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.benchmark.Timer.blocked_autorange",
        "api_url": "https://pytorch.org/docs/stable/benchmark_utils.html#torch.utils.benchmark.Timer.blocked_autorange",
        "api_signature": "blocked_autorange(callback=None, min_run_time=0.2)",
        "api_description": "Measure many replicates while keeping timer overhead to a minimum.",
        "return_value": "A Measurement object that contains measured runtimes and\nrepetition counts, and can be used to compute statistics.\n(mean, median, etc.)\n",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.bmm",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.bmm.html#torch.bmm",
        "api_signature": "torch.bmm(input, mat2, *, out=None)",
        "api_description": "Performs a batch matrix-matrix product of matrices stored in input\nand mat2.",
        "return_value": "",
        "parameters": "input (Tensor) – the first batch of matrices to be multiplied\nmat2 (Tensor) – the second batch of matrices to be multiplied\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.bmm",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.bmm.html#torch.Tensor.bmm",
        "api_signature": "Tensor.bmm(batch2)",
        "api_description": "See torch.bmm()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.intrinsic.BNReLU2d",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.nn.intrinsic.BNReLU2d.html#torch.ao.nn.intrinsic.BNReLU2d",
        "api_signature": "torch.ao.nn.intrinsic.BNReLU2d(batch_norm, relu)",
        "api_description": "This is a sequential container which calls the BatchNorm 2d and ReLU modules.\nDuring quantization this will be replaced with the corresponding fused module.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.intrinsic.quantized.BNReLU2d",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.nn.intrinsic.quantized.BNReLU2d.html#torch.ao.nn.intrinsic.quantized.BNReLU2d",
        "api_signature": "torch.ao.nn.intrinsic.quantized.BNReLU2d(num_features, eps=1e-05, momentum=0.1, device=None, dtype=None)",
        "api_description": "A BNReLU2d module is a fused module of BatchNorm2d and ReLU",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.intrinsic.BNReLU3d",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.nn.intrinsic.BNReLU3d.html#torch.ao.nn.intrinsic.BNReLU3d",
        "api_signature": "torch.ao.nn.intrinsic.BNReLU3d(batch_norm, relu)",
        "api_description": "This is a sequential container which calls the BatchNorm 3d and ReLU modules.\nDuring quantization this will be replaced with the corresponding fused module.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.intrinsic.quantized.BNReLU3d",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.nn.intrinsic.quantized.BNReLU3d.html#torch.ao.nn.intrinsic.quantized.BNReLU3d",
        "api_signature": "torch.ao.nn.intrinsic.quantized.BNReLU3d(num_features, eps=1e-05, momentum=0.1, device=None, dtype=None)",
        "api_description": "A BNReLU3d module is a fused module of BatchNorm3d and ReLU",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.bool",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.bool.html#torch.Tensor.bool",
        "api_signature": "Tensor.bool(memory_format=torch.preserve_format)",
        "api_description": "self.bool() is equivalent to self.to(torch.bool). See to().",
        "return_value": "",
        "parameters": "memory_format (torch.memory_format, optional) – the desired memory format of\nreturned Tensor. Default: torch.preserve_format.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.TypedStorage.bool",
        "api_url": "https://pytorch.org/docs/stable/storage.html#torch.TypedStorage.bool",
        "api_signature": "bool()",
        "api_description": "Casts this storage to bool type.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.UntypedStorage.bool",
        "api_url": "https://pytorch.org/docs/stable/storage.html#torch.UntypedStorage.bool",
        "api_signature": "bool()",
        "api_description": "Casts this storage to bool type.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.BoolStorage",
        "api_url": "https://pytorch.org/docs/stable/storage.html#torch.BoolStorage",
        "api_signature": "torch.BoolStorage(*args, wrap_storage=None, dtype=None, device=None, _internal=False)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.symbolic_shapes.ShapeEnv.bound_sympy",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.fx.experimental.symbolic_shapes.ShapeEnv.html#torch.fx.experimental.symbolic_shapes.ShapeEnv.bound_sympy",
        "api_signature": "bound_sympy(expr, size_oblivious=False)",
        "api_description": "Given a sympy expression, computes a ValueRanges bound for what values it can be",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.Interpreter.boxed_run",
        "api_url": "https://pytorch.org/docs/stable/fx.html#torch.fx.Interpreter.boxed_run",
        "api_signature": "boxed_run(args_list)",
        "api_description": "Run module via interpretation and return the result.  This uses the “boxed”\ncalling convention, where you pass a list of arguments, which will be cleared\nby the interpreter.  This ensures that input tensors are promptly deallocated.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.breakpoint",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#torch.distributed.breakpoint",
        "api_signature": "torch.distributed.breakpoint(rank=0)",
        "api_description": "Set a breakpoint, but only on a single rank.  All other ranks will wait for you to be\ndone with the breakpoint before continuing.",
        "return_value": "",
        "parameters": "rank (int) – Which rank to break on.  Default: 0",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda.comm.broadcast",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.cuda.comm.broadcast.html#torch.cuda.comm.broadcast",
        "api_signature": "torch.cuda.comm.broadcast(tensor, devices=None, *, out=None)",
        "api_description": "Broadcasts a tensor to specified GPU devices.",
        "return_value": "\n\nIf devices is specified,a tuple containing copies of tensor, placed on\ndevices.\n\n\n\n\nIf out is specified,a tuple containing out tensors, each containing a copy of\ntensor.\n\n\n\n\n\n",
        "parameters": "tensor (Tensor) – tensor to broadcast. Can be on CPU or GPU.\ndevices (Iterable[torch.device, str or int], optional) – an iterable of\nGPU devices, among which to broadcast.\nout (Sequence[Tensor], optional, keyword-only) – the GPU tensors to\nstore output results.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.broadcast",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#torch.distributed.broadcast",
        "api_signature": "torch.distributed.broadcast(tensor, src, group=None, async_op=False)",
        "api_description": "Broadcasts the tensor to the whole group.",
        "return_value": "Async work handle, if async_op is set to True.\nNone, if not async_op or if not part of the group\n",
        "parameters": "tensor (Tensor) – Data to be sent if src is the rank of current\nprocess, and tensor to be used to save received data otherwise.\nsrc (int) – Source rank on global process group (regardless of group argument).\ngroup (ProcessGroup, optional) – The process group to work on. If None,\nthe default process group will be used.\nasync_op (bool, optional) – Whether this op should be an async op",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda.comm.broadcast_coalesced",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.cuda.comm.broadcast_coalesced.html#torch.cuda.comm.broadcast_coalesced",
        "api_signature": "torch.cuda.comm.broadcast_coalesced(tensors, devices, buffer_size=10485760)",
        "api_description": "Broadcast a sequence of tensors to the specified GPUs.",
        "return_value": "A tuple containing copies of tensor, placed on devices.\n",
        "parameters": "tensors (sequence) – tensors to broadcast. Must be on the same device,\neither CPU or GPU.\ndevices (Iterable[torch.device, str or int]) – an iterable of GPU\ndevices, among which to broadcast.\nbuffer_size (int) – maximum size of the buffer used for coalescing",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.broadcast_object_list",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#torch.distributed.broadcast_object_list",
        "api_signature": "torch.distributed.broadcast_object_list(object_list, src=0, group=None, device=None)",
        "api_description": "Broadcasts picklable objects in object_list to the whole group.",
        "return_value": "None. If rank is part of the group, object_list will contain the\nbroadcasted objects from src rank.\n",
        "parameters": "object_list (List[Any]) – List of input objects to broadcast.\nEach object must be picklable. Only objects on the src rank will\nbe broadcast, but each rank must provide lists of equal sizes.\nsrc (int) – Source rank from which to broadcast object_list.\nSource rank is based on global process group (regardless of group argument)\ngroup – (ProcessGroup, optional): The process group to work on. If None,\nthe default process group will be used. Default is None.\ndevice (torch.device, optional) – If not None, the objects are\nserialized and converted to tensors which are moved to the\ndevice before broadcasting. Default is None.",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> # Note: Process group initialization omitted on each rank.\n>>> import torch.distributed as dist\n>>> if dist.get_rank() == 0:\n>>>     # Assumes world_size of 3.\n>>>     objects = [\"foo\", 12, {1: 2}] # any picklable object\n>>> else:\n>>>     objects = [None, None, None]\n>>> # Assumes backend is not NCCL\n>>> device = torch.device(\"cpu\")\n>>> dist.broadcast_object_list(objects, src=0, device=device)\n>>> objects\n['foo', 12, {1: 2}]\n\n\n"
    },
    {
        "api_name": "torch.broadcast_shapes",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.broadcast_shapes.html#torch.broadcast_shapes",
        "api_signature": "torch.broadcast_shapes(*shapes)",
        "api_description": "Similar to broadcast_tensors() but for shapes.",
        "return_value": "A shape compatible with all input shapes.\n",
        "parameters": "*shapes (torch.Size) – Shapes of tensors.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.broadcast_tensors",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.broadcast_tensors.html#torch.broadcast_tensors",
        "api_signature": "torch.broadcast_tensors(*tensors)",
        "api_description": "Broadcasts the given tensors according to Broadcasting semantics.",
        "return_value": "",
        "parameters": "*tensors – any number of tensors of the same type",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.broadcast_to",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.broadcast_to.html#torch.broadcast_to",
        "api_signature": "torch.broadcast_to(input, shape)",
        "api_description": "Broadcasts input to the shape shape.\nEquivalent to calling input.expand(shape). See expand() for details.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.\nshape (list, tuple, or torch.Size) – the new shape.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.broadcast_to",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.broadcast_to.html#torch.Tensor.broadcast_to",
        "api_signature": "Tensor.broadcast_to(shape)",
        "api_description": "See torch.broadcast_to().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.checkpoint.format_utils.BroadcastingTorchSaveReader",
        "api_url": "https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.format_utils.BroadcastingTorchSaveReader",
        "api_signature": "torch.distributed.checkpoint.format_utils.BroadcastingTorchSaveReader(checkpoint_id=None, coordinator_rank=0)",
        "api_description": "StorageReader for reading a Torch Save file. This reader will read the entire checkpoint\non the coordinator rank, and then broadcast and shard each tensor to all ranks.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.bucketize",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.bucketize.html#torch.bucketize",
        "api_signature": "torch.bucketize(input, boundaries, *, out_int32=False, right=False, out=None)",
        "api_description": "Returns the indices of the buckets to which each value in the input belongs, where the\nboundaries of the buckets are set by boundaries. Return a new tensor with the same size\nas input. If right is False (default), then the left boundary is open. Note that\nthis behavior is opposite the behavior of\nnumpy.digitize.\nMore formally, the returned index satisfies the following rules:",
        "return_value": "",
        "parameters": "input (Tensor or Scalar) – N-D tensor or a Scalar containing the search value(s).\nboundaries (Tensor) – 1-D tensor, must contain a strictly increasing sequence, or the return value is undefined.\nout_int32 (bool, optional) – indicate the output data type. torch.int32 if True, torch.int64 otherwise.\nDefault value is False, i.e. default output data type is torch.int64.\nright (bool, optional) – if False, return the first suitable location that is found. If True, return the\nlast such index. If no suitable index found, return 0 for non-numerical value\n(eg. nan, inf) or the size of boundaries (one pass the last index).\nIn other words, if False, gets the lower bound index for each value in input\nfrom boundaries. If True, gets the upper bound index instead.\nDefault value is False.\nout (Tensor, optional) – the output tensor, must be the same size as input if provided.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.GradBucket.buffer",
        "api_url": "https://pytorch.org/docs/stable/ddp_comm_hooks.html#torch.distributed.GradBucket.buffer",
        "api_signature": "torch.distributed.GradBucket.buffer(self: torch._C._distributed_c10d.GradBucket)",
        "api_description": "A flattened 1D torch.Tensor buffer,\nwhich can be further decomposed into a list of per-parameter tensors within this bucket.",
        "return_value": "A flattened 1D torch.Tensor buffer,\nwhich can be further decomposed into a list of per-parameter tensors within this bucket.\n",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.export.ExportedProgram.buffers",
        "api_url": "https://pytorch.org/docs/stable/export.html#torch.export.ExportedProgram.buffers",
        "api_signature": "buffers()",
        "api_description": "Returns an iterator over original module buffers.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.jit.ScriptModule.buffers",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.jit.ScriptModule.html#torch.jit.ScriptModule.buffers",
        "api_signature": "buffers(recurse=True)",
        "api_description": "Return an iterator over module buffers.",
        "return_value": "",
        "parameters": "recurse (bool) – if True, then yields buffers of this module\nand all submodules. Otherwise, yields only buffers that\nare direct members of this module.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.Module.buffers",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.buffers",
        "api_signature": "buffers(recurse=True)",
        "api_description": "Return an iterator over module buffers.",
        "return_value": "",
        "parameters": "recurse (bool) – if True, then yields buffers of this module\nand all submodules. Otherwise, yields only buffers that\nare direct members of this module.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.cpp_extension.BuildExtension",
        "api_url": "https://pytorch.org/docs/stable/cpp_extension.html#torch.utils.cpp_extension.BuildExtension",
        "api_signature": "torch.utils.cpp_extension.BuildExtension(*args, **kwargs)",
        "api_description": "A custom setuptools build extension .",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.byte",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.byte.html#torch.Tensor.byte",
        "api_signature": "Tensor.byte(memory_format=torch.preserve_format)",
        "api_description": "self.byte() is equivalent to self.to(torch.uint8). See to().",
        "return_value": "",
        "parameters": "memory_format (torch.memory_format, optional) – the desired memory format of\nreturned Tensor. Default: torch.preserve_format.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.TypedStorage.byte",
        "api_url": "https://pytorch.org/docs/stable/storage.html#torch.TypedStorage.byte",
        "api_signature": "byte()",
        "api_description": "Casts this storage to byte type.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.UntypedStorage.byte",
        "api_url": "https://pytorch.org/docs/stable/storage.html#torch.UntypedStorage.byte",
        "api_signature": "byte()",
        "api_description": "Casts this storage to byte type.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ByteStorage",
        "api_url": "https://pytorch.org/docs/stable/storage.html#torch.ByteStorage",
        "api_signature": "torch.ByteStorage(*args, wrap_storage=None, dtype=None, device=None, _internal=False)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.UntypedStorage.byteswap",
        "api_url": "https://pytorch.org/docs/stable/storage.html#torch.UntypedStorage.byteswap",
        "api_signature": "byteswap(dtype)",
        "api_description": "Swap bytes in underlying data.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.rendezvous.c10d_rendezvous_backend.C10dRendezvousBackend",
        "api_url": "https://pytorch.org/docs/stable/elastic/rendezvous.html#torch.distributed.elastic.rendezvous.c10d_rendezvous_backend.C10dRendezvousBackend",
        "api_signature": "torch.distributed.elastic.rendezvous.c10d_rendezvous_backend.C10dRendezvousBackend(store, run_id)",
        "api_description": "Represents a C10d-backed rendezvous backend.",
        "return_value": "",
        "parameters": "store (Store) – The torch.distributed.Store instance to use to\ncommunicate with the C10d store.\nrun_id (str) – The run id of the rendezvous.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.utils.parametrize.cached",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.utils.parametrize.cached.html#torch.nn.utils.parametrize.cached",
        "api_signature": "torch.nn.utils.parametrize.cached()",
        "api_description": "Context manager that enables the caching system within parametrizations registered with register_parametrization().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda.caching_allocator_alloc",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.cuda.caching_allocator_alloc.html#torch.cuda.caching_allocator_alloc",
        "api_signature": "torch.cuda.caching_allocator_alloc(size, device=None, stream=None)",
        "api_description": "Perform a memory allocation using the CUDA memory allocator.",
        "return_value": "",
        "parameters": "size (int) – number of bytes to be allocated.\ndevice (torch.device or int, optional) – selected device. If it is\nNone the default CUDA device is used.\nstream (torch.cuda.Stream or int, optional) – selected stream. If is None then\nthe default stream for the selected device is used.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda.caching_allocator_delete",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.cuda.caching_allocator_delete.html#torch.cuda.caching_allocator_delete",
        "api_signature": "torch.cuda.caching_allocator_delete(mem_ptr)",
        "api_description": "Delete memory allocated using the CUDA memory allocator.",
        "return_value": "",
        "parameters": "mem_ptr (int) – memory address to be freed by the allocator.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.init.calculate_gain",
        "api_url": "https://pytorch.org/docs/stable/nn.init.html#torch.nn.init.calculate_gain",
        "api_signature": "torch.nn.init.calculate_gain(nonlinearity, param=None)",
        "api_description": "Return the recommended gain value for the given nonlinearity function.",
        "return_value": "",
        "parameters": "nonlinearity – the non-linear function (nn.functional name)\nparam – optional parameter for the non-linear function",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.observer.MinMaxObserver.calculate_qparams",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.quantization.observer.MinMaxObserver.html#torch.ao.quantization.observer.MinMaxObserver.calculate_qparams",
        "api_signature": "calculate_qparams()",
        "api_description": "Calculates the quantization parameters.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.Graph.call_function",
        "api_url": "https://pytorch.org/docs/stable/fx.html#torch.fx.Graph.call_function",
        "api_signature": "call_function(the_function, args=None, kwargs=None, type_expr=None)",
        "api_description": "Insert a call_function Node into the Graph. A call_function node\nrepresents a call to a Python callable, specified by the_function.",
        "return_value": "The newly created and inserted call_function node.\n",
        "parameters": "operator, Python function, or member of the builtins or operator\nnamespaces.\nargs (Optional[Tuple[Argument, ...]]) – The positional arguments to be passed\nto the called function.\nkwargs (Optional[Dict[str, Argument]]) – The keyword arguments to be passed\nto the called function\ntype_expr (Optional[Any]) – an optional type annotation representing the\nPython type the output of this node will have.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.Interpreter.call_function",
        "api_url": "https://pytorch.org/docs/stable/fx.html#torch.fx.Interpreter.call_function",
        "api_signature": "call_function(target, args, kwargs)",
        "api_description": "Execute a call_function node and return the result.",
        "return_value": "",
        "parameters": "target (Target) – The call target for this node. See\nNode for\ndetails on semantics\nargs (Tuple) – Tuple of positional args for this invocation\nkwargs (Dict) – Dict of keyword arguments for this invocation",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.Transformer.call_function",
        "api_url": "https://pytorch.org/docs/stable/fx.html#torch.fx.Transformer.call_function",
        "api_signature": "call_function(target, args, kwargs)",
        "api_description": "Note",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.Graph.call_method",
        "api_url": "https://pytorch.org/docs/stable/fx.html#torch.fx.Graph.call_method",
        "api_signature": "call_method(method_name, args=None, kwargs=None, type_expr=None)",
        "api_description": "Insert a call_method Node into the Graph. A call_method node\nrepresents a call to a given method on the 0th element of args.",
        "return_value": "The newly created and inserted call_method node.\n",
        "parameters": "method_name (str) – The name of the method to apply to the self argument.\nFor example, if args[0] is a Node representing a Tensor,\nthen to call relu() on that Tensor, pass relu to method_name.\nargs (Optional[Tuple[Argument, ...]]) – The positional arguments to be passed\nto the called method. Note that this should include a self argument.\nkwargs (Optional[Dict[str, Argument]]) – The keyword arguments to be passed\nto the called method\ntype_expr (Optional[Any]) – an optional type annotation representing the\nPython type the output of this node will have.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.Interpreter.call_method",
        "api_url": "https://pytorch.org/docs/stable/fx.html#torch.fx.Interpreter.call_method",
        "api_signature": "call_method(target, args, kwargs)",
        "api_description": "Execute a call_method node and return the result.",
        "return_value": "",
        "parameters": "target (Target) – The call target for this node. See\nNode for\ndetails on semantics\nargs (Tuple) – Tuple of positional args for this invocation\nkwargs (Dict) – Dict of keyword arguments for this invocation",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.Graph.call_module",
        "api_url": "https://pytorch.org/docs/stable/fx.html#torch.fx.Graph.call_module",
        "api_signature": "call_module(module_name, args=None, kwargs=None, type_expr=None)",
        "api_description": "Insert a call_module Node into the Graph. A call_module node\nrepresents a call to the forward() function of a Module in the Module\nhierarchy.",
        "return_value": "The newly-created and inserted call_module node.\n",
        "parameters": "module_name (str) – The qualified name of the Module in the Module\nhierarchy to be called. For example, if the traced Module has a\nsubmodule named foo, which has a submodule named bar, the\nqualified name foo.bar should be passed as module_name to\ncall that module.\nargs (Optional[Tuple[Argument, ...]]) – The positional arguments to be passed\nto the called method. Note that this should not include a self argument.\nkwargs (Optional[Dict[str, Argument]]) – The keyword arguments to be passed\nto the called method\ntype_expr (Optional[Any]) – an optional type annotation representing the\nPython type the output of this node will have.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.Interpreter.call_module",
        "api_url": "https://pytorch.org/docs/stable/fx.html#torch.fx.Interpreter.call_module",
        "api_signature": "call_module(target, args, kwargs)",
        "api_description": "Execute a call_module node and return the result.",
        "return_value": "",
        "parameters": "target (Target) – The call target for this node. See\nNode for\ndetails on semantics\nargs (Tuple) – Tuple of positional args for this invocation\nkwargs (Dict) – Dict of keyword arguments for this invocation",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.Tracer.call_module",
        "api_url": "https://pytorch.org/docs/stable/fx.html#torch.fx.Tracer.call_module",
        "api_signature": "call_module(m, forward, args, kwargs)",
        "api_description": "Method that specifies the behavior of this Tracer when it encounters\na call to an nn.Module instance.",
        "return_value": "The return value from the Module call. In the case that a call_module\nnode was emitted, this is a Proxy value. Otherwise, it is whatever\nvalue was returned from the Module invocation.\n",
        "parameters": "m (Module) – The module for which a call is being emitted\nforward (Callable) – The forward() method of the Module to be invoked\nargs (Tuple) – args of the module callsite\nkwargs (Dict) – kwargs of the module callsite",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.Transformer.call_module",
        "api_url": "https://pytorch.org/docs/stable/fx.html#torch.fx.Transformer.call_module",
        "api_signature": "call_module(target, args, kwargs)",
        "api_description": "Note",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.benchmark.CallgrindStats",
        "api_url": "https://pytorch.org/docs/stable/benchmark_utils.html#torch.utils.benchmark.CallgrindStats",
        "api_signature": "torch.utils.benchmark.CallgrindStats(task_spec, number_per_run, built_with_debug_symbols, baseline_inclusive_stats, baseline_exclusive_stats, stmt_inclusive_stats, stmt_exclusive_stats, stmt_callgrind_out)",
        "api_description": "Top level container for Callgrind results collected by Timer.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.can_cast",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.can_cast.html#torch.can_cast",
        "api_signature": "torch.can_cast(from, to)",
        "api_description": "Determines if a type conversion is allowed under PyTorch casting rules\ndescribed in the type promotion documentation.",
        "return_value": "",
        "parameters": "from (dtype) – The original torch.dtype.\nto (dtype) – The target torch.dtype.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda.can_device_access_peer",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.cuda.can_device_access_peer.html#torch.cuda.can_device_access_peer",
        "api_signature": "torch.cuda.can_device_access_peer(device, peer_device)",
        "api_description": "Check if peer access between two devices is possible.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.backends.cuda.can_use_efficient_attention",
        "api_url": "https://pytorch.org/docs/stable/backends.html#torch.backends.cuda.can_use_efficient_attention",
        "api_signature": "torch.backends.cuda.can_use_efficient_attention(params, debug=False)",
        "api_description": "Check if efficient_attention can be utilized in scaled_dot_product_attention.",
        "return_value": "True if efficient_attention can be used with the given parameters; otherwise, False.\n",
        "parameters": "params (_SDPAParams) – An instance of SDPAParams containing the tensors for query,\nkey, value, an optional attention mask, dropout rate, and\na flag indicating if the attention is causal.\ndebug (bool) – Whether to logging.warn with information as to why efficient_attention could not be run.\nDefaults to False.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.backends.cuda.can_use_flash_attention",
        "api_url": "https://pytorch.org/docs/stable/backends.html#torch.backends.cuda.can_use_flash_attention",
        "api_signature": "torch.backends.cuda.can_use_flash_attention(params, debug=False)",
        "api_description": "Check if FlashAttention can be utilized in scaled_dot_product_attention.",
        "return_value": "True if FlashAttention can be used with the given parameters; otherwise, False.\n",
        "parameters": "params (_SDPAParams) – An instance of SDPAParams containing the tensors for query,\nkey, value, an optional attention mask, dropout rate, and\na flag indicating if the attention is causal.\ndebug (bool) – Whether to logging.warn debug information as to why FlashAttention could not be run.\nDefaults to False.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.symbolic_shapes.canonicalize_bool_expr",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.fx.experimental.symbolic_shapes.canonicalize_bool_expr.html#torch.fx.experimental.symbolic_shapes.canonicalize_bool_expr",
        "api_signature": "torch.fx.experimental.symbolic_shapes.canonicalize_bool_expr(expr)",
        "api_description": "Canonicalize a boolean expression by transforming it into a lt / le\ninequality and moving all the non-constant terms to the rhs.\nWe canonicalize And / Ors / Not via cnf and then canonicalize their subexpr\nrecursively\nnb. sympy.Rel.canonical is not good enough https://github.com/sympy/sympy/issues/25924",
        "return_value": "",
        "parameters": "expr (sympy.Expr) – Expression to canonicalize",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda.CUDAGraph.capture_begin",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.cuda.CUDAGraph.html#torch.cuda.CUDAGraph.capture_begin",
        "api_signature": "capture_begin(pool=None, capture_error_mode='global')",
        "api_description": "Begin capturing CUDA work on the current stream.",
        "return_value": "",
        "parameters": "pool (optional) – Token (returned by graph_pool_handle() or\nother_Graph_instance.pool()) that hints this graph may share memory\nwith the indicated pool.  See Graph memory management.\ncapture_error_mode (str, optional) – specifies the cudaStreamCaptureMode for the graph capture stream.\nCan be “global”, “thread_local” or “relaxed”. During cuda graph capture, some actions, such as cudaMalloc,\nmay be unsafe. “global” will error on actions in other threads, “thread_local” will only error for\nactions in the current thread, and “relaxed” will not error on these actions. Do NOT change this setting\nunless you’re familiar with cudaStreamCaptureMode",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda.CUDAGraph.capture_end",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.cuda.CUDAGraph.html#torch.cuda.CUDAGraph.capture_end",
        "api_signature": "capture_end()",
        "api_description": "End CUDA graph capture on the current stream.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cartesian_prod",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.cartesian_prod.html#torch.cartesian_prod",
        "api_signature": "torch.cartesian_prod(*tensors)",
        "api_description": "Do cartesian product of the given sequence of tensors. The behavior is similar to\npython’s itertools.product.",
        "return_value": "A tensor equivalent to converting all the input tensors into lists,\ndo itertools.product on these lists, and finally convert the resulting list\ninto tensor.\n",
        "parameters": "*tensors (Tensor) – any number of 1 dimensional tensors.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.constraints.cat",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.constraints.cat",
        "api_signature": null,
        "api_description": "alias of _Cat",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cat",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.cat.html#torch.cat",
        "api_signature": "torch.cat(tensors, dim=0, *, out=None)",
        "api_description": "Concatenates the given sequence of seq tensors in the given dimension.\nAll tensors must either have the same shape (except in the concatenating\ndimension) or be a 1-D empty tensor with size (0,).",
        "return_value": "",
        "parameters": "tensors (sequence of Tensors) – any python sequence of tensors of the same type.\nNon-empty tensors provided must have the same shape, except in the\ncat dimension.\ndim (int, optional) – the dimension over which the tensors are concatenated\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.ns._numeric_suite.Shadow.cat",
        "api_url": "https://pytorch.org/docs/stable/torch.ao.ns._numeric_suite.html#torch.ao.ns._numeric_suite.Shadow.cat",
        "api_signature": "cat(x, dim=0)",
        "api_description": "Tensor",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.categorical.Categorical",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.categorical.Categorical",
        "api_signature": "torch.distributions.categorical.Categorical(probs=None, logits=None, validate_args=None)",
        "api_description": "Bases: Distribution",
        "return_value": "",
        "parameters": "probs (Tensor) – event probabilities\nlogits (Tensor) – event log probabilities (unnormalized)",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.transforms.CatTransform",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.transforms.CatTransform",
        "api_signature": "torch.distributions.transforms.CatTransform(tseq, dim=0, lengths=None, cache_size=0)",
        "api_description": "Transform functor that applies a sequence of transforms tseq\ncomponent-wise to each submatrix at dim, of length lengths[dim],\nin a way compatible with torch.cat().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.cauchy.Cauchy",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.cauchy.Cauchy",
        "api_signature": "torch.distributions.cauchy.Cauchy(loc, scale, validate_args=None)",
        "api_description": "Bases: Distribution",
        "return_value": "",
        "parameters": "loc (float or Tensor) – mode or median of the distribution.\nscale (float or Tensor) – half width at half maximum.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.cauchy_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.cauchy_.html#torch.Tensor.cauchy_",
        "api_signature": "Tensor.cauchy_(median=0, sigma=1, *, generator=None)",
        "api_description": "Fills the tensor with numbers drawn from the Cauchy distribution:",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.attention.bias.causal_lower_right",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.attention.bias.causal_lower_right.html#torch.nn.attention.bias.causal_lower_right",
        "api_signature": "torch.nn.attention.bias.causal_lower_right(*size)",
        "api_description": "Creates a lower-right triangular causal bias.",
        "return_value": "The LOWER_RIGHT triangular causal bias variant.\n",
        "parameters": "size – The size of the bias matrix.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.attention.bias.causal_upper_left",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.attention.bias.causal_upper_left.html#torch.nn.attention.bias.causal_upper_left",
        "api_signature": "torch.nn.attention.bias.causal_upper_left(*size)",
        "api_description": "Creates an upper-left triangular causal bias.",
        "return_value": "The UPPER_LEFT triangular causal bias variant.\n",
        "parameters": "size – The size of the bias matrix.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.attention.bias.CausalBias",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.attention.bias.CausalBias.html#torch.nn.attention.bias.CausalBias",
        "api_signature": "torch.nn.attention.bias.CausalBias(variant, seq_len_q, seq_len_kv)",
        "api_description": "A bias representing causal attention patterns. For an overview of the bias structure, see the CausalVariant enum.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.attention.bias.CausalVariant",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.attention.bias.CausalVariant.html#torch.nn.attention.bias.CausalVariant",
        "api_signature": "torch.nn.attention.bias.CausalVariant(value)",
        "api_description": "Enum for causal variants used in attention mechanisms.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.ccol_indices",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.ccol_indices.html#torch.Tensor.ccol_indices",
        "api_signature": "Tensor.ccol_indices()",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.cauchy.Cauchy.cdf",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.cauchy.Cauchy.cdf",
        "api_signature": "cdf(value)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.continuous_bernoulli.ContinuousBernoulli.cdf",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.continuous_bernoulli.ContinuousBernoulli.cdf",
        "api_signature": "cdf(value)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.distribution.Distribution.cdf",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.distribution.Distribution.cdf",
        "api_signature": "cdf(value)",
        "api_description": "Returns the cumulative density/mass function evaluated at\nvalue.",
        "return_value": "",
        "parameters": "value (Tensor) –",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.exponential.Exponential.cdf",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.exponential.Exponential.cdf",
        "api_signature": "cdf(value)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.gamma.Gamma.cdf",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.gamma.Gamma.cdf",
        "api_signature": "cdf(value)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.half_cauchy.HalfCauchy.cdf",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.half_cauchy.HalfCauchy.cdf",
        "api_signature": "cdf(value)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.half_normal.HalfNormal.cdf",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.half_normal.HalfNormal.cdf",
        "api_signature": "cdf(value)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.laplace.Laplace.cdf",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.laplace.Laplace.cdf",
        "api_signature": "cdf(value)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.mixture_same_family.MixtureSameFamily.cdf",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.mixture_same_family.MixtureSameFamily.cdf",
        "api_signature": "cdf(x)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.normal.Normal.cdf",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.normal.Normal.cdf",
        "api_signature": "cdf(value)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.transformed_distribution.TransformedDistribution.cdf",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.transformed_distribution.TransformedDistribution.cdf",
        "api_signature": "cdf(value)",
        "api_description": "Computes the cumulative distribution function by inverting the\ntransform(s) and computing the score of the base distribution.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.uniform.Uniform.cdf",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.uniform.Uniform.cdf",
        "api_signature": "cdf(value)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cdist",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.cdist.html#torch.cdist",
        "api_signature": "torch.cdist(x1, x2, p=2.0, compute_mode='use_mm_for_euclid_dist_if_necessary')",
        "api_description": "Computes batched the p-norm distance between each pair of the two collections of row vectors.",
        "return_value": "",
        "parameters": "x1 (Tensor) – input tensor of shape B×P×MB \\times P \\times MB×P×M.\nx2 (Tensor) – input tensor of shape B×R×MB \\times R \\times MB×R×M.\np (float) – p value for the p-norm distance to calculate between each vector pair\n∈[0,∞]\\in [0, \\infty]∈[0,∞].\ncompute_mode (str) – ‘use_mm_for_euclid_dist_if_necessary’ - will use matrix multiplication approach to calculate\neuclidean distance (p = 2) if P > 25 or R > 25\n‘use_mm_for_euclid_dist’ - will always use matrix multiplication approach to calculate\neuclidean distance (p = 2)\n‘donot_use_mm_for_euclid_dist’ - will never use matrix multiplication approach to calculate\neuclidean distance (p = 2)\nDefault: use_mm_for_euclid_dist_if_necessary.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.cdouble",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.cdouble.html#torch.Tensor.cdouble",
        "api_signature": "Tensor.cdouble(memory_format=torch.preserve_format)",
        "api_description": "self.cdouble() is equivalent to self.to(torch.complex128). See to().",
        "return_value": "",
        "parameters": "memory_format (torch.memory_format, optional) – the desired memory format of\nreturned Tensor. Default: torch.preserve_format.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ceil",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ceil.html#torch.ceil",
        "api_signature": "torch.ceil(input, *, out=None)",
        "api_description": "Returns a new tensor with the ceil of the elements of input,\nthe smallest integer greater than or equal to each element.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.ceil",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.ceil.html#torch.Tensor.ceil",
        "api_signature": "Tensor.ceil()",
        "api_description": "See torch.ceil()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.ceil_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.ceil_.html#torch.Tensor.ceil_",
        "api_signature": "Tensor.ceil_()",
        "api_description": "In-place version of ceil()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.quantized.functional.celu",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.nn.quantized.functional.celu.html#torch.ao.nn.quantized.functional.celu",
        "api_signature": "torch.ao.nn.quantized.functional.celu(input, scale, zero_point, alpha=1.)",
        "api_description": "Applies the quantized CELU function element-wise.",
        "return_value": "",
        "parameters": "input (Tensor) – quantized input\nalpha (float) – the α\\alphaα value for the CELU formulation. Default: 1.0",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.CELU",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.CELU.html#torch.nn.CELU",
        "api_signature": "torch.nn.CELU(alpha=1.0, inplace=False)",
        "api_description": "Applies the CELU function element-wise.",
        "return_value": "",
        "parameters": "alpha (float) – the α\\alphaα value for the CELU formulation. Default: 1.0\ninplace (bool) – can optionally do the operation in-place. Default: False",
        "input_shape": "\nInput: (∗)(*)(∗), where ∗*∗ means any number of dimensions.\nOutput: (∗)(*)(∗), same shape as the input.\n\n",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.functional.celu",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.functional.celu.html#torch.nn.functional.celu",
        "api_signature": "torch.nn.functional.celu(input, alpha=1., inplace=False)",
        "api_description": "Applies element-wise,\nCELU(x)=max⁡(0,x)+min⁡(0,α∗(exp⁡(x/α)−1))\\text{CELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x/\\alpha) - 1))CELU(x)=max(0,x)+min(0,α∗(exp(x/α)−1)).",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.cfloat",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.cfloat.html#torch.Tensor.cfloat",
        "api_signature": "Tensor.cfloat(memory_format=torch.preserve_format)",
        "api_description": "self.cfloat() is equivalent to self.to(torch.complex64). See to().",
        "return_value": "",
        "parameters": "memory_format (torch.memory_format, optional) – the desired memory format of\nreturned Tensor. Default: torch.preserve_format.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.chain_matmul",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.chain_matmul.html#torch.chain_matmul",
        "api_signature": "torch.chain_matmul(*matrices, out=None)",
        "api_description": "Returns the matrix product of the NNN 2-D tensors. This product is efficiently computed\nusing the matrix chain order algorithm which selects the order in which incurs the lowest cost in terms\nof arithmetic operations ([CLRS]). Note that since this is a function to compute the product, NNN\nneeds to be greater than or equal to 2; if equal to 2 then a trivial matrix-matrix product is returned.\nIf NNN is 1, then this is a no-op - the original matrix is returned as is.",
        "return_value": "if the ithi^{th}ith tensor was of dimensions pi×pi+1p_{i} \\times p_{i + 1}pi​×pi+1​, then the product\nwould be of dimensions p1×pN+1p_{1} \\times p_{N + 1}p1​×pN+1​.\n",
        "parameters": "matrices (Tensors...) – a sequence of 2 or more 2-D tensors whose product is to be determined.\nout (Tensor, optional) – the output tensor. Ignored if out = None.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.data.ChainDataset",
        "api_url": "https://pytorch.org/docs/stable/data.html#torch.utils.data.ChainDataset",
        "api_signature": "torch.utils.data.ChainDataset(datasets)",
        "api_description": "Dataset for chaining multiple IterableDataset s.",
        "return_value": "",
        "parameters": "datasets (iterable of IterableDataset) – datasets to be chained together",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.lr_scheduler.ChainedScheduler",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.ChainedScheduler.html#torch.optim.lr_scheduler.ChainedScheduler",
        "api_signature": "torch.optim.lr_scheduler.ChainedScheduler(schedulers)",
        "api_description": "Chains list of learning rate schedulers. It takes a list of chainable learning\nrate schedulers and performs consecutive step() functions belonging to them by just\none call.",
        "return_value": "",
        "parameters": "schedulers (list) – List of chained schedulers.\nstate_dict (dict) – scheduler state. Should be an object returned\nfrom a call to state_dict().",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.chalf",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.chalf.html#torch.Tensor.chalf",
        "api_signature": "Tensor.chalf(memory_format=torch.preserve_format)",
        "api_description": "self.chalf() is equivalent to self.to(torch.complex32). See to().",
        "return_value": "",
        "parameters": "memory_format (torch.memory_format, optional) – the desired memory format of\nreturned Tensor. Default: torch.preserve_format.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda.change_current_allocator",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.cuda.change_current_allocator.html#torch.cuda.change_current_allocator",
        "api_signature": "torch.cuda.change_current_allocator(allocator)",
        "api_description": "Change the currently used memory allocator to be the one provided.",
        "return_value": "",
        "parameters": "allocator (torch.cuda.memory._CUDAAllocator) – allocator to be set as the active one.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.ChannelShuffle",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.ChannelShuffle.html#torch.nn.ChannelShuffle",
        "api_signature": "torch.nn.ChannelShuffle(groups)",
        "api_description": "Divides and rearranges the channels in a tensor.",
        "return_value": "",
        "parameters": "groups (int) – number of groups to divide channels in.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.char",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.char.html#torch.Tensor.char",
        "api_signature": "Tensor.char(memory_format=torch.preserve_format)",
        "api_description": "self.char() is equivalent to self.to(torch.int8). See to().",
        "return_value": "",
        "parameters": "memory_format (torch.memory_format, optional) – the desired memory format of\nreturned Tensor. Default: torch.preserve_format.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.TypedStorage.char",
        "api_url": "https://pytorch.org/docs/stable/storage.html#torch.TypedStorage.char",
        "api_signature": "char()",
        "api_description": "Casts this storage to char type.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.UntypedStorage.char",
        "api_url": "https://pytorch.org/docs/stable/storage.html#torch.UntypedStorage.char",
        "api_signature": "char()",
        "api_description": "Casts this storage to char type.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.CharStorage",
        "api_url": "https://pytorch.org/docs/stable/storage.html#torch.CharStorage",
        "api_signature": "torch.CharStorage(*args, wrap_storage=None, dtype=None, device=None, _internal=False)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.rendezvous.etcd_store.EtcdStore.check",
        "api_url": "https://pytorch.org/docs/stable/elastic/rendezvous.html#torch.distributed.elastic.rendezvous.etcd_store.EtcdStore.check",
        "api_signature": "check(keys)",
        "api_description": "Check if all of the keys are immediately present (without waiting).",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.constraints.Constraint.check",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.constraints.Constraint.check",
        "api_signature": "check(value)",
        "api_description": "Returns a byte tensor of sample_shape + batch_shape indicating\nwhether each event in value satisfies this constraint.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.symbolic_shapes.ShapeEnv.check_equal",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.fx.experimental.symbolic_shapes.ShapeEnv.html#torch.fx.experimental.symbolic_shapes.ShapeEnv.check_equal",
        "api_signature": "check_equal(other)",
        "api_description": "Compare another ShapeEnv for equivalence",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.fsdp.FullyShardedDataParallel.check_is_root",
        "api_url": "https://pytorch.org/docs/stable/fsdp.html#torch.distributed.fsdp.FullyShardedDataParallel.check_is_root",
        "api_signature": "check_is_root()",
        "api_description": "Check if this instance is a root FSDP module.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.sparse.check_sparse_tensor_invariants",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.sparse.check_sparse_tensor_invariants.html#torch.sparse.check_sparse_tensor_invariants",
        "api_signature": "torch.sparse.check_sparse_tensor_invariants(enable=True)",
        "api_description": "A tool to control checking sparse tensor invariants.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.checkpoint.checkpoint",
        "api_url": "https://pytorch.org/docs/stable/checkpoint.html#torch.utils.checkpoint.checkpoint",
        "api_signature": "torch.utils.checkpoint.checkpoint(function, *args, use_reentrant=None, context_fn=<function noop_context_fn>, determinism_check='default', debug=False, **kwargs)",
        "api_description": "Checkpoint a model or part of the model.",
        "return_value": "Output of running function on *args\n",
        "parameters": "function – describes what to run in the forward pass of the model or\npart of the model. It should also know how to handle the inputs\npassed as the tuple. For example, in LSTM, if user passes\n(activation, hidden), function should correctly use the\nfirst input as activation and the second input as hidden\npreserve_rng_state (bool, optional) – Omit stashing and restoring\nthe RNG state during each checkpoint. Note that under torch.compile,\nthis flag doesn’t take effect and we always preserve RNG state.\nDefault: True\nuse_reentrant (bool) – specify whether to use the activation checkpoint variant that\nrequires reentrant autograd. This parameter should be passed\nexplicitly. In version 2.4 we will raise an exception if\nuse_reentrant is not passed. If use_reentrant=False,\ncheckpoint will use an implementation that does not require\nreentrant autograd. This allows checkpoint to support additional\nfunctionality, such as working as expected with\ntorch.autograd.grad and support for keyword arguments input into\nthe checkpointed function.\ncontext_fn (Callable, optional) – A callable returning a tuple of two\ncontext managers. The function and its recomputation will be run\nunder the first and second context managers respectively.\nThis argument is only supported if use_reentrant=False.\ndeterminism_check (str, optional) – A string specifying the determinism\ncheck to perform. By default it is set to \"default\" which\ncompares the shapes, dtypes, and devices of the recomputed tensors\nagainst those the saved tensors. To turn off this check, specify\n\"none\". Currently these are the only two supported values.\nPlease open an issue if you would like to see more determinism\nchecks. This argument is only supported if use_reentrant=False,\nif use_reentrant=True, the determinism check is always disabled.\ndebug (bool, optional) – If True, error messages will also include\na trace of the operators ran during the original forward computation\nas well as the recomputation. This argument is only supported if\nuse_reentrant=False.\nargs – tuple containing inputs to the function",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.checkpoint.checkpoint_sequential",
        "api_url": "https://pytorch.org/docs/stable/checkpoint.html#torch.utils.checkpoint.checkpoint_sequential",
        "api_signature": "torch.utils.checkpoint.checkpoint_sequential(functions, segments, input, use_reentrant=None, **kwargs)",
        "api_description": "Checkpoint a sequential model to save memory.",
        "return_value": "Output of running functions sequentially on *inputs\n",
        "parameters": "functions – A torch.nn.Sequential or the list of modules or\nfunctions (comprising the model) to run sequentially.\nsegments – Number of chunks to create in the model\ninput – A Tensor that is input to functions\npreserve_rng_state (bool, optional) – Omit stashing and restoring\nthe RNG state during each checkpoint.\nDefault: True\nuse_reentrant (bool) – specify whether to use the activation checkpoint variant that\nrequires reentrant autograd. This parameter should be passed\nexplicitly. In version 2.4 we will raise an exception if\nuse_reentrant is not passed. If use_reentrant=False,\ncheckpoint will use an implementation that does not require\nreentrant autograd. This allows checkpoint to support additional\nfunctionality, such as working as expected with\ntorch.autograd.grad and support for keyword arguments input into\nthe checkpointed function.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.chi2.Chi2",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.chi2.Chi2",
        "api_signature": "torch.distributions.chi2.Chi2(df, validate_args=None)",
        "api_description": "Bases: Gamma",
        "return_value": "",
        "parameters": "df (float or Tensor) – shape parameter of the distribution",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.multiprocessing.errors.ChildFailedError",
        "api_url": "https://pytorch.org/docs/stable/elastic/errors.html#torch.distributed.elastic.multiprocessing.errors.ChildFailedError",
        "api_signature": "torch.distributed.elastic.multiprocessing.errors.ChildFailedError(name, failures)",
        "api_description": "Special exception type that can be raised from a function annotated with the\n@record decorator to have the child process’ (root exception) propagate\nup the stack as-is (e.g. without being wrapped in the parent’s traceback).",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.jit.ScriptModule.children",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.jit.ScriptModule.html#torch.jit.ScriptModule.children",
        "api_signature": "children()",
        "api_description": "Return an iterator over immediate children modules.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.Module.children",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.children",
        "api_signature": "children()",
        "api_description": "Return an iterator over immediate children modules.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cholesky",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.cholesky.html#torch.cholesky",
        "api_signature": "torch.cholesky(input, upper=False, *, out=None)",
        "api_description": "Computes the Cholesky decomposition of a symmetric positive-definite\nmatrix AAA or for batches of symmetric positive-definite matrices.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor AAA of size (∗,n,n)(*, n, n)(∗,n,n) where * is zero or more\nbatch dimensions consisting of symmetric positive-definite matrices.\nupper (bool, optional) – flag that indicates whether to return a\nupper or lower triangular matrix. Default: False\nout (Tensor, optional) – the output matrix",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.linalg.cholesky",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.linalg.cholesky.html#torch.linalg.cholesky",
        "api_signature": "torch.linalg.cholesky(A, *, upper=False, out=None)",
        "api_description": "Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.",
        "return_value": "",
        "parameters": "A (Tensor) – tensor of shape (*, n, n) where * is zero or more batch dimensions\nconsisting of symmetric or Hermitian positive-definite matrices.\nupper (bool, optional) – whether to return an upper triangular matrix.\nThe tensor returned with upper=True is the conjugate transpose of the tensor\nreturned with upper=False.\nout (Tensor, optional) – output tensor. Ignored if None. Default: None.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.cholesky",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.cholesky.html#torch.Tensor.cholesky",
        "api_signature": "Tensor.cholesky(upper=False)",
        "api_description": "See torch.cholesky()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.linalg.cholesky_ex",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.linalg.cholesky_ex.html#torch.linalg.cholesky_ex",
        "api_signature": "torch.linalg.cholesky_ex(A, *, upper=False, check_errors=False, out=None)",
        "api_description": "Computes the Cholesky decomposition of a complex Hermitian or real\nsymmetric positive-definite matrix.",
        "return_value": "",
        "parameters": "A (Tensor) – the Hermitian n times n matrix or the batch of such matrices of size\n(*, n, n) where * is one or more batch dimensions.\nupper (bool, optional) – whether to return an upper triangular matrix.\nThe tensor returned with upper=True is the conjugate transpose of the tensor\nreturned with upper=False.\ncheck_errors (bool, optional) – controls whether to check the content of infos. Default: False.\nout (tuple, optional) – tuple of two tensors to write the output to. Ignored if None. Default: None.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cholesky_inverse",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.cholesky_inverse.html#torch.cholesky_inverse",
        "api_signature": "torch.cholesky_inverse(L, upper=False, *, out=None)",
        "api_description": "Computes the inverse of a complex Hermitian or real symmetric\npositive-definite matrix given its Cholesky decomposition.",
        "return_value": "",
        "parameters": "L (Tensor) – tensor of shape (*, n, n) where * is zero or more batch dimensions\nconsisting of lower or upper triangular Cholesky decompositions of\nsymmetric or Hermitian positive-definite matrices.\nupper (bool, optional) – flag that indicates whether LLL is lower triangular\nor upper triangular. Default: False\nout (Tensor, optional) – output tensor. Ignored if None. Default: None.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.cholesky_inverse",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.cholesky_inverse.html#torch.Tensor.cholesky_inverse",
        "api_signature": "Tensor.cholesky_inverse(upper=False)",
        "api_description": "See torch.cholesky_inverse()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cholesky_solve",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.cholesky_solve.html#torch.cholesky_solve",
        "api_signature": "torch.cholesky_solve(B, L, upper=False, *, out=None)",
        "api_description": "Computes the solution of a system of linear equations with complex Hermitian\nor real symmetric positive-definite lhs given its Cholesky decomposition.",
        "return_value": "",
        "parameters": "B (Tensor) – right-hand side tensor of shape (*, n, k)\nwhere ∗*∗ is zero or more batch dimensions\nL (Tensor) – tensor of shape (*, n, n) where * is zero or more batch dimensions\nconsisting of lower or upper triangular Cholesky decompositions of\nsymmetric or Hermitian positive-definite matrices.\nupper (bool, optional) – flag that indicates whether LLL is lower triangular\nor upper triangular. Default: False.\nout (Tensor, optional) – output tensor. Ignored if None. Default: None.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.cholesky_solve",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.cholesky_solve.html#torch.Tensor.cholesky_solve",
        "api_signature": "Tensor.cholesky_solve(input2, upper=False)",
        "api_description": "See torch.cholesky_solve()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.chunk",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.chunk.html#torch.chunk",
        "api_signature": "torch.chunk(input, chunks, dim=0)",
        "api_description": "Attempts to split a tensor into the specified number of chunks. Each chunk is a view of\nthe input tensor.",
        "return_value": "",
        "parameters": "input (Tensor) – the tensor to split\nchunks (int) – number of chunks to return\ndim (int) – dimension along which to split the tensor",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.chunk",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.chunk.html#torch.Tensor.chunk",
        "api_signature": "Tensor.chunk(chunks, dim=0)",
        "api_description": "See torch.chunk()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.CircularPad1d",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.CircularPad1d.html#torch.nn.CircularPad1d",
        "api_signature": "torch.nn.CircularPad1d(padding)",
        "api_description": "Pads the input tensor using circular padding of the input boundary.",
        "return_value": "",
        "parameters": "padding (int, tuple) – the size of the padding. If is int, uses the same\npadding in all boundaries. If a 2-tuple, uses\n(padding_left\\text{padding\\_left}padding_left, padding_right\\text{padding\\_right}padding_right)",
        "input_shape": "\nInput: (C,Win)(C, W_{in})(C,Win​) or (N,C,Win)(N, C, W_{in})(N,C,Win​).\nOutput: (C,Wout)(C, W_{out})(C,Wout​) or (N,C,Wout)(N, C, W_{out})(N,C,Wout​), where\nWout=Win+padding_left+padding_rightW_{out} = W_{in} + \\text{padding\\_left} + \\text{padding\\_right}Wout​=Win​+padding_left+padding_right\n\n\n",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.CircularPad2d",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.CircularPad2d.html#torch.nn.CircularPad2d",
        "api_signature": "torch.nn.CircularPad2d(padding)",
        "api_description": "Pads the input tensor using circular padding of the input boundary.",
        "return_value": "",
        "parameters": "padding (int, tuple) – the size of the padding. If is int, uses the same\npadding in all boundaries. If a 4-tuple, uses (padding_left\\text{padding\\_left}padding_left,\npadding_right\\text{padding\\_right}padding_right, padding_top\\text{padding\\_top}padding_top, padding_bottom\\text{padding\\_bottom}padding_bottom)",
        "input_shape": "\nInput: (N,C,Hin,Win)(N, C, H_{in}, W_{in})(N,C,Hin​,Win​) or (C,Hin,Win)(C, H_{in}, W_{in})(C,Hin​,Win​).\nOutput: (N,C,Hout,Wout)(N, C, H_{out}, W_{out})(N,C,Hout​,Wout​) or (C,Hout,Wout)(C, H_{out}, W_{out})(C,Hout​,Wout​), where\nHout=Hin+padding_top+padding_bottomH_{out} = H_{in} + \\text{padding\\_top} + \\text{padding\\_bottom}Hout​=Hin​+padding_top+padding_bottom\nWout=Win+padding_left+padding_rightW_{out} = W_{in} + \\text{padding\\_left} + \\text{padding\\_right}Wout​=Win​+padding_left+padding_right\n\n\n",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.CircularPad3d",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.CircularPad3d.html#torch.nn.CircularPad3d",
        "api_signature": "torch.nn.CircularPad3d(padding)",
        "api_description": "Pads the input tensor using circular padding of the input boundary.",
        "return_value": "",
        "parameters": "padding (int, tuple) – the size of the padding. If is int, uses the same\npadding in all boundaries. If a 6-tuple, uses\n(padding_left\\text{padding\\_left}padding_left, padding_right\\text{padding\\_right}padding_right,\npadding_top\\text{padding\\_top}padding_top, padding_bottom\\text{padding\\_bottom}padding_bottom,\npadding_front\\text{padding\\_front}padding_front, padding_back\\text{padding\\_back}padding_back)",
        "input_shape": "\nInput: (N,C,Din,Hin,Win)(N, C, D_{in}, H_{in}, W_{in})(N,C,Din​,Hin​,Win​) or (C,Din,Hin,Win)(C, D_{in}, H_{in}, W_{in})(C,Din​,Hin​,Win​).\nOutput: (N,C,Dout,Hout,Wout)(N, C, D_{out}, H_{out}, W_{out})(N,C,Dout​,Hout​,Wout​) or (C,Dout,Hout,Wout)(C, D_{out}, H_{out}, W_{out})(C,Dout​,Hout​,Wout​),\nwhere\nDout=Din+padding_front+padding_backD_{out} = D_{in} + \\text{padding\\_front} + \\text{padding\\_back}Dout​=Din​+padding_front+padding_back\nHout=Hin+padding_top+padding_bottomH_{out} = H_{in} + \\text{padding\\_top} + \\text{padding\\_bottom}Hout​=Hin​+padding_top+padding_bottom\nWout=Win+padding_left+padding_rightW_{out} = W_{in} + \\text{padding\\_left} + \\text{padding\\_right}Wout​=Win​+padding_left+padding_right\n\n\n",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.quantized.functional.clamp",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.nn.quantized.functional.clamp.html#torch.ao.nn.quantized.functional.clamp",
        "api_signature": "torch.ao.nn.quantized.functional.clamp(input, min_, max_)",
        "api_description": "float(input, min_, max_) -> Tensor",
        "return_value": "",
        "parameters": "input (Tensor) – quantized input\nmin – minimum value for clamping\nmax – maximum value for clamping",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.clamp",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.clamp.html#torch.clamp",
        "api_signature": "torch.clamp(input, min=None, max=None, *, out=None)",
        "api_description": "Clamps all elements in input into the range [ min, max ].\nLetting min_value and max_value be min and max, respectively, this returns:",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.\nmin (Number or Tensor, optional) – lower-bound of the range to be clamped to\nmax (Number or Tensor, optional) – upper-bound of the range to be clamped to\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.clamp",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.clamp.html#torch.Tensor.clamp",
        "api_signature": "Tensor.clamp(min=None, max=None)",
        "api_description": "See torch.clamp()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.clamp_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.clamp_.html#torch.Tensor.clamp_",
        "api_signature": "Tensor.clamp_(min=None, max=None)",
        "api_description": "In-place version of clamp()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.symbolic_shapes.ShapeEnv.cleanup",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.fx.experimental.symbolic_shapes.ShapeEnv.html#torch.fx.experimental.symbolic_shapes.ShapeEnv.cleanup",
        "api_signature": "cleanup()",
        "api_description": "Break reference cycles.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.backends.cuda.cufft_plan_cache.clear",
        "api_url": "https://pytorch.org/docs/stable/backends.html#torch.backends.cuda.cufft_plan_cache.clear",
        "api_signature": "torch.backends.cuda.cufft_plan_cache.clear()",
        "api_description": "Clears a cuFFT plan cache.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.autograd.profiler_util.StringTable.clear",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.autograd.profiler_util.StringTable.html#torch.autograd.profiler_util.StringTable.clear",
        "api_signature": "clear()",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.ModuleDict.clear",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.ModuleDict.html#torch.nn.ModuleDict.clear",
        "api_signature": "clear()",
        "api_description": "Remove all items from the ModuleDict.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.ParameterDict.clear",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.ParameterDict.html#torch.nn.ParameterDict.clear",
        "api_signature": "clear()",
        "api_description": "Remove all items from the ParameterDict.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.onnx.verification.GraphInfo.clear",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.onnx.verification.GraphInfo.html#torch.onnx.verification.GraphInfo.clear",
        "api_signature": "clear()",
        "api_description": "Clear states and results of previous verification.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.timer.TimerServer.clear_timers",
        "api_url": "https://pytorch.org/docs/stable/elastic/timer.html#torch.distributed.elastic.timer.TimerServer.clear_timers",
        "api_signature": "clear_timers(worker_ids)",
        "api_description": "Clears all timers for the given worker_ids.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.clip",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.clip.html#torch.clip",
        "api_signature": "torch.clip(input, min=None, max=None, *, out=None)",
        "api_description": "Alias for torch.clamp().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.clip",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.clip.html#torch.Tensor.clip",
        "api_signature": "Tensor.clip(min=None, max=None)",
        "api_description": "Alias for clamp().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.clip_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.clip_.html#torch.Tensor.clip_",
        "api_signature": "Tensor.clip_(min=None, max=None)",
        "api_description": "Alias for clamp_().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.utils.clip_grad_norm",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_norm.html#torch.nn.utils.clip_grad_norm",
        "api_signature": "torch.nn.utils.clip_grad_norm(parameters, max_norm, norm_type=2.0, error_if_nonfinite=False, foreach=None)",
        "api_description": "Clip the gradient norm of an iterable of parameters.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.utils.clip_grad_norm_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_norm_.html#torch.nn.utils.clip_grad_norm_",
        "api_signature": "torch.nn.utils.clip_grad_norm_(parameters, max_norm, norm_type=2.0, error_if_nonfinite=False, foreach=None)",
        "api_description": "Clip the gradient norm of an iterable of parameters.",
        "return_value": "Total norm of the parameter gradients (viewed as a single vector).\n",
        "parameters": "parameters (Iterable[Tensor] or Tensor) – an iterable of Tensors or a\nsingle Tensor that will have gradients normalized\nmax_norm (float) – max norm of the gradients\nnorm_type (float) – type of the used p-norm. Can be 'inf' for\ninfinity norm.\nerror_if_nonfinite (bool) – if True, an error is thrown if the total\nnorm of the gradients from parameters is nan,\ninf, or -inf. Default: False (will switch to True in the future)\nforeach (bool) – use the faster foreach-based implementation.\nIf None, use the foreach implementation for CUDA and CPU native tensors and silently\nfall back to the slow implementation for other device types.\nDefault: None",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.fsdp.FullyShardedDataParallel.clip_grad_norm_",
        "api_url": "https://pytorch.org/docs/stable/fsdp.html#torch.distributed.fsdp.FullyShardedDataParallel.clip_grad_norm_",
        "api_signature": "clip_grad_norm_(max_norm, norm_type=2.0)",
        "api_description": "Clip the gradient norm of all parameters.",
        "return_value": "Total norm of the parameters (viewed as a single vector).\n",
        "parameters": "max_norm (float or int) – max norm of the gradients\nnorm_type (float or int) – type of the used p-norm. Can be 'inf'\nfor infinity norm.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.utils.clip_grad_value_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_value_.html#torch.nn.utils.clip_grad_value_",
        "api_signature": "torch.nn.utils.clip_grad_value_(parameters, clip_value, foreach=None)",
        "api_description": "Clip the gradients of an iterable of parameters at specified value.",
        "return_value": "",
        "parameters": "parameters (Iterable[Tensor] or Tensor) – an iterable of Tensors or a\nsingle Tensor that will have gradients normalized\nclip_value (float) – maximum allowed value of the gradients.\nThe gradients are clipped in the range\n[-clip_value,clip_value]\\left[\\text{-clip\\_value}, \\text{clip\\_value}\\right][-clip_value,clip_value]\nforeach (bool) – use the faster foreach-based implementation\nIf None, use the foreach implementation for CUDA and CPU native tensors and\nsilently fall back to the slow implementation for other device types.\nDefault: None",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda.clock_rate",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.cuda.clock_rate.html#torch.cuda.clock_rate",
        "api_signature": "torch.cuda.clock_rate(device=None)",
        "api_description": "Return the clock speed of the GPU SM in Hz Hertz over the past sample period as given by nvidia-smi.",
        "return_value": "",
        "parameters": "device (torch.device or int, optional) – selected device. Returns\nstatistic for the current device, given by current_device(),\nif device is None (default).",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.clone",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.clone.html#torch.clone",
        "api_signature": "torch.clone(input, *, memory_format=torch.preserve_format)",
        "api_description": "Returns a copy of input.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.\nmemory_format (torch.memory_format, optional) – the desired memory format of\nreturned tensor. Default: torch.preserve_format.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.autograd.grad_mode.inference_mode.clone",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.autograd.grad_mode.inference_mode.html#torch.autograd.grad_mode.inference_mode.clone",
        "api_signature": "clone()",
        "api_description": "Create a copy of this class",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.autograd.grad_mode.set_grad_enabled.clone",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.autograd.grad_mode.set_grad_enabled.html#torch.autograd.grad_mode.set_grad_enabled.clone",
        "api_signature": "clone()",
        "api_description": "Create a copy of this class",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.autograd.grad_mode.set_multithreading_enabled.clone",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.autograd.grad_mode.set_multithreading_enabled.html#torch.autograd.grad_mode.set_multithreading_enabled.clone",
        "api_signature": "clone()",
        "api_description": "Create a copy of this class",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.clone",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.clone.html#torch.Tensor.clone",
        "api_signature": "Tensor.clone(*, memory_format=torch.preserve_format)",
        "api_description": "See torch.clone()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.TypedStorage.clone",
        "api_url": "https://pytorch.org/docs/stable/storage.html#torch.TypedStorage.clone",
        "api_signature": "clone()",
        "api_description": "Return a copy of this storage.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.UntypedStorage.clone",
        "api_url": "https://pytorch.org/docs/stable/storage.html#torch.UntypedStorage.clone",
        "api_signature": "clone()",
        "api_description": "Return a copy of this storage.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.rendezvous.dynamic_rendezvous.RendezvousTimeout.close",
        "api_url": "https://pytorch.org/docs/stable/elastic/rendezvous.html#torch.distributed.elastic.rendezvous.dynamic_rendezvous.RendezvousTimeout.close",
        "api_signature": null,
        "api_description": "Get the close timeout.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.package.PackageExporter.close",
        "api_url": "https://pytorch.org/docs/stable/package.html#torch.package.PackageExporter.close",
        "api_signature": "close()",
        "api_description": "Write the package to the filesystem. Any calls after close() are now invalid.\nIt is preferable to use resource guard syntax instead:",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.tensorboard.writer.SummaryWriter.close",
        "api_url": "https://pytorch.org/docs/stable/tensorboard.html#torch.utils.tensorboard.writer.SummaryWriter.close",
        "api_signature": "close()",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.LazyBatchNorm1d.cls_to_become",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.LazyBatchNorm1d.html#torch.nn.LazyBatchNorm1d.cls_to_become",
        "api_signature": null,
        "api_description": "alias of BatchNorm1d",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.LazyBatchNorm2d.cls_to_become",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.LazyBatchNorm2d.html#torch.nn.LazyBatchNorm2d.cls_to_become",
        "api_signature": null,
        "api_description": "alias of BatchNorm2d",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.LazyBatchNorm3d.cls_to_become",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.LazyBatchNorm3d.html#torch.nn.LazyBatchNorm3d.cls_to_become",
        "api_signature": null,
        "api_description": "alias of BatchNorm3d",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.LazyConv1d.cls_to_become",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.LazyConv1d.html#torch.nn.LazyConv1d.cls_to_become",
        "api_signature": null,
        "api_description": "alias of Conv1d",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.LazyConv2d.cls_to_become",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.LazyConv2d.html#torch.nn.LazyConv2d.cls_to_become",
        "api_signature": null,
        "api_description": "alias of Conv2d",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.LazyConv3d.cls_to_become",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.LazyConv3d.html#torch.nn.LazyConv3d.cls_to_become",
        "api_signature": null,
        "api_description": "alias of Conv3d",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.LazyConvTranspose1d.cls_to_become",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.LazyConvTranspose1d.html#torch.nn.LazyConvTranspose1d.cls_to_become",
        "api_signature": null,
        "api_description": "alias of ConvTranspose1d",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.LazyConvTranspose2d.cls_to_become",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.LazyConvTranspose2d.html#torch.nn.LazyConvTranspose2d.cls_to_become",
        "api_signature": null,
        "api_description": "alias of ConvTranspose2d",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.LazyConvTranspose3d.cls_to_become",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.LazyConvTranspose3d.html#torch.nn.LazyConvTranspose3d.cls_to_become",
        "api_signature": null,
        "api_description": "alias of ConvTranspose3d",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.LazyInstanceNorm1d.cls_to_become",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.LazyInstanceNorm1d.html#torch.nn.LazyInstanceNorm1d.cls_to_become",
        "api_signature": null,
        "api_description": "alias of InstanceNorm1d",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.LazyInstanceNorm2d.cls_to_become",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.LazyInstanceNorm2d.html#torch.nn.LazyInstanceNorm2d.cls_to_become",
        "api_signature": null,
        "api_description": "alias of InstanceNorm2d",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.LazyInstanceNorm3d.cls_to_become",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.LazyInstanceNorm3d.html#torch.nn.LazyInstanceNorm3d.cls_to_become",
        "api_signature": null,
        "api_description": "alias of InstanceNorm3d",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.LazyLinear.cls_to_become",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.LazyLinear.html#torch.nn.LazyLinear.cls_to_become",
        "api_signature": null,
        "api_description": "alias of Linear",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.parameter.UninitializedParameter.cls_to_become",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.parameter.UninitializedParameter.html#torch.nn.parameter.UninitializedParameter.cls_to_become",
        "api_signature": null,
        "api_description": "alias of Parameter",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.coalesce",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.coalesce.html#torch.Tensor.coalesce",
        "api_signature": "Tensor.coalesce()",
        "api_description": "Returns a coalesced copy of self if self is an\nuncoalesced tensor.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.GraphModule.code",
        "api_url": "https://pytorch.org/docs/stable/fx.html#torch.fx.GraphModule.code",
        "api_signature": null,
        "api_description": "Return the Python code generated from the Graph underlying this\nGraphModule.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.jit.ScriptModule.code",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.jit.ScriptModule.html#torch.jit.ScriptModule.code",
        "api_signature": null,
        "api_description": "Return a pretty-printed representation (as valid Python syntax) of the internal graph for the forward method.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.jit.ScriptModule.code_with_constants",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.jit.ScriptModule.html#torch.jit.ScriptModule.code_with_constants",
        "api_signature": null,
        "api_description": "Return a tuple.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.col_indices",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.col_indices.html#torch.Tensor.col_indices",
        "api_signature": "Tensor.col_indices()",
        "api_description": "Returns the tensor containing the column indices of the self\ntensor when self is a sparse CSR tensor of layout sparse_csr.\nThe col_indices tensor is strictly of shape (self.nnz())\nand of type int32 or int64.  When using MKL routines such as sparse\nmatrix multiplication, it is necessary to use int32 indexing in order\nto avoid downcasting and potentially losing information.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> csr = torch.eye(5,5).to_sparse_csr()\n>>> csr.col_indices()\ntensor([0, 1, 2, 3, 4], dtype=torch.int32)\n\n\n"
    },
    {
        "api_name": "torch.utils.data._utils.collate.collate",
        "api_url": "https://pytorch.org/docs/stable/data.html#torch.utils.data._utils.collate.collate",
        "api_signature": "torch.utils.data._utils.collate.collate(batch, *, collate_fn_map=None)",
        "api_description": "General collate function that handles collection type of element within each batch.",
        "return_value": "",
        "parameters": "batch – a single batch to be collated\ncollate_fn_map (Optional[Dict[Union[Type, Tuple[Type, ...]], Callable]]) – Optional dictionary mapping from element type to the corresponding collate function.\nIf the element type isn’t present in this dictionary,\nthis function will go through each key of the dictionary in the insertion order to\ninvoke the corresponding collate function if the element type is a subclass of the key.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.futures.collect_all",
        "api_url": "https://pytorch.org/docs/stable/futures.html#torch.futures.collect_all",
        "api_signature": "torch.futures.collect_all(futures)",
        "api_description": "Collects the provided Future objects into a single\ncombined Future that is completed when all of the\nsub-futures are completed.",
        "return_value": "Returns a Future object to a list of the passed\nin Futures.\n",
        "parameters": "futures (list) – a list of Future objects.",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> fut0 = torch.futures.Future()\n>>> fut1 = torch.futures.Future()\n>>> fut = torch.futures.collect_all([fut0, fut1])\n>>> fut0.set_result(0)\n>>> fut1.set_result(1)\n>>> fut_list = fut.wait()\n>>> print(f\"fut0 result = {fut_list[0].wait()}\")\nfut0 result = 0\n>>> print(f\"fut1 result = {fut_list[1].wait()}\")\nfut1 result = 1\n\n\n"
    },
    {
        "api_name": "torch.utils.benchmark.Timer.collect_callgrind",
        "api_url": "https://pytorch.org/docs/stable/benchmark_utils.html#torch.utils.benchmark.Timer.collect_callgrind",
        "api_signature": "collect_callgrind(number: int, *, repeats: None, collect_baseline: bool, retain_out_file: bool)",
        "api_description": "",
        "return_value": "A CallgrindStats object which provides instruction counts and\nsome basic facilities for analyzing and manipulating results.\n",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.column_stack",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.column_stack.html#torch.column_stack",
        "api_signature": "torch.column_stack(tensors, *, out=None)",
        "api_description": "Creates a new tensor by horizontally stacking the tensors in tensors.",
        "return_value": "",
        "parameters": "tensors (sequence of Tensors) – sequence of tensors to concatenate\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.tensor.parallel.ColwiseParallel",
        "api_url": "https://pytorch.org/docs/stable/distributed.tensor.parallel.html#torch.distributed.tensor.parallel.ColwiseParallel",
        "api_signature": "torch.distributed.tensor.parallel.ColwiseParallel(*, input_layouts=None, output_layouts=None, use_local_output=True)",
        "api_description": "Partition a compatible nn.Module in a column-wise fashion. Currently supports nn.Linear and nn.Embedding.\nUsers can compose it together with RowwiseParallel to achieve the sharding of more complicated modules.\n(i.e. MLP, Attention)",
        "return_value": "A ParallelStyle object that represents Colwise sharding of the nn.Module.\n",
        "parameters": "input_layouts (Placement, optional) – The DTensor layout of input tensor for the nn.Module, this is used to annotate the input tensor to\nbecome a DTensor. If not specified, we assume the input tensor to be replicated.\noutput_layouts (Placement, optional) – The DTensor layout of the output for the nn.Module, this is used to ensure the output of the nn.Module\nwith the user desired layout. If not specified, the output tensor is sharded on the last dimension.\nuse_local_output (bool, optional) – Whether to use local torch.Tensor instead of DTensor for the module output, default: True.",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> from torch.distributed.tensor.parallel import parallelize_module, ColwiseParallel\n>>> from torch.distributed.device_mesh import init_device_mesh\n>>> ...\n>>> m = Model(...)  # m is a nn.Module that contains a \"w1\" nn.Linear submodule\n>>> tp_mesh = init_device_mesh(\"cuda\", (8,))\n>>>\n>>> # By default, the input of the \"w1\" Linear will be converted to Replicated DTensor\n>>> # and the output of \"w1\" will return :class:`torch.Tensor` that shards on the last dim.\n>>>\n>>> sharded_mod = parallelize_module(m, tp_mesh, {\"w1\": ColwiseParallel()})\n>>> ...\n\n\n"
    },
    {
        "api_name": "torch.combinations",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.combinations.html#torch.combinations",
        "api_signature": "torch.combinations(input, r=2, with_replacement=False)",
        "api_description": "Compute combinations of length rrr of the given tensor. The behavior is similar to\npython’s itertools.combinations when with_replacement is set to False, and\nitertools.combinations_with_replacement when with_replacement is set to True.",
        "return_value": "A tensor equivalent to converting all the input tensors into lists, do\nitertools.combinations or itertools.combinations_with_replacement on these\nlists, and finally convert the resulting list into tensor.\n",
        "parameters": "input (Tensor) – 1D vector.\nr (int, optional) – number of elements to combine\nwith_replacement (bool, optional) – whether to allow duplication in combination",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.checkpoint.LoadPlanner.commit_tensor",
        "api_url": "https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.LoadPlanner.commit_tensor",
        "api_signature": "commit_tensor(read_item, tensor)",
        "api_description": "Call once the StorageReader finished loading data into tensor.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.ns._numeric_suite.compare_model_outputs",
        "api_url": "https://pytorch.org/docs/stable/torch.ao.ns._numeric_suite.html#torch.ao.ns._numeric_suite.compare_model_outputs",
        "api_signature": "torch.ao.ns._numeric_suite.compare_model_outputs(float_model, q_model, *data, logger_cls=<class 'torch.ao.ns._numeric_suite.OutputLogger'>, allow_list=None)",
        "api_description": "Compare output activations between float and quantized models at\ncorresponding locations for the same input. Return a dict with key corresponding\nto quantized module names and each entry being a dictionary with two keys\n‘float’ and ‘quantized’, containing the activations of quantized model and\nfloat model at matching locations. This dict can be used to compare and\ncompute the propagation quantization error.",
        "return_value": "dict with key corresponding to quantized module names\nand each entry being a dictionary with two keys ‘float’ and ‘quantized’,\ncontaining the matching float and quantized activations\n",
        "parameters": "float_model (Module) – float model used to generate the q_model\nq_model (Module) – model quantized from float_model\ndata – input data used to run the prepared float_model and q_model\nlogger_cls – type of logger to be attached to float_module and q_module\nallow_list – list of module types to attach logger",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.ns._numeric_suite.compare_model_stub",
        "api_url": "https://pytorch.org/docs/stable/torch.ao.ns._numeric_suite.html#torch.ao.ns._numeric_suite.compare_model_stub",
        "api_signature": "torch.ao.ns._numeric_suite.compare_model_stub(float_model, q_model, module_swap_list, *data, logger_cls=<class 'torch.ao.ns._numeric_suite.ShadowLogger'>)",
        "api_description": "Compare quantized module in a model with its floating point counterpart,\nfeeding both of them the same input. Return a dict with key corresponding to\nmodule names and each entry being a dictionary with two keys ‘float’ and\n‘quantized’, containing the output tensors of quantized and its matching\nfloat shadow module. This dict can be used to compare and compute the module\nlevel quantization error.",
        "return_value": "",
        "parameters": "float_model (Module) – float model used to generate the q_model\nq_model (Module) – model quantized from float_model\nmodule_swap_list (Set[type]) – list of float module types at which shadow modules will\nbe attached.\ndata – input data used to run the prepared q_model\nlogger_cls – type of logger to be used in shadow module to process the outputs of\nquantized module and its float shadow module",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.Store.compare_set",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#torch.distributed.Store.compare_set",
        "api_signature": "torch.distributed.Store.compare_set(self: torch._C._distributed_c10d.Store, arg0: str, arg1: str, arg2: str)",
        "api_description": "Inserts the key-value pair into the store based on the supplied key and\nperforms comparison between expected_value and desired_value before inserting. desired_value\nwill only be set if expected_value for the key already exists in the store or if expected_value\nis an empty string.",
        "return_value": "",
        "parameters": "key (str) – The key to be checked in the store.\nexpected_value (str) – The value associated with key to be checked before insertion.\ndesired_value (str) – The value associated with key to be added to the store.",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n>>> store.set(\"key\", \"first_value\")\n>>> store.compare_set(\"key\", \"first_value\", \"second_value\")\n>>> # Should return \"second_value\"\n>>> store.get(\"key\")\n\n\n"
    },
    {
        "api_name": "torch.ao.ns._numeric_suite.compare_weights",
        "api_url": "https://pytorch.org/docs/stable/torch.ao.ns._numeric_suite.html#torch.ao.ns._numeric_suite.compare_weights",
        "api_signature": "torch.ao.ns._numeric_suite.compare_weights(float_dict, quantized_dict)",
        "api_description": "Compare the weights of the float module with its corresponding quantized\nmodule. Return a dict with key corresponding to module names and each entry being\na dictionary with two keys ‘float’ and ‘quantized’, containing the float and\nquantized weights. This dict can be used to compare and compute the quantization\nerror of the weights of float and quantized models.",
        "return_value": "dict with key corresponding to module names and each entry being\na dictionary with two keys ‘float’ and ‘quantized’, containing the float and\nquantized weights\n",
        "parameters": "float_dict (Dict[str, Any]) – state dict of the float model\nquantized_dict (Dict[str, Any]) – state dict of the quantized model",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.compile",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.compile.html#torch.compile",
        "api_signature": "torch.compile(model=None, *, fullgraph=False, dynamic=None, backend='inductor', mode=None, options=None, disable=False)",
        "api_description": "Optimizes given model/function using TorchDynamo and specified backend.",
        "return_value": "",
        "parameters": "model (Callable) – Module/function to optimize\nfullgraph (bool) – If False (default), torch.compile attempts to discover compileable regions\nin the function that it will optimize. If True, then we require that the entire function be\ncapturable into a single graph. If this is not possible (that is, if there are graph breaks),\nthen this will raise an error.\ndynamic (bool or None) – Use dynamic shape tracing.  When this is True, we will up-front attempt\nto generate a kernel that is as dynamic as possible to avoid recompilations when\nsizes change.  This may not always work as some operations/optimizations will\nforce specialization; use TORCH_LOGS=dynamic to debug overspecialization.\nWhen this is False, we will NEVER generate dynamic kernels, we will always specialize.\nBy default (None), we automatically detect if dynamism has occurred and compile a more\ndynamic kernel upon recompile.\nbackend (str or Callable) – backend to be used\n”inductor” is the default backend, which is a good balance between performance and overhead\nNon experimental in-tree backends can be seen with torch._dynamo.list_backends()\nExperimental or debug in-tree backends can be seen with torch._dynamo.list_backends(None)\nmode (str) – Can be either “default”, “reduce-overhead”, “max-autotune” or “max-autotune-no-cudagraphs”\n”default” is the default mode, which is a good balance between performance and overhead\n”reduce-overhead” is a mode that reduces the overhead of python with CUDA graphs,\nuseful for small batches.  Reduction of overhead can come at the cost of more memory\nusage, as we will cache the workspace memory required for the invocation so that we\ndo not have to reallocate it on subsequent runs.  Reduction of overhead is not guaranteed\nto work; today, we only reduce overhead for CUDA only graphs which do not mutate inputs.\nThere are other circumstances where CUDA graphs are not applicable; use TORCH_LOG=perf_hints\nto debug.\n”max-autotune” is a mode that leverages Triton based matrix multiplications and convolutions\nIt enables CUDA graphs by default.\n”max-autotune-no-cudagraphs” is a mode similar to “max-autotune” but without CUDA graphs\nTo see the exact configs that each mode sets you can call torch._inductor.list_mode_options()\noptions (dict) – A dictionary of options to pass to the backend. Some notable ones to try out are\nepilogue_fusion which fuses pointwise ops into templates. Requires max_autotune to also be set\nmax_autotune which will profile to pick the best matmul configuration\nfallback_random which is useful when debugging accuracy issues\nshape_padding which pads matrix shapes to better align loads on GPUs especially for tensor cores\ntriton.cudagraphs which will reduce the overhead of python with CUDA graphs\ntrace.enabled which is the most useful debugging flag to turn on\ntrace.graph_diagram which will show you a picture of your graph after fusion\nFor inductor you can see the full list of configs that it supports by calling torch._inductor.list_options()\ndisable (bool) – Turn torch.compile() into a no-op for testing",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.compiler.compile",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.compiler.compile.html#torch.compiler.compile",
        "api_signature": "torch.compiler.compile(*args, **kwargs)",
        "api_description": "See torch.compile() for details on the arguments for this function.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.jit.ScriptModule.compile",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.jit.ScriptModule.html#torch.jit.ScriptModule.compile",
        "api_signature": "compile(*args, **kwargs)",
        "api_description": "Compile this Module’s forward using torch.compile().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.Module.compile",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.compile",
        "api_signature": "compile(*args, **kwargs)",
        "api_description": "Compile this Module’s forward using torch.compile().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.compiled_with_cxx11_abi",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.compiled_with_cxx11_abi.html#torch.compiled_with_cxx11_abi",
        "api_signature": "torch.compiled_with_cxx11_abi()",
        "api_description": "Returns whether PyTorch was built with _GLIBCXX_USE_CXX11_ABI=1",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.complex",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.complex.html#torch.complex",
        "api_signature": "torch.complex(real, imag, *, out=None)",
        "api_description": "Constructs a complex tensor with its real part equal to real and its\nimaginary part equal to imag.",
        "return_value": "",
        "parameters": "real (Tensor) – The real part of the complex tensor. Must be half, float or double.\nimag (Tensor) – The imaginary part of the complex tensor. Must be same dtype\nas real.\nout (Tensor) – If the inputs are torch.float32, must be\ntorch.complex64. If the inputs are torch.float64, must be\ntorch.complex128.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.TypedStorage.complex_double",
        "api_url": "https://pytorch.org/docs/stable/storage.html#torch.TypedStorage.complex_double",
        "api_signature": "complex_double()",
        "api_description": "Casts this storage to complex double type.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.UntypedStorage.complex_double",
        "api_url": "https://pytorch.org/docs/stable/storage.html#torch.UntypedStorage.complex_double",
        "api_signature": "complex_double()",
        "api_description": "Casts this storage to complex double type.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.TypedStorage.complex_float",
        "api_url": "https://pytorch.org/docs/stable/storage.html#torch.TypedStorage.complex_float",
        "api_signature": "complex_float()",
        "api_description": "Casts this storage to complex float type.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.UntypedStorage.complex_float",
        "api_url": "https://pytorch.org/docs/stable/storage.html#torch.UntypedStorage.complex_float",
        "api_signature": "complex_float()",
        "api_description": "Casts this storage to complex float type.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ComplexDoubleStorage",
        "api_url": "https://pytorch.org/docs/stable/storage.html#torch.ComplexDoubleStorage",
        "api_signature": "torch.ComplexDoubleStorage(*args, wrap_storage=None, dtype=None, device=None, _internal=False)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ComplexFloatStorage",
        "api_url": "https://pytorch.org/docs/stable/storage.html#torch.ComplexFloatStorage",
        "api_signature": "torch.ComplexFloatStorage(*args, wrap_storage=None, dtype=None, device=None, _internal=False)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.mixture_same_family.MixtureSameFamily.component_distribution",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.mixture_same_family.MixtureSameFamily.component_distribution",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.transforms.ComposeTransform",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.transforms.ComposeTransform",
        "api_signature": "torch.distributions.transforms.ComposeTransform(parts, cache_size=0)",
        "api_description": "Composes multiple transforms in a chain.\nThe transforms being composed are responsible for caching.",
        "return_value": "",
        "parameters": "parts (list of Transform) – A list of transforms to compose.\ncache_size (int) – Size of cache. If zero, no caching is done. If one,\nthe latest single value is cached. Only 0 and 1 are supported.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.ns.fx.utils.compute_cosine_similarity",
        "api_url": "https://pytorch.org/docs/stable/torch.ao.ns._numeric_suite_fx.html#torch.ao.ns.fx.utils.compute_cosine_similarity",
        "api_signature": "torch.ao.ns.fx.utils.compute_cosine_similarity(x, y)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.utils.prune.BasePruningMethod.compute_mask",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.BasePruningMethod.html#torch.nn.utils.prune.BasePruningMethod.compute_mask",
        "api_signature": "compute_mask(t, default_mask)",
        "api_description": "Compute and returns a mask for the input tensor t.",
        "return_value": "mask to apply to t, of same dims as t\n",
        "parameters": "t (torch.Tensor) – tensor representing the importance scores of the\nprune. (parameter to) –\ndefault_mask (torch.Tensor) – Base mask from previous pruning\niterations –\nis (that need to be respected after the new mask) –\nt. (applied. Same dims as) –",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.utils.prune.LnStructured.compute_mask",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.LnStructured.html#torch.nn.utils.prune.LnStructured.compute_mask",
        "api_signature": "compute_mask(t, default_mask)",
        "api_description": "Compute and returns a mask for the input tensor t.",
        "return_value": "mask to apply to t, of same dims as t\n",
        "parameters": "t (torch.Tensor) – tensor representing the parameter to prune\ndefault_mask (torch.Tensor) – Base mask from previous pruning\niterations, that need to be respected after the new mask is\napplied.  Same dims as t.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.utils.prune.PruningContainer.compute_mask",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.PruningContainer.html#torch.nn.utils.prune.PruningContainer.compute_mask",
        "api_signature": "compute_mask(t, default_mask)",
        "api_description": "Apply the latest method by computing the new partial masks and returning its combination with the default_mask.",
        "return_value": "new mask that combines the effects\nof the default_mask and the new mask from the current\npruning method (of same dimensions as default_mask and\nt).\n",
        "parameters": "t (torch.Tensor) – tensor representing the parameter to prune\n(of same dimensions as default_mask).\ndefault_mask (torch.Tensor) – mask from previous pruning iteration.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.utils.prune.RandomStructured.compute_mask",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.RandomStructured.html#torch.nn.utils.prune.RandomStructured.compute_mask",
        "api_signature": "compute_mask(t, default_mask)",
        "api_description": "Compute and returns a mask for the input tensor t.",
        "return_value": "mask to apply to t, of same dims as t\n",
        "parameters": "t (torch.Tensor) – tensor representing the parameter to prune\ndefault_mask (torch.Tensor) – Base mask from previous pruning\niterations, that need to be respected after the new mask is\napplied. Same dims as t.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.ns.fx.utils.compute_normalized_l2_error",
        "api_url": "https://pytorch.org/docs/stable/torch.ao.ns._numeric_suite_fx.html#torch.ao.ns.fx.utils.compute_normalized_l2_error",
        "api_signature": "torch.ao.ns.fx.utils.compute_normalized_l2_error(x, y)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.ns.fx.utils.compute_sqnr",
        "api_url": "https://pytorch.org/docs/stable/torch.ao.ns._numeric_suite_fx.html#torch.ao.ns.fx.utils.compute_sqnr",
        "api_signature": "torch.ao.ns.fx.utils.compute_sqnr(x, y)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.concat",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.concat.html#torch.concat",
        "api_signature": "torch.concat(tensors, dim=0, *, out=None)",
        "api_description": "Alias of torch.cat().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.data.ConcatDataset",
        "api_url": "https://pytorch.org/docs/stable/data.html#torch.utils.data.ConcatDataset",
        "api_signature": "torch.utils.data.ConcatDataset(datasets)",
        "api_description": "Dataset as a concatenation of multiple datasets.",
        "return_value": "",
        "parameters": "datasets (sequence) – List of datasets to be concatenated",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.concatenate",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.concatenate.html#torch.concatenate",
        "api_signature": "torch.concatenate(tensors, axis=0, out=None)",
        "api_description": "Alias of torch.cat().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.inverse_gamma.InverseGamma.concentration",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.inverse_gamma.InverseGamma.concentration",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.beta.Beta.concentration0",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.beta.Beta.concentration0",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.beta.Beta.concentration1",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.beta.Beta.concentration1",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cond",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.cond.html#torch.cond",
        "api_signature": "torch.cond(pred, true_fn, false_fn, operands)",
        "api_description": "Conditionally applies true_fn or false_fn.",
        "return_value": "",
        "parameters": "pred (Union[bool, torch.Tensor]) – A boolean expression or a tensor with one element,\nindicating which branch function to apply.\ntrue_fn (Callable) – A callable function (a -> b) that is within the\nscope that is being traced.\nfalse_fn (Callable) – A callable function (a -> b) that is within the\nscope that is being traced. The true branch and false branch must\nhave consistent input and outputs, meaning the inputs have to be\nthe same, and the outputs have to be the same type and shape.\noperands (Tuple of possibly nested dict/list/tuple of torch.Tensor) – A tuple of inputs to the true/false functions.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch._higher_order_ops.cond.cond",
        "api_url": "https://pytorch.org/docs/stable/cond.html#torch._higher_order_ops.cond.cond",
        "api_signature": "torch._higher_order_ops.cond.cond(pred, true_fn, false_fn, operands)",
        "api_description": "Conditionally applies true_fn or false_fn.",
        "return_value": "",
        "parameters": "pred (Union[bool, torch.Tensor]) – A boolean expression or a tensor with one element,\nindicating which branch function to apply.\ntrue_fn (Callable) – A callable function (a -> b) that is within the\nscope that is being traced.\nfalse_fn (Callable) – A callable function (a -> b) that is within the\nscope that is being traced. The true branch and false branch must\nhave consistent input and outputs, meaning the inputs have to be\nthe same, and the outputs have to be the same type and shape.\noperands (Tuple of possibly nested dict/list/tuple of torch.Tensor) – A tuple of inputs to the true/false functions.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.linalg.cond",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.linalg.cond.html#torch.linalg.cond",
        "api_signature": "torch.linalg.cond(A, p=None, *, out=None)",
        "api_description": "Computes the condition number of a matrix with respect to a matrix norm.",
        "return_value": "A real-valued tensor, even when A is complex.\n",
        "parameters": "A (Tensor) – tensor of shape (*, m, n) where * is zero or more batch dimensions\nfor p in (2, -2), and of shape (*, n, n) where every matrix\nis invertible for p in (‘fro’, ‘nuc’, inf, -inf, 1, -1).\np (int, inf, -inf, 'fro', 'nuc', optional) – the type of the matrix norm to use in the computations (see above). Default: None\nout (Tensor, optional) – output tensor. Ignored if None. Default: None.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.backend_config.BackendConfig.configs",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.quantization.backend_config.BackendConfig.html#torch.ao.quantization.backend_config.BackendConfig.configs",
        "api_signature": null,
        "api_description": "Return a copy of the list of configs set in this BackendConfig.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.metrics.configure",
        "api_url": "https://pytorch.org/docs/stable/elastic/metrics.html#torch.distributed.elastic.metrics.configure",
        "api_signature": "torch.distributed.elastic.metrics.configure(handler, group=None)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.timer.configure",
        "api_url": "https://pytorch.org/docs/stable/elastic/timer.html#torch.distributed.elastic.timer.configure",
        "api_signature": "torch.distributed.elastic.timer.configure(timer_client)",
        "api_description": "Configures a timer client. Must be called before using expires.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.rpc.PyRRef.confirmed_by_owner",
        "api_url": "https://pytorch.org/docs/stable/rpc.html#torch.distributed.rpc.PyRRef.confirmed_by_owner",
        "api_signature": "confirmed_by_owner(self: torch._C._distributed_rpc.PyRRef)",
        "api_description": "Returns whether this RRef has been confirmed by the owner.\nOwnerRRef always returns true, while UserRRef only\nreturns true when the owner knowns about this UserRRef.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.conj",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.conj.html#torch.conj",
        "api_signature": "torch.conj(input)",
        "api_description": "Returns a view of input with a flipped conjugate bit. If input has a non-complex dtype,\nthis function just returns input.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.conj",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.conj.html#torch.Tensor.conj",
        "api_signature": "Tensor.conj()",
        "api_description": "See torch.conj()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.conj_physical",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.conj_physical.html#torch.conj_physical",
        "api_signature": "torch.conj_physical(input, *, out=None)",
        "api_description": "Computes the element-wise conjugate of the given input tensor.\nIf input has a non-complex dtype, this function just returns input.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.conj_physical",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.conj_physical.html#torch.Tensor.conj_physical",
        "api_signature": "Tensor.conj_physical()",
        "api_description": "See torch.conj_physical()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.conj_physical_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.conj_physical_.html#torch.Tensor.conj_physical_",
        "api_signature": "Tensor.conj_physical_()",
        "api_description": "In-place version of conj_physical()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.metrics.api.ConsoleMetricHandler",
        "api_url": "https://pytorch.org/docs/stable/elastic/metrics.html#torch.distributed.elastic.metrics.api.ConsoleMetricHandler",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.optim.ZeroRedundancyOptimizer.consolidate_state_dict",
        "api_url": "https://pytorch.org/docs/stable/distributed.optim.html#torch.distributed.optim.ZeroRedundancyOptimizer.consolidate_state_dict",
        "api_signature": "consolidate_state_dict(to=0)",
        "api_description": "Consolidate a list of state_dict s (one per rank) on the target rank.",
        "return_value": "",
        "parameters": "to (int) – the rank that receives the optimizer states (default: 0).",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.init.constant_",
        "api_url": "https://pytorch.org/docs/stable/nn.init.html#torch.nn.init.constant_",
        "api_signature": "torch.nn.init.constant_(tensor, val)",
        "api_description": "Fill the input Tensor with the value val\\text{val}val.",
        "return_value": "",
        "parameters": "tensor (Tensor) – an n-dimensional torch.Tensor\nval (float) – the value to fill the tensor with",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.lr_scheduler.ConstantLR",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.ConstantLR.html#torch.optim.lr_scheduler.ConstantLR",
        "api_signature": "torch.optim.lr_scheduler.ConstantLR(optimizer, factor=0.3333333333333333, total_iters=5, last_epoch=-1, verbose='deprecated')",
        "api_description": "Multiply the learning rate of each parameter group by a small constant factor until the\nnumber of epoch reaches a pre-defined milestone: total_iters.\nNotice that such multiplication of the small constant factor can\nhappen simultaneously with other changes to the learning rate from outside this scheduler.\nWhen last_epoch=-1, sets initial lr as lr.",
        "return_value": "",
        "parameters": "optimizer (Optimizer) – Wrapped optimizer.\nfactor (float) – The number we multiply learning rate until the milestone. Default: 1./3.\ntotal_iters (int) – The number of steps that the scheduler multiplies the learning rate by the factor.\nDefault: 5.\nlast_epoch (int) – The index of the last epoch. Default: -1.\nverbose (bool) – If True, prints a message to stdout for\neach update. Default: False.\nDeprecated since version 2.2: verbose is deprecated. Please use get_last_lr() to access the\nlearning rate.\nstate_dict (dict) – scheduler state. Should be an object returned\nfrom a call to state_dict().",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.ConstantPad1d",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.ConstantPad1d.html#torch.nn.ConstantPad1d",
        "api_signature": "torch.nn.ConstantPad1d(padding, value)",
        "api_description": "Pads the input tensor boundaries with a constant value.",
        "return_value": "",
        "parameters": "padding (int, tuple) – the size of the padding. If is int, uses the same\npadding in both boundaries. If a 2-tuple, uses\n(padding_left\\text{padding\\_left}padding_left, padding_right\\text{padding\\_right}padding_right)",
        "input_shape": "\nInput: (C,Win)(C, W_{in})(C,Win​) or (N,C,Win)(N, C, W_{in})(N,C,Win​).\nOutput: (C,Wout)(C, W_{out})(C,Wout​) or (N,C,Wout)(N, C, W_{out})(N,C,Wout​), where\nWout=Win+padding_left+padding_rightW_{out} = W_{in} + \\text{padding\\_left} + \\text{padding\\_right}Wout​=Win​+padding_left+padding_right\n\n\n",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.ConstantPad2d",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.ConstantPad2d.html#torch.nn.ConstantPad2d",
        "api_signature": "torch.nn.ConstantPad2d(padding, value)",
        "api_description": "Pads the input tensor boundaries with a constant value.",
        "return_value": "",
        "parameters": "padding (int, tuple) – the size of the padding. If is int, uses the same\npadding in all boundaries. If a 4-tuple, uses (padding_left\\text{padding\\_left}padding_left,\npadding_right\\text{padding\\_right}padding_right, padding_top\\text{padding\\_top}padding_top, padding_bottom\\text{padding\\_bottom}padding_bottom)",
        "input_shape": "\nInput: (N,C,Hin,Win)(N, C, H_{in}, W_{in})(N,C,Hin​,Win​) or (C,Hin,Win)(C, H_{in}, W_{in})(C,Hin​,Win​).\nOutput: (N,C,Hout,Wout)(N, C, H_{out}, W_{out})(N,C,Hout​,Wout​) or (C,Hout,Wout)(C, H_{out}, W_{out})(C,Hout​,Wout​), where\nHout=Hin+padding_top+padding_bottomH_{out} = H_{in} + \\text{padding\\_top} + \\text{padding\\_bottom}Hout​=Hin​+padding_top+padding_bottom\nWout=Win+padding_left+padding_rightW_{out} = W_{in} + \\text{padding\\_left} + \\text{padding\\_right}Wout​=Win​+padding_left+padding_right\n\n\n",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.ConstantPad3d",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.ConstantPad3d.html#torch.nn.ConstantPad3d",
        "api_signature": "torch.nn.ConstantPad3d(padding, value)",
        "api_description": "Pads the input tensor boundaries with a constant value.",
        "return_value": "",
        "parameters": "padding (int, tuple) – the size of the padding. If is int, uses the same\npadding in all boundaries. If a 6-tuple, uses\n(padding_left\\text{padding\\_left}padding_left, padding_right\\text{padding\\_right}padding_right,\npadding_top\\text{padding\\_top}padding_top, padding_bottom\\text{padding\\_bottom}padding_bottom,\npadding_front\\text{padding\\_front}padding_front, padding_back\\text{padding\\_back}padding_back)",
        "input_shape": "\nInput: (N,C,Din,Hin,Win)(N, C, D_{in}, H_{in}, W_{in})(N,C,Din​,Hin​,Win​) or (C,Din,Hin,Win)(C, D_{in}, H_{in}, W_{in})(C,Din​,Hin​,Win​).\nOutput: (N,C,Dout,Hout,Wout)(N, C, D_{out}, H_{out}, W_{out})(N,C,Dout​,Hout​,Wout​) or\n(C,Dout,Hout,Wout)(C, D_{out}, H_{out}, W_{out})(C,Dout​,Hout​,Wout​), where\nDout=Din+padding_front+padding_backD_{out} = D_{in} + \\text{padding\\_front} + \\text{padding\\_back}Dout​=Din​+padding_front+padding_back\nHout=Hin+padding_top+padding_bottomH_{out} = H_{in} + \\text{padding\\_top} + \\text{padding\\_bottom}Hout​=Hin​+padding_top+padding_bottom\nWout=Win+padding_left+padding_rightW_{out} = W_{in} + \\text{padding\\_left} + \\text{padding\\_right}Wout​=Win​+padding_left+padding_right\n\n\n",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.symbolic_shapes.constrain_range",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.fx.experimental.symbolic_shapes.constrain_range.html#torch.fx.experimental.symbolic_shapes.constrain_range",
        "api_signature": "torch.fx.experimental.symbolic_shapes.constrain_range(a, *, min, max=None)",
        "api_description": "Applies a constraint that the passed in SymInt must lie between min-max\ninclusive-inclusive, WITHOUT introducing a guard on the SymInt (meaning\nthat it can be used on unbacked SymInts).  If min/max are None, we assume\nthat the dimension is unbounded in that direction.  Repeated application\nof constrain_range intersects the ranges.  This is a fairly low level API\nthat doesn’t have a lot of safety guarantees (TODO: provide higher level\nAPIs).",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.symbolic_shapes.constrain_unify",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.fx.experimental.symbolic_shapes.constrain_unify.html#torch.fx.experimental.symbolic_shapes.constrain_unify",
        "api_signature": "torch.fx.experimental.symbolic_shapes.constrain_unify(a, b)",
        "api_description": "Given two SymInts, constrain them so that they must be equal.  NB:\nthis will not work with SymInts that represent nontrivial expressions\n(yet!)",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.constraints.Constraint",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.constraints.Constraint",
        "api_signature": null,
        "api_description": "Abstract base class for constraints.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.export.Constraint",
        "api_url": "https://pytorch.org/docs/stable/export.html#torch.export.Constraint",
        "api_signature": null,
        "api_description": "alias of Union[_Constraint, _DerivedConstraint]",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.constraint_registry.ConstraintRegistry",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.constraint_registry.ConstraintRegistry",
        "api_signature": null,
        "api_description": "Registry to link constraints to transforms.",
        "return_value": "",
        "parameters": "constraint (subclass of Constraint) – A subclass of Constraint, or\na singleton object of the desired class.\nfactory (Callable) – A callable that inputs a constraint object and returns\na  Transform object.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.autograd.context",
        "api_url": "https://pytorch.org/docs/stable/rpc.html#torch.distributed.autograd.context",
        "api_signature": null,
        "api_description": "Context object to wrap forward and backward passes when using\ndistributed autograd. The context_id generated in the with\nstatement  is required to uniquely identify a distributed backward pass\non all workers. Each worker stores metadata associated with this\ncontext_id, which is required to correctly execute a distributed\nautograd pass.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import torch.distributed.autograd as dist_autograd\n>>> with dist_autograd.context() as context_id:\n>>>     t1 = torch.rand((3, 3), requires_grad=True)\n>>>     t2 = torch.rand((3, 3), requires_grad=True)\n>>>     loss = rpc.rpc_sync(\"worker1\", torch.add, args=(t1, t2)).sum()\n>>>     dist_autograd.backward(context_id, [loss])\n\n\n"
    },
    {
        "api_name": "torch.Tensor.contiguous",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.contiguous.html#torch.Tensor.contiguous",
        "api_signature": "Tensor.contiguous(memory_format=torch.contiguous_format)",
        "api_description": "Returns a contiguous in memory tensor containing the same data as self tensor. If\nself tensor is already in the specified memory format, this function returns the\nself tensor.",
        "return_value": "",
        "parameters": "memory_format (torch.memory_format, optional) – the desired memory format of\nreturned Tensor. Default: torch.contiguous_format.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.continuous_bernoulli.ContinuousBernoulli",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.continuous_bernoulli.ContinuousBernoulli",
        "api_signature": "torch.distributions.continuous_bernoulli.ContinuousBernoulli(probs=None, logits=None, lims=(0.499, 0.501)",
        "api_description": "Bases: ExponentialFamily",
        "return_value": "",
        "parameters": "probs (Number, Tensor) – (0,1) valued parameters\nlogits (Number, Tensor) – real valued parameters whose sigmoid matches ‘probs’",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.quantized.Conv1d",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.nn.quantized.Conv1d.html#torch.ao.nn.quantized.Conv1d",
        "api_signature": "torch.ao.nn.quantized.Conv1d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None)",
        "api_description": "Applies a 1D convolution over a quantized input signal composed of\nseveral quantized input planes.",
        "return_value": "",
        "parameters": "mod (Module) – a float module, either produced by torch.ao.quantization\nutilities or provided by the user",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.quantized.functional.conv1d",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.nn.quantized.functional.conv1d.html#torch.ao.nn.quantized.functional.conv1d",
        "api_signature": "torch.ao.nn.quantized.functional.conv1d(input, weight, bias, stride=1, padding=0, dilation=1, groups=1, padding_mode='zeros', scale=1.0, zero_point=0, dtype=torch.quint8)",
        "api_description": "Applies a 1D convolution over a quantized 1D input composed of several input\nplanes.",
        "return_value": "",
        "parameters": "input – quantized input tensor of shape (minibatch,in_channels,iW)(\\text{minibatch} , \\text{in\\_channels} , iW)(minibatch,in_channels,iW)\nweight – quantized filters of shape (out_channels,in_channelsgroups,iW)(\\text{out\\_channels} , \\frac{\\text{in\\_channels}}{\\text{groups}} , iW)(out_channels,groupsin_channels​,iW)\nbias – non-quantized bias tensor of shape (out_channels)(\\text{out\\_channels})(out_channels). The tensor type must be torch.float.\nstride – the stride of the convolving kernel. Can be a single number or a\ntuple (sW,). Default: 1\npadding – implicit paddings on both sides of the input. Can be a\nsingle number or a tuple (padW,). Default: 0\ndilation – the spacing between kernel elements. Can be a single number or\na tuple (dW,). Default: 1\ngroups – split input into groups, in_channels\\text{in\\_channels}in_channels should be divisible by the\nnumber of groups. Default: 1\npadding_mode – the padding mode to use. Only “zeros” is supported for quantized convolution at the moment. Default: “zeros”\nscale – quantization scale for the output. Default: 1.0\nzero_point – quantization zero_point for the output. Default: 0\ndtype – quantization data type to use. Default: torch.quint8",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.Conv1d",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html#torch.nn.Conv1d",
        "api_signature": "torch.nn.Conv1d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None)",
        "api_description": "Applies a 1D convolution over an input signal composed of several input\nplanes.",
        "return_value": "",
        "parameters": "in_channels (int) – Number of channels in the input image\nout_channels (int) – Number of channels produced by the convolution\nkernel_size (int or tuple) – Size of the convolving kernel\nstride (int or tuple, optional) – Stride of the convolution. Default: 1\npadding (int, tuple or str, optional) – Padding added to both sides of\nthe input. Default: 0\npadding_mode (str, optional) – 'zeros', 'reflect',\n'replicate' or 'circular'. Default: 'zeros'\ndilation (int or tuple, optional) – Spacing between kernel\nelements. Default: 1\ngroups (int, optional) – Number of blocked connections from input\nchannels to output channels. Default: 1\nbias (bool, optional) – If True, adds a learnable bias to the\noutput. Default: True",
        "input_shape": "\nInput: (N,Cin,Lin)(N, C_{in}, L_{in})(N,Cin​,Lin​) or (Cin,Lin)(C_{in}, L_{in})(Cin​,Lin​)\nOutput: (N,Cout,Lout)(N, C_{out}, L_{out})(N,Cout​,Lout​) or (Cout,Lout)(C_{out}, L_{out})(Cout​,Lout​), where\n\nLout=⌊Lin+2×padding−dilation×(kernel_size−1)−1stride+1⌋L_{out} = \\left\\lfloor\\frac{L_{in} + 2 \\times \\text{padding} - \\text{dilation}\n          \\times (\\text{kernel\\_size} - 1) - 1}{\\text{stride}} + 1\\right\\rfloor\n\nLout​=⌊strideLin​+2×padding−dilation×(kernel_size−1)−1​+1⌋\n\n",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.functional.conv1d",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.functional.conv1d.html#torch.nn.functional.conv1d",
        "api_signature": "torch.nn.functional.conv1d(input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1)",
        "api_description": "Applies a 1D convolution over an input signal composed of several input\nplanes.",
        "return_value": "",
        "parameters": "input – input tensor of shape (minibatch,in_channels,iW)(\\text{minibatch} , \\text{in\\_channels} , iW)(minibatch,in_channels,iW)\nweight – filters of shape (out_channels,in_channelsgroups,kW)(\\text{out\\_channels} , \\frac{\\text{in\\_channels}}{\\text{groups}} , kW)(out_channels,groupsin_channels​,kW)\nbias – optional bias of shape (out_channels)(\\text{out\\_channels})(out_channels). Default: None\nstride – the stride of the convolving kernel. Can be a single number or\na one-element tuple (sW,). Default: 1\npadding – implicit paddings on both sides of the input. Can be a string {‘valid’, ‘same’},\nsingle number or a one-element tuple (padW,). Default: 0\npadding='valid' is the same as no padding. padding='same' pads\nthe input so the output has the same shape as the input. However, this mode\ndoesn’t support any stride values other than 1.\nWarning\nFor padding='same', if the weight is even-length and\ndilation is odd in any dimension, a full pad() operation\nmay be needed internally. Lowering performance.\ndilation – the spacing between kernel elements. Can be a single number or\na one-element tuple (dW,). Default: 1\ngroups – split input into groups, in_channels\\text{in\\_channels}in_channels should be divisible by\nthe number of groups. Default: 1",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.qat.Conv2d",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.nn.qat.Conv2d.html#torch.ao.nn.qat.Conv2d",
        "api_signature": "torch.ao.nn.qat.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', qconfig=None, device=None, dtype=None)",
        "api_description": "A Conv2d module attached with FakeQuantize modules for weight,\nused for quantization aware training.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.quantized.Conv2d",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.nn.quantized.Conv2d.html#torch.ao.nn.quantized.Conv2d",
        "api_signature": "torch.ao.nn.quantized.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None)",
        "api_description": "Applies a 2D convolution over a quantized input signal composed of\nseveral quantized input planes.",
        "return_value": "",
        "parameters": "mod (Module) – a float module, either produced by torch.ao.quantization\nutilities or provided by the user",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.quantized.functional.conv2d",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.nn.quantized.functional.conv2d.html#torch.ao.nn.quantized.functional.conv2d",
        "api_signature": "torch.ao.nn.quantized.functional.conv2d(input, weight, bias, stride=1, padding=0, dilation=1, groups=1, padding_mode='zeros', scale=1.0, zero_point=0, dtype=torch.quint8)",
        "api_description": "Applies a 2D convolution over a quantized 2D input composed of several input\nplanes.",
        "return_value": "",
        "parameters": "input – quantized input tensor of shape (minibatch,in_channels,iH,iW)(\\text{minibatch} , \\text{in\\_channels} , iH , iW)(minibatch,in_channels,iH,iW)\nweight – quantized filters of shape (out_channels,in_channelsgroups,kH,kW)(\\text{out\\_channels} , \\frac{\\text{in\\_channels}}{\\text{groups}} , kH , kW)(out_channels,groupsin_channels​,kH,kW)\nbias – non-quantized bias tensor of shape (out_channels)(\\text{out\\_channels})(out_channels). The tensor type must be torch.float.\nstride – the stride of the convolving kernel. Can be a single number or a\ntuple (sH, sW). Default: 1\npadding – implicit paddings on both sides of the input. Can be a\nsingle number or a tuple (padH, padW). Default: 0\ndilation – the spacing between kernel elements. Can be a single number or\na tuple (dH, dW). Default: 1\ngroups – split input into groups, in_channels\\text{in\\_channels}in_channels should be divisible by the\nnumber of groups. Default: 1\npadding_mode – the padding mode to use. Only “zeros” is supported for quantized convolution at the moment. Default: “zeros”\nscale – quantization scale for the output. Default: 1.0\nzero_point – quantization zero_point for the output. Default: 0\ndtype – quantization data type to use. Default: torch.quint8",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.Conv2d",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d",
        "api_signature": "torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None)",
        "api_description": "Applies a 2D convolution over an input signal composed of several input\nplanes.",
        "return_value": "",
        "parameters": "in_channels (int) – Number of channels in the input image\nout_channels (int) – Number of channels produced by the convolution\nkernel_size (int or tuple) – Size of the convolving kernel\nstride (int or tuple, optional) – Stride of the convolution. Default: 1\npadding (int, tuple or str, optional) – Padding added to all four sides of\nthe input. Default: 0\npadding_mode (str, optional) – 'zeros', 'reflect',\n'replicate' or 'circular'. Default: 'zeros'\ndilation (int or tuple, optional) – Spacing between kernel elements. Default: 1\ngroups (int, optional) – Number of blocked connections from input\nchannels to output channels. Default: 1\nbias (bool, optional) – If True, adds a learnable bias to the\noutput. Default: True",
        "input_shape": "\nInput: (N,Cin,Hin,Win)(N, C_{in}, H_{in}, W_{in})(N,Cin​,Hin​,Win​) or (Cin,Hin,Win)(C_{in}, H_{in}, W_{in})(Cin​,Hin​,Win​)\nOutput: (N,Cout,Hout,Wout)(N, C_{out}, H_{out}, W_{out})(N,Cout​,Hout​,Wout​) or (Cout,Hout,Wout)(C_{out}, H_{out}, W_{out})(Cout​,Hout​,Wout​), where\n\nHout=⌊Hin+2×padding[0]−dilation[0]×(kernel_size[0]−1)−1stride[0]+1⌋H_{out} = \\left\\lfloor\\frac{H_{in}  + 2 \\times \\text{padding}[0] - \\text{dilation}[0]\n          \\times (\\text{kernel\\_size}[0] - 1) - 1}{\\text{stride}[0]} + 1\\right\\rfloor\n\nHout​=⌊stride[0]Hin​+2×padding[0]−dilation[0]×(kernel_size[0]−1)−1​+1⌋\nWout=⌊Win+2×padding[1]−dilation[1]×(kernel_size[1]−1)−1stride[1]+1⌋W_{out} = \\left\\lfloor\\frac{W_{in}  + 2 \\times \\text{padding}[1] - \\text{dilation}[1]\n          \\times (\\text{kernel\\_size}[1] - 1) - 1}{\\text{stride}[1]} + 1\\right\\rfloor\n\nWout​=⌊stride[1]Win​+2×padding[1]−dilation[1]×(kernel_size[1]−1)−1​+1⌋\n\n",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.functional.conv2d",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.functional.conv2d.html#torch.nn.functional.conv2d",
        "api_signature": "torch.nn.functional.conv2d(input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1)",
        "api_description": "Applies a 2D convolution over an input image composed of several input\nplanes.",
        "return_value": "",
        "parameters": "input – input tensor of shape (minibatch,in_channels,iH,iW)(\\text{minibatch} , \\text{in\\_channels} , iH , iW)(minibatch,in_channels,iH,iW)\nweight – filters of shape (out_channels,in_channelsgroups,kH,kW)(\\text{out\\_channels} , \\frac{\\text{in\\_channels}}{\\text{groups}} , kH , kW)(out_channels,groupsin_channels​,kH,kW)\nbias – optional bias tensor of shape (out_channels)(\\text{out\\_channels})(out_channels). Default: None\nstride – the stride of the convolving kernel. Can be a single number or a\ntuple (sH, sW). Default: 1\npadding – implicit paddings on both sides of the input. Can be a string {‘valid’, ‘same’},\nsingle number or a tuple (padH, padW). Default: 0\npadding='valid' is the same as no padding. padding='same' pads\nthe input so the output has the same shape as the input. However, this mode\ndoesn’t support any stride values other than 1.\nWarning\nFor padding='same', if the weight is even-length and\ndilation is odd in any dimension, a full pad() operation\nmay be needed internally. Lowering performance.\ndilation – the spacing between kernel elements. Can be a single number or\na tuple (dH, dW). Default: 1\ngroups – split input into groups, both in_channels\\text{in\\_channels}in_channels and out_channels\\text{out\\_channels}out_channels\nshould be divisible by the number of groups. Default: 1",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.qat.Conv3d",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.nn.qat.Conv3d.html#torch.ao.nn.qat.Conv3d",
        "api_signature": "torch.ao.nn.qat.Conv3d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', qconfig=None, device=None, dtype=None)",
        "api_description": "A Conv3d module attached with FakeQuantize modules for weight,\nused for quantization aware training.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.quantized.Conv3d",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.nn.quantized.Conv3d.html#torch.ao.nn.quantized.Conv3d",
        "api_signature": "torch.ao.nn.quantized.Conv3d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None)",
        "api_description": "Applies a 3D convolution over a quantized input signal composed of\nseveral quantized input planes.",
        "return_value": "",
        "parameters": "mod (Module) – a float module, either produced by torch.ao.quantization\nutilities or provided by the user",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.quantized.functional.conv3d",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.nn.quantized.functional.conv3d.html#torch.ao.nn.quantized.functional.conv3d",
        "api_signature": "torch.ao.nn.quantized.functional.conv3d(input, weight, bias, stride=1, padding=0, dilation=1, groups=1, padding_mode='zeros', scale=1.0, zero_point=0, dtype=torch.quint8)",
        "api_description": "Applies a 3D convolution over a quantized 3D input composed of several input\nplanes.",
        "return_value": "",
        "parameters": "input – quantized input tensor of shape\n(minibatch,in_channels,iD,iH,iW)(\\text{minibatch} , \\text{in\\_channels} , iD , iH , iW)(minibatch,in_channels,iD,iH,iW)\nweight – quantized filters of shape\n(out_channels,in_channelsgroups,kD,kH,kW)(\\text{out\\_channels} , \\frac{\\text{in\\_channels}}{\\text{groups}} , kD , kH , kW)(out_channels,groupsin_channels​,kD,kH,kW)\nbias – non-quantized bias tensor of shape\n(out_channels)(\\text{out\\_channels})(out_channels). The tensor type must be torch.float.\nstride – the stride of the convolving kernel. Can be a single number or a\ntuple (sD, sH, sW). Default: 1\npadding – implicit paddings on both sides of the input. Can be a\nsingle number or a tuple (padD, padH, padW). Default: 0\ndilation – the spacing between kernel elements. Can be a single number or\na tuple (dD, dH, dW). Default: 1\ngroups – split input into groups, in_channels\\text{in\\_channels}in_channels should be\ndivisible by the number of groups. Default: 1\npadding_mode – the padding mode to use. Only “zeros” is supported for\nquantized convolution at the moment. Default: “zeros”\nscale – quantization scale for the output. Default: 1.0\nzero_point – quantization zero_point for the output. Default: 0\ndtype – quantization data type to use. Default: torch.quint8",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.Conv3d",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.Conv3d.html#torch.nn.Conv3d",
        "api_signature": "torch.nn.Conv3d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None)",
        "api_description": "Applies a 3D convolution over an input signal composed of several input\nplanes.",
        "return_value": "",
        "parameters": "in_channels (int) – Number of channels in the input image\nout_channels (int) – Number of channels produced by the convolution\nkernel_size (int or tuple) – Size of the convolving kernel\nstride (int or tuple, optional) – Stride of the convolution. Default: 1\npadding (int, tuple or str, optional) – Padding added to all six sides of\nthe input. Default: 0\npadding_mode (str, optional) – 'zeros', 'reflect', 'replicate' or 'circular'. Default: 'zeros'\ndilation (int or tuple, optional) – Spacing between kernel elements. Default: 1\ngroups (int, optional) – Number of blocked connections from input channels to output channels. Default: 1\nbias (bool, optional) – If True, adds a learnable bias to the output. Default: True",
        "input_shape": "\nInput: (N,Cin,Din,Hin,Win)(N, C_{in}, D_{in}, H_{in}, W_{in})(N,Cin​,Din​,Hin​,Win​) or (Cin,Din,Hin,Win)(C_{in}, D_{in}, H_{in}, W_{in})(Cin​,Din​,Hin​,Win​)\nOutput: (N,Cout,Dout,Hout,Wout)(N, C_{out}, D_{out}, H_{out}, W_{out})(N,Cout​,Dout​,Hout​,Wout​) or (Cout,Dout,Hout,Wout)(C_{out}, D_{out}, H_{out}, W_{out})(Cout​,Dout​,Hout​,Wout​),\nwhere\n\nDout=⌊Din+2×padding[0]−dilation[0]×(kernel_size[0]−1)−1stride[0]+1⌋D_{out} = \\left\\lfloor\\frac{D_{in} + 2 \\times \\text{padding}[0] - \\text{dilation}[0]\n      \\times (\\text{kernel\\_size}[0] - 1) - 1}{\\text{stride}[0]} + 1\\right\\rfloor\n\nDout​=⌊stride[0]Din​+2×padding[0]−dilation[0]×(kernel_size[0]−1)−1​+1⌋\nHout=⌊Hin+2×padding[1]−dilation[1]×(kernel_size[1]−1)−1stride[1]+1⌋H_{out} = \\left\\lfloor\\frac{H_{in} + 2 \\times \\text{padding}[1] - \\text{dilation}[1]\n      \\times (\\text{kernel\\_size}[1] - 1) - 1}{\\text{stride}[1]} + 1\\right\\rfloor\n\nHout​=⌊stride[1]Hin​+2×padding[1]−dilation[1]×(kernel_size[1]−1)−1​+1⌋\nWout=⌊Win+2×padding[2]−dilation[2]×(kernel_size[2]−1)−1stride[2]+1⌋W_{out} = \\left\\lfloor\\frac{W_{in} + 2 \\times \\text{padding}[2] - \\text{dilation}[2]\n      \\times (\\text{kernel\\_size}[2] - 1) - 1}{\\text{stride}[2]} + 1\\right\\rfloor\n\nWout​=⌊stride[2]Win​+2×padding[2]−dilation[2]×(kernel_size[2]−1)−1​+1⌋\n\n",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.functional.conv3d",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.functional.conv3d.html#torch.nn.functional.conv3d",
        "api_signature": "torch.nn.functional.conv3d(input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1)",
        "api_description": "Applies a 3D convolution over an input image composed of several input\nplanes.",
        "return_value": "",
        "parameters": "input – input tensor of shape (minibatch,in_channels,iT,iH,iW)(\\text{minibatch} , \\text{in\\_channels} , iT , iH , iW)(minibatch,in_channels,iT,iH,iW)\nweight – filters of shape (out_channels,in_channelsgroups,kT,kH,kW)(\\text{out\\_channels} , \\frac{\\text{in\\_channels}}{\\text{groups}} , kT , kH , kW)(out_channels,groupsin_channels​,kT,kH,kW)\nbias – optional bias tensor of shape (out_channels)(\\text{out\\_channels})(out_channels). Default: None\nstride – the stride of the convolving kernel. Can be a single number or a\ntuple (sT, sH, sW). Default: 1\npadding – implicit paddings on both sides of the input. Can be a string {‘valid’, ‘same’},\nsingle number or a tuple (padT, padH, padW). Default: 0\npadding='valid' is the same as no padding. padding='same' pads\nthe input so the output has the same shape as the input. However, this mode\ndoesn’t support any stride values other than 1.\nWarning\nFor padding='same', if the weight is even-length and\ndilation is odd in any dimension, a full pad() operation\nmay be needed internally. Lowering performance.\ndilation – the spacing between kernel elements. Can be a single number or\na tuple (dT, dH, dW). Default: 1\ngroups – split input into groups, in_channels\\text{in\\_channels}in_channels should be divisible by\nthe number of groups. Default: 1",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.functional.conv_transpose1d",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.functional.conv_transpose1d.html#torch.nn.functional.conv_transpose1d",
        "api_signature": "torch.nn.functional.conv_transpose1d(input, weight, bias=None, stride=1, padding=0, output_padding=0, groups=1, dilation=1)",
        "api_description": "Applies a 1D transposed convolution operator over an input signal\ncomposed of several input planes, sometimes also called “deconvolution”.",
        "return_value": "",
        "parameters": "input – input tensor of shape (minibatch,in_channels,iW)(\\text{minibatch} , \\text{in\\_channels} , iW)(minibatch,in_channels,iW)\nweight – filters of shape (in_channels,out_channelsgroups,kW)(\\text{in\\_channels} , \\frac{\\text{out\\_channels}}{\\text{groups}} , kW)(in_channels,groupsout_channels​,kW)\nbias – optional bias of shape (out_channels)(\\text{out\\_channels})(out_channels). Default: None\nstride – the stride of the convolving kernel. Can be a single number or a\ntuple (sW,). Default: 1\npadding – dilation * (kernel_size - 1) - padding zero-padding will be added to both\nsides of each dimension in the input. Can be a single number or a tuple\n(padW,). Default: 0\noutput_padding – additional size added to one side of each dimension in the\noutput shape. Can be a single number or a tuple (out_padW). Default: 0\ngroups – split input into groups, in_channels\\text{in\\_channels}in_channels should be divisible by the\nnumber of groups. Default: 1\ndilation – the spacing between kernel elements. Can be a single number or\na tuple (dW,). Default: 1",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.functional.conv_transpose2d",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.functional.conv_transpose2d.html#torch.nn.functional.conv_transpose2d",
        "api_signature": "torch.nn.functional.conv_transpose2d(input, weight, bias=None, stride=1, padding=0, output_padding=0, groups=1, dilation=1)",
        "api_description": "Applies a 2D transposed convolution operator over an input image\ncomposed of several input planes, sometimes also called “deconvolution”.",
        "return_value": "",
        "parameters": "input – input tensor of shape (minibatch,in_channels,iH,iW)(\\text{minibatch} , \\text{in\\_channels} , iH , iW)(minibatch,in_channels,iH,iW)\nweight – filters of shape (in_channels,out_channelsgroups,kH,kW)(\\text{in\\_channels} , \\frac{\\text{out\\_channels}}{\\text{groups}} , kH , kW)(in_channels,groupsout_channels​,kH,kW)\nbias – optional bias of shape (out_channels)(\\text{out\\_channels})(out_channels). Default: None\nstride – the stride of the convolving kernel. Can be a single number or a\ntuple (sH, sW). Default: 1\npadding – dilation * (kernel_size - 1) - padding zero-padding will be added to both\nsides of each dimension in the input. Can be a single number or a tuple\n(padH, padW). Default: 0\noutput_padding – additional size added to one side of each dimension in the\noutput shape. Can be a single number or a tuple (out_padH, out_padW).\nDefault: 0\ngroups – split input into groups, in_channels\\text{in\\_channels}in_channels should be divisible by the\nnumber of groups. Default: 1\ndilation – the spacing between kernel elements. Can be a single number or\na tuple (dH, dW). Default: 1",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.functional.conv_transpose3d",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.functional.conv_transpose3d.html#torch.nn.functional.conv_transpose3d",
        "api_signature": "torch.nn.functional.conv_transpose3d(input, weight, bias=None, stride=1, padding=0, output_padding=0, groups=1, dilation=1)",
        "api_description": "Applies a 3D transposed convolution operator over an input image\ncomposed of several input planes, sometimes also called “deconvolution”",
        "return_value": "",
        "parameters": "input – input tensor of shape (minibatch,in_channels,iT,iH,iW)(\\text{minibatch} , \\text{in\\_channels} , iT , iH , iW)(minibatch,in_channels,iT,iH,iW)\nweight – filters of shape (in_channels,out_channelsgroups,kT,kH,kW)(\\text{in\\_channels} , \\frac{\\text{out\\_channels}}{\\text{groups}} , kT , kH , kW)(in_channels,groupsout_channels​,kT,kH,kW)\nbias – optional bias of shape (out_channels)(\\text{out\\_channels})(out_channels). Default: None\nstride – the stride of the convolving kernel. Can be a single number or a\ntuple (sT, sH, sW). Default: 1\npadding – dilation * (kernel_size - 1) - padding zero-padding will be added to both\nsides of each dimension in the input. Can be a single number or a tuple\n(padT, padH, padW). Default: 0\noutput_padding – additional size added to one side of each dimension in the\noutput shape. Can be a single number or a tuple\n(out_padT, out_padH, out_padW). Default: 0\ngroups – split input into groups, in_channels\\text{in\\_channels}in_channels should be divisible by the\nnumber of groups. Default: 1\ndilation – the spacing between kernel elements. Can be a single number or\na tuple (dT, dH, dW). Default: 1",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.intrinsic.ConvBn1d",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.nn.intrinsic.ConvBn1d.html#torch.ao.nn.intrinsic.ConvBn1d",
        "api_signature": "torch.ao.nn.intrinsic.ConvBn1d(conv, bn)",
        "api_description": "This is a sequential container which calls the Conv 1d and Batch Norm 1d modules.\nDuring quantization this will be replaced with the corresponding fused module.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.intrinsic.qat.ConvBn1d",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.nn.intrinsic.qat.ConvBn1d.html#torch.ao.nn.intrinsic.qat.ConvBn1d",
        "api_signature": "torch.ao.nn.intrinsic.qat.ConvBn1d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=None, padding_mode='zeros', eps=1e-05, momentum=0.1, freeze_bn=False, qconfig=None)",
        "api_description": "A ConvBn1d module is a module fused from Conv1d and BatchNorm1d,\nattached with FakeQuantize modules for weight,\nused in quantization aware training.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.intrinsic.ConvBn2d",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.nn.intrinsic.ConvBn2d.html#torch.ao.nn.intrinsic.ConvBn2d",
        "api_signature": "torch.ao.nn.intrinsic.ConvBn2d(conv, bn)",
        "api_description": "This is a sequential container which calls the Conv 2d and Batch Norm 2d modules.\nDuring quantization this will be replaced with the corresponding fused module.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.intrinsic.qat.ConvBn2d",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.nn.intrinsic.qat.ConvBn2d.html#torch.ao.nn.intrinsic.qat.ConvBn2d",
        "api_signature": "torch.ao.nn.intrinsic.qat.ConvBn2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=None, padding_mode='zeros', eps=1e-05, momentum=0.1, freeze_bn=False, qconfig=None)",
        "api_description": "A ConvBn2d module is a module fused from Conv2d and BatchNorm2d,\nattached with FakeQuantize modules for weight,\nused in quantization aware training.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.intrinsic.ConvBn3d",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.nn.intrinsic.ConvBn3d.html#torch.ao.nn.intrinsic.ConvBn3d",
        "api_signature": "torch.ao.nn.intrinsic.ConvBn3d(conv, bn)",
        "api_description": "This is a sequential container which calls the Conv 3d and Batch Norm 3d modules.\nDuring quantization this will be replaced with the corresponding fused module.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.intrinsic.qat.ConvBn3d",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.nn.intrinsic.qat.ConvBn3d.html#torch.ao.nn.intrinsic.qat.ConvBn3d",
        "api_signature": "torch.ao.nn.intrinsic.qat.ConvBn3d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=None, padding_mode='zeros', eps=1e-05, momentum=0.1, freeze_bn=False, qconfig=None)",
        "api_description": "A ConvBn3d module is a module fused from Conv3d and BatchNorm3d,\nattached with FakeQuantize modules for weight,\nused in quantization aware training.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.intrinsic.ConvBnReLU1d",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.nn.intrinsic.ConvBnReLU1d.html#torch.ao.nn.intrinsic.ConvBnReLU1d",
        "api_signature": "torch.ao.nn.intrinsic.ConvBnReLU1d(conv, bn, relu)",
        "api_description": "This is a sequential container which calls the Conv 1d, Batch Norm 1d, and ReLU modules.\nDuring quantization this will be replaced with the corresponding fused module.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.intrinsic.qat.ConvBnReLU1d",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.nn.intrinsic.qat.ConvBnReLU1d.html#torch.ao.nn.intrinsic.qat.ConvBnReLU1d",
        "api_signature": "torch.ao.nn.intrinsic.qat.ConvBnReLU1d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=None, padding_mode='zeros', eps=1e-05, momentum=0.1, freeze_bn=False, qconfig=None)",
        "api_description": "A ConvBnReLU1d module is a module fused from Conv1d, BatchNorm1d and ReLU,\nattached with FakeQuantize modules for weight,\nused in quantization aware training.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.intrinsic.ConvBnReLU2d",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.nn.intrinsic.ConvBnReLU2d.html#torch.ao.nn.intrinsic.ConvBnReLU2d",
        "api_signature": "torch.ao.nn.intrinsic.ConvBnReLU2d(conv, bn, relu)",
        "api_description": "This is a sequential container which calls the Conv 2d, Batch Norm 2d, and ReLU modules.\nDuring quantization this will be replaced with the corresponding fused module.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.intrinsic.qat.ConvBnReLU2d",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.nn.intrinsic.qat.ConvBnReLU2d.html#torch.ao.nn.intrinsic.qat.ConvBnReLU2d",
        "api_signature": "torch.ao.nn.intrinsic.qat.ConvBnReLU2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=None, padding_mode='zeros', eps=1e-05, momentum=0.1, freeze_bn=False, qconfig=None)",
        "api_description": "A ConvBnReLU2d module is a module fused from Conv2d, BatchNorm2d and ReLU,\nattached with FakeQuantize modules for weight,\nused in quantization aware training.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.intrinsic.ConvBnReLU3d",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.nn.intrinsic.ConvBnReLU3d.html#torch.ao.nn.intrinsic.ConvBnReLU3d",
        "api_signature": "torch.ao.nn.intrinsic.ConvBnReLU3d(conv, bn, relu)",
        "api_description": "This is a sequential container which calls the Conv 3d, Batch Norm 3d, and ReLU modules.\nDuring quantization this will be replaced with the corresponding fused module.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.intrinsic.qat.ConvBnReLU3d",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.nn.intrinsic.qat.ConvBnReLU3d.html#torch.ao.nn.intrinsic.qat.ConvBnReLU3d",
        "api_signature": "torch.ao.nn.intrinsic.qat.ConvBnReLU3d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=None, padding_mode='zeros', eps=1e-05, momentum=0.1, freeze_bn=False, qconfig=None)",
        "api_description": "A ConvBnReLU3d module is a module fused from Conv3d, BatchNorm3d and ReLU,\nattached with FakeQuantize modules for weight,\nused in quantization aware training.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.convert",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.quantization.convert.html#torch.ao.quantization.convert",
        "api_signature": "torch.ao.quantization.convert(module, mapping=None, inplace=False, remove_qconfig=True, is_reference=False, convert_custom_config_dict=None)",
        "api_description": "Converts submodules in input module to a different module according to mapping\nby calling from_float method on the target module class. And remove qconfig at the\nend if remove_qconfig is set to True.",
        "return_value": "",
        "parameters": "module – prepared and calibrated module\nmapping – a dictionary that maps from source module type to target\nmodule type, can be overwritten to allow swapping user defined\nModules\ninplace – carry out model transformations in-place, the original module\nis mutated\nconvert_custom_config_dict – custom configuration dictionary for convert function",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.utils.convert_conv2d_weight_memory_format",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.utils.convert_conv2d_weight_memory_format.html#torch.nn.utils.convert_conv2d_weight_memory_format",
        "api_signature": "torch.nn.utils.convert_conv2d_weight_memory_format(module, memory_format)",
        "api_description": "Convert memory_format of nn.Conv2d.weight to memory_format.",
        "return_value": "The original module with updated nn.Conv2d\n",
        "parameters": "module (nn.Module) – nn.Conv2d & nn.ConvTranspose2d or container\nnn.Module\nmemory_format – user specified memory_format,\ne.g. torch.channels_last or torch.contiguous_format",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.utils.convert_conv3d_weight_memory_format",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.utils.convert_conv3d_weight_memory_format.html#torch.nn.utils.convert_conv3d_weight_memory_format",
        "api_signature": "torch.nn.utils.convert_conv3d_weight_memory_format(module, memory_format)",
        "api_description": "Convert memory_format of nn.Conv3d.weight to memory_format\nThe conversion recursively applies to nested nn.Module, including module.\nNote that it only changes the memory_format, but not the semantics of each dimensions.\nThis function is used to facilitate the computation to adopt NHWC kernels, which\nprovides considerable speed up for fp16 data on CUDA devices with compute capability >= 7.0",
        "return_value": "The original module with updated nn.Conv3d\n",
        "parameters": "module (nn.Module) – nn.Conv3d & nn.ConvTranspose3d or container\nnn.Module\nmemory_format – user specified memory_format,\ne.g. torch.channels_last or torch.contiguous_format",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.quantize_fx.convert_fx",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.quantization.quantize_fx.convert_fx.html#torch.ao.quantization.quantize_fx.convert_fx",
        "api_signature": "torch.ao.quantization.quantize_fx.convert_fx(graph_module, convert_custom_config=None, _remove_qconfig=True, qconfig_mapping=None, backend_config=None)",
        "api_description": "Convert a calibrated or trained model to a quantized model",
        "return_value": "A quantized model (torch.nn.Module)\n",
        "parameters": "graph_module (*) – A prepared and calibrated/trained model (GraphModule)\nconvert_custom_config (*) – custom configurations for convert function.\nSee ConvertCustomConfig for more details\n_remove_qconfig (*) – Option to remove the qconfig attributes in the model after convert.\nqconfig_mapping (*) – config for specifying how to convert a model for quantization.\nThe keys must include the ones in the qconfig_mapping passed to prepare_fx or prepare_qat_fx,\nwith the same values or None. Additional keys can be specified with values set to None.\nFor each entry whose value is set to None, we skip quantizing that entry in the model:\nqconfig_mapping = QConfigMapping\n.set_global(qconfig_from_prepare)\n.set_object_type(torch.nn.functional.add, None)  # skip quantizing torch.nn.functional.add\n.set_object_type(torch.nn.functional.linear, qconfig_from_prepare)\n.set_module_name(\"foo.bar\", None)  # skip quantizing module \"foo.bar\"\nbackend_config (BackendConfig): A configuration for the backend which describes howoperators should be quantized in the backend, this includes quantization\nmode support (static/dynamic/weight_only), dtype support (quint8/qint8 etc.),\nobserver placement for each operators and fused operators.\nSee BackendConfig for more details",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.ns._numeric_suite_fx.convert_n_shadows_model",
        "api_url": "https://pytorch.org/docs/stable/torch.ao.ns._numeric_suite_fx.html#torch.ao.ns._numeric_suite_fx.convert_n_shadows_model",
        "api_signature": "torch.ao.ns._numeric_suite_fx.convert_n_shadows_model(model, custom_convert_fn=None, custom_convert_kwargs=None)",
        "api_description": "Given a model from prepare_n_shadows_model, runs convert_fx\non each shadow submodule.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.SyncBatchNorm.convert_sync_batchnorm",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.SyncBatchNorm.html#torch.nn.SyncBatchNorm.convert_sync_batchnorm",
        "api_signature": "convert_sync_batchnorm(module, process_group=None)",
        "api_description": "Converts all BatchNorm*D layers in the model to torch.nn.SyncBatchNorm layers.",
        "return_value": "The original module with the converted torch.nn.SyncBatchNorm\nlayers. If the original module is a BatchNorm*D layer,\na new torch.nn.SyncBatchNorm layer object will be returned\ninstead.\n",
        "parameters": "module (nn.Module) – module containing one or more BatchNorm*D layers\nprocess_group (optional) – process group to scope synchronization,\ndefault is the whole world",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.fx.custom_config.ConvertCustomConfig",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.quantization.fx.custom_config.ConvertCustomConfig.html#torch.ao.quantization.fx.custom_config.ConvertCustomConfig",
        "api_signature": null,
        "api_description": "Custom configuration for convert_fx().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.intrinsic.ConvReLU1d",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.nn.intrinsic.ConvReLU1d.html#torch.ao.nn.intrinsic.ConvReLU1d",
        "api_signature": "torch.ao.nn.intrinsic.ConvReLU1d(conv, relu)",
        "api_description": "This is a sequential container which calls the Conv1d and ReLU modules.\nDuring quantization this will be replaced with the corresponding fused module.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.intrinsic.quantized.ConvReLU1d",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.nn.intrinsic.quantized.ConvReLU1d.html#torch.ao.nn.intrinsic.quantized.ConvReLU1d",
        "api_signature": "torch.ao.nn.intrinsic.quantized.ConvReLU1d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None)",
        "api_description": "A ConvReLU1d module is a fused module of Conv1d and ReLU",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.intrinsic.ConvReLU2d",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.nn.intrinsic.ConvReLU2d.html#torch.ao.nn.intrinsic.ConvReLU2d",
        "api_signature": "torch.ao.nn.intrinsic.ConvReLU2d(conv, relu)",
        "api_description": "This is a sequential container which calls the Conv2d and ReLU modules.\nDuring quantization this will be replaced with the corresponding fused module.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.intrinsic.qat.ConvReLU2d",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.nn.intrinsic.qat.ConvReLU2d.html#torch.ao.nn.intrinsic.qat.ConvReLU2d",
        "api_signature": "torch.ao.nn.intrinsic.qat.ConvReLU2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', qconfig=None)",
        "api_description": "A ConvReLU2d module is a fused module of Conv2d and ReLU, attached with\nFakeQuantize modules for weight for\nquantization aware training.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.intrinsic.quantized.ConvReLU2d",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.nn.intrinsic.quantized.ConvReLU2d.html#torch.ao.nn.intrinsic.quantized.ConvReLU2d",
        "api_signature": "torch.ao.nn.intrinsic.quantized.ConvReLU2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None)",
        "api_description": "A ConvReLU2d module is a fused module of Conv2d and ReLU",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.intrinsic.ConvReLU3d",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.nn.intrinsic.ConvReLU3d.html#torch.ao.nn.intrinsic.ConvReLU3d",
        "api_signature": "torch.ao.nn.intrinsic.ConvReLU3d(conv, relu)",
        "api_description": "This is a sequential container which calls the Conv3d and ReLU modules.\nDuring quantization this will be replaced with the corresponding fused module.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.intrinsic.qat.ConvReLU3d",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.nn.intrinsic.qat.ConvReLU3d.html#torch.ao.nn.intrinsic.qat.ConvReLU3d",
        "api_signature": "torch.ao.nn.intrinsic.qat.ConvReLU3d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', qconfig=None)",
        "api_description": "A ConvReLU3d module is a fused module of Conv3d and ReLU, attached with\nFakeQuantize modules for weight for\nquantization aware training.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.intrinsic.quantized.ConvReLU3d",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.nn.intrinsic.quantized.ConvReLU3d.html#torch.ao.nn.intrinsic.quantized.ConvReLU3d",
        "api_signature": "torch.ao.nn.intrinsic.quantized.ConvReLU3d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None)",
        "api_description": "A ConvReLU3d module is a fused module of Conv3d and ReLU",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.quantized.ConvTranspose1d",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.nn.quantized.ConvTranspose1d.html#torch.ao.nn.quantized.ConvTranspose1d",
        "api_signature": "torch.ao.nn.quantized.ConvTranspose1d(in_channels, out_channels, kernel_size, stride=1, padding=0, output_padding=0, groups=1, bias=True, dilation=1, padding_mode='zeros', device=None, dtype=None)",
        "api_description": "Applies a 1D transposed convolution operator over an input image\ncomposed of several input planes.\nFor details on input arguments, parameters, and implementation see\nConvTranspose1d.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.ConvTranspose1d",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose1d.html#torch.nn.ConvTranspose1d",
        "api_signature": "torch.nn.ConvTranspose1d(in_channels, out_channels, kernel_size, stride=1, padding=0, output_padding=0, groups=1, bias=True, dilation=1, padding_mode='zeros', device=None, dtype=None)",
        "api_description": "Applies a 1D transposed convolution operator over an input image\ncomposed of several input planes.",
        "return_value": "",
        "parameters": "in_channels (int) – Number of channels in the input image\nout_channels (int) – Number of channels produced by the convolution\nkernel_size (int or tuple) – Size of the convolving kernel\nstride (int or tuple, optional) – Stride of the convolution. Default: 1\npadding (int or tuple, optional) – dilation * (kernel_size - 1) - padding zero-padding\nwill be added to both sides of the input. Default: 0\noutput_padding (int or tuple, optional) – Additional size added to one side\nof the output shape. Default: 0\ngroups (int, optional) – Number of blocked connections from input channels to output channels. Default: 1\nbias (bool, optional) – If True, adds a learnable bias to the output. Default: True\ndilation (int or tuple, optional) – Spacing between kernel elements. Default: 1",
        "input_shape": "\nInput: (N,Cin,Lin)(N, C_{in}, L_{in})(N,Cin​,Lin​) or (Cin,Lin)(C_{in}, L_{in})(Cin​,Lin​)\nOutput: (N,Cout,Lout)(N, C_{out}, L_{out})(N,Cout​,Lout​) or (Cout,Lout)(C_{out}, L_{out})(Cout​,Lout​), where\n\nLout=(Lin−1)×stride−2×padding+dilation×(kernel_size−1)+output_padding+1L_{out} = (L_{in} - 1) \\times \\text{stride} - 2 \\times \\text{padding} + \\text{dilation}\n          \\times (\\text{kernel\\_size} - 1) + \\text{output\\_padding} + 1\n\nLout​=(Lin​−1)×stride−2×padding+dilation×(kernel_size−1)+output_padding+1\n\n",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.quantized.ConvTranspose2d",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.nn.quantized.ConvTranspose2d.html#torch.ao.nn.quantized.ConvTranspose2d",
        "api_signature": "torch.ao.nn.quantized.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=1, padding=0, output_padding=0, groups=1, bias=True, dilation=1, padding_mode='zeros', device=None, dtype=None)",
        "api_description": "Applies a 2D transposed convolution operator over an input image\ncomposed of several input planes.\nFor details on input arguments, parameters, and implementation see\nConvTranspose2d.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.ConvTranspose2d",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d.html#torch.nn.ConvTranspose2d",
        "api_signature": "torch.nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=1, padding=0, output_padding=0, groups=1, bias=True, dilation=1, padding_mode='zeros', device=None, dtype=None)",
        "api_description": "Applies a 2D transposed convolution operator over an input image\ncomposed of several input planes.",
        "return_value": "",
        "parameters": "in_channels (int) – Number of channels in the input image\nout_channels (int) – Number of channels produced by the convolution\nkernel_size (int or tuple) – Size of the convolving kernel\nstride (int or tuple, optional) – Stride of the convolution. Default: 1\npadding (int or tuple, optional) – dilation * (kernel_size - 1) - padding zero-padding\nwill be added to both sides of each dimension in the input. Default: 0\noutput_padding (int or tuple, optional) – Additional size added to one side\nof each dimension in the output shape. Default: 0\ngroups (int, optional) – Number of blocked connections from input channels to output channels. Default: 1\nbias (bool, optional) – If True, adds a learnable bias to the output. Default: True\ndilation (int or tuple, optional) – Spacing between kernel elements. Default: 1",
        "input_shape": "\nInput: (N,Cin,Hin,Win)(N, C_{in}, H_{in}, W_{in})(N,Cin​,Hin​,Win​) or (Cin,Hin,Win)(C_{in}, H_{in}, W_{in})(Cin​,Hin​,Win​)\nOutput: (N,Cout,Hout,Wout)(N, C_{out}, H_{out}, W_{out})(N,Cout​,Hout​,Wout​) or (Cout,Hout,Wout)(C_{out}, H_{out}, W_{out})(Cout​,Hout​,Wout​), where\n\n\nHout=(Hin−1)×stride[0]−2×padding[0]+dilation[0]×(kernel_size[0]−1)+output_padding[0]+1H_{out} = (H_{in} - 1) \\times \\text{stride}[0] - 2 \\times \\text{padding}[0] + \\text{dilation}[0]\n          \\times (\\text{kernel\\_size}[0] - 1) + \\text{output\\_padding}[0] + 1\n\nHout​=(Hin​−1)×stride[0]−2×padding[0]+dilation[0]×(kernel_size[0]−1)+output_padding[0]+1\nWout=(Win−1)×stride[1]−2×padding[1]+dilation[1]×(kernel_size[1]−1)+output_padding[1]+1W_{out} = (W_{in} - 1) \\times \\text{stride}[1] - 2 \\times \\text{padding}[1] + \\text{dilation}[1]\n          \\times (\\text{kernel\\_size}[1] - 1) + \\text{output\\_padding}[1] + 1\n\nWout​=(Win​−1)×stride[1]−2×padding[1]+dilation[1]×(kernel_size[1]−1)+output_padding[1]+1",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.quantized.ConvTranspose3d",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.nn.quantized.ConvTranspose3d.html#torch.ao.nn.quantized.ConvTranspose3d",
        "api_signature": "torch.ao.nn.quantized.ConvTranspose3d(in_channels, out_channels, kernel_size, stride=1, padding=0, output_padding=0, groups=1, bias=True, dilation=1, padding_mode='zeros', device=None, dtype=None)",
        "api_description": "Applies a 3D transposed convolution operator over an input image\ncomposed of several input planes.\nFor details on input arguments, parameters, and implementation see\nConvTranspose3d.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.ConvTranspose3d",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose3d.html#torch.nn.ConvTranspose3d",
        "api_signature": "torch.nn.ConvTranspose3d(in_channels, out_channels, kernel_size, stride=1, padding=0, output_padding=0, groups=1, bias=True, dilation=1, padding_mode='zeros', device=None, dtype=None)",
        "api_description": "Applies a 3D transposed convolution operator over an input image composed of several input\nplanes.\nThe transposed convolution operator multiplies each input value element-wise by a learnable kernel,\nand sums over the outputs from all input feature planes.",
        "return_value": "",
        "parameters": "in_channels (int) – Number of channels in the input image\nout_channels (int) – Number of channels produced by the convolution\nkernel_size (int or tuple) – Size of the convolving kernel\nstride (int or tuple, optional) – Stride of the convolution. Default: 1\npadding (int or tuple, optional) – dilation * (kernel_size - 1) - padding zero-padding\nwill be added to both sides of each dimension in the input. Default: 0\noutput_padding (int or tuple, optional) – Additional size added to one side\nof each dimension in the output shape. Default: 0\ngroups (int, optional) – Number of blocked connections from input channels to output channels. Default: 1\nbias (bool, optional) – If True, adds a learnable bias to the output. Default: True\ndilation (int or tuple, optional) – Spacing between kernel elements. Default: 1",
        "input_shape": "\nInput: (N,Cin,Din,Hin,Win)(N, C_{in}, D_{in}, H_{in}, W_{in})(N,Cin​,Din​,Hin​,Win​) or (Cin,Din,Hin,Win)(C_{in}, D_{in}, H_{in}, W_{in})(Cin​,Din​,Hin​,Win​)\nOutput: (N,Cout,Dout,Hout,Wout)(N, C_{out}, D_{out}, H_{out}, W_{out})(N,Cout​,Dout​,Hout​,Wout​) or\n(Cout,Dout,Hout,Wout)(C_{out}, D_{out}, H_{out}, W_{out})(Cout​,Dout​,Hout​,Wout​), where\n\n\nDout=(Din−1)×stride[0]−2×padding[0]+dilation[0]×(kernel_size[0]−1)+output_padding[0]+1D_{out} = (D_{in} - 1) \\times \\text{stride}[0] - 2 \\times \\text{padding}[0] + \\text{dilation}[0]\n          \\times (\\text{kernel\\_size}[0] - 1) + \\text{output\\_padding}[0] + 1\n\nDout​=(Din​−1)×stride[0]−2×padding[0]+dilation[0]×(kernel_size[0]−1)+output_padding[0]+1\nHout=(Hin−1)×stride[1]−2×padding[1]+dilation[1]×(kernel_size[1]−1)+output_padding[1]+1H_{out} = (H_{in} - 1) \\times \\text{stride}[1] - 2 \\times \\text{padding}[1] + \\text{dilation}[1]\n          \\times (\\text{kernel\\_size}[1] - 1) + \\text{output\\_padding}[1] + 1\n\nHout​=(Hin​−1)×stride[1]−2×padding[1]+dilation[1]×(kernel_size[1]−1)+output_padding[1]+1\nWout=(Win−1)×stride[2]−2×padding[2]+dilation[2]×(kernel_size[2]−1)+output_padding[2]+1W_{out} = (W_{in} - 1) \\times \\text{stride}[2] - 2 \\times \\text{padding}[2] + \\text{dilation}[2]\n          \\times (\\text{kernel\\_size}[2] - 1) + \\text{output\\_padding}[2] + 1\n\nWout​=(Win​−1)×stride[2]−2×padding[2]+dilation[2]×(kernel_size[2]−1)+output_padding[2]+1",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.autograd.profiler_util.StringTable.copy",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.autograd.profiler_util.StringTable.html#torch.autograd.profiler_util.StringTable.copy",
        "api_signature": "copy()",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.ParameterDict.copy",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.ParameterDict.html#torch.nn.ParameterDict.copy",
        "api_signature": "copy()",
        "api_description": "Return a copy of this ParameterDict instance.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.copy_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.copy_.html#torch.Tensor.copy_",
        "api_signature": "Tensor.copy_(src, non_blocking=False)",
        "api_description": "Copies the elements from src into self tensor and returns\nself.",
        "return_value": "",
        "parameters": "src (Tensor) – the source tensor to copy from\nnon_blocking (bool) – if True and this copy is between CPU and GPU,\nthe copy may occur asynchronously with respect to the host. For other\ncases, this argument has no effect.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.TypedStorage.copy_",
        "api_url": "https://pytorch.org/docs/stable/storage.html#torch.TypedStorage.copy_",
        "api_signature": "copy_(source, non_blocking=None)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.UntypedStorage.copy_",
        "api_url": "https://pytorch.org/docs/stable/storage.html#torch.UntypedStorage.copy_",
        "api_signature": "copy_()",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.copysign",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.copysign.html#torch.copysign",
        "api_signature": "torch.copysign(input, other, *, out=None)",
        "api_description": "Create a new floating-point tensor with the magnitude of input and the sign of other, elementwise.",
        "return_value": "",
        "parameters": "input (Tensor) – magnitudes.\nother (Tensor or Number) – contains value(s) whose signbit(s) are\napplied to the magnitudes in input.\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.copysign",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.copysign.html#torch.Tensor.copysign",
        "api_signature": "Tensor.copysign(other)",
        "api_description": "See torch.copysign()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.copysign_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.copysign_.html#torch.Tensor.copysign_",
        "api_signature": "Tensor.copysign_(other)",
        "api_description": "In-place version of copysign()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.transforms.CorrCholeskyTransform",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.transforms.CorrCholeskyTransform",
        "api_signature": "torch.distributions.transforms.CorrCholeskyTransform(cache_size=0)",
        "api_description": "Transforms an uncontrained real vector xxx with length D∗(D−1)/2D*(D-1)/2D∗(D−1)/2 into the\nCholesky factor of a D-dimension correlation matrix. This Cholesky factor is a lower\ntriangular matrix with positive diagonals and unit Euclidean norm for each row.\nThe transform is processed as follows:",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.corrcoef",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.corrcoef.html#torch.corrcoef",
        "api_signature": "torch.corrcoef(input)",
        "api_description": "Estimates the Pearson product-moment correlation coefficient matrix of the variables given by the input matrix,\nwhere rows are the variables and columns are the observations.",
        "return_value": "(Tensor) The correlation coefficient matrix of the variables.\n",
        "parameters": "input (Tensor) – A 2D matrix containing multiple variables and observations, or a\nScalar or 1D vector representing a single variable.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.corrcoef",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.corrcoef.html#torch.Tensor.corrcoef",
        "api_signature": "Tensor.corrcoef()",
        "api_description": "See torch.corrcoef()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cos",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.cos.html#torch.cos",
        "api_signature": "torch.cos(input, *, out=None)",
        "api_description": "Returns a new tensor with the cosine  of the elements of input.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.cos",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.cos.html#torch.Tensor.cos",
        "api_signature": "Tensor.cos()",
        "api_description": "See torch.cos()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.cos_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.cos_.html#torch.Tensor.cos_",
        "api_signature": "Tensor.cos_()",
        "api_description": "In-place version of cos()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cosh",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.cosh.html#torch.cosh",
        "api_signature": "torch.cosh(input, *, out=None)",
        "api_description": "Returns a new tensor with the hyperbolic cosine  of the elements of\ninput.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.cosh",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.cosh.html#torch.Tensor.cosh",
        "api_signature": "Tensor.cosh()",
        "api_description": "See torch.cosh()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.cosh_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.cosh_.html#torch.Tensor.cosh_",
        "api_signature": "Tensor.cosh_()",
        "api_description": "In-place version of cosh()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.signal.windows.cosine",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.signal.windows.cosine.html#torch.signal.windows.cosine",
        "api_signature": "torch.signal.windows.cosine(M, *, sym=True, dtype=None, layout=torch.strided, device=None, requires_grad=False)",
        "api_description": "Computes a window with a simple cosine waveform, following the same implementation as SciPy.\nThis window is also known as the sine window.",
        "return_value": "",
        "parameters": "M (int) – the length of the window.\nIn other words, the number of points of the returned window.\nsym (bool, optional) – If False, returns a periodic window suitable for use in spectral analysis.\nIf True, returns a symmetric window suitable for use in filter design. Default: True.\ndtype (torch.dtype, optional) – the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_dtype()).\nlayout (torch.layout, optional) – the desired layout of returned Tensor.\nDefault: torch.strided.\ndevice (torch.device, optional) – the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_device()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types.\nrequires_grad (bool, optional) – If autograd should record operations on the\nreturned tensor. Default: False.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.functional.cosine_embedding_loss",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.functional.cosine_embedding_loss.html#torch.nn.functional.cosine_embedding_loss",
        "api_signature": "torch.nn.functional.cosine_embedding_loss(input1, input2, target, margin=0, size_average=None, reduce=None, reduction='mean')",
        "api_description": "See CosineEmbeddingLoss for details.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.functional.cosine_similarity",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.functional.cosine_similarity.html#torch.nn.functional.cosine_similarity",
        "api_signature": "torch.nn.functional.cosine_similarity(x1, x2, dim=1, eps=1e-8)",
        "api_description": "Returns cosine similarity between x1 and x2, computed along dim. x1 and x2 must be broadcastable\nto a common shape. dim refers to the dimension in this common shape. Dimension dim of the output is\nsqueezed (see torch.squeeze()), resulting in the\noutput tensor having 1 fewer dimension.",
        "return_value": "",
        "parameters": "x1 (Tensor) – First input.\nx2 (Tensor) – Second input.\ndim (int, optional) – Dimension along which cosine similarity is computed. Default: 1\neps (float, optional) – Small value to avoid division by zero.\nDefault: 1e-8",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.lr_scheduler.CosineAnnealingLR",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.CosineAnnealingLR.html#torch.optim.lr_scheduler.CosineAnnealingLR",
        "api_signature": "torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max, eta_min=0, last_epoch=-1, verbose='deprecated')",
        "api_description": "Set the learning rate of each parameter group using a cosine annealing\nschedule, where ηmax\\eta_{max}ηmax​ is set to the initial lr and\nTcurT_{cur}Tcur​ is the number of epochs since the last restart in SGDR:",
        "return_value": "",
        "parameters": "optimizer (Optimizer) – Wrapped optimizer.\nT_max (int) – Maximum number of iterations.\neta_min (float) – Minimum learning rate. Default: 0.\nlast_epoch (int) – The index of last epoch. Default: -1.\nverbose (bool) – If True, prints a message to stdout for\neach update. Default: False.\nDeprecated since version 2.2: verbose is deprecated. Please use get_last_lr() to access the\nlearning rate.\nstate_dict (dict) – scheduler state. Should be an object returned\nfrom a call to state_dict().",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.lr_scheduler.CosineAnnealingWarmRestarts",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.CosineAnnealingWarmRestarts.html#torch.optim.lr_scheduler.CosineAnnealingWarmRestarts",
        "api_signature": "torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0, T_mult=1, eta_min=0, last_epoch=-1, verbose='deprecated')",
        "api_description": "Set the learning rate of each parameter group using a cosine annealing\nschedule, where ηmax\\eta_{max}ηmax​ is set to the initial lr, TcurT_{cur}Tcur​\nis the number of epochs since the last restart and TiT_{i}Ti​ is the number\nof epochs between two warm restarts in SGDR:",
        "return_value": "",
        "parameters": "optimizer (Optimizer) – Wrapped optimizer.\nT_0 (int) – Number of iterations for the first restart.\nT_mult (int, optional) – A factor increases TiT_{i}Ti​ after a restart. Default: 1.\neta_min (float, optional) – Minimum learning rate. Default: 0.\nlast_epoch (int, optional) – The index of last epoch. Default: -1.\nverbose (bool) – If True, prints a message to stdout for\neach update. Default: False.\nDeprecated since version 2.2: verbose is deprecated. Please use get_last_lr() to access the\nlearning rate.\nstate_dict (dict) – scheduler state. Should be an object returned\nfrom a call to state_dict().",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.CosineEmbeddingLoss",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.CosineEmbeddingLoss.html#torch.nn.CosineEmbeddingLoss",
        "api_signature": "torch.nn.CosineEmbeddingLoss(margin=0.0, size_average=None, reduce=None, reduction='mean')",
        "api_description": "Creates a criterion that measures the loss given input tensors\nx1x_1x1​, x2x_2x2​ and a Tensor label yyy with values 1 or -1.\nUse (y=1y=1y=1) to maximize the cosine similarity of two inputs, and (y=−1y=-1y=−1) otherwise.\nThis is typically used for learning nonlinear\nembeddings or semi-supervised learning.",
        "return_value": "",
        "parameters": "margin (float, optional) – Should be a number from −1-1−1 to 111,\n000 to 0.50.50.5 is suggested. If margin is missing, the\ndefault value is 000.\nsize_average (bool, optional) – Deprecated (see reduction). By default,\nthe losses are averaged over each loss element in the batch. Note that for\nsome losses, there are multiple elements per sample. If the field size_average\nis set to False, the losses are instead summed for each minibatch. Ignored\nwhen reduce is False. Default: True\nreduce (bool, optional) – Deprecated (see reduction). By default, the\nlosses are averaged or summed over observations for each minibatch depending\non size_average. When reduce is False, returns a loss per\nbatch element instead and ignores size_average. Default: True\nreduction (str, optional) – Specifies the reduction to apply to the output:\n'none' | 'mean' | 'sum'. 'none': no reduction will be applied,\n'mean': the sum of the output will be divided by the number of\nelements in the output, 'sum': the output will be summed. Note: size_average\nand reduce are in the process of being deprecated, and in the meantime,\nspecifying either of those two args will override reduction. Default: 'mean'",
        "input_shape": "\nInput1: (N,D)(N, D)(N,D) or (D)(D)(D), where N is the batch size and D is the embedding dimension.\nInput2: (N,D)(N, D)(N,D) or (D)(D)(D), same shape as Input1.\nTarget: (N)(N)(N) or ()()().\nOutput: If reduction is 'none', then (N)(N)(N), otherwise scalar.\n\n",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.CosineSimilarity",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.CosineSimilarity.html#torch.nn.CosineSimilarity",
        "api_signature": "torch.nn.CosineSimilarity(dim=1, eps=1e-08)",
        "api_description": "Returns cosine similarity between x1x_1x1​ and x2x_2x2​, computed along dim.",
        "return_value": "",
        "parameters": "dim (int, optional) – Dimension where cosine similarity is computed. Default: 1\neps (float, optional) – Small value to avoid division by zero.\nDefault: 1e-8",
        "input_shape": "\nInput1: (∗1,D,∗2)(\\ast_1, D, \\ast_2)(∗1​,D,∗2​) where D is at position dim\n\nInput2: (∗1,D,∗2)(\\ast_1, D, \\ast_2)(∗1​,D,∗2​), same number of dimensions as x1, matching x1 size at dimension dim,and broadcastable with x1 at other dimensions.\n\n\n\nOutput: (∗1,∗2)(\\ast_1, \\ast_2)(∗1​,∗2​)\n\n",
        "notes": "",
        "code_example": ">>> input1 = torch.randn(100, 128)\n>>> input2 = torch.randn(100, 128)\n>>> cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n>>> output = cos(input1, input2)\n\n\n"
    },
    {
        "api_name": "torch.monitor.Stat.count",
        "api_url": "https://pytorch.org/docs/stable/monitor.html#torch.monitor.Stat.count",
        "api_signature": null,
        "api_description": "Number of data points that have currently been collected. Resets\nonce the event has been logged.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.autograd.forward_ad.UnpackedDualTensor.count",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.autograd.forward_ad.UnpackedDualTensor.html#torch.autograd.forward_ad.UnpackedDualTensor.count",
        "api_signature": "count(value, /)",
        "api_description": "Return number of occurrences of value.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.autograd.profiler_util.Kernel.count",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.autograd.profiler_util.Kernel.html#torch.autograd.profiler_util.Kernel.count",
        "api_signature": "count(value, /)",
        "api_description": "Return number of occurrences of value.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.jit.Attribute.count",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.jit.Attribute.html#torch.jit.Attribute.count",
        "api_signature": "count(value, /)",
        "api_description": "Return number of occurrences of value.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.utils.rnn.PackedSequence.count",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.PackedSequence.html#torch.nn.utils.rnn.PackedSequence.count",
        "api_signature": "count(value, /)",
        "api_description": "Return number of occurrences of value.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.count_nonzero",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.count_nonzero.html#torch.count_nonzero",
        "api_signature": "torch.count_nonzero(input, dim=None)",
        "api_description": "Counts the number of non-zero values in the tensor input along the given dim.\nIf no dim is specified then all non-zeros in the tensor are counted.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.\ndim (int or tuple of ints, optional) – Dim or tuple of dims along which to count non-zeros.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.count_nonzero",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.count_nonzero.html#torch.Tensor.count_nonzero",
        "api_signature": "Tensor.count_nonzero(dim=None)",
        "api_description": "See torch.count_nonzero()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.benchmark.CallgrindStats.counts",
        "api_url": "https://pytorch.org/docs/stable/benchmark_utils.html#torch.utils.benchmark.CallgrindStats.counts",
        "api_signature": "counts(*, denoise=False)",
        "api_description": "Returns the total number of instructions executed.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cov",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.cov.html#torch.cov",
        "api_signature": "torch.cov(input, *, correction=1, fweights=None, aweights=None)",
        "api_description": "Estimates the covariance matrix of the variables given by the input matrix, where rows are\nthe variables and columns are the observations.",
        "return_value": "(Tensor) The covariance matrix of the variables.\n",
        "parameters": "input (Tensor) – A 2D matrix containing multiple variables and observations, or a\nScalar or 1D vector representing a single variable.\ncorrection (int, optional) – difference between the sample size and sample degrees of freedom.\nDefaults to Bessel’s correction, correction = 1 which returns the unbiased estimate,\neven if both fweights and aweights are specified. correction = 0\nwill return the simple average. Defaults to 1.\nfweights (tensor, optional) – A Scalar or 1D tensor of observation vector frequencies representing the number of\ntimes each observation should be repeated. Its numel must equal the number of columns of input.\nMust have integral dtype. Ignored if None. Defaults to None.\naweights (tensor, optional) – A Scalar or 1D array of observation vector weights.\nThese relative weights are typically large for observations considered “important” and smaller for\nobservations considered less “important”. Its numel must equal the number of columns of input.\nMust have floating point dtype. Ignored if None. Defaults to None.",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> x = torch.tensor([[0, 2], [1, 1], [2, 0]]).T\n>>> x\ntensor([[0, 1, 2],\n        [2, 1, 0]])\n>>> torch.cov(x)\ntensor([[ 1., -1.],\n        [-1.,  1.]])\n>>> torch.cov(x, correction=0)\ntensor([[ 0.6667, -0.6667],\n        [-0.6667,  0.6667]])\n>>> fw = torch.randint(1, 10, (3,))\n>>> fw\ntensor([1, 6, 9])\n>>> aw = torch.rand(3)\n>>> aw\ntensor([0.4282, 0.0255, 0.4144])\n>>> torch.cov(x, fweights=fw, aweights=aw)\ntensor([[ 0.4169, -0.4169],\n        [-0.4169,  0.4169]])\n\n\n"
    },
    {
        "api_name": "torch.Tensor.cov",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.cov.html#torch.Tensor.cov",
        "api_signature": "Tensor.cov(*, correction=1, fweights=None, aweights=None)",
        "api_description": "See torch.cov()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.covariance_matrix",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.covariance_matrix",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.multivariate_normal.MultivariateNormal.covariance_matrix",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.multivariate_normal.MultivariateNormal.covariance_matrix",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.wishart.Wishart.covariance_matrix",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.wishart.Wishart.covariance_matrix",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.cpp_extension.CppExtension",
        "api_url": "https://pytorch.org/docs/stable/cpp_extension.html#torch.utils.cpp_extension.CppExtension",
        "api_signature": "torch.utils.cpp_extension.CppExtension(name, sources, *args, **kwargs)",
        "api_description": "Create a setuptools.Extension for C++.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.jit.ScriptModule.cpu",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.jit.ScriptModule.html#torch.jit.ScriptModule.cpu",
        "api_signature": "cpu()",
        "api_description": "Move all model parameters and buffers to the CPU.",
        "return_value": "self\n",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.Module.cpu",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.cpu",
        "api_signature": "cpu()",
        "api_description": "Move all model parameters and buffers to the CPU.",
        "return_value": "self\n",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.cpu",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.cpu.html#torch.Tensor.cpu",
        "api_signature": "Tensor.cpu(memory_format=torch.preserve_format)",
        "api_description": "Returns a copy of this object in CPU memory.",
        "return_value": "",
        "parameters": "memory_format (torch.memory_format, optional) – the desired memory format of\nreturned Tensor. Default: torch.preserve_format.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.TypedStorage.cpu",
        "api_url": "https://pytorch.org/docs/stable/storage.html#torch.TypedStorage.cpu",
        "api_signature": "cpu()",
        "api_description": "Return a CPU copy of this storage if it’s not already on the CPU.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.UntypedStorage.cpu",
        "api_url": "https://pytorch.org/docs/stable/storage.html#torch.UntypedStorage.cpu",
        "api_signature": "cpu()",
        "api_description": "Return a CPU copy of this storage if it’s not already on the CPU.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.fsdp.CPUOffload",
        "api_url": "https://pytorch.org/docs/stable/fsdp.html#torch.distributed.fsdp.CPUOffload",
        "api_signature": "torch.distributed.fsdp.CPUOffload(offload_params=False)",
        "api_description": "This configures CPU offloading.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.Tracer.create_arg",
        "api_url": "https://pytorch.org/docs/stable/fx.html#torch.fx.Tracer.create_arg",
        "api_signature": "create_arg(a)",
        "api_description": "A method to specify the behavior of tracing when preparing values to\nbe used as arguments to nodes in the Graph.",
        "return_value": "The value a converted into the appropriate Argument\n",
        "parameters": "a (Any) – The value to be emitted as an Argument in the Graph.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.Tracer.create_args_for_root",
        "api_url": "https://pytorch.org/docs/stable/fx.html#torch.fx.Tracer.create_args_for_root",
        "api_signature": "create_args_for_root(root_fn, is_module, concrete_args=None)",
        "api_description": "Create placeholder nodes corresponding to the signature of the root\nModule. This method introspects root’s signature and emits those\nnodes accordingly, also supporting *args and **kwargs.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.rendezvous.c10d_rendezvous_backend.create_backend",
        "api_url": "https://pytorch.org/docs/stable/elastic/rendezvous.html#torch.distributed.elastic.rendezvous.c10d_rendezvous_backend.create_backend",
        "api_signature": "torch.distributed.elastic.rendezvous.c10d_rendezvous_backend.create_backend(params)",
        "api_description": "Create a new C10dRendezvousBackend from the specified parameters.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.rendezvous.etcd_rendezvous_backend.create_backend",
        "api_url": "https://pytorch.org/docs/stable/elastic/rendezvous.html#torch.distributed.elastic.rendezvous.etcd_rendezvous_backend.create_backend",
        "api_signature": "torch.distributed.elastic.rendezvous.etcd_rendezvous_backend.create_backend(params)",
        "api_description": "Create a new EtcdRendezvousBackend from the specified parameters.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.checkpoint.LoadPlanner.create_global_plan",
        "api_url": "https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.LoadPlanner.create_global_plan",
        "api_signature": "create_global_plan(global_plan)",
        "api_description": "Compute the global load plan and return plans for each rank.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.checkpoint.SavePlanner.create_global_plan",
        "api_url": "https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.SavePlanner.create_global_plan",
        "api_signature": "create_global_plan(all_plans)",
        "api_description": "Compute the global checkpoint plan and return the local plan of each rank.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.rendezvous.dynamic_rendezvous.create_handler",
        "api_url": "https://pytorch.org/docs/stable/elastic/rendezvous.html#torch.distributed.elastic.rendezvous.dynamic_rendezvous.create_handler",
        "api_signature": "torch.distributed.elastic.rendezvous.dynamic_rendezvous.create_handler(store, backend, params)",
        "api_description": "Create a new DynamicRendezvousHandler from the specified parameters.",
        "return_value": "",
        "parameters": "store (Store) – The C10d store to return as part of the rendezvous.\nbackend (RendezvousBackend) – The backend to use to hold the rendezvous state.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.rendezvous.RendezvousHandlerRegistry.create_handler",
        "api_url": "https://pytorch.org/docs/stable/elastic/rendezvous.html#torch.distributed.elastic.rendezvous.RendezvousHandlerRegistry.create_handler",
        "api_signature": "create_handler(params)",
        "api_description": "Create a new RendezvousHandler.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.checkpoint.LoadPlanner.create_local_plan",
        "api_url": "https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.LoadPlanner.create_local_plan",
        "api_signature": "create_local_plan()",
        "api_description": "Create a LoadPlan based on state_dict and metadata provided by set_up_planner.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.checkpoint.SavePlanner.create_local_plan",
        "api_url": "https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.SavePlanner.create_local_plan",
        "api_signature": "create_local_plan()",
        "api_description": "Compute the save plan for the current rank.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.Graph.create_node",
        "api_url": "https://pytorch.org/docs/stable/fx.html#torch.fx.Graph.create_node",
        "api_signature": "create_node(op, target, args=None, kwargs=None, name=None, type_expr=None)",
        "api_description": "Create a Node and add it to the Graph at the current insert-point.\nNote that the current insert-point can be set via Graph.inserting_before()\nand Graph.inserting_after().",
        "return_value": "The newly-created and inserted node.\n",
        "parameters": "op (str) – the opcode for this Node. One of ‘call_function’, ‘call_method’, ‘get_attr’,\n‘call_module’, ‘placeholder’, or ‘output’. The semantics of these opcodes are\ndescribed in the Graph docstring.\nargs (Optional[Tuple[Argument, ...]]) – is a tuple of arguments to this node.\nkwargs (Optional[Dict[str, Argument]]) – the kwargs of this Node\nname (Optional[str]) – an optional string name for the Node.\nThis will influence the name of the value assigned to in the\nPython generated code.\ntype_expr (Optional[Any]) – an optional type annotation representing the\nPython type the output of this node will have.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.Tracer.create_node",
        "api_url": "https://pytorch.org/docs/stable/fx.html#torch.fx.Tracer.create_node",
        "api_signature": "create_node(kind, target, args, kwargs, name=None, type_expr=None)",
        "api_description": "Inserts a graph node given target, args, kwargs, and name.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.Tracer.create_proxy",
        "api_url": "https://pytorch.org/docs/stable/fx.html#torch.fx.Tracer.create_proxy",
        "api_signature": "create_proxy(kind, target, args, kwargs, name=None, type_expr=None, proxy_factory_fn=None)",
        "api_description": "Create a Node from the given arguments, then return the Node\nwrapped in a Proxy object.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.symbolic_shapes.ShapeEnv.create_symbol",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.fx.experimental.symbolic_shapes.ShapeEnv.html#torch.fx.experimental.symbolic_shapes.ShapeEnv.create_symbol",
        "api_signature": "create_symbol(val, source, dynamic_dim=DimDynamic.DUCK, constraint_dim=None, positive=True, do_not_specialize_zero_one=False, symbolic_context=None)",
        "api_description": "Create a new symbol which is tracked by this ShapeEnv",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.symbolic_shapes.ShapeEnv.create_symbolic_sizes_strides_storage_offset",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.fx.experimental.symbolic_shapes.ShapeEnv.html#torch.fx.experimental.symbolic_shapes.ShapeEnv.create_symbolic_sizes_strides_storage_offset",
        "api_signature": "create_symbolic_sizes_strides_storage_offset(ex, source, *, symbolic_context=None)",
        "api_description": "Returns a list of symbolic sizes and strides for the given tensor.\nWe try our best to express stride in terms of the sizes, so as to not\nintroduce new symbolic variables.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.symbolic_shapes.ShapeEnv.create_symboolnode",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.fx.experimental.symbolic_shapes.ShapeEnv.html#torch.fx.experimental.symbolic_shapes.ShapeEnv.create_symboolnode",
        "api_signature": "create_symboolnode(sym)",
        "api_description": "Create a SymBool object from a sympy boolean expression",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.symbolic_shapes.ShapeEnv.create_symintnode",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.fx.experimental.symbolic_shapes.ShapeEnv.html#torch.fx.experimental.symbolic_shapes.ShapeEnv.create_symintnode",
        "api_signature": "create_symintnode(sym, *, hint, source=None)",
        "api_description": "Create a SymInt value from a symbolic expression",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.symbolic_shapes.ShapeEnv.create_unbacked_symbool",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.fx.experimental.symbolic_shapes.ShapeEnv.html#torch.fx.experimental.symbolic_shapes.ShapeEnv.create_unbacked_symbool",
        "api_signature": "create_unbacked_symbool()",
        "api_description": "Create a symbolic boolean without a hint value",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.symbolic_shapes.ShapeEnv.create_unbacked_symfloat",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.fx.experimental.symbolic_shapes.ShapeEnv.html#torch.fx.experimental.symbolic_shapes.ShapeEnv.create_unbacked_symfloat",
        "api_signature": "create_unbacked_symfloat()",
        "api_description": "Create a symbolic float without a hint value",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.symbolic_shapes.ShapeEnv.create_unbacked_symint",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.fx.experimental.symbolic_shapes.ShapeEnv.html#torch.fx.experimental.symbolic_shapes.ShapeEnv.create_unbacked_symint",
        "api_signature": "create_unbacked_symint()",
        "api_description": "Create a symbolic integer without a hint value",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.symbolic_shapes.ShapeEnv.create_unspecified_symbol",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.fx.experimental.symbolic_shapes.ShapeEnv.html#torch.fx.experimental.symbolic_shapes.ShapeEnv.create_unspecified_symbol",
        "api_signature": "create_unspecified_symbol(val, source, dynamic_dim=DimDynamic.DUCK, constraint_dim=None)",
        "api_description": "Create a symbol with an unspecified value",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.symbolic_shapes.ShapeEnv.create_unspecified_symint_and_symbol",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.fx.experimental.symbolic_shapes.ShapeEnv.html#torch.fx.experimental.symbolic_shapes.ShapeEnv.create_unspecified_symint_and_symbol",
        "api_signature": "create_unspecified_symint_and_symbol(value, source, dynamic_dim)",
        "api_description": "Create a SymInt wrapping a new unspecified symbol",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cross",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.cross.html#torch.cross",
        "api_signature": "torch.cross(input, other, dim=None, *, out=None)",
        "api_description": "Returns the cross product of vectors in dimension dim of input\nand other.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.\nother (Tensor) – the second input tensor\ndim (int, optional) – the dimension to take the cross-product in.\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.linalg.cross",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.linalg.cross.html#torch.linalg.cross",
        "api_signature": "torch.linalg.cross(input, other, *, dim=-1, out=None)",
        "api_description": "Computes the cross product of two 3-dimensional vectors.",
        "return_value": "",
        "parameters": "input (Tensor) – the first input tensor.\nother (Tensor) – the second input tensor.\ndim (int, optional) – the dimension along which to take the cross-product. Default: -1.\nout (Tensor, optional) – the output tensor. Ignored if None. Default: None.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.cross",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.cross.html#torch.Tensor.cross",
        "api_signature": "Tensor.cross(other, dim=None)",
        "api_description": "See torch.cross()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.functional.cross_entropy",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.functional.cross_entropy.html#torch.nn.functional.cross_entropy",
        "api_signature": "torch.nn.functional.cross_entropy(input, target, weight=None, size_average=None, ignore_index=-100, reduce=None, reduction='mean', label_smoothing=0.0)",
        "api_description": "Compute the cross entropy loss between input logits and target.",
        "return_value": "",
        "parameters": "input (Tensor) – Predicted unnormalized logits;\nsee Shape section below for supported shapes.\ntarget (Tensor) – Ground truth class indices or class probabilities;\nsee Shape section below for supported shapes.\nweight (Tensor, optional) – a manual rescaling weight given to each\nclass. If given, has to be a Tensor of size C\nsize_average (bool, optional) – Deprecated (see reduction). By default,\nthe losses are averaged over each loss element in the batch. Note that for\nsome losses, there multiple elements per sample. If the field size_average\nis set to False, the losses are instead summed for each minibatch. Ignored\nwhen reduce is False. Default: True\nignore_index (int, optional) – Specifies a target value that is ignored\nand does not contribute to the input gradient. When size_average is\nTrue, the loss is averaged over non-ignored targets. Note that\nignore_index is only applicable when the target contains class indices.\nDefault: -100\nreduce (bool, optional) – Deprecated (see reduction). By default, the\nlosses are averaged or summed over observations for each minibatch depending\non size_average. When reduce is False, returns a loss per\nbatch element instead and ignores size_average. Default: True\nreduction (str, optional) – Specifies the reduction to apply to the output:\n'none' | 'mean' | 'sum'. 'none': no reduction will be applied,\n'mean': the sum of the output will be divided by the number of\nelements in the output, 'sum': the output will be summed. Note: size_average\nand reduce are in the process of being deprecated, and in the meantime,\nspecifying either of those two args will override reduction. Default: 'mean'\nlabel_smoothing (float, optional) – A float in [0.0, 1.0]. Specifies the amount\nof smoothing when computing the loss, where 0.0 means no smoothing. The targets\nbecome a mixture of the original ground truth and a uniform distribution as described in\nRethinking the Inception Architecture for Computer Vision. Default: 0.00.00.0.",
        "input_shape": "\nInput: Shape (C)(C)(C), (N,C)(N, C)(N,C) or (N,C,d1,d2,...,dK)(N, C, d_1, d_2, ..., d_K)(N,C,d1​,d2​,...,dK​) with K≥1K \\geq 1K≥1\nin the case of K-dimensional loss.\nTarget: If containing class indices, shape ()()(), (N)(N)(N) or (N,d1,d2,...,dK)(N, d_1, d_2, ..., d_K)(N,d1​,d2​,...,dK​) with\nK≥1K \\geq 1K≥1 in the case of K-dimensional loss where each value should be between [0,C)[0, C)[0,C).\nIf containing class probabilities, same shape as the input and each value should be between [0,1][0, 1][0,1].\n\nwhere:\n\nC=number of classesN=batch size\\begin{aligned}\n    C ={} & \\text{number of classes} \\\\\n    N ={} & \\text{batch size} \\\\\n\\end{aligned}\n\nC=N=​number of classesbatch size​",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.CrossEntropyLoss",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss",
        "api_signature": "torch.nn.CrossEntropyLoss(weight=None, size_average=None, ignore_index=-100, reduce=None, reduction='mean', label_smoothing=0.0)",
        "api_description": "This criterion computes the cross entropy loss between input logits\nand target.",
        "return_value": "",
        "parameters": "weight (Tensor, optional) – a manual rescaling weight given to each class.\nIf given, has to be a Tensor of size C and floating point dtype\nsize_average (bool, optional) – Deprecated (see reduction). By default,\nthe losses are averaged over each loss element in the batch. Note that for\nsome losses, there are multiple elements per sample. If the field size_average\nis set to False, the losses are instead summed for each minibatch. Ignored\nwhen reduce is False. Default: True\nignore_index (int, optional) – Specifies a target value that is ignored\nand does not contribute to the input gradient. When size_average is\nTrue, the loss is averaged over non-ignored targets. Note that\nignore_index is only applicable when the target contains class indices.\nreduce (bool, optional) – Deprecated (see reduction). By default, the\nlosses are averaged or summed over observations for each minibatch depending\non size_average. When reduce is False, returns a loss per\nbatch element instead and ignores size_average. Default: True\nreduction (str, optional) – Specifies the reduction to apply to the output:\n'none' | 'mean' | 'sum'. 'none': no reduction will\nbe applied, 'mean': the weighted mean of the output is taken,\n'sum': the output will be summed. Note: size_average\nand reduce are in the process of being deprecated, and in\nthe meantime, specifying either of those two args will override\nreduction. Default: 'mean'\nlabel_smoothing (float, optional) – A float in [0.0, 1.0]. Specifies the amount\nof smoothing when computing the loss, where 0.0 means no smoothing. The targets\nbecome a mixture of the original ground truth and a uniform distribution as described in\nRethinking the Inception Architecture for Computer Vision. Default: 0.00.00.0.",
        "input_shape": "\nInput: Shape (C)(C)(C), (N,C)(N, C)(N,C) or (N,C,d1,d2,...,dK)(N, C, d_1, d_2, ..., d_K)(N,C,d1​,d2​,...,dK​) with K≥1K \\geq 1K≥1\nin the case of K-dimensional loss.\nTarget: If containing class indices, shape ()()(), (N)(N)(N) or (N,d1,d2,...,dK)(N, d_1, d_2, ..., d_K)(N,d1​,d2​,...,dK​) with\nK≥1K \\geq 1K≥1 in the case of K-dimensional loss where each value should be between [0,C)[0, C)[0,C).\nIf containing class probabilities, same shape as the input and each value should be between [0,1][0, 1][0,1].\nOutput: If reduction is ‘none’, shape ()()(), (N)(N)(N) or (N,d1,d2,...,dK)(N, d_1, d_2, ..., d_K)(N,d1​,d2​,...,dK​) with K≥1K \\geq 1K≥1\nin the case of K-dimensional loss, depending on the shape of the input. Otherwise, scalar.\n\nwhere:\n\nC=number of classesN=batch size\\begin{aligned}\n    C ={} & \\text{number of classes} \\\\\n    N ={} & \\text{batch size} \\\\\n\\end{aligned}\n\nC=N=​number of classesbatch size​",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.crow_indices",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.crow_indices.html#torch.Tensor.crow_indices",
        "api_signature": "Tensor.crow_indices()",
        "api_description": "Returns the tensor containing the compressed row indices of the self\ntensor when self is a sparse CSR tensor of layout sparse_csr.\nThe crow_indices tensor is strictly of shape (self.size(0) + 1)\nand of type int32 or int64. When using MKL routines such as sparse\nmatrix multiplication, it is necessary to use int32 indexing in order\nto avoid downcasting and potentially losing information.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> csr = torch.eye(5,5).to_sparse_csr()\n>>> csr.crow_indices()\ntensor([0, 1, 2, 3, 4, 5], dtype=torch.int32)\n\n\n"
    },
    {
        "api_name": "torch.nn.functional.ctc_loss",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.functional.ctc_loss.html#torch.nn.functional.ctc_loss",
        "api_signature": "torch.nn.functional.ctc_loss(log_probs, targets, input_lengths, target_lengths, blank=0, reduction='mean', zero_infinity=False)",
        "api_description": "Apply the Connectionist Temporal Classification loss.",
        "return_value": "",
        "parameters": "log_probs (Tensor) – (T,N,C)(T, N, C)(T,N,C) or (T,C)(T, C)(T,C) where C = number of characters in alphabet including blank,\nT = input length, and N = batch size.\nThe logarithmized probabilities of the outputs\n(e.g. obtained with torch.nn.functional.log_softmax()).\ntargets (Tensor) – (N,S)(N, S)(N,S) or (sum(target_lengths)).\nTargets cannot be blank. In the second form, the targets are assumed to be concatenated.\ninput_lengths (Tensor) – (N)(N)(N) or ()()().\nLengths of the inputs (must each be ≤T\\leq T≤T)\ntarget_lengths (Tensor) – (N)(N)(N) or ()()().\nLengths of the targets\nblank (int, optional) – Blank label. Default 000.\nreduction (str, optional) – Specifies the reduction to apply to the output:\n'none' | 'mean' | 'sum'. 'none': no reduction will be applied,\n'mean': the output losses will be divided by the target lengths and\nthen the mean over the batch is taken, 'sum': the output will be\nsummed. Default: 'mean'\nzero_infinity (bool, optional) – Whether to zero infinite losses and the associated gradients.\nDefault: False\nInfinite losses mainly occur when the inputs are too short\nto be aligned to the targets.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.CTCLoss",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.CTCLoss.html#torch.nn.CTCLoss",
        "api_signature": "torch.nn.CTCLoss(blank=0, reduction='mean', zero_infinity=False)",
        "api_description": "The Connectionist Temporal Classification loss.",
        "return_value": "",
        "parameters": "blank (int, optional) – blank label. Default 000.\nreduction (str, optional) – Specifies the reduction to apply to the output:\n'none' | 'mean' | 'sum'. 'none': no reduction will be applied,\n'mean': the output losses will be divided by the target lengths and\nthen the mean over the batch is taken, 'sum': the output losses will be summed.\nDefault: 'mean'\nzero_infinity (bool, optional) – Whether to zero infinite losses and the associated gradients.\nDefault: False\nInfinite losses mainly occur when the inputs are too short\nto be aligned to the targets.",
        "input_shape": "\nLog_probs: Tensor of size (T,N,C)(T, N, C)(T,N,C) or (T,C)(T, C)(T,C),\nwhere T=input lengthT = \\text{input length}T=input length,\nN=batch sizeN = \\text{batch size}N=batch size, and\nC=number of classes (including blank)C = \\text{number of classes (including blank)}C=number of classes (including blank).\nThe logarithmized probabilities of the outputs (e.g. obtained with\ntorch.nn.functional.log_softmax()).\nTargets: Tensor of size (N,S)(N, S)(N,S) or\n(sum⁡(target_lengths))(\\operatorname{sum}(\\text{target\\_lengths}))(sum(target_lengths)),\nwhere N=batch sizeN = \\text{batch size}N=batch size and\nS=max target length, if shape is (N,S)S = \\text{max target length, if shape is } (N, S)S=max target length, if shape is (N,S).\nIt represent the target sequences. Each element in the target\nsequence is a class index. And the target index cannot be blank (default=0).\nIn the (N,S)(N, S)(N,S) form, targets are padded to the\nlength of the longest sequence, and stacked.\nIn the (sum⁡(target_lengths))(\\operatorname{sum}(\\text{target\\_lengths}))(sum(target_lengths)) form,\nthe targets are assumed to be un-padded and\nconcatenated within 1 dimension.\nInput_lengths: Tuple or tensor of size (N)(N)(N) or ()()(),\nwhere N=batch sizeN = \\text{batch size}N=batch size. It represent the lengths of the\ninputs (must each be ≤T\\leq T≤T). And the lengths are specified\nfor each sequence to achieve masking under the assumption that sequences\nare padded to equal lengths.\nTarget_lengths: Tuple or tensor of size (N)(N)(N) or ()()(),\nwhere N=batch sizeN = \\text{batch size}N=batch size. It represent lengths of the targets.\nLengths are specified for each sequence to achieve masking under the\nassumption that sequences are padded to equal lengths. If target shape is\n(N,S)(N,S)(N,S), target_lengths are effectively the stop index\nsns_nsn​ for each target sequence, such that target_n = targets[n,0:s_n] for\neach target in a batch. Lengths must each be ≤S\\leq S≤S\nIf the targets are given as a 1d tensor that is the concatenation of individual\ntargets, the target_lengths must add up to the total length of the tensor.\nOutput: scalar if reduction is 'mean' (default) or\n'sum'. If reduction is 'none', then (N)(N)(N) if input is batched or\n()()() if input is unbatched, where N=batch sizeN = \\text{batch size}N=batch size.\n\n",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.jit.ScriptModule.cuda",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.jit.ScriptModule.html#torch.jit.ScriptModule.cuda",
        "api_signature": "cuda(device=None)",
        "api_description": "Move all model parameters and buffers to the GPU.",
        "return_value": "self\n",
        "parameters": "device (int, optional) – if specified, all parameters will be\ncopied to that device",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.Module.cuda",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.cuda",
        "api_signature": "cuda(device=None)",
        "api_description": "Move all model parameters and buffers to the GPU.",
        "return_value": "self\n",
        "parameters": "device (int, optional) – if specified, all parameters will be\ncopied to that device",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.cuda",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.cuda.html#torch.Tensor.cuda",
        "api_signature": "Tensor.cuda(device=None, non_blocking=False, memory_format=torch.preserve_format)",
        "api_description": "Returns a copy of this object in CUDA memory.",
        "return_value": "",
        "parameters": "device (torch.device) – The destination GPU device.\nDefaults to the current CUDA device.\nnon_blocking (bool) – If True and the source is in pinned memory,\nthe copy will be asynchronous with respect to the host.\nOtherwise, the argument has no effect. Default: False.\nmemory_format (torch.memory_format, optional) – the desired memory format of\nreturned Tensor. Default: torch.preserve_format.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.TypedStorage.cuda",
        "api_url": "https://pytorch.org/docs/stable/storage.html#torch.TypedStorage.cuda",
        "api_signature": "cuda(device=None, non_blocking=False, **kwargs)",
        "api_description": "Returns a copy of this object in CUDA memory.",
        "return_value": "",
        "parameters": "device (int) – The destination GPU id. Defaults to the current device.\nnon_blocking (bool) – If True and the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect.\n**kwargs – For compatibility, may contain the key async in place of\nthe non_blocking argument.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.UntypedStorage.cuda",
        "api_url": "https://pytorch.org/docs/stable/storage.html#torch.UntypedStorage.cuda",
        "api_signature": "cuda(device=None, non_blocking=False, **kwargs)",
        "api_description": "Returns a copy of this object in CUDA memory.",
        "return_value": "",
        "parameters": "device (int) – The destination GPU id. Defaults to the current device.\nnon_blocking (bool) – If True and the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect.\n**kwargs – For compatibility, may contain the key async in place of\nthe non_blocking argument.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.cpp_extension.CUDAExtension",
        "api_url": "https://pytorch.org/docs/stable/cpp_extension.html#torch.utils.cpp_extension.CUDAExtension",
        "api_signature": "torch.utils.cpp_extension.CUDAExtension(name, sources, *args, **kwargs)",
        "api_description": "Create a setuptools.Extension for CUDA/C++.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda.CUDAGraph",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.cuda.CUDAGraph.html#torch.cuda.CUDAGraph",
        "api_signature": null,
        "api_description": "Wrapper around a CUDA graph.",
        "return_value": "",
        "parameters": "pool (optional) – Token (returned by graph_pool_handle() or\nother_Graph_instance.pool()) that hints this graph may share memory\nwith the indicated pool.  See Graph memory management.\ncapture_error_mode (str, optional) – specifies the cudaStreamCaptureMode for the graph capture stream.\nCan be “global”, “thread_local” or “relaxed”. During cuda graph capture, some actions, such as cudaMalloc,\nmay be unsafe. “global” will error on actions in other threads, “thread_local” will only error for\nactions in the current thread, and “relaxed” will not error on these actions. Do NOT change this setting\nunless you’re familiar with cudaStreamCaptureMode\ndebug_path (required) – Path to dump the graph to.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.compiler.cudagraph_mark_step_begin",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.compiler.cudagraph_mark_step_begin.html#torch.compiler.cudagraph_mark_step_begin",
        "api_signature": "torch.compiler.cudagraph_mark_step_begin()",
        "api_description": "Indicates that a new iteration of inference or training is about to begin.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda.CUDAPluggableAllocator",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.cuda.CUDAPluggableAllocator.html#torch.cuda.CUDAPluggableAllocator",
        "api_signature": "torch.cuda.CUDAPluggableAllocator(path_to_so_file, alloc_fn_name, free_fn_name)",
        "api_description": "CUDA memory allocator loaded from a so file.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.backends.cuda.cudnn_sdp_enabled",
        "api_url": "https://pytorch.org/docs/stable/backends.html#torch.backends.cuda.cudnn_sdp_enabled",
        "api_signature": "torch.backends.cuda.cudnn_sdp_enabled()",
        "api_description": "Warning",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.backends.cuda.cufft_plan_cache",
        "api_url": "https://pytorch.org/docs/stable/backends.html#torch.backends.cuda.cufft_plan_cache",
        "api_signature": null,
        "api_description": "cufft_plan_cache contains the cuFFT plan caches for each CUDA device.\nQuery a specific device i’s cache via torch.backends.cuda.cufft_plan_cache[i].",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cummax",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.cummax.html#torch.cummax",
        "api_signature": "torch.cummax(input, dim, *, out=None)",
        "api_description": "Returns a namedtuple (values, indices) where values is the cumulative maximum of\nelements of input in the dimension dim. And indices is the index\nlocation of each maximum value found in the dimension dim.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.\ndim (int) – the dimension to do the operation over\nout (tuple, optional) – the result tuple of two output tensors (values, indices)",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.cummax",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.cummax.html#torch.Tensor.cummax",
        "api_signature": "Tensor.cummax(dim)",
        "api_description": "See torch.cummax()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cummin",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.cummin.html#torch.cummin",
        "api_signature": "torch.cummin(input, dim, *, out=None)",
        "api_description": "Returns a namedtuple (values, indices) where values is the cumulative minimum of\nelements of input in the dimension dim. And indices is the index\nlocation of each maximum value found in the dimension dim.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.\ndim (int) – the dimension to do the operation over\nout (tuple, optional) – the result tuple of two output tensors (values, indices)",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.cummin",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.cummin.html#torch.Tensor.cummin",
        "api_signature": "Tensor.cummin(dim)",
        "api_description": "See torch.cummin()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cumprod",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.cumprod.html#torch.cumprod",
        "api_signature": "torch.cumprod(input, dim, *, dtype=None, out=None)",
        "api_description": "Returns the cumulative product of elements of input in the dimension\ndim.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.\ndim (int) – the dimension to do the operation over\ndtype (torch.dtype, optional) – the desired data type of returned tensor.\nIf specified, the input tensor is casted to dtype before the operation\nis performed. This is useful for preventing data type overflows. Default: None.\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.cumprod",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.cumprod.html#torch.Tensor.cumprod",
        "api_signature": "Tensor.cumprod(dim, dtype=None)",
        "api_description": "See torch.cumprod()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.cumprod_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.cumprod_.html#torch.Tensor.cumprod_",
        "api_signature": "Tensor.cumprod_(dim, dtype=None)",
        "api_description": "In-place version of cumprod()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cumsum",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.cumsum.html#torch.cumsum",
        "api_signature": "torch.cumsum(input, dim, *, dtype=None, out=None)",
        "api_description": "Returns the cumulative sum of elements of input in the dimension\ndim.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.\ndim (int) – the dimension to do the operation over\ndtype (torch.dtype, optional) – the desired data type of returned tensor.\nIf specified, the input tensor is casted to dtype before the operation\nis performed. This is useful for preventing data type overflows. Default: None.\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.cumsum",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.cumsum.html#torch.Tensor.cumsum",
        "api_signature": "Tensor.cumsum(dim, dtype=None)",
        "api_description": "See torch.cumsum()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.cumsum_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.cumsum_.html#torch.Tensor.cumsum_",
        "api_signature": "Tensor.cumsum_(dim, dtype=None)",
        "api_description": "In-place version of cumsum()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cumulative_trapezoid",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.cumulative_trapezoid.html#torch.cumulative_trapezoid",
        "api_signature": "torch.cumulative_trapezoid(y, x=None, *, dx=None, dim=-1)",
        "api_description": "Cumulatively computes the trapezoidal rule\nalong dim. By default the spacing between elements is assumed to be 1, but\ndx can be used to specify a different constant spacing, and x can be\nused to specify arbitrary spacing along dim.",
        "return_value": "",
        "parameters": "y (Tensor) – Values to use when computing the trapezoidal rule.\nx (Tensor) – If specified, defines spacing between values as specified above.\ndx (float) – constant spacing between values. If neither x or dx\nare specified then this defaults to 1. Effectively multiplies the result by its value.\ndim (int) – The dimension along which to compute the trapezoidal rule.\nThe last (inner-most) dimension by default.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.transforms.CumulativeDistributionTransform",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.transforms.CumulativeDistributionTransform",
        "api_signature": "torch.distributions.transforms.CumulativeDistributionTransform(distribution, cache_size=0)",
        "api_description": "Transform via the cumulative distribution function of a probability distribution.",
        "return_value": "",
        "parameters": "distribution (Distribution) – Distribution whose cumulative distribution function to use for\nthe transformation.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.mps.current_allocated_memory",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.mps.current_allocated_memory.html#torch.mps.current_allocated_memory",
        "api_signature": "torch.mps.current_allocated_memory()",
        "api_description": "Returns the current GPU memory occupied by tensors in bytes.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda.current_blas_handle",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.cuda.current_blas_handle.html#torch.cuda.current_blas_handle",
        "api_signature": "torch.cuda.current_blas_handle()",
        "api_description": "Return cublasHandle_t pointer to current cuBLAS handle",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cpu.current_device",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.cpu.current_device.html#torch.cpu.current_device",
        "api_signature": "torch.cpu.current_device()",
        "api_description": "Returns current device for cpu. Always ‘cpu’.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda.current_device",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.cuda.current_device.html#torch.cuda.current_device",
        "api_signature": "torch.cuda.current_device()",
        "api_description": "Return the index of a currently selected device.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.xpu.current_device",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.xpu.current_device.html#torch.xpu.current_device",
        "api_signature": "torch.xpu.current_device()",
        "api_description": "Return the index of a currently selected device.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.autograd.profiler.KinetoStepTracker.current_step",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.autograd.profiler.KinetoStepTracker.html#torch.autograd.profiler.KinetoStepTracker.current_step",
        "api_signature": "current_step()",
        "api_description": "Get the latest step for any requester",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cpu.current_stream",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.cpu.current_stream.html#torch.cpu.current_stream",
        "api_signature": "torch.cpu.current_stream(device=None)",
        "api_description": "Returns the currently selected Stream for a given device.",
        "return_value": "",
        "parameters": "device (torch.device or int, optional) – Ignored.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda.current_stream",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.cuda.current_stream.html#torch.cuda.current_stream",
        "api_signature": "torch.cuda.current_stream(device=None)",
        "api_description": "Return the currently selected Stream for a given device.",
        "return_value": "",
        "parameters": "device (torch.device or int, optional) – selected device. Returns\nthe currently selected Stream for the current device, given\nby current_device(), if device is None\n(default).",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.xpu.current_stream",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.xpu.current_stream.html#torch.xpu.current_stream",
        "api_signature": "torch.xpu.current_stream(device=None)",
        "api_description": "Return the currently selected Stream for a given device.",
        "return_value": "",
        "parameters": "device (torch.device or int, optional) – selected device. Returns\nthe currently selected Stream for the current device, given\nby current_device(), if device is None\n(default).",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda.amp.custom_bwd",
        "api_url": "https://pytorch.org/docs/stable/amp.html#torch.cuda.amp.custom_bwd",
        "api_signature": "torch.cuda.amp.custom_bwd(bwd)",
        "api_description": "Create a helper decorator for backward methods of custom autograd functions.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.utils.prune.custom_from_mask",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.custom_from_mask.html#torch.nn.utils.prune.custom_from_mask",
        "api_signature": "torch.nn.utils.prune.custom_from_mask(module, name, mask)",
        "api_description": "Prune tensor corresponding to parameter called name in module by applying the pre-computed mask in mask.",
        "return_value": "modified (i.e. pruned) version of the input module\n",
        "parameters": "module (nn.Module) – module containing the tensor to prune\nname (str) – parameter name within module on which pruning\nwill act.\nmask (Tensor) – binary mask to be applied to the parameter.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda.amp.custom_fwd",
        "api_url": "https://pytorch.org/docs/stable/amp.html#torch.cuda.amp.custom_fwd",
        "api_signature": "torch.cuda.amp.custom_fwd(fwd=None, *, cast_inputs=None)",
        "api_description": "Create a helper decorator for forward methods of custom autograd functions.",
        "return_value": "",
        "parameters": "cast_inputs (torch.dtype or None, optional, default=None) – If not None,\nwhen forward runs in an autocast-enabled region, casts incoming\nfloating-point CUDA Tensors to the target dtype (non-floating-point Tensors are not affected),\nthen executes forward with autocast disabled.\nIf None, forward’s internal ops execute with the current autocast state.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.utils.prune.CustomFromMask",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.CustomFromMask.html#torch.nn.utils.prune.CustomFromMask",
        "api_signature": "torch.nn.utils.prune.CustomFromMask(mask)",
        "api_description": "Add pruning on the fly and reparametrization of a tensor.",
        "return_value": "pruned version of the input tensor\npruned version of tensor t.\n",
        "parameters": "module (nn.Module) – module containing the tensor to prune\nname (str) – parameter name within module on which pruning\nwill act.\nmodule (nn.Module) – module containing the tensor to prune\nt (torch.Tensor) – tensor to prune (of same dimensions as\ndefault_mask).\nimportance_scores (torch.Tensor) – tensor of importance scores (of\nsame shape as t) used to compute mask for pruning t.\nThe values in this tensor indicate the importance of the\ncorresponding elements in the t that is being pruned.\nIf unspecified or None, the tensor t will be used in its place.\ndefault_mask (torch.Tensor, optional) – mask from previous pruning\niteration, if any. To be considered when determining what\nportion of the tensor that pruning should act on. If None,\ndefault to a mask of ones.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.export.graph_signature.CustomObjArgument",
        "api_url": "https://pytorch.org/docs/stable/export.html#torch.export.graph_signature.CustomObjArgument",
        "api_signature": "torch.export.graph_signature.CustomObjArgument(name: str, class_fqn: str)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.lr_scheduler.CyclicLR",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.CyclicLR.html#torch.optim.lr_scheduler.CyclicLR",
        "api_signature": "torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr, max_lr, step_size_up=2000, step_size_down=None, mode='triangular', gamma=1.0, scale_fn=None, scale_mode='cycle', cycle_momentum=True, base_momentum=0.8, max_momentum=0.9, last_epoch=-1, verbose='deprecated')",
        "api_description": "Sets the learning rate of each parameter group according to\ncyclical learning rate policy (CLR). The policy cycles the learning\nrate between two boundaries with a constant frequency, as detailed in\nthe paper Cyclical Learning Rates for Training Neural Networks.\nThe distance between the two boundaries can be scaled on a per-iteration\nor per-cycle basis.",
        "return_value": "",
        "parameters": "optimizer (Optimizer) – Wrapped optimizer.\nbase_lr (float or list) – Initial learning rate which is the\nlower boundary in the cycle for each parameter group.\nmax_lr (float or list) – Upper learning rate boundaries in the cycle\nfor each parameter group. Functionally,\nit defines the cycle amplitude (max_lr - base_lr).\nThe lr at any cycle is the sum of base_lr\nand some scaling of the amplitude; therefore\nmax_lr may not actually be reached depending on\nscaling function.\nstep_size_up (int) – Number of training iterations in the\nincreasing half of a cycle. Default: 2000\nstep_size_down (int) – Number of training iterations in the\ndecreasing half of a cycle. If step_size_down is None,\nit is set to step_size_up. Default: None\nmode (str) – One of {triangular, triangular2, exp_range}.\nValues correspond to policies detailed above.\nIf scale_fn is not None, this argument is ignored.\nDefault: ‘triangular’\ngamma (float) – Constant in ‘exp_range’ scaling function:\ngamma**(cycle iterations)\nDefault: 1.0\nscale_fn (function) – Custom scaling policy defined by a single\nargument lambda function, where\n0 <= scale_fn(x) <= 1 for all x >= 0.\nIf specified, then ‘mode’ is ignored.\nDefault: None\nscale_mode (str) – {‘cycle’, ‘iterations’}.\nDefines whether scale_fn is evaluated on\ncycle number or cycle iterations (training\niterations since start of cycle).\nDefault: ‘cycle’\ncycle_momentum (bool) – If True, momentum is cycled inversely\nto learning rate between ‘base_momentum’ and ‘max_momentum’.\nDefault: True\nbase_momentum (float or list) – Lower momentum boundaries in the cycle\nfor each parameter group. Note that momentum is cycled inversely\nto learning rate; at the peak of a cycle, momentum is\n‘base_momentum’ and learning rate is ‘max_lr’.\nDefault: 0.8\nmax_momentum (float or list) – Upper momentum boundaries in the cycle\nfor each parameter group. Functionally,\nit defines the cycle amplitude (max_momentum - base_momentum).\nThe momentum at any cycle is the difference of max_momentum\nand some scaling of the amplitude; therefore\nbase_momentum may not actually be reached depending on\nscaling function. Note that momentum is cycled inversely\nto learning rate; at the start of a cycle, momentum is ‘max_momentum’\nand learning rate is ‘base_lr’\nDefault: 0.9\nlast_epoch (int) – The index of the last batch. This parameter is used when\nresuming a training job. Since step() should be invoked after each\nbatch instead of after each epoch, this number represents the total\nnumber of batches computed, not the total number of epochs computed.\nWhen last_epoch=-1, the schedule is started from the beginning.\nDefault: -1\nverbose (bool) – If True, prints a message to stdout for\neach update. Default: False.\nDeprecated since version 2.2: verbose is deprecated. Please use get_last_lr() to access the\nlearning rate.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.monitor.Event.data",
        "api_url": "https://pytorch.org/docs/stable/monitor.html#torch.monitor.Event.data",
        "api_signature": null,
        "api_description": "The structured data contained within the Event.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.utils.rnn.PackedSequence.data",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.PackedSequence.html#torch.nn.utils.rnn.PackedSequence.data",
        "api_signature": null,
        "api_description": "Alias for field number 0",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.parallel.data_parallel",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.functional.torch.nn.parallel.data_parallel.html#torch.nn.parallel.data_parallel",
        "api_signature": "torch.nn.parallel.data_parallel(module, inputs, device_ids=None, output_device=None, dim=0, module_kwargs=None)",
        "api_description": "Evaluate module(input) in parallel across the GPUs given in device_ids.",
        "return_value": "a Tensor containing the result of module(input) located on\noutput_device\n",
        "parameters": "module (Module) – the module to evaluate in parallel\ninputs (Tensor) – inputs to the module\ndevice_ids (list of int or torch.device) – GPU ids on which to replicate module\noutput_device (list of int or torch.device) – GPU location of the output  Use -1 to indicate the CPU.\n(default: device_ids[0])",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.data_ptr",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.data_ptr.html#torch.Tensor.data_ptr",
        "api_signature": "Tensor.data_ptr()",
        "api_description": "Returns the address of the first element of self tensor.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.TypedStorage.data_ptr",
        "api_url": "https://pytorch.org/docs/stable/storage.html#torch.TypedStorage.data_ptr",
        "api_signature": "data_ptr()",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.UntypedStorage.data_ptr",
        "api_url": "https://pytorch.org/docs/stable/storage.html#torch.UntypedStorage.data_ptr",
        "api_signature": "data_ptr()",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.monitor.data_value_t",
        "api_url": "https://pytorch.org/docs/stable/monitor.html#torch.monitor.data_value_t",
        "api_signature": null,
        "api_description": "data_value_t is one of str, float, int, bool.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.data.DataLoader",
        "api_url": "https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader",
        "api_signature": "torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=None, sampler=None, batch_sampler=None, num_workers=0, collate_fn=None, pin_memory=False, drop_last=False, timeout=0, worker_init_fn=None, multiprocessing_context=None, generator=None, *, prefetch_factor=None, persistent_workers=False, pin_memory_device='')",
        "api_description": "Data loader combines a dataset and a sampler, and provides an iterable over the given dataset.",
        "return_value": "",
        "parameters": "dataset (Dataset) – dataset from which to load the data.\nbatch_size (int, optional) – how many samples per batch to load\n(default: 1).\nshuffle (bool, optional) – set to True to have the data reshuffled\nat every epoch (default: False).\nsampler (Sampler or Iterable, optional) – defines the strategy to draw\nsamples from the dataset. Can be any Iterable with __len__\nimplemented. If specified, shuffle must not be specified.\nbatch_sampler (Sampler or Iterable, optional) – like sampler, but\nreturns a batch of indices at a time. Mutually exclusive with\nbatch_size, shuffle, sampler,\nand drop_last.\nnum_workers (int, optional) – how many subprocesses to use for data\nloading. 0 means that the data will be loaded in the main process.\n(default: 0)\ncollate_fn (Callable, optional) – merges a list of samples to form a\nmini-batch of Tensor(s).  Used when using batched loading from a\nmap-style dataset.\npin_memory (bool, optional) – If True, the data loader will copy Tensors\ninto device/CUDA pinned memory before returning them.  If your data elements\nare a custom type, or your collate_fn returns a batch that is a custom type,\nsee the example below.\ndrop_last (bool, optional) – set to True to drop the last incomplete batch,\nif the dataset size is not divisible by the batch size. If False and\nthe size of dataset is not divisible by the batch size, then the last batch\nwill be smaller. (default: False)\ntimeout (numeric, optional) – if positive, the timeout value for collecting a batch\nfrom workers. Should always be non-negative. (default: 0)\nworker_init_fn (Callable, optional) – If not None, this will be called on each\nworker subprocess with the worker id (an int in [0, num_workers - 1]) as\ninput, after seeding and before data loading. (default: None)\nmultiprocessing_context (str or multiprocessing.context.BaseContext, optional) – If\nNone, the default multiprocessing context of your operating system will\nbe used. (default: None)\ngenerator (torch.Generator, optional) – If not None, this RNG will be used\nby RandomSampler to generate random indexes and multiprocessing to generate\nbase_seed for workers. (default: None)\nprefetch_factor (int, optional, keyword-only arg) – Number of batches loaded\nin advance by each worker. 2 means there will be a total of\n2 * num_workers batches prefetched across all workers. (default value depends\non the set value for num_workers. If value of num_workers=0 default is None.\nOtherwise, if value of num_workers > 0 default is 2).\npersistent_workers (bool, optional) – If True, the data loader will not shut down\nthe worker processes after a dataset has been consumed once. This allows to\nmaintain the workers Dataset instances alive. (default: False)\npin_memory_device (str, optional) – the device to pin_memory to if pin_memory is\nTrue.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.DataParallel",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.DataParallel.html#torch.nn.DataParallel",
        "api_signature": "torch.nn.DataParallel(module, device_ids=None, output_device=None, dim=0)",
        "api_description": "Implements data parallelism at the module level.",
        "return_value": "",
        "parameters": "module (Module) – module to be parallelized\ndevice_ids (list of int or torch.device) – CUDA devices (default: all devices)\noutput_device (int or torch.device) – device location of output (default: device_ids[0])",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.data.Dataset",
        "api_url": "https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset",
        "api_signature": "torch.utils.data.Dataset(*args, **kwds)",
        "api_description": "An abstract class representing a Dataset.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.checkpoint.format_utils.dcp_to_torch_save",
        "api_url": "https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.format_utils.dcp_to_torch_save",
        "api_signature": "torch.distributed.checkpoint.format_utils.dcp_to_torch_save(dcp_checkpoint_dir, torch_save_path)",
        "api_description": "Given a directory containing a DCP checkpoint, this function will convert it into a\nTorch save file.",
        "return_value": "",
        "parameters": "dcp_checkpoint_dir (Union[str, PathLike]) – Directory containing the DCP checkpoint.\ntorch_save_path (Union[str, PathLike]) – Filename to store the converted Torch save file.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda.CUDAGraph.debug_dump",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.cuda.CUDAGraph.html#torch.cuda.CUDAGraph.debug_dump",
        "api_signature": "debug_dump(debug_path)",
        "api_description": "debug_path (required) – Path to dump the graph to.",
        "return_value": "",
        "parameters": "debug_path (required) – Path to dump the graph to.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.qconfig.default_activation_only_qconfig",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.quantization.qconfig.default_activation_only_qconfig.html#torch.ao.quantization.qconfig.default_activation_only_qconfig",
        "api_signature": null,
        "api_description": "Default qconfig for quantizing activations only.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.data.default_collate",
        "api_url": "https://pytorch.org/docs/stable/data.html#torch.utils.data.default_collate",
        "api_signature": "torch.utils.data.default_collate(batch)",
        "api_description": "Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.",
        "return_value": "",
        "parameters": "batch – a single batch to be collated",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.data.default_convert",
        "api_url": "https://pytorch.org/docs/stable/data.html#torch.utils.data.default_convert",
        "api_signature": "torch.utils.data.default_convert(data)",
        "api_description": "Convert each NumPy array element into a torch.Tensor.",
        "return_value": "",
        "parameters": "data – a single data point to be converted",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.observer.default_debug_observer",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.quantization.observer.default_debug_observer.html#torch.ao.quantization.observer.default_debug_observer",
        "api_signature": null,
        "api_description": "Default debug-only observer.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.qconfig.default_debug_qconfig",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.quantization.qconfig.default_debug_qconfig.html#torch.ao.quantization.qconfig.default_debug_qconfig",
        "api_signature": null,
        "api_description": "Default qconfig configuration for debugging.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.qconfig.default_dynamic_qconfig",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.quantization.qconfig.default_dynamic_qconfig.html#torch.ao.quantization.qconfig.default_dynamic_qconfig",
        "api_signature": null,
        "api_description": "Default dynamic qconfig.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.observer.default_dynamic_quant_observer",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.quantization.observer.default_dynamic_quant_observer.html#torch.ao.quantization.observer.default_dynamic_quant_observer",
        "api_signature": null,
        "api_description": "Default observer for dynamic quantization.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.default_eval_fn",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.quantization.default_eval_fn.html#torch.ao.quantization.default_eval_fn",
        "api_signature": "torch.ao.quantization.default_eval_fn(model, calib_data)",
        "api_description": "Define the default evaluation function.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.autograd.profiler_util.StringTable.default_factory",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.autograd.profiler_util.StringTable.html#torch.autograd.profiler_util.StringTable.default_factory",
        "api_signature": null,
        "api_description": "Factory for default value called by __missing__().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.fake_quantize.default_fake_quant",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.quantization.fake_quantize.default_fake_quant.html#torch.ao.quantization.fake_quantize.default_fake_quant",
        "api_signature": null,
        "api_description": "Default fake_quant for activations.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.observer.default_float_qparams_observer",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.quantization.observer.default_float_qparams_observer.html#torch.ao.quantization.observer.default_float_qparams_observer",
        "api_signature": null,
        "api_description": "Default observer for a floating point zero-point.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.fake_quantize.default_fused_act_fake_quant",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.quantization.fake_quantize.default_fused_act_fake_quant.html#torch.ao.quantization.fake_quantize.default_fused_act_fake_quant",
        "api_signature": null,
        "api_description": "Fused version of default_fake_quant, with improved performance.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.fake_quantize.default_fused_per_channel_wt_fake_quant",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.quantization.fake_quantize.default_fused_per_channel_wt_fake_quant.html#torch.ao.quantization.fake_quantize.default_fused_per_channel_wt_fake_quant",
        "api_signature": null,
        "api_description": "Fused version of default_per_channel_weight_fake_quant, with improved performance.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.fake_quantize.default_fused_wt_fake_quant",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.quantization.fake_quantize.default_fused_wt_fake_quant.html#torch.ao.quantization.fake_quantize.default_fused_wt_fake_quant",
        "api_signature": null,
        "api_description": "Fused version of default_weight_fake_quant, with improved performance.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.torch.default_generator",
        "api_url": "https://pytorch.org/docs/stable/torch.html#torch.torch.default_generator",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.fake_quantize.default_histogram_fake_quant",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.quantization.fake_quantize.default_histogram_fake_quant.html#torch.ao.quantization.fake_quantize.default_histogram_fake_quant",
        "api_signature": null,
        "api_description": "Fake_quant for activations using a histogram..",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.observer.default_histogram_observer",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.quantization.observer.default_histogram_observer.html#torch.ao.quantization.observer.default_histogram_observer",
        "api_signature": null,
        "api_description": "Default histogram observer, usually used for PTQ.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.observer.default_observer",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.quantization.observer.default_observer.html#torch.ao.quantization.observer.default_observer",
        "api_signature": null,
        "api_description": "Default observer for static quantization, usually used for debugging.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.qconfig.default_per_channel_qconfig",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.quantization.qconfig.default_per_channel_qconfig.html#torch.ao.quantization.qconfig.default_per_channel_qconfig",
        "api_signature": null,
        "api_description": "Default qconfig configuration for per channel weight quantization.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.fake_quantize.default_per_channel_weight_fake_quant",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.quantization.fake_quantize.default_per_channel_weight_fake_quant.html#torch.ao.quantization.fake_quantize.default_per_channel_weight_fake_quant",
        "api_signature": null,
        "api_description": "Default fake_quant for per-channel weights.\nObserver is memoryless since averaging_constant is 1.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.observer.default_per_channel_weight_observer",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.quantization.observer.default_per_channel_weight_observer.html#torch.ao.quantization.observer.default_per_channel_weight_observer",
        "api_signature": null,
        "api_description": "Default per-channel weight observer, usually used on backends where per-channel\nweight quantization is supported, such as fbgemm.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.observer.default_placeholder_observer",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.quantization.observer.default_placeholder_observer.html#torch.ao.quantization.observer.default_placeholder_observer",
        "api_signature": null,
        "api_description": "Default placeholder observer, usually used for quantization to torch.float16.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.qconfig.default_qat_qconfig",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.quantization.qconfig.default_qat_qconfig.html#torch.ao.quantization.qconfig.default_qat_qconfig",
        "api_signature": null,
        "api_description": "Default qconfig for QAT.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.qconfig.default_qat_qconfig_v2",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.quantization.qconfig.default_qat_qconfig_v2.html#torch.ao.quantization.qconfig.default_qat_qconfig_v2",
        "api_signature": null,
        "api_description": "Fused version of default_qat_config, has performance benefits.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.qconfig.default_qconfig",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.quantization.qconfig.default_qconfig.html#torch.ao.quantization.qconfig.default_qconfig",
        "api_signature": null,
        "api_description": "Default qconfig configuration.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda.default_stream",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.cuda.default_stream.html#torch.cuda.default_stream",
        "api_signature": "torch.cuda.default_stream(device=None)",
        "api_description": "Return the default Stream for a given device.",
        "return_value": "",
        "parameters": "device (torch.device or int, optional) – selected device. Returns\nthe default Stream for the current device, given by\ncurrent_device(), if device is None\n(default).",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.fake_quantize.default_weight_fake_quant",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.quantization.fake_quantize.default_weight_fake_quant.html#torch.ao.quantization.fake_quantize.default_weight_fake_quant",
        "api_signature": null,
        "api_description": "Default fake_quant for weights.\nObserver is memoryless since averaging_constant is 1.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.observer.default_weight_observer",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.quantization.observer.default_weight_observer.html#torch.ao.quantization.observer.default_weight_observer",
        "api_signature": null,
        "api_description": "Default weight observer.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.qconfig.default_weight_only_qconfig",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.quantization.qconfig.default_weight_only_qconfig.html#torch.ao.quantization.qconfig.default_weight_only_qconfig",
        "api_signature": null,
        "api_description": "Default qconfig for quantizing weights only.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.checkpoint.DefaultLoadPlanner",
        "api_url": "https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.DefaultLoadPlanner",
        "api_signature": "torch.distributed.checkpoint.DefaultLoadPlanner(flatten_state_dict=True, flatten_sharded_tensors=True)",
        "api_description": "DefaultLoadPlanner that adds multiple features on top of LoadPlanner.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.multiprocessing.api.DefaultLogsSpecs",
        "api_url": "https://pytorch.org/docs/stable/elastic/multiprocessing.html#torch.distributed.elastic.multiprocessing.api.DefaultLogsSpecs",
        "api_signature": "torch.distributed.elastic.multiprocessing.api.DefaultLogsSpecs(log_dir=None, redirects=Std.NONE, tee=Std.NONE, local_ranks_filter=None)",
        "api_description": "Default LogsSpecs implementation:",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.checkpoint.DefaultSavePlanner",
        "api_url": "https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.DefaultSavePlanner",
        "api_signature": "torch.distributed.checkpoint.DefaultSavePlanner(flatten_state_dict=True, flatten_sharded_tensors=True, dedup_replicated_tensors=None)",
        "api_description": "Extension from the planner interface to make it easy to extend the default planner.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.symbolic_shapes.ShapeEnv.defer_runtime_assert",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.fx.experimental.symbolic_shapes.ShapeEnv.html#torch.fx.experimental.symbolic_shapes.ShapeEnv.defer_runtime_assert",
        "api_signature": "defer_runtime_assert(orig_expr, msg, fx_node=None)",
        "api_description": "Create an assert that is checked at runtime",
        "return_value": "",
        "parameters": "orig_expr (sympy.Expr) – Boolean expression to assert is true\nmsg (str) – Message to display on assertion failure\nfx_node (Optional, torch.fx.Node) – node in self.graph corresponding\nto the expression, if applicable",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.library.define",
        "api_url": "https://pytorch.org/docs/stable/library.html#torch.library.define",
        "api_signature": "torch.library.define(qualname, schema, *, lib=None, tags=()",
        "api_description": "",
        "return_value": "",
        "parameters": "qualname (str) – The qualified name for the operator. Should be\na string that looks like “namespace::name”, e.g. “aten::sin”.\navoid name collisions; a given operator may only be created once.\nIf you are writing a Python library, we recommend the namespace to\nbe the name of your top-level module.\nschema (str) – The schema of the operator. E.g. “(Tensor x) -> Tensor”\nfor an op that accepts one Tensor and returns one Tensor. It does\nnot contain the operator name (that is passed in qualname).\nlib (Optional[Library]) – If provided, the lifetime of this operator\nwill be tied to the lifetime of the Library object.\ntags (Tag | Sequence[Tag]) – one or more torch.Tag to apply to this\noperator. Tagging an operator changes the operator’s behavior\ntorch.Tag carefully before applying it.",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import torch\n>>> import numpy as np\n>>>\n>>> # Define the operator\n>>> torch.library.define(\"mylib::sin\", \"(Tensor x) -> Tensor\")\n>>>\n>>> # Add implementations for the operator\n>>> @torch.library.impl(\"mylibrary::sin\", \"cpu\")\n>>> def f(x):\n>>>     return torch.from_numpy(np.sin(x.numpy()))\n>>>\n>>> # Call the new operator from torch.ops.\n>>> x = torch.randn(3)\n>>> y = torch.ops.mylib.sin(x)\n>>> assert torch.allclose(y, x)\n\n\n"
    },
    {
        "api_name": "torch.library.Library.define",
        "api_url": "https://pytorch.org/docs/stable/library.html#torch.library.Library.define",
        "api_signature": "define(schema, alias_analysis='', *, tags=()",
        "api_description": "Defines a new operator and its semantics in the ns namespace.",
        "return_value": "name of the operator as inferred from the schema.\n",
        "parameters": "schema – function schema to define a new operator.\nalias_analysis (optional) – Indicates if the aliasing properties of the operator arguments can be\ninferred from the schema (default behavior) or not (“CONSERVATIVE”).\ntags (Tag | Sequence[Tag]) – one or more torch.Tag to apply to this\noperator. Tagging an operator changes the operator’s behavior\ntorch.Tag carefully before applying it.",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> my_lib = Library(\"foo\", \"DEF\")\n>>> my_lib.define(\"sum(Tensor self) -> Tensor\")\n\n\n"
    },
    {
        "api_name": "torch.fx.experimental.symbolic_shapes.definitely_false",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.fx.experimental.symbolic_shapes.definitely_false.html#torch.fx.experimental.symbolic_shapes.definitely_false",
        "api_signature": "torch.fx.experimental.symbolic_shapes.definitely_false(a)",
        "api_description": "Returns True only if we can tell that a is False, possibly introducing\na guard in the process.  If a depends on some unbacked SymInt, we may\nreturn False even though there may exist a possible value of the SymInt\nthat would cause the expression a to be False.  See definitely_true\nfor more usage guidance.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.symbolic_shapes.definitely_true",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.fx.experimental.symbolic_shapes.definitely_true.html#torch.fx.experimental.symbolic_shapes.definitely_true",
        "api_signature": "torch.fx.experimental.symbolic_shapes.definitely_true(a)",
        "api_description": "Returns True only if we can tell that a is True, possibly introducing\na guard in the process.  If a depends on some unbacked SymInt, we may\nreturn False even though there may exist a possible value of the SymInt\nthat would cause the expression to return True.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.deg2rad",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.deg2rad.html#torch.deg2rad",
        "api_signature": "torch.deg2rad(input, *, out=None)",
        "api_description": "Returns a new tensor with each of the elements of input\nconverted from angles in degrees to radians.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.deg2rad",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.deg2rad.html#torch.Tensor.deg2rad",
        "api_signature": "Tensor.deg2rad()",
        "api_description": "See torch.deg2rad()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.GraphModule.delete_all_unused_submodules",
        "api_url": "https://pytorch.org/docs/stable/fx.html#torch.fx.GraphModule.delete_all_unused_submodules",
        "api_signature": "delete_all_unused_submodules()",
        "api_description": "Deletes all unused submodules from self.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.Store.delete_key",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#torch.distributed.Store.delete_key",
        "api_signature": "torch.distributed.Store.delete_key(self: torch._C._distributed_c10d.Store, arg0: str)",
        "api_description": "Deletes the key-value pair associated with key from the store. Returns\ntrue if the key was successfully deleted, and false if it was not.",
        "return_value": "True if key was deleted, otherwise False.\n",
        "parameters": "key (str) – The key to be deleted from the store",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> # Using TCPStore as an example, HashStore can also be used\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n>>> store.set(\"first_key\")\n>>> # This should return true\n>>> store.delete_key(\"first_key\")\n>>> # This should return false\n>>> store.delete_key(\"bad_key\")\n\n\n"
    },
    {
        "api_name": "torch.fx.GraphModule.delete_submodule",
        "api_url": "https://pytorch.org/docs/stable/fx.html#torch.fx.GraphModule.delete_submodule",
        "api_signature": "delete_submodule(target)",
        "api_description": "Deletes the given submodule from self.",
        "return_value": "\nWhether or not the target string referenced asubmodule we want to delete. A return value of False\nmeans that the target was not a valid reference to\na submodule.\n\n\n\n",
        "parameters": "target (str) – The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.)",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.benchmark.CallgrindStats.delta",
        "api_url": "https://pytorch.org/docs/stable/benchmark_utils.html#torch.utils.benchmark.CallgrindStats.delta",
        "api_signature": "delta(other, inclusive=False)",
        "api_description": "Diff two sets of counts.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.package.PackageExporter.denied_modules",
        "api_url": "https://pytorch.org/docs/stable/package.html#torch.package.PackageExporter.denied_modules",
        "api_signature": "denied_modules()",
        "api_description": "Return all modules that are currently denied.",
        "return_value": "A list containing the names of modules which will be\ndenied in this package.\n",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.benchmark.FunctionCounts.denoise",
        "api_url": "https://pytorch.org/docs/stable/benchmark_utils.html#torch.utils.benchmark.FunctionCounts.denoise",
        "api_signature": "denoise()",
        "api_description": "Remove known noisy instructions.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.dense_dim",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.dense_dim.html#torch.Tensor.dense_dim",
        "api_signature": "Tensor.dense_dim()",
        "api_description": "Return the number of dense dimensions in a sparse tensor self.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.package.PackageExporter.deny",
        "api_url": "https://pytorch.org/docs/stable/package.html#torch.package.PackageExporter.deny",
        "api_signature": "deny(include, *, exclude=()",
        "api_description": "Blocklist modules who names match the given glob patterns from the list of modules the package can import.\nIf a dependency on any matching packages is found, a PackagingError is raised.",
        "return_value": "",
        "parameters": "include (Union[List[str], str]) – A string e.g. \"my_package.my_subpackage\", or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as described in mock().\nexclude (Union[List[str], str]) – An optional pattern that excludes some patterns that match the include string.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.package.PackageExporter.dependency_graph_string",
        "api_url": "https://pytorch.org/docs/stable/package.html#torch.package.PackageExporter.dependency_graph_string",
        "api_signature": "dependency_graph_string()",
        "api_description": "Returns digraph string representation of dependencies in package.",
        "return_value": "A string representation of dependencies in package.\n",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.constraints.dependent_property",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.constraints.dependent_property",
        "api_signature": null,
        "api_description": "alias of _DependentProperty",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.dequantize",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.dequantize.html#torch.dequantize",
        "api_signature": "torch.dequantize(tensor)",
        "api_description": "Returns an fp32 Tensor by dequantizing a quantized Tensor",
        "return_value": "",
        "parameters": "tensor (Tensor) – A quantized Tensor\ntensors (sequence of Tensors) – A list of quantized Tensors",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.quantizable.MultiheadAttention.dequantize",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.nn.quantizable.MultiheadAttention.html#torch.ao.nn.quantizable.MultiheadAttention.dequantize",
        "api_signature": "dequantize()",
        "api_description": "Utility to convert the quantized MHA back to float.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.dequantize",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.dequantize.html#torch.Tensor.dequantize",
        "api_signature": "Tensor.dequantize()",
        "api_description": "Given a quantized Tensor, dequantize it and return the dequantized float Tensor.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.DeQuantStub",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.quantization.DeQuantStub.html#torch.ao.quantization.DeQuantStub",
        "api_signature": "torch.ao.quantization.DeQuantStub(qconfig=None)",
        "api_description": "Dequantize stub module, before calibration, this is same as identity,\nthis will be swapped as nnq.DeQuantize in convert.",
        "return_value": "",
        "parameters": "qconfig – quantization configuration for the tensor,\nif qconfig is not provided, we will get qconfig from parent modules",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.det",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.det.html#torch.det",
        "api_signature": "torch.det(input)",
        "api_description": "Alias for torch.linalg.det()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.linalg.det",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.linalg.det.html#torch.linalg.det",
        "api_signature": "torch.linalg.det(A, *, out=None)",
        "api_description": "Computes the determinant of a square matrix.",
        "return_value": "",
        "parameters": "A (Tensor) – tensor of shape (*, n, n) where * is zero or more batch dimensions.\nout (Tensor, optional) – output tensor. Ignored if None. Default: None.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.det",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.det.html#torch.Tensor.det",
        "api_signature": "Tensor.det()",
        "api_description": "See torch.det()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.detach",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.detach.html#torch.Tensor.detach",
        "api_signature": "Tensor.detach()",
        "api_description": "Returns a new Tensor, detached from the current graph.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.detach_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.detach_.html#torch.Tensor.detach_",
        "api_signature": "Tensor.detach_()",
        "api_description": "Detaches the Tensor from the graph that created it, making it a leaf.\nViews cannot be detached in-place.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.autograd.detect_anomaly",
        "api_url": "https://pytorch.org/docs/stable/autograd.html#torch.autograd.detect_anomaly",
        "api_signature": "torch.autograd.detect_anomaly(check_nan=True)",
        "api_description": "Context-manager that enable anomaly detection for the autograd engine.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.backends.cudnn.deterministic",
        "api_url": "https://pytorch.org/docs/stable/backends.html#torch.backends.cudnn.deterministic",
        "api_signature": null,
        "api_description": "A bool that, if True, causes cuDNN to only use deterministic convolution algorithms.\nSee also torch.are_deterministic_algorithms_enabled() and\ntorch.use_deterministic_algorithms().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.device",
        "api_url": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.device",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda.device",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.cuda.device.html#torch.cuda.device",
        "api_signature": "torch.cuda.device(device)",
        "api_description": "Context-manager that changes the selected device.",
        "return_value": "",
        "parameters": "device (torch.device or int) – device index to select. It’s a no-op if\nthis argument is a negative integer or None.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.xpu.device",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.xpu.device.html#torch.xpu.device",
        "api_signature": "torch.xpu.device(device)",
        "api_description": "Context-manager that changes the selected device.",
        "return_value": "",
        "parameters": "device (torch.device or int or str) – device index to select. It’s a no-op if\nthis argument is a negative integer or None.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.autograd.profiler_util.Kernel.device",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.autograd.profiler_util.Kernel.html#torch.autograd.profiler_util.Kernel.device",
        "api_signature": null,
        "api_description": "Alias for field number 1",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Generator.device",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator.device",
        "api_signature": null,
        "api_description": "Generator.device -> device",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.device",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.device.html#torch.Tensor.device",
        "api_signature": null,
        "api_description": "Is the torch.device where this Tensor is.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.TypedStorage.device",
        "api_url": "https://pytorch.org/docs/stable/storage.html#torch.TypedStorage.device",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.UntypedStorage.device",
        "api_url": "https://pytorch.org/docs/stable/storage.html#torch.UntypedStorage.device",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cpu.device_count",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.cpu.device_count.html#torch.cpu.device_count",
        "api_signature": "torch.cpu.device_count()",
        "api_description": "Returns number of CPU devices (not cores). Always 1.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda.device_count",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.cuda.device_count.html#torch.cuda.device_count",
        "api_signature": "torch.cuda.device_count()",
        "api_description": "Return the number of GPUs available.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.xpu.device_count",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.xpu.device_count.html#torch.xpu.device_count",
        "api_signature": "torch.xpu.device_count()",
        "api_description": "Return the number of XPU device available.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.rpc.TensorPipeRpcBackendOptions.device_maps",
        "api_url": "https://pytorch.org/docs/stable/rpc.html#torch.distributed.rpc.TensorPipeRpcBackendOptions.device_maps",
        "api_signature": null,
        "api_description": "The device map locations.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda.device_of",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.cuda.device_of.html#torch.cuda.device_of",
        "api_signature": "torch.cuda.device_of(obj)",
        "api_description": "Context-manager that changes the current device to that of given object.",
        "return_value": "",
        "parameters": "obj (Tensor or Storage) – object allocated on the selected device.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.xpu.device_of",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.xpu.device_of.html#torch.xpu.device_of",
        "api_signature": "torch.xpu.device_of(obj)",
        "api_description": "Context-manager that changes the current device to that of given object.",
        "return_value": "",
        "parameters": "obj (Tensor or Storage) – object allocated on the selected device.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.device_mesh.DeviceMesh",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#torch.distributed.device_mesh.DeviceMesh",
        "api_signature": "torch.distributed.device_mesh.DeviceMesh(device_type, mesh, *, mesh_dim_names=None, _init_backend=True)",
        "api_description": "DeviceMesh represents a mesh of devices, where layout of devices could be\nrepresented as a n-d dimension array, and each value of the n-d dimensional\narray is the global id of the default process group ranks.",
        "return_value": "A DeviceMesh object representing the device layout.\n",
        "parameters": "device_type (str) – The device type of the mesh. Currently supports: “cpu”, “cuda/cuda-like”.\nmesh (ndarray) – A multi-dimensional array or an integer tensor describing the layout\nof devices, where the IDs are global IDs of the default process group.",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> from torch.distributed.device_mesh import DeviceMesh\n>>>\n>>> # Initialize device mesh as (2, 4) to represent the topology\n>>> # of cross-host(dim 0), and within-host (dim 1).\n>>> mesh = DeviceMesh(device_type=\"cuda\", mesh=[[0, 1, 2, 3],[4, 5, 6, 7]])\n\n\n"
    },
    {
        "api_name": "torch.distributed.rpc.TensorPipeRpcBackendOptions.devices",
        "api_url": "https://pytorch.org/docs/stable/rpc.html#torch.distributed.rpc.TensorPipeRpcBackendOptions.devices",
        "api_signature": null,
        "api_description": "All devices used by the local agent.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.chi2.Chi2.df",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.chi2.Chi2.df",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.diag",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.diag.html#torch.diag",
        "api_signature": "torch.diag(input, diagonal=0, *, out=None)",
        "api_description": "If input is a vector (1-D tensor), then returns a 2-D square tensor\nwith the elements of input as the diagonal.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.\ndiagonal (int, optional) – the diagonal to consider\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.diag",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.diag.html#torch.Tensor.diag",
        "api_signature": "Tensor.diag(diagonal=0)",
        "api_description": "See torch.diag()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.diag_embed",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.diag_embed.html#torch.diag_embed",
        "api_signature": "torch.diag_embed(input, offset=0, dim1=-2, dim2=-1)",
        "api_description": "Creates a tensor whose diagonals of certain 2D planes (specified by\ndim1 and dim2) are filled by input.\nTo facilitate creating batched diagonal matrices, the 2D planes formed by\nthe last two dimensions of the returned tensor are chosen by default.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor. Must be at least 1-dimensional.\noffset (int, optional) – which diagonal to consider. Default: 0\n(main diagonal).\ndim1 (int, optional) – first dimension with respect to which to\ntake diagonal. Default: -2.\ndim2 (int, optional) – second dimension with respect to which to\ntake diagonal. Default: -1.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.diag_embed",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.diag_embed.html#torch.Tensor.diag_embed",
        "api_signature": "Tensor.diag_embed(offset=0, dim1=-2, dim2=-1)",
        "api_description": "See torch.diag_embed()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.diagflat",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.diagflat.html#torch.diagflat",
        "api_signature": "torch.diagflat(input, offset=0)",
        "api_description": "If input is a vector (1-D tensor), then returns a 2-D square tensor\nwith the elements of input as the diagonal.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.\noffset (int, optional) – the diagonal to consider. Default: 0 (main\ndiagonal).",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.diagflat",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.diagflat.html#torch.Tensor.diagflat",
        "api_signature": "Tensor.diagflat(offset=0)",
        "api_description": "See torch.diagflat()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.onnx.ONNXProgram.diagnostic_context",
        "api_url": "https://pytorch.org/docs/stable/onnx_dynamo.html#torch.onnx.ONNXProgram.diagnostic_context",
        "api_signature": null,
        "api_description": "The diagnostic context associated with the export.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.onnx.DiagnosticOptions",
        "api_url": "https://pytorch.org/docs/stable/onnx_dynamo.html#torch.onnx.DiagnosticOptions",
        "api_signature": "torch.onnx.DiagnosticOptions(verbosity_level=20, warnings_as_errors=False)",
        "api_description": "Options for diagnostic context.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.diagonal",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.diagonal.html#torch.diagonal",
        "api_signature": "torch.diagonal(input, offset=0, dim1=0, dim2=1)",
        "api_description": "Returns a partial view of input with the its diagonal elements\nwith respect to dim1 and dim2 appended as a dimension\nat the end of the shape.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor. Must be at least 2-dimensional.\noffset (int, optional) – which diagonal to consider. Default: 0\n(main diagonal).\ndim1 (int, optional) – first dimension with respect to which to\ntake diagonal. Default: 0.\ndim2 (int, optional) – second dimension with respect to which to\ntake diagonal. Default: 1.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.linalg.diagonal",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.linalg.diagonal.html#torch.linalg.diagonal",
        "api_signature": "torch.linalg.diagonal(A, *, offset=0, dim1=-2, dim2=-1)",
        "api_description": "Alias for torch.diagonal() with defaults dim1= -2, dim2= -1.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.diagonal",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.diagonal.html#torch.Tensor.diagonal",
        "api_signature": "Tensor.diagonal(offset=0, dim1=0, dim2=1)",
        "api_description": "See torch.diagonal()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.diagonal_scatter",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.diagonal_scatter.html#torch.diagonal_scatter",
        "api_signature": "torch.diagonal_scatter(input, src, offset=0, dim1=0, dim2=1)",
        "api_description": "Embeds the values of the src tensor into input along\nthe diagonal elements of input, with respect to dim1\nand dim2.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor. Must be at least 2-dimensional.\nsrc (Tensor) – the tensor to embed into input.\noffset (int, optional) – which diagonal to consider. Default: 0\n(main diagonal).\ndim1 (int, optional) – first dimension with respect to which to\ntake diagonal. Default: 0.\ndim2 (int, optional) – second dimension with respect to which to\ntake diagonal. Default: 1.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.diagonal_scatter",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.diagonal_scatter.html#torch.Tensor.diagonal_scatter",
        "api_signature": "Tensor.diagonal_scatter(src, offset=0, dim1=0, dim2=1)",
        "api_description": "See torch.diagonal_scatter()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.diff",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.diff.html#torch.diff",
        "api_signature": "torch.diff(input, n=1, dim=-1, prepend=None, append=None)",
        "api_description": "Computes the n-th forward difference along the given dimension.",
        "return_value": "",
        "parameters": "input (Tensor) – the tensor to compute the differences on\nn (int, optional) – the number of times to recursively compute the difference\ndim (int, optional) – the dimension to compute the difference along.\nDefault is the last dimension.\nprepend (Tensor, optional) – values to prepend or append to\ninput along dim before computing the difference.\nTheir dimensions must be equivalent to that of input, and their shapes\nmust match input’s shape except on dim.\nappend (Tensor, optional) – values to prepend or append to\ninput along dim before computing the difference.\nTheir dimensions must be equivalent to that of input, and their shapes\nmust match input’s shape except on dim.\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.diff",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.diff.html#torch.Tensor.diff",
        "api_signature": "Tensor.diff(n=1, dim=-1, prepend=None, append=None)",
        "api_description": "See torch.diff()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.digamma",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.digamma.html#torch.digamma",
        "api_signature": "torch.digamma(input, *, out=None)",
        "api_description": "Alias for torch.special.digamma().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.special.digamma",
        "api_url": "https://pytorch.org/docs/stable/special.html#torch.special.digamma",
        "api_signature": "torch.special.digamma(input, *, out=None)",
        "api_description": "Computes the logarithmic derivative of the gamma function on input.",
        "return_value": "",
        "parameters": "input (Tensor) – the tensor to compute the digamma function on\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.digamma",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.digamma.html#torch.Tensor.digamma",
        "api_signature": "Tensor.digamma()",
        "api_description": "See torch.digamma()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.digamma_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.digamma_.html#torch.Tensor.digamma_",
        "api_signature": "Tensor.digamma_()",
        "api_description": "In-place version of digamma()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.export.dynamic_shapes.Dim",
        "api_url": "https://pytorch.org/docs/stable/export.html#torch.export.dynamic_shapes.Dim",
        "api_signature": "torch.export.dynamic_shapes.Dim(name, *, min=None, max=None)",
        "api_description": "Dim() constructs a type analogous to a named symbolic integer with a range.\nIt can be used to describe multiple possible values of a dynamic tensor dimension.\nNote that different dynamic dimensions of the same tensor, or of different tensors,\ncan be described by the same type.",
        "return_value": "A type that can be used in dynamic shape specifications for tensors.\n",
        "parameters": "name (str) – Human-readable name for debugging.\nmin (Optional[int]) – Minimum possible value of given symbol (inclusive)\nmax (Optional[int]) – Maximum possible value of given symbol (inclusive)",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.dim",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.dim.html#torch.Tensor.dim",
        "api_signature": "Tensor.dim()",
        "api_description": "Returns the number of dimensions of self tensor.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.dim_order",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.dim_order.html#torch.Tensor.dim_order",
        "api_signature": "Tensor.dim_order()",
        "api_description": "Returns a tuple of int describing the dim order or physical layout of self.",
        "return_value": "",
        "parameters": "None –",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> torch.empty((2, 3, 5, 7)).dim_order()\n(0, 1, 2, 3)\n>>> torch.empty((2, 3, 5, 7), memory_format=torch.channels_last).dim_order()\n(0, 2, 3, 1)\n\n\n"
    },
    {
        "api_name": "torch.fx.experimental.symbolic_shapes.DimConstraints",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.fx.experimental.symbolic_shapes.DimConstraints.html#torch.fx.experimental.symbolic_shapes.DimConstraints",
        "api_signature": "torch.fx.experimental.symbolic_shapes.DimConstraints(symbol_to_source, var_to_val, marked_dynamic, source_name_to_debug_name)",
        "api_description": "Custom solver for a system of constraints on symbolic dimensions.\nSolutions are “static” values or simplified “dynamic” constraints.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.symbolic_shapes.DimDynamic",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.fx.experimental.symbolic_shapes.DimDynamic.html#torch.fx.experimental.symbolic_shapes.DimDynamic",
        "api_signature": "torch.fx.experimental.symbolic_shapes.DimDynamic(value)",
        "api_description": "Controls how to perform symbol allocation for a dimension.  It is always\nsound to default this to DYNAMIC, but the policies DUCK and STATIC can\nresult in better trace-time and compile-time performance, as they reduce\nthe number of allocated symbols and generally make your graph more static.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.export.dims",
        "api_url": "https://pytorch.org/docs/stable/export.html#torch.export.dims",
        "api_signature": "torch.export.dims(*names, min=None, max=None)",
        "api_description": "Util to create multiple Dim() types.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.init.dirac_",
        "api_url": "https://pytorch.org/docs/stable/nn.init.html#torch.nn.init.dirac_",
        "api_signature": "torch.nn.init.dirac_(tensor, groups=1)",
        "api_description": "Fill the {3, 4, 5}-dimensional input Tensor with the Dirac delta function.",
        "return_value": "",
        "parameters": "tensor – a {3, 4, 5}-dimensional torch.Tensor\ngroups (int, optional) – number of groups in the conv layer (default: 1)",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.package.Directory",
        "api_url": "https://pytorch.org/docs/stable/package.html#torch.package.Directory",
        "api_signature": "torch.package.Directory(name, is_dir)",
        "api_description": "A file structure representation. Organized as Directory nodes that have lists of\ntheir Directory children. Directories for a package are created by calling\nPackageImporter.file_structure().",
        "return_value": "If a Directory contains the specified file.\n",
        "parameters": "filename (str) – Path of file to search for.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.dirichlet.Dirichlet",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.dirichlet.Dirichlet",
        "api_signature": "torch.distributions.dirichlet.Dirichlet(concentration, validate_args=None)",
        "api_description": "Bases: ExponentialFamily",
        "return_value": "",
        "parameters": "concentration (Tensor) – concentration parameter of the distribution\n(often referred to as alpha)",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.compiler.disable",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.compiler.disable.html#torch.compiler.disable",
        "api_signature": "torch.compiler.disable(fn=None, recursive=True)",
        "api_description": "This function provides both a decorator and a context manager to disable compilation on a function\nIt also provides the option of recursively disabling called functions",
        "return_value": "",
        "parameters": "fn (optional) – The function to disable\nrecursive (optional) – A boolean value indicating whether the disabling should be recursive.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.sparse.check_sparse_tensor_invariants.disable",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.sparse.check_sparse_tensor_invariants.html#torch.sparse.check_sparse_tensor_invariants.disable",
        "api_signature": "disable()",
        "api_description": "Disable sparse tensor invariants checking in sparse tensor constructors.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.fake_quantize.disable_fake_quant",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.quantization.fake_quantize.disable_fake_quant.html#torch.ao.quantization.fake_quantize.disable_fake_quant",
        "api_signature": "torch.ao.quantization.fake_quantize.disable_fake_quant(mod)",
        "api_description": "Disable fake quantization for the module.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.onnx.disable_log",
        "api_url": "https://pytorch.org/docs/stable/onnx_torchscript.html#torch.onnx.disable_log",
        "api_signature": "torch.onnx.disable_log()",
        "api_description": "Disables ONNX logging.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.fake_quantize.disable_observer",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.quantization.fake_quantize.disable_observer.html#torch.ao.quantization.fake_quantize.disable_observer",
        "api_signature": "torch.ao.quantization.fake_quantize.disable_observer(mod)",
        "api_description": "Disable observation for this module.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.autograd.graph.disable_saved_tensors_hooks",
        "api_url": "https://pytorch.org/docs/stable/autograd.html#torch.autograd.graph.disable_saved_tensors_hooks",
        "api_signature": "torch.autograd.graph.disable_saved_tensors_hooks(error_message)",
        "api_description": "Context-manager that disables the saved tensors default hooks feature.",
        "return_value": "",
        "parameters": "error_message (str) – When saved tensors default hooks are used when they\nhave been are disabled, a RuntimeError with this\nerror message gets raised.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.dist",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.dist.html#torch.dist",
        "api_signature": "torch.dist(input, other, p=2)",
        "api_description": "Returns the p-norm of (input - other)",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.\nother (Tensor) – the Right-hand-side input tensor\np (float, optional) – the norm to be computed",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.dist",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.dist.html#torch.Tensor.dist",
        "api_signature": "Tensor.dist(other, p=2)",
        "api_description": "See torch.dist()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.DistBackendError",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#torch.distributed.DistBackendError",
        "api_signature": null,
        "api_description": "Exception raised when a backend error occurs in distributed",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.DistError",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#torch.distributed.DistError",
        "api_signature": null,
        "api_description": "Exception raised when an error occurs in the distributed library",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.DistNetworkError",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#torch.distributed.DistNetworkError",
        "api_signature": null,
        "api_description": "Exception raised when a network error occurs in distributed",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.parallel.DistributedDataParallel",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel",
        "api_signature": "torch.nn.parallel.DistributedDataParallel(module, device_ids=None, output_device=None, dim=0, broadcast_buffers=True, process_group=None, bucket_cap_mb=25, find_unused_parameters=False, check_reduction=False, gradient_as_bucket_view=False, static_graph=False, delay_all_reduce_named_params=None, param_to_hook_all_reduce=None, mixed_precision=None, device_mesh=None)",
        "api_description": "Implement distributed data parallelism based on torch.distributed at module level.",
        "return_value": "",
        "parameters": "module (Module) – module to be parallelized\ndevice_ids (list of int or torch.device) – CUDA devices.\n1) For single-device modules, device_ids can\ncontain exactly one device id, which represents the only\nCUDA device where the input module corresponding to this process resides.\nAlternatively, device_ids can also be None.\n2) For multi-device modules and CPU modules,\ndevice_ids must be None.\nWhen device_ids is None for both cases,\nboth the input data for the forward pass and the actual module\nmust be placed on the correct device.\n(default: None)\noutput_device (int or torch.device) – Device location of output for\nsingle-device CUDA modules. For multi-device modules and\nCPU modules, it must be None, and the module itself\ndictates the output location. (default: device_ids[0]\nfor single-device modules)\nbroadcast_buffers (bool) – Flag that enables syncing (broadcasting)\nbuffers of the module at beginning of the forward\nfunction. (default: True)\nprocess_group – The process group to be used for distributed data\nall-reduction. If None, the default process group, which\nis created by torch.distributed.init_process_group(),\nwill be used. (default: None)\nbucket_cap_mb – DistributedDataParallel will bucket parameters into\nmultiple buckets so that gradient reduction of each\nbucket can potentially overlap with backward computation.\nbucket_cap_mb controls the bucket size in\nMegaBytes (MB). (default: 25)\nfind_unused_parameters (bool) – Traverse the autograd graph from all\ntensors contained in the return value of the\nwrapped module’s forward function. Parameters\nthat don’t receive gradients as part of this\ngraph are preemptively marked as being ready to\nbe reduced. In addition, parameters that may have\nbeen used in the wrapped module’s forward\nfunction but were not part of loss computation and\nthus would also not receive gradients are\npreemptively marked as ready to be reduced.\n(default: False)\ncheck_reduction – This argument is deprecated.\ngradient_as_bucket_view (bool) – When set to True, gradients will be views\npointing to different offsets of allreduce communication\nbuckets. This can reduce peak memory usage, where the\nsaved memory size will be equal to the total gradients\nsize. Moreover, it avoids the overhead of copying between\ngradients and allreduce communication buckets. When\ngradients are views, detach_() cannot be called on the\ngradients. If hitting such errors, please fix it by\nreferring to the zero_grad()\nfunction in torch/optim/optimizer.py as a solution.\nNote that gradients will be views after first iteration, so\nthe peak memory saving should be checked after first iteration.\nstatic_graph (bool) – When set to True, DDP knows the trained graph is\nstatic. Static graph means 1) The set of used and unused\nparameters will not change during the whole training loop; in\nthis case, it does not matter whether users set\nfind_unused_parameters = True or not. 2) How the graph is trained\nwill not change during the whole training loop (meaning there is\nno control flow depending on iterations).\nWhen static_graph is set to be True, DDP will support cases that\ncan not be supported in the past:\n1) Reentrant backwards.\n2) Activation checkpointing multiple times.\n3) Activation checkpointing when model has unused parameters.\n4) There are model parameters that are outside of forward function.\n5) Potentially improve performance when there are unused parameters,\nas DDP will not search graph in each iteration to detect unused\nparameters when static_graph is set to be True.\nTo check whether you can set static_graph to be True, one way is to\ncheck ddp logging data at the end of your previous model training,\nif ddp_logging_data.get(\"can_set_static_graph\") == True, mostly you\ncan set static_graph = True as well.\nExample::>>> model_DDP = torch.nn.parallel.DistributedDataParallel(model)\n>>> # Training loop\n>>> ...\n>>> ddp_logging_data = model_DDP._get_ddp_logging_data()\n>>> static_graph = ddp_logging_data.get(\"can_set_static_graph\")\ndelay_all_reduce_named_params (list of tuple of str and torch.nn.Parameter) – a list\nof named parameters whose all reduce will be delayed when the gradient of\nthe parameter specified in param_to_hook_all_reduce is ready. Other\narguments of DDP do not apply to named params specified in this argument\nas these named params will be ignored by DDP reducer.\nparam_to_hook_all_reduce (torch.nn.Parameter) – a parameter to hook delayed all reduce\nof parameters specified in delay_all_reduce_named_params.\ndivide_by_initial_world_size (bool) – If True, will divide\ngradients by the initial world_size DDP training was launched\nwith. If False, will compute the effective world size\n(number of ranks that have not depleted their inputs yet) and\ndivide gradients by that during allreduce. Set\ndivide_by_initial_world_size=True to ensure every input\nsample including the uneven inputs have equal weight in terms of\nhow much they contribute to the global gradient. This is\nachieved by always dividing the gradient by the initial\nworld_size even when we encounter uneven inputs. If you set\nthis to False, we divide the gradient by the remaining\nnumber of nodes. This ensures parity with training on a smaller\nworld_size although it also means the uneven inputs would\ncontribute more towards the global gradient. Typically, you\nwould want to set this to True for cases where the last few\ninputs of your training job are uneven. In extreme cases, where\nthere is a large discrepancy in the number of inputs, setting\nthis to False might provide better results.\nenable (bool) – Whether to enable uneven input detection or not. Pass\nin enable=False to disable in cases where you know that\ninputs are even across participating processes. Default is\nTrue.\nthrow_on_early_termination (bool) – Whether to throw an error\nor continue training when at least one rank has exhausted\ninputs. If True, will throw upon the first rank reaching end\nof data. If False, will continue training with a smaller\neffective world size until all ranks are joined. Note that if\nthis flag is specified, then the flag\ndivide_by_initial_world_size would be ignored. Default\nis False.\nkwargs (dict) – a dict containing any keyword arguments\nto modify the behavior of the join hook at run time; all\nJoinable instances sharing the same join context\nmanager are forwarded the same value for kwargs.\nstate (object) – Passed to the hook to maintain any state information during the training process.\nExamples include error feedback in gradient compression,\npeers to communicate with next in GossipGrad, etc.\nIt is locally stored by each worker\nand shared by all the gradient tensors on the worker.\nhook (Callable) – Callable with the following signature:\nhook(state: object, bucket: dist.GradBucket) -> torch.futures.Future[torch.Tensor]:\nThis function is called once the bucket is ready. The\nhook can perform whatever processing is needed and return\na Future indicating completion of any async work (ex: allreduce).\nIf the hook doesn’t perform any communication, it still\nmust return a completed Future. The Future should hold the\nnew value of grad bucket’s tensors. Once a bucket is ready,\nc10d reducer would call this hook and use the tensors returned\nby the Future and copy grads to individual parameters.\nNote that the future’s return type must be a single tensor.\nWe also provide an API called get_future to retrieve a\nFuture associated with the completion of c10d.ProcessGroup.Work.\nget_future is currently supported for NCCL and also supported for most\noperations on GLOO and MPI, except for peer to peer operations (send/recv).",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> model_DDP = torch.nn.parallel.DistributedDataParallel(model)\n>>> # Training loop\n>>> ...\n>>> ddp_logging_data = model_DDP._get_ddp_logging_data()\n>>> static_graph = ddp_logging_data.get(\"can_set_static_graph\")\n\n\nBelow is an example of a noop hook that returns the same tensor.\n>>> def noop(state: object, bucket: dist.GradBucket) -> torch.futures.Future[torch.Tensor]:\n>>>     fut = torch.futures.Future()\n>>>     fut.set_result(bucket.buffer())\n>>>     return fut\n>>> ddp.register_comm_hook(state=None, hook=noop)\n\n\nBelow is an example of a Parallel SGD algorithm where gradients are encoded before\nallreduce, and then decoded after allreduce.\n>>> def encode_and_decode(state: object, bucket: dist.GradBucket) -> torch.futures.Future[torch.Tensor]:\n>>>     encoded_tensor = encode(bucket.buffer())  # encode gradients\n>>>     fut = torch.distributed.all_reduce(encoded_tensor).get_future()\n>>>     # Define the then callback to decode.\n>>>     def decode(fut):\n>>>         decoded_tensor = decode(fut.value()[0])  # decode gradients\n>>>         return decoded_tensor\n>>>     return fut.then(decode)\n>>> ddp.register_comm_hook(state=None, hook=encode_and_decode)\n\n\n"
    },
    {
        "api_name": "torch.distributed.optim.DistributedOptimizer",
        "api_url": "https://pytorch.org/docs/stable/distributed.optim.html#torch.distributed.optim.DistributedOptimizer",
        "api_signature": "torch.distributed.optim.DistributedOptimizer(optimizer_class, params_rref, *args, **kwargs)",
        "api_description": "DistributedOptimizer takes remote references to parameters scattered\nacross workers and applies the given optimizer locally for each parameter.",
        "return_value": "",
        "parameters": "optimizer_class (optim.Optimizer) – the class of optimizer to\ninstantiate on each worker.\nparams_rref (list[RRef]) – list of RRefs to local or remote parameters\nto optimize.\nargs – arguments to pass to the optimizer constructor on each worker.\nkwargs – arguments to pass to the optimizer constructor on each worker.\ncontext_id – the autograd context id for which we should run the\noptimizer step.",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import torch.distributed.autograd as dist_autograd\n>>> import torch.distributed.rpc as rpc\n>>> from torch import optim\n>>> from torch.distributed.optim import DistributedOptimizer\n>>>\n>>> with dist_autograd.context() as context_id:\n>>>   # Forward pass.\n>>>   rref1 = rpc.remote(\"worker1\", torch.add, args=(torch.ones(2), 3))\n>>>   rref2 = rpc.remote(\"worker1\", torch.add, args=(torch.ones(2), 1))\n>>>   loss = rref1.to_here() + rref2.to_here()\n>>>\n>>>   # Backward pass.\n>>>   dist_autograd.backward(context_id, [loss.sum()])\n>>>\n>>>   # Optimizer.\n>>>   dist_optim = DistributedOptimizer(\n>>>      optim.SGD,\n>>>      [rref1, rref2],\n>>>      lr=0.05,\n>>>   )\n>>>   dist_optim.step(context_id)\n\n\n"
    },
    {
        "api_name": "torch.utils.data.distributed.DistributedSampler",
        "api_url": "https://pytorch.org/docs/stable/data.html#torch.utils.data.distributed.DistributedSampler",
        "api_signature": "torch.utils.data.distributed.DistributedSampler(dataset, num_replicas=None, rank=None, shuffle=True, seed=0, drop_last=False)",
        "api_description": "Sampler that restricts data loading to a subset of the dataset.",
        "return_value": "",
        "parameters": "dataset – Dataset used for sampling.\nnum_replicas (int, optional) – Number of processes participating in\ndistributed training. By default, world_size is retrieved from the\ncurrent distributed group.\nrank (int, optional) – Rank of the current process within num_replicas.\nBy default, rank is retrieved from the current distributed\ngroup.\nshuffle (bool, optional) – If True (default), sampler will shuffle the\nindices.\nseed (int, optional) – random seed used to shuffle the sampler if\nshuffle=True. This number should be identical across all\nprocesses in the distributed group. Default: 0.\ndrop_last (bool, optional) – if True, then the sampler will drop the\ntail of the data to make it evenly divisible across the number of\nreplicas. If False, the sampler will add extra indices to make\nthe data evenly divisible across the replicas. Default: False.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.distribution.Distribution",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.distribution.Distribution",
        "api_signature": "torch.distributions.distribution.Distribution(batch_shape=torch.Size([])",
        "api_description": "Bases: object",
        "return_value": "Tensor of shape batch_shape.\nTensor iterating over dimension 0.\nNew distribution instance with batch dimensions expanded to\nbatch_size.\nTensor of shape batch_shape.\n",
        "parameters": "value (Tensor) –\nexpand (bool) – whether to expand the support over the\nbatch dims to match the distribution’s batch_shape.\nbatch_shape (torch.Size) – the desired expanded size.\n_instance – new instance provided by subclasses that\nneed to override .expand.\nvalue (Tensor) –\nvalue (Tensor) –\nvalue (bool) – Whether to enable validation.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.DistStoreError",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#torch.distributed.DistStoreError",
        "api_signature": null,
        "api_description": "Exception raised when an error occurs in the distributed store",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.div",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.div.html#torch.div",
        "api_signature": "torch.div(input, other, *, rounding_mode=None, out=None)",
        "api_description": "Divides each element of the input input by the corresponding element of\nother.",
        "return_value": "",
        "parameters": "input (Tensor) – the dividend\nother (Tensor or Number) – the divisor\nrounding_mode (str, optional) – Type of rounding applied to the result:\nNone - default behavior. Performs no rounding and, if both input and\nother are integer types, promotes the inputs to the default scalar type.\nEquivalent to true division in Python (the / operator) and NumPy’s np.true_divide.\n\"trunc\" - rounds the results of the division towards zero.\nEquivalent to C-style integer division.\n\"floor\" - rounds the results of the division down.\nEquivalent to floor division in Python (the // operator) and NumPy’s np.floor_divide.\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.div",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.div.html#torch.Tensor.div",
        "api_signature": "Tensor.div(value, *, rounding_mode=None)",
        "api_description": "See torch.div()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.div_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.div_.html#torch.Tensor.div_",
        "api_signature": "Tensor.div_(value, *, rounding_mode=None)",
        "api_description": "In-place version of div()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.divide",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.divide.html#torch.divide",
        "api_signature": "torch.divide(input, other, *, rounding_mode=None, out=None)",
        "api_description": "Alias for torch.div().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.divide",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.divide.html#torch.Tensor.divide",
        "api_signature": "Tensor.divide(value, *, rounding_mode=None)",
        "api_description": "See torch.divide()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.divide_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.divide_.html#torch.Tensor.divide_",
        "api_signature": "Tensor.divide_(value, *, rounding_mode=None)",
        "api_description": "In-place version of divide()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.futures.Future.done",
        "api_url": "https://pytorch.org/docs/stable/futures.html#torch.futures.Future.done",
        "api_signature": "done()",
        "api_description": "Return True if this Future is done. A Future is done if it\nhas a result or an exception.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.dot",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.dot.html#torch.dot",
        "api_signature": "torch.dot(input, other, *, out=None)",
        "api_description": "Computes the dot product of two 1D tensors.",
        "return_value": "",
        "parameters": "input (Tensor) – first tensor in the dot product, must be 1D.\nother (Tensor) – second tensor in the dot product, must be 1D.\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.dot",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.dot.html#torch.Tensor.dot",
        "api_signature": "Tensor.dot(other)",
        "api_description": "See torch.dot()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.jit.ScriptModule.double",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.jit.ScriptModule.html#torch.jit.ScriptModule.double",
        "api_signature": "double()",
        "api_description": "Casts all floating point parameters and buffers to double datatype.",
        "return_value": "self\n",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.Module.double",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.double",
        "api_signature": "double()",
        "api_description": "Casts all floating point parameters and buffers to double datatype.",
        "return_value": "self\n",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.double",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.double.html#torch.Tensor.double",
        "api_signature": "Tensor.double(memory_format=torch.preserve_format)",
        "api_description": "self.double() is equivalent to self.to(torch.float64). See to().",
        "return_value": "",
        "parameters": "memory_format (torch.memory_format, optional) – the desired memory format of\nreturned Tensor. Default: torch.preserve_format.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.TypedStorage.double",
        "api_url": "https://pytorch.org/docs/stable/storage.html#torch.TypedStorage.double",
        "api_signature": "double()",
        "api_description": "Casts this storage to double type.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.UntypedStorage.double",
        "api_url": "https://pytorch.org/docs/stable/storage.html#torch.UntypedStorage.double",
        "api_signature": "double()",
        "api_description": "Casts this storage to double type.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.DoubleStorage",
        "api_url": "https://pytorch.org/docs/stable/storage.html#torch.DoubleStorage",
        "api_signature": "torch.DoubleStorage(*args, wrap_storage=None, dtype=None, device=None, _internal=False)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.hub.download_url_to_file",
        "api_url": "https://pytorch.org/docs/stable/hub.html#torch.hub.download_url_to_file",
        "api_signature": "torch.hub.download_url_to_file(url, dst, hash_prefix=None, progress=True)",
        "api_description": "Download object at the given URL to a local path.",
        "return_value": "",
        "parameters": "url (str) – URL of the object to download\ndst (str) – Full path where object will be saved, e.g. /tmp/temporary_file\nhash_prefix (str, optional) – If not None, the SHA256 downloaded file should start with hash_prefix.\nDefault: None\nprogress (bool, optional) – whether or not to display a progress bar to stderr\nDefault: True",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.quasirandom.SobolEngine.draw",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.quasirandom.SobolEngine.html#torch.quasirandom.SobolEngine.draw",
        "api_signature": "draw(n=1, out=None, dtype=torch.float32)",
        "api_description": "Function to draw a sequence of n points from a Sobol sequence.\nNote that the samples are dependent on the previous samples. The size\nof the result is (n,dimension)(n, dimension)(n,dimension).",
        "return_value": "",
        "parameters": "n (Int, optional) – The length of sequence of points to draw.\nDefault: 1\nout (Tensor, optional) – The output tensor\ndtype (torch.dtype, optional) – the desired data type of the\nreturned tensor.\nDefault: torch.float32",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.quasirandom.SobolEngine.draw_base2",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.quasirandom.SobolEngine.html#torch.quasirandom.SobolEngine.draw_base2",
        "api_signature": "draw_base2(m, out=None, dtype=torch.float32)",
        "api_description": "Function to draw a sequence of 2**m points from a Sobol sequence.\nNote that the samples are dependent on the previous samples. The size\nof the result is (2∗∗m,dimension)(2**m, dimension)(2∗∗m,dimension).",
        "return_value": "",
        "parameters": "m (Int) – The (base2) exponent of the number of points to draw.\nout (Tensor, optional) – The output tensor\ndtype (torch.dtype, optional) – the desired data type of the\nreturned tensor.\nDefault: torch.float32",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.mps.driver_allocated_memory",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.mps.driver_allocated_memory.html#torch.mps.driver_allocated_memory",
        "api_signature": "torch.mps.driver_allocated_memory()",
        "api_description": "Returns total GPU memory allocated by Metal driver for the process in bytes.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.Dropout",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html#torch.nn.Dropout",
        "api_signature": "torch.nn.Dropout(p=0.5, inplace=False)",
        "api_description": "During training, randomly zeroes some of the elements of the input tensor with probability p.",
        "return_value": "",
        "parameters": "p (float) – probability of an element to be zeroed. Default: 0.5\ninplace (bool) – If set to True, will do this operation in-place. Default: False",
        "input_shape": "\nInput: (∗)(*)(∗). Input can be of any shape\nOutput: (∗)(*)(∗). Output is of the same shape as input\n\n",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.functional.dropout",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.functional.dropout.html#torch.nn.functional.dropout",
        "api_signature": "torch.nn.functional.dropout(input, p=0.5, training=True, inplace=False)",
        "api_description": "During training, randomly zeroes some elements of the input tensor with probability p.",
        "return_value": "",
        "parameters": "p (float) – probability of an element to be zeroed. Default: 0.5\ntraining (bool) – apply dropout if is True. Default: True\ninplace (bool) – If set to True, will do this operation in-place. Default: False",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.Dropout1d",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.Dropout1d.html#torch.nn.Dropout1d",
        "api_signature": "torch.nn.Dropout1d(p=0.5, inplace=False)",
        "api_description": "Randomly zero out entire channels.",
        "return_value": "",
        "parameters": "p (float, optional) – probability of an element to be zero-ed.\ninplace (bool, optional) – If set to True, will do this operation\nin-place",
        "input_shape": "\nInput: (N,C,L)(N, C, L)(N,C,L) or (C,L)(C, L)(C,L).\nOutput: (N,C,L)(N, C, L)(N,C,L) or (C,L)(C, L)(C,L) (same shape as input).\n\n",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.functional.dropout1d",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.functional.dropout1d.html#torch.nn.functional.dropout1d",
        "api_signature": "torch.nn.functional.dropout1d(input, p=0.5, training=True, inplace=False)",
        "api_description": "Randomly zero out entire channels (a channel is a 1D feature map).",
        "return_value": "",
        "parameters": "p (float) – probability of a channel to be zeroed. Default: 0.5\ntraining (bool) – apply dropout if is True. Default: True\ninplace (bool) – If set to True, will do this operation in-place. Default: False",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.Dropout2d",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.Dropout2d.html#torch.nn.Dropout2d",
        "api_signature": "torch.nn.Dropout2d(p=0.5, inplace=False)",
        "api_description": "Randomly zero out entire channels.",
        "return_value": "",
        "parameters": "p (float, optional) – probability of an element to be zero-ed.\ninplace (bool, optional) – If set to True, will do this operation\nin-place",
        "input_shape": "\nInput: (N,C,H,W)(N, C, H, W)(N,C,H,W) or (N,C,L)(N, C, L)(N,C,L).\nOutput: (N,C,H,W)(N, C, H, W)(N,C,H,W) or (N,C,L)(N, C, L)(N,C,L) (same shape as input).\n\n",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.functional.dropout2d",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.functional.dropout2d.html#torch.nn.functional.dropout2d",
        "api_signature": "torch.nn.functional.dropout2d(input, p=0.5, training=True, inplace=False)",
        "api_description": "Randomly zero out entire channels (a channel is a 2D feature map).",
        "return_value": "",
        "parameters": "p (float) – probability of a channel to be zeroed. Default: 0.5\ntraining (bool) – apply dropout if is True. Default: True\ninplace (bool) – If set to True, will do this operation in-place. Default: False",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.Dropout3d",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.Dropout3d.html#torch.nn.Dropout3d",
        "api_signature": "torch.nn.Dropout3d(p=0.5, inplace=False)",
        "api_description": "Randomly zero out entire channels.",
        "return_value": "",
        "parameters": "p (float, optional) – probability of an element to be zeroed.\ninplace (bool, optional) – If set to True, will do this operation\nin-place",
        "input_shape": "\nInput: (N,C,D,H,W)(N, C, D, H, W)(N,C,D,H,W) or (C,D,H,W)(C, D, H, W)(C,D,H,W).\nOutput: (N,C,D,H,W)(N, C, D, H, W)(N,C,D,H,W) or (C,D,H,W)(C, D, H, W)(C,D,H,W) (same shape as input).\n\n",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.functional.dropout3d",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.functional.dropout3d.html#torch.nn.functional.dropout3d",
        "api_signature": "torch.nn.functional.dropout3d(input, p=0.5, training=True, inplace=False)",
        "api_description": "Randomly zero out entire channels (a channel is a 3D feature map).",
        "return_value": "",
        "parameters": "p (float) – probability of a channel to be zeroed. Default: 0.5\ntraining (bool) – apply dropout if is True. Default: True\ninplace (bool) – If set to True, will do this operation in-place. Default: False",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.dsplit",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.dsplit.html#torch.dsplit",
        "api_signature": "torch.dsplit(input, indices_or_sections)",
        "api_description": "Splits input, a tensor with three or more dimensions, into multiple tensors\ndepthwise according to indices_or_sections. Each split is a view of\ninput.",
        "return_value": "",
        "parameters": "input (Tensor) – tensor to split.\nindices_or_sections (int or list or tuple of ints) – See argument in torch.tensor_split().",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> t = torch.arange(16.0).reshape(2, 2, 4)\n>>> t\ntensor([[[ 0.,  1.,  2.,  3.],\n         [ 4.,  5.,  6.,  7.]],\n        [[ 8.,  9., 10., 11.],\n         [12., 13., 14., 15.]]])\n>>> torch.dsplit(t, 2)\n(tensor([[[ 0.,  1.],\n        [ 4.,  5.]],\n       [[ 8.,  9.],\n        [12., 13.]]]),\n tensor([[[ 2.,  3.],\n          [ 6.,  7.]],\n         [[10., 11.],\n          [14., 15.]]]))\n\n\n>>> torch.dsplit(t, [3, 6])\n(tensor([[[ 0.,  1.,  2.],\n          [ 4.,  5.,  6.]],\n         [[ 8.,  9., 10.],\n          [12., 13., 14.]]]),\n tensor([[[ 3.],\n          [ 7.]],\n         [[11.],\n          [15.]]]),\n tensor([], size=(2, 2, 0)))\n\n\n"
    },
    {
        "api_name": "torch.Tensor.dsplit",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.dsplit.html#torch.Tensor.dsplit",
        "api_signature": "Tensor.dsplit(split_size_or_sections)",
        "api_description": "See torch.dsplit()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.dstack",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.dstack.html#torch.dstack",
        "api_signature": "torch.dstack(tensors, *, out=None)",
        "api_description": "Stack tensors in sequence depthwise (along third axis).",
        "return_value": "",
        "parameters": "tensors (sequence of Tensors) – sequence of tensors to concatenate\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.dtype",
        "api_url": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.dtype",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.BFloat16Storage.dtype",
        "api_url": "https://pytorch.org/docs/stable/storage.html#torch.BFloat16Storage.dtype",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.BoolStorage.dtype",
        "api_url": "https://pytorch.org/docs/stable/storage.html#torch.BoolStorage.dtype",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ByteStorage.dtype",
        "api_url": "https://pytorch.org/docs/stable/storage.html#torch.ByteStorage.dtype",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.CharStorage.dtype",
        "api_url": "https://pytorch.org/docs/stable/storage.html#torch.CharStorage.dtype",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ComplexDoubleStorage.dtype",
        "api_url": "https://pytorch.org/docs/stable/storage.html#torch.ComplexDoubleStorage.dtype",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ComplexFloatStorage.dtype",
        "api_url": "https://pytorch.org/docs/stable/storage.html#torch.ComplexFloatStorage.dtype",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.DoubleStorage.dtype",
        "api_url": "https://pytorch.org/docs/stable/storage.html#torch.DoubleStorage.dtype",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.FloatStorage.dtype",
        "api_url": "https://pytorch.org/docs/stable/storage.html#torch.FloatStorage.dtype",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.HalfStorage.dtype",
        "api_url": "https://pytorch.org/docs/stable/storage.html#torch.HalfStorage.dtype",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.IntStorage.dtype",
        "api_url": "https://pytorch.org/docs/stable/storage.html#torch.IntStorage.dtype",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.LongStorage.dtype",
        "api_url": "https://pytorch.org/docs/stable/storage.html#torch.LongStorage.dtype",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.QInt32Storage.dtype",
        "api_url": "https://pytorch.org/docs/stable/storage.html#torch.QInt32Storage.dtype",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.QInt8Storage.dtype",
        "api_url": "https://pytorch.org/docs/stable/storage.html#torch.QInt8Storage.dtype",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.QUInt2x4Storage.dtype",
        "api_url": "https://pytorch.org/docs/stable/storage.html#torch.QUInt2x4Storage.dtype",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.QUInt4x2Storage.dtype",
        "api_url": "https://pytorch.org/docs/stable/storage.html#torch.QUInt4x2Storage.dtype",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.QUInt8Storage.dtype",
        "api_url": "https://pytorch.org/docs/stable/storage.html#torch.QUInt8Storage.dtype",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ShortStorage.dtype",
        "api_url": "https://pytorch.org/docs/stable/storage.html#torch.ShortStorage.dtype",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.TypedStorage.dtype",
        "api_url": "https://pytorch.org/docs/stable/storage.html#torch.TypedStorage.dtype",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.onnx.JitScalarType.dtype",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.onnx.JitScalarType.html#torch.onnx.JitScalarType.dtype",
        "api_signature": "dtype()",
        "api_description": "Convert a JitScalarType to a torch dtype.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.backend_config.DTypeConfig",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.quantization.backend_config.DTypeConfig.html#torch.ao.quantization.backend_config.DTypeConfig",
        "api_signature": "torch.ao.quantization.backend_config.DTypeConfig(input_dtype=None, output_dtype=None, weight_dtype=None, bias_dtype=None, is_dynamic=None)",
        "api_description": "Config object that specifies the supported data types passed as arguments to\nquantize ops in the reference model spec, for input and output activations,\nweights, and biases.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.backend_config.DTypeWithConstraints",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.quantization.backend_config.DTypeWithConstraints.html#torch.ao.quantization.backend_config.DTypeWithConstraints",
        "api_signature": "torch.ao.quantization.backend_config.DTypeWithConstraints(dtype=None, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None)",
        "api_description": "Config for specifying additional constraints for a given dtype, such as quantization\nvalue ranges, scale value ranges, and fixed quantization params, to be used in\nDTypeConfig.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.autograd.forward_ad.dual_level",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.autograd.forward_ad.dual_level.html#torch.autograd.forward_ad.dual_level",
        "api_signature": null,
        "api_description": "Context-manager for forward AD, where all forward AD computation must occur within the dual_level context.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.autograd.profiler_util.Kernel.duration",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.autograd.profiler_util.Kernel.html#torch.autograd.profiler_util.Kernel.duration",
        "api_signature": null,
        "api_description": "Alias for field number 2",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.export.dynamic_shapes.dynamic_dim",
        "api_url": "https://pytorch.org/docs/stable/export.html#torch.export.dynamic_shapes.dynamic_dim",
        "api_signature": "torch.export.dynamic_shapes.dynamic_dim(t, index, debug_name=None)",
        "api_description": "Warning",
        "return_value": "A _Constraint object that describes shape dynamism. It can be passed to export() so\nthat export() does not assume static size of specified tensor, i.e. keeping it dynamic\nas a symbolic size rather than specializing according to size of example tracing input.\n",
        "parameters": "t (torch.Tensor) – Example input tensor that have dynamic dimension size(s)\nindex (int) – Index of dynamic dimension",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.checkpoint.format_utils.DynamicMetaLoadPlanner",
        "api_url": "https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.format_utils.DynamicMetaLoadPlanner",
        "api_signature": "torch.distributed.checkpoint.format_utils.DynamicMetaLoadPlanner(flatten_state_dict=True, flatten_sharded_tensors=True)",
        "api_description": "Extension of DefaultLoadPlanner, which creates a new Metadata object based on the passed in state dict,\navoiding the need to read metadata from disk. This is useful when reading formats which don’t have a\nmetadata file, like Torch Save files.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.rendezvous.dynamic_rendezvous.DynamicRendezvousHandler",
        "api_url": "https://pytorch.org/docs/stable/elastic/rendezvous.html#torch.distributed.elastic.rendezvous.dynamic_rendezvous.DynamicRendezvousHandler",
        "api_signature": null,
        "api_description": "Represent a handler that sets up a rendezvous among a set of nodes.",
        "return_value": "",
        "parameters": "run_id (str) – The run id of the rendezvous.\nstore (Store) – The C10d store to return as part of the rendezvous.\nbackend (RendezvousBackend) – The backend to use to hold the rendezvous state.\nmin_nodes (int) – The minimum number of nodes to admit to the rendezvous.\nmax_nodes (int) – The maximum number of nodes to admit to the rendezvous.\nlocal_addr (Optional[str]) – The local node address.\ntimeout (Optional[RendezvousTimeout]) – The timeout configuration of the rendezvous.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.onnx.dynamo_export",
        "api_url": "https://pytorch.org/docs/stable/onnx_dynamo.html#torch.onnx.dynamo_export",
        "api_signature": "torch.onnx.dynamo_export(model, /, *model_args, export_options=None, **model_kwargs)",
        "api_description": "Export a torch.nn.Module to an ONNX graph.",
        "return_value": "An in-memory representation of the exported ONNX model.\n",
        "parameters": "model_args – Positional inputs to model.\nmodel_kwargs – Keyword inputs to model.\nexport_options (Optional[ExportOptions]) – Options to influence the export to ONNX.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.linalg.eig",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.linalg.eig.html#torch.linalg.eig",
        "api_signature": "torch.linalg.eig(A, *, out=None)",
        "api_description": "Computes the eigenvalue decomposition of a square matrix if it exists.",
        "return_value": "A named tuple (eigenvalues, eigenvectors) which corresponds to Λ\\LambdaΛ and VVV above.\neigenvalues and eigenvectors will always be complex-valued, even when A is real. The eigenvectors\nwill be given by the columns of eigenvectors.\n\n",
        "parameters": "A (Tensor) – tensor of shape (*, n, n) where * is zero or more batch dimensions\nconsisting of diagonalizable matrices.\nout (tuple, optional) – output tuple of two tensors. Ignored if None. Default: None.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.linalg.eigh",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.linalg.eigh.html#torch.linalg.eigh",
        "api_signature": "torch.linalg.eigh(A, UPLO='L', *, out=None)",
        "api_description": "Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix.",
        "return_value": "A named tuple (eigenvalues, eigenvectors) which corresponds to Λ\\LambdaΛ and QQQ above.\neigenvalues will always be real-valued, even when A is complex.\nIt will also be ordered in ascending order.\neigenvectors will have the same dtype as A and will contain the eigenvectors as its columns.\n\n",
        "parameters": "A (Tensor) – tensor of shape (*, n, n) where * is zero or more batch dimensions\nconsisting of symmetric or Hermitian matrices.\nUPLO ('L', 'U', optional) – controls whether to use the upper or lower triangular part\nof A in the computations. Default: ‘L’.\nout (tuple, optional) – output tuple of two tensors. Ignored if None. Default: None.",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> A = torch.randn(2, 2, dtype=torch.complex128)\n>>> A = A + A.T.conj()  # creates a Hermitian matrix\n>>> A\ntensor([[2.9228+0.0000j, 0.2029-0.0862j],\n        [0.2029+0.0862j, 0.3464+0.0000j]], dtype=torch.complex128)\n>>> L, Q = torch.linalg.eigh(A)\n>>> L\ntensor([0.3277, 2.9415], dtype=torch.float64)\n>>> Q\ntensor([[-0.0846+-0.0000j, -0.9964+0.0000j],\n        [ 0.9170+0.3898j, -0.0779-0.0331j]], dtype=torch.complex128)\n>>> torch.dist(Q @ torch.diag(L.cdouble()) @ Q.T.conj(), A)\ntensor(6.1062e-16, dtype=torch.float64)\n\n\n>>> A = torch.randn(3, 2, 2, dtype=torch.float64)\n>>> A = A + A.mT  # creates a batch of symmetric matrices\n>>> L, Q = torch.linalg.eigh(A)\n>>> torch.dist(Q @ torch.diag_embed(L) @ Q.mH, A)\ntensor(1.5423e-15, dtype=torch.float64)\n\n\n"
    },
    {
        "api_name": "torch.linalg.eigvals",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.linalg.eigvals.html#torch.linalg.eigvals",
        "api_signature": "torch.linalg.eigvals(A, *, out=None)",
        "api_description": "Computes the eigenvalues of a square matrix.",
        "return_value": "A complex-valued tensor containing the eigenvalues even when A is real.\n",
        "parameters": "A (Tensor) – tensor of shape (*, n, n) where * is zero or more batch dimensions.\nout (Tensor, optional) – output tensor. Ignored if None. Default: None.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.linalg.eigvalsh",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.linalg.eigvalsh.html#torch.linalg.eigvalsh",
        "api_signature": "torch.linalg.eigvalsh(A, UPLO='L', *, out=None)",
        "api_description": "Computes the eigenvalues of a complex Hermitian or real symmetric matrix.",
        "return_value": "A real-valued tensor containing the eigenvalues even when A is complex.\nThe eigenvalues are returned in ascending order.\n",
        "parameters": "A (Tensor) – tensor of shape (*, n, n) where * is zero or more batch dimensions\nconsisting of symmetric or Hermitian matrices.\nUPLO ('L', 'U', optional) – controls whether to use the upper or lower triangular part\nof A in the computations. Default: ‘L’.\nout (Tensor, optional) – output tensor. Ignored if None. Default: None.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.einsum",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.einsum.html#torch.einsum",
        "api_signature": "torch.einsum(equation, *operands)",
        "api_description": "Sums the product of the elements of the input operands along dimensions specified using a notation\nbased on the Einstein summation convention.",
        "return_value": "",
        "parameters": "equation (str) – The subscripts for the Einstein summation.\noperands (List[Tensor]) – The tensors to compute the Einstein summation of.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda.Event.elapsed_time",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.cuda.Event.html#torch.cuda.Event.elapsed_time",
        "api_signature": "elapsed_time(end_event)",
        "api_description": "Return the time elapsed.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.mps.event.Event.elapsed_time",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.mps.event.Event.html#torch.mps.event.Event.elapsed_time",
        "api_signature": "elapsed_time(end_event)",
        "api_description": "Returns the time elapsed in milliseconds after the event was\nrecorded and before the end_event was recorded.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.xpu.Event.elapsed_time",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.xpu.Event.html#torch.xpu.Event.elapsed_time",
        "api_signature": "elapsed_time(end_event)",
        "api_description": "Return the time elapsed.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.autograd.profiler_util.Interval.elapsed_us",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.autograd.profiler_util.Interval.html#torch.autograd.profiler_util.Interval.elapsed_us",
        "api_signature": "elapsed_us()",
        "api_description": "Returns the length of the interval",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.agent.server.ElasticAgent",
        "api_url": "https://pytorch.org/docs/stable/elastic/agent.html#torch.distributed.elastic.agent.server.ElasticAgent",
        "api_signature": null,
        "api_description": "An agent process responsible for managing one or more worker processes.",
        "return_value": "The result of the execution, containing the return values or\nfailure details for each worker mapped by the worker’s global rank.\n",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.element_size",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.element_size.html#torch.Tensor.element_size",
        "api_signature": "Tensor.element_size()",
        "api_description": "Returns the size in bytes of an individual element.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.TypedStorage.element_size",
        "api_url": "https://pytorch.org/docs/stable/storage.html#torch.TypedStorage.element_size",
        "api_signature": "element_size()",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.UntypedStorage.element_size",
        "api_url": "https://pytorch.org/docs/stable/storage.html#torch.UntypedStorage.element_size",
        "api_signature": "element_size()",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.Graph.eliminate_dead_code",
        "api_url": "https://pytorch.org/docs/stable/fx.html#torch.fx.Graph.eliminate_dead_code",
        "api_signature": "eliminate_dead_code()",
        "api_description": "Remove all dead code from the graph, based on each node’s number of\nusers, and whether the nodes have any side effects. The graph must be\ntopologically sorted before calling.",
        "return_value": "Whether the graph was changed as a result of the pass.\n",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.quantized.ELU",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.nn.quantized.ELU.html#torch.ao.nn.quantized.ELU",
        "api_signature": "torch.ao.nn.quantized.ELU(scale, zero_point, alpha=1.0)",
        "api_description": "This is the quantized equivalent of ELU.",
        "return_value": "",
        "parameters": "scale – quantization scale of the output tensor\nzero_point – quantization zero point of the output tensor\nalpha (float) – the alpha constant",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.quantized.functional.elu",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.nn.quantized.functional.elu.html#torch.ao.nn.quantized.functional.elu",
        "api_signature": "torch.ao.nn.quantized.functional.elu(input, scale, zero_point, alpha=1.0)",
        "api_description": "This is the quantized version of elu().",
        "return_value": "",
        "parameters": "input (Tensor) – quantized input\nscale (float) – quantization scale of the output tensor\nzero_point (int) – quantization zero point of the output tensor\nalpha (float) – the alpha constant",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.ELU",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.ELU.html#torch.nn.ELU",
        "api_signature": "torch.nn.ELU(alpha=1.0, inplace=False)",
        "api_description": "Applies the Exponential Linear Unit (ELU) function, element-wise.",
        "return_value": "",
        "parameters": "alpha (float) – the α\\alphaα value for the ELU formulation. Default: 1.0\ninplace (bool) – can optionally do the operation in-place. Default: False",
        "input_shape": "\nInput: (∗)(*)(∗), where ∗*∗ means any number of dimensions.\nOutput: (∗)(*)(∗), same shape as the input.\n\n",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.functional.elu",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.functional.elu.html#torch.nn.functional.elu",
        "api_signature": "torch.nn.functional.elu(input, alpha=1.0, inplace=False)",
        "api_description": "Apply the Exponential Linear Unit (ELU) function element-wise.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.functional.elu_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.functional.elu_.html#torch.nn.functional.elu_",
        "api_signature": "torch.nn.functional.elu_(input, alpha=1.)",
        "api_description": "In-place version of elu().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.quantized.Embedding",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.nn.quantized.Embedding.html#torch.ao.nn.quantized.Embedding",
        "api_signature": "torch.ao.nn.quantized.Embedding(num_embeddings, embedding_dim, padding_idx=None, max_norm=None, norm_type=2.0, scale_grad_by_freq=False, sparse=False, _weight=None, dtype=torch.quint8)",
        "api_description": "A quantized Embedding module with quantized packed weights as inputs.\nWe adopt the same interface as torch.nn.Embedding, please see\nhttps://pytorch.org/docs/stable/nn.html#torch.nn.Embedding for documentation.",
        "return_value": "",
        "parameters": "mod (Module) – a float module, either produced by torch.ao.quantization\nutilities or provided by user",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> m = nn.quantized.Embedding(num_embeddings=10, embedding_dim=12)\n>>> indices = torch.tensor([9, 6, 5, 7, 8, 8, 9, 2, 8])\n>>> output = m(indices)\n>>> print(output.size())\ntorch.Size([9, 12])\n\n\n"
    },
    {
        "api_name": "torch.nn.Embedding",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html#torch.nn.Embedding",
        "api_signature": "torch.nn.Embedding(num_embeddings, embedding_dim, padding_idx=None, max_norm=None, norm_type=2.0, scale_grad_by_freq=False, sparse=False, _weight=None, _freeze=False, device=None, dtype=None)",
        "api_description": "A simple lookup table that stores embeddings of a fixed dictionary and size.",
        "return_value": "",
        "parameters": "num_embeddings (int) – size of the dictionary of embeddings\nembedding_dim (int) – the size of each embedding vector\npadding_idx (int, optional) – If specified, the entries at padding_idx do not contribute to the gradient;\ntherefore, the embedding vector at padding_idx is not updated during training,\ni.e. it remains as a fixed “pad”. For a newly constructed Embedding,\nthe embedding vector at padding_idx will default to all zeros,\nbut can be updated to another value to be used as the padding vector.\nmax_norm (float, optional) – If given, each embedding vector with norm larger than max_norm\nis renormalized to have norm max_norm.\nnorm_type (float, optional) – The p of the p-norm to compute for the max_norm option. Default 2.\nscale_grad_by_freq (bool, optional) – If given, this will scale gradients by the inverse of frequency of\nthe words in the mini-batch. Default False.\nsparse (bool, optional) – If True, gradient w.r.t. weight matrix will be a sparse tensor.\nSee Notes for more details regarding sparse gradients.\nembeddings (Tensor) – FloatTensor containing weights for the Embedding.\nFirst dimension is being passed to Embedding as num_embeddings, second as embedding_dim.\nfreeze (bool, optional) – If True, the tensor does not get updated in the learning process.\nEquivalent to embedding.weight.requires_grad = False. Default: True\npadding_idx (int, optional) – If specified, the entries at padding_idx do not contribute to the gradient;\ntherefore, the embedding vector at padding_idx is not updated during training,\ni.e. it remains as a fixed “pad”.\nmax_norm (float, optional) – See module initialization documentation.\nnorm_type (float, optional) – See module initialization documentation. Default 2.\nscale_grad_by_freq (bool, optional) – See module initialization documentation. Default False.\nsparse (bool, optional) – See module initialization documentation.",
        "input_shape": "\nInput: (∗)(*)(∗), IntTensor or LongTensor of arbitrary shape containing the indices to extract\nOutput: (∗,H)(*, H)(∗,H), where * is the input shape and H=embedding_dimH=\\text{embedding\\_dim}H=embedding_dim\n\n",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.functional.embedding",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.functional.embedding.html#torch.nn.functional.embedding",
        "api_signature": "torch.nn.functional.embedding(input, weight, padding_idx=None, max_norm=None, norm_type=2.0, scale_grad_by_freq=False, sparse=False)",
        "api_description": "Generate a simple lookup table that looks up embeddings in a fixed dictionary and size.",
        "return_value": "",
        "parameters": "input (LongTensor) – Tensor containing indices into the embedding matrix\nweight (Tensor) – The embedding matrix with number of rows equal to the maximum possible index + 1,\nand number of columns equal to the embedding size\npadding_idx (int, optional) – If specified, the entries at padding_idx do not contribute to the gradient;\ntherefore, the embedding vector at padding_idx is not updated during training,\ni.e. it remains as a fixed “pad”.\nmax_norm (float, optional) – If given, each embedding vector with norm larger than max_norm\nis renormalized to have norm max_norm.\nNote: this will modify weight in-place.\nnorm_type (float, optional) – The p of the p-norm to compute for the max_norm option. Default 2.\nscale_grad_by_freq (bool, optional) – If given, this will scale gradients by the inverse of frequency of\nthe words in the mini-batch. Default False.\nsparse (bool, optional) – If True, gradient w.r.t. weight will be a sparse tensor. See Notes under\ntorch.nn.Embedding for more details regarding sparse gradients.",
        "input_shape": "\nInput: LongTensor of arbitrary shape containing the indices to extract\nWeight: Embedding matrix of floating point type with shape (V, embedding_dim),\nwhere V = maximum index + 1 and embedding_dim = the embedding size\nOutput: (*, embedding_dim), where * is the input shape\n\n",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.functional.embedding_bag",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.functional.embedding_bag.html#torch.nn.functional.embedding_bag",
        "api_signature": "torch.nn.functional.embedding_bag(input, weight, offsets=None, max_norm=None, norm_type=2, scale_grad_by_freq=False, mode='mean', sparse=False, per_sample_weights=None, include_last_offset=False, padding_idx=None)",
        "api_description": "Compute sums, means or maxes of bags of embeddings.",
        "return_value": "",
        "parameters": "input (LongTensor) – Tensor containing bags of indices into the embedding matrix\nweight (Tensor) – The embedding matrix with number of rows equal to the maximum possible index + 1,\nand number of columns equal to the embedding size\noffsets (LongTensor, optional) – Only used when input is 1D. offsets determines\nthe starting index position of each bag (sequence) in input.\nmax_norm (float, optional) – If given, each embedding vector with norm larger than max_norm\nis renormalized to have norm max_norm.\nNote: this will modify weight in-place.\nnorm_type (float, optional) – The p in the p-norm to compute for the max_norm option.\nDefault 2.\nscale_grad_by_freq (bool, optional) – if given, this will scale gradients by the inverse of frequency of\nthe words in the mini-batch. Default False.\nNote: this option is not supported when mode=\"max\".\nmode (str, optional) – \"sum\", \"mean\" or \"max\". Specifies the way to reduce the bag.\nDefault: \"mean\"\nsparse (bool, optional) – if True, gradient w.r.t. weight will be a sparse tensor. See Notes under\ntorch.nn.Embedding for more details regarding sparse gradients.\nNote: this option is not supported when mode=\"max\".\nper_sample_weights (Tensor, optional) – a tensor of float / double weights, or None\nto indicate all weights should be taken to be 1. If specified, per_sample_weights\nmust have exactly the same shape as input and is treated as having the same\noffsets, if those are not None.\ninclude_last_offset (bool, optional) – if True, the size of offsets is equal to the number of bags + 1.\nThe last element is the size of the input, or the ending index position of the last bag (sequence).\npadding_idx (int, optional) – If specified, the entries at padding_idx do not contribute to the\ngradient; therefore, the embedding vector at padding_idx is not updated\nduring training, i.e. it remains as a fixed “pad”. Note that the embedding\nvector at padding_idx is excluded from the reduction.",
        "input_shape": "\ninput (LongTensor) and offsets (LongTensor, optional)\n\nIf input is 2D of shape (B, N), it will be treated as B bags (sequences)\neach of fixed length N, and this will return B values aggregated in a way\ndepending on the mode. offsets is ignored and required to be None in this case.\nIf input is 1D of shape (N), it will be treated as a concatenation of\nmultiple bags (sequences). offsets is required to be a 1D tensor containing\nthe starting index positions of each bag in input. Therefore, for offsets\nof shape (B), input will be viewed as having B bags.\nEmpty bags (i.e., having 0-length) will have returned vectors filled by zeros.\n\n\nweight (Tensor): the learnable weights of the module of shape (num_embeddings, embedding_dim)\nper_sample_weights (Tensor, optional). Has the same shape as input.\noutput: aggregated embedding values of shape (B, embedding_dim)\n\n",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.quantized.EmbeddingBag",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.nn.quantized.EmbeddingBag.html#torch.ao.nn.quantized.EmbeddingBag",
        "api_signature": "torch.ao.nn.quantized.EmbeddingBag(num_embeddings, embedding_dim, max_norm=None, norm_type=2.0, scale_grad_by_freq=False, mode='sum', sparse=False, _weight=None, include_last_offset=False, dtype=torch.quint8)",
        "api_description": "A quantized EmbeddingBag module with quantized packed weights as inputs.\nWe adopt the same interface as torch.nn.EmbeddingBag, please see\nhttps://pytorch.org/docs/stable/nn.html#torch.nn.EmbeddingBag for documentation.",
        "return_value": "",
        "parameters": "mod (Module) – a float module, either produced by torch.ao.quantization\nutilities or provided by user",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> m = nn.quantized.EmbeddingBag(num_embeddings=10, embedding_dim=12, include_last_offset=True, mode='sum')\n>>> indices = torch.tensor([9, 6, 5, 7, 8, 8, 9, 2, 8, 6, 6, 9, 1, 6, 8, 8, 3, 2, 3, 6, 3, 6, 5, 7, 0, 8, 4, 6, 5, 8, 2, 3])\n>>> offsets = torch.tensor([0, 19, 20, 28, 28, 32])\n>>> output = m(indices, offsets)\n>>> print(output.size())\ntorch.Size([5, 12])\n\n\n"
    },
    {
        "api_name": "torch.nn.EmbeddingBag",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.EmbeddingBag.html#torch.nn.EmbeddingBag",
        "api_signature": "torch.nn.EmbeddingBag(num_embeddings, embedding_dim, max_norm=None, norm_type=2.0, scale_grad_by_freq=False, mode='mean', sparse=False, _weight=None, include_last_offset=False, padding_idx=None, device=None, dtype=None)",
        "api_description": "Compute sums or means of ‘bags’ of embeddings, without instantiating the intermediate embeddings.",
        "return_value": "Tensor output shape of (B, embedding_dim).\n",
        "parameters": "num_embeddings (int) – size of the dictionary of embeddings\nembedding_dim (int) – the size of each embedding vector\nmax_norm (float, optional) – If given, each embedding vector with norm larger than max_norm\nis renormalized to have norm max_norm.\nnorm_type (float, optional) – The p of the p-norm to compute for the max_norm option. Default 2.\nscale_grad_by_freq (bool, optional) – if given, this will scale gradients by the inverse of frequency of\nthe words in the mini-batch. Default False.\nNote: this option is not supported when mode=\"max\".\nmode (str, optional) – \"sum\", \"mean\" or \"max\". Specifies the way to reduce the bag.\n\"sum\" computes the weighted sum, taking per_sample_weights\ninto consideration. \"mean\" computes the average of the values\nin the bag, \"max\" computes the max value over each bag.\nDefault: \"mean\"\nsparse (bool, optional) – if True, gradient w.r.t. weight matrix will be a sparse tensor. See\nNotes for more details regarding sparse gradients. Note: this option is not\nsupported when mode=\"max\".\ninclude_last_offset (bool, optional) – if True, offsets has one additional element, where the last element\nis equivalent to the size of indices. This matches the CSR format.\npadding_idx (int, optional) – If specified, the entries at padding_idx do not contribute to the\ngradient; therefore, the embedding vector at padding_idx is not updated\nduring training, i.e. it remains as a fixed “pad”. For a newly constructed\nEmbeddingBag, the embedding vector at padding_idx will default to all\nzeros, but can be updated to another value to be used as the padding vector.\nNote that the embedding vector at padding_idx is excluded from the\nreduction.\ninput (Tensor) – Tensor containing bags of indices into the embedding matrix.\noffsets (Tensor, optional) – Only used when input is 1D. offsets determines\nthe starting index position of each bag (sequence) in input.\nper_sample_weights (Tensor, optional) – a tensor of float / double weights, or None\nto indicate all weights should be taken to be 1. If specified, per_sample_weights\nmust have exactly the same shape as input and is treated as having the same\noffsets, if those are not None. Only supported for mode='sum'.\nembeddings (Tensor) – FloatTensor containing weights for the EmbeddingBag.\nFirst dimension is being passed to EmbeddingBag as ‘num_embeddings’, second as ‘embedding_dim’.\nfreeze (bool, optional) – If True, the tensor does not get updated in the learning process.\nEquivalent to embeddingbag.weight.requires_grad = False. Default: True\nmax_norm (float, optional) – See module initialization documentation. Default: None\nnorm_type (float, optional) – See module initialization documentation. Default 2.\nscale_grad_by_freq (bool, optional) – See module initialization documentation. Default False.\nmode (str, optional) – See module initialization documentation. Default: \"mean\"\nsparse (bool, optional) – See module initialization documentation. Default: False.\ninclude_last_offset (bool, optional) – See module initialization documentation. Default: False.\npadding_idx (int, optional) – See module initialization documentation. Default: None.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.autograd.profiler.emit_itt",
        "api_url": "https://pytorch.org/docs/stable/autograd.html#torch.autograd.profiler.emit_itt",
        "api_signature": "torch.autograd.profiler.emit_itt(enabled=True, record_shapes=False)",
        "api_description": "Context manager that makes every autograd operation emit an ITT range.",
        "return_value": "",
        "parameters": "enabled (bool, optional) – Setting enabled=False makes this context manager a no-op.\nDefault: True.\nrecord_shapes (bool, optional) – If record_shapes=True, the itt range wrapping\neach autograd op will append information about the sizes of Tensor arguments received\nby that op, in the following format:\n[[arg0.size(0), arg0.size(1), ...], [arg1.size(0), arg1.size(1), ...], ...]\nNon-tensor arguments will be represented by [].\nArguments will be listed in the order they are received by the backend op.\nPlease note that this order may not match the order in which those arguments were passed\non the Python side.  Also note that shape recording may increase the overhead of itt range creation.\nDefault: False",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.autograd.profiler.emit_nvtx",
        "api_url": "https://pytorch.org/docs/stable/autograd.html#torch.autograd.profiler.emit_nvtx",
        "api_signature": "torch.autograd.profiler.emit_nvtx(enabled=True, record_shapes=False)",
        "api_description": "Context manager that makes every autograd operation emit an NVTX range.",
        "return_value": "",
        "parameters": "enabled (bool, optional) – Setting enabled=False makes this context manager a no-op.\nDefault: True.\nrecord_shapes (bool, optional) – If record_shapes=True, the nvtx range wrapping\neach autograd op will append information about the sizes of Tensor arguments received\nby that op, in the following format:\n[[arg0.size(0), arg0.size(1), ...], [arg1.size(0), arg1.size(1), ...], ...]\nNon-tensor arguments will be represented by [].\nArguments will be listed in the order they are received by the backend op.\nPlease note that this order may not match the order in which those arguments were passed\non the Python side.  Also note that shape recording may increase the overhead of nvtx range creation.\nDefault: False",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.empty",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.empty.html#torch.empty",
        "api_signature": "torch.empty(*size, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False, pin_memory=False, memory_format=torch.contiguous_format)",
        "api_description": "Returns a tensor filled with uninitialized data. The shape of the tensor is\ndefined by the variable argument size.",
        "return_value": "",
        "parameters": "size (int...) – a sequence of integers defining the shape of the output tensor.\nCan be a variable number of arguments or a collection like a list or tuple.\nout (Tensor, optional) – the output tensor.\ndtype (torch.dtype, optional) – the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_dtype()).\nlayout (torch.layout, optional) – the desired layout of returned Tensor.\nDefault: torch.strided.\ndevice (torch.device, optional) – the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_device()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types.\nrequires_grad (bool, optional) – If autograd should record operations on the\nreturned tensor. Default: False.\npin_memory (bool, optional) – If set, returned tensor would be allocated in\nthe pinned memory. Works only for CPU tensors. Default: False.\nmemory_format (torch.memory_format, optional) – the desired memory format of\nreturned Tensor. Default: torch.contiguous_format.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda.empty_cache",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.cuda.empty_cache.html#torch.cuda.empty_cache",
        "api_signature": "torch.cuda.empty_cache()",
        "api_description": "Release all unoccupied cached memory currently held by the caching\nallocator so that those can be used in other GPU application and visible in\nnvidia-smi.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.mps.empty_cache",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.mps.empty_cache.html#torch.mps.empty_cache",
        "api_signature": "torch.mps.empty_cache()",
        "api_description": "Releases all unoccupied cached memory currently held by the caching\nallocator so that those can be used in other GPU applications.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.xpu.empty_cache",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.xpu.empty_cache.html#torch.xpu.empty_cache",
        "api_signature": "torch.xpu.empty_cache()",
        "api_description": "Release all unoccupied cached memory currently held by the caching\nallocator so that those can be used in other XPU application.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.empty_like",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.empty_like.html#torch.empty_like",
        "api_signature": "torch.empty_like(input, *, dtype=None, layout=None, device=None, requires_grad=False, memory_format=torch.preserve_format)",
        "api_description": "Returns an uninitialized tensor with the same size as input.\ntorch.empty_like(input) is equivalent to\ntorch.empty(input.size(), dtype=input.dtype, layout=input.layout, device=input.device).",
        "return_value": "",
        "parameters": "input (Tensor) – the size of input will determine size of the output tensor.\ndtype (torch.dtype, optional) – the desired data type of returned Tensor.\nDefault: if None, defaults to the dtype of input.\nlayout (torch.layout, optional) – the desired layout of returned tensor.\nDefault: if None, defaults to the layout of input.\ndevice (torch.device, optional) – the desired device of returned tensor.\nDefault: if None, defaults to the device of input.\nrequires_grad (bool, optional) – If autograd should record operations on the\nreturned tensor. Default: False.\nmemory_format (torch.memory_format, optional) – the desired memory format of\nreturned Tensor. Default: torch.preserve_format.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.empty_strided",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.empty_strided.html#torch.empty_strided",
        "api_signature": "torch.empty_strided(size, stride, *, dtype=None, layout=None, device=None, requires_grad=False, pin_memory=False)",
        "api_description": "Creates a tensor with the specified size and stride and filled with undefined data.",
        "return_value": "",
        "parameters": "size (tuple of int) – the shape of the output tensor\nstride (tuple of int) – the strides of the output tensor\ndtype (torch.dtype, optional) – the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_dtype()).\nlayout (torch.layout, optional) – the desired layout of returned Tensor.\nDefault: torch.strided.\ndevice (torch.device, optional) – the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_device()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types.\nrequires_grad (bool, optional) – If autograd should record operations on the\nreturned tensor. Default: False.\npin_memory (bool, optional) – If set, returned tensor would be allocated in\nthe pinned memory. Works only for CPU tensors. Default: False.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.package.EmptyMatchError",
        "api_url": "https://pytorch.org/docs/stable/package.html#torch.package.EmptyMatchError",
        "api_signature": null,
        "api_description": "This is an exception that is thrown when a mock or extern is marked as\nallow_empty=False, and is not matched with any module during packaging.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.sparse.check_sparse_tensor_invariants.enable",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.sparse.check_sparse_tensor_invariants.html#torch.sparse.check_sparse_tensor_invariants.enable",
        "api_signature": "enable()",
        "api_description": "Enable sparse tensor invariants checking in sparse tensor constructors.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda._sanitizer.enable_cuda_sanitizer",
        "api_url": "https://pytorch.org/docs/stable/cuda._sanitizer.html#torch.cuda._sanitizer.enable_cuda_sanitizer",
        "api_signature": "torch.cuda._sanitizer.enable_cuda_sanitizer()",
        "api_description": "Enable CUDA Sanitizer.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.backends.cuda.enable_cudnn_sdp",
        "api_url": "https://pytorch.org/docs/stable/backends.html#torch.backends.cuda.enable_cudnn_sdp",
        "api_signature": "torch.backends.cuda.enable_cudnn_sdp(enabled)",
        "api_description": "Warning",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda.CUDAGraph.enable_debug_mode",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.cuda.CUDAGraph.html#torch.cuda.CUDAGraph.enable_debug_mode",
        "api_signature": "enable_debug_mode()",
        "api_description": "Enable debugging mode for CUDAGraph.debug_dump.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.onnx.enable_fake_mode",
        "api_url": "https://pytorch.org/docs/stable/onnx_dynamo.html#torch.onnx.enable_fake_mode",
        "api_signature": "torch.onnx.enable_fake_mode()",
        "api_description": "Enable fake mode for the duration of the context.",
        "return_value": "A ONNXFakeContext object that must be passed to dynamo_export()\nthrough the ExportOptions.fake_context argument.\n",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.fake_quantize.enable_fake_quant",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.quantization.fake_quantize.enable_fake_quant.html#torch.ao.quantization.fake_quantize.enable_fake_quant",
        "api_signature": "torch.ao.quantization.fake_quantize.enable_fake_quant(mod)",
        "api_description": "Enable fake quantization for the module.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.backends.cuda.enable_flash_sdp",
        "api_url": "https://pytorch.org/docs/stable/backends.html#torch.backends.cuda.enable_flash_sdp",
        "api_signature": "torch.backends.cuda.enable_flash_sdp(enabled)",
        "api_description": "Warning",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.enable_grad",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.enable_grad.html#torch.enable_grad",
        "api_signature": "torch.enable_grad(orig_func=None)",
        "api_description": "Context-manager that enables gradient calculation.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> x = torch.tensor([1.], requires_grad=True)\n>>> with torch.no_grad():\n...     with torch.enable_grad():\n...         y = x * 2\n>>> y.requires_grad\nTrue\n>>> y.backward()\n>>> x.grad\ntensor([2.])\n>>> @torch.enable_grad()\n... def doubler(x):\n...     return x * 2\n>>> with torch.no_grad():\n...     z = doubler(x)\n>>> z.requires_grad\nTrue\n>>> @torch.enable_grad\n... def tripler(x):\n...     return x * 3\n>>> with torch.no_grad():\n...     z = tripler(x)\n>>> z.requires_grad\nTrue\n\n\n"
    },
    {
        "api_name": "torch.onnx.enable_log",
        "api_url": "https://pytorch.org/docs/stable/onnx_torchscript.html#torch.onnx.enable_log",
        "api_signature": "torch.onnx.enable_log()",
        "api_description": "Enables ONNX logging.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.backends.cuda.enable_math_sdp",
        "api_url": "https://pytorch.org/docs/stable/backends.html#torch.backends.cuda.enable_math_sdp",
        "api_signature": "torch.backends.cuda.enable_math_sdp(enabled)",
        "api_description": "Warning",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.backends.cuda.enable_mem_efficient_sdp",
        "api_url": "https://pytorch.org/docs/stable/backends.html#torch.backends.cuda.enable_mem_efficient_sdp",
        "api_signature": "torch.backends.cuda.enable_mem_efficient_sdp(enabled)",
        "api_description": "Warning",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.fake_quantize.enable_observer",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.quantization.fake_quantize.enable_observer.html#torch.ao.quantization.fake_quantize.enable_observer",
        "api_signature": "torch.ao.quantization.fake_quantize.enable_observer(mod)",
        "api_description": "Enable observation for this module.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.jit.enable_onednn_fusion",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.jit.enable_onednn_fusion.html#torch.jit.enable_onednn_fusion",
        "api_signature": "torch.jit.enable_onednn_fusion(enabled)",
        "api_description": "Enable or disables onednn JIT fusion based on the parameter enabled.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.backends.cudnn.enabled",
        "api_url": "https://pytorch.org/docs/stable/backends.html#torch.backends.cudnn.enabled",
        "api_signature": null,
        "api_description": "A bool that controls whether cuDNN is enabled.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.backends.opt_einsum.enabled",
        "api_url": "https://pytorch.org/docs/stable/backends.html#torch.backends.opt_einsum.enabled",
        "api_signature": null,
        "api_description": "A :class:bool that controls whether opt_einsum is enabled (True by default). If so,\ntorch.einsum will use opt_einsum (https://optimized-einsum.readthedocs.io/en/stable/path_finding.html)\nif available to calculate an optimal path of contraction for faster performance.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.autograd.profiler.EnforceUnique",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.autograd.profiler.EnforceUnique.html#torch.autograd.profiler.EnforceUnique",
        "api_signature": null,
        "api_description": "Raises an error if a key is seen more than once.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.autograd.forward_ad.enter_dual_level",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.autograd.forward_ad.enter_dual_level.html#torch.autograd.forward_ad.enter_dual_level",
        "api_signature": "torch.autograd.forward_ad.enter_dual_level()",
        "api_description": "Enter a new forward grad level.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.special.entr",
        "api_url": "https://pytorch.org/docs/stable/special.html#torch.special.entr",
        "api_signature": "torch.special.entr(input, *, out=None)",
        "api_description": "Computes the entropy on input (as defined below), elementwise.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> a = torch.arange(-0.5, 1, 0.5)\n>>> a\ntensor([-0.5000,  0.0000,  0.5000])\n>>> torch.special.entr(a)\ntensor([  -inf, 0.0000, 0.3466])\n\n\n"
    },
    {
        "api_name": "torch.distributions.bernoulli.Bernoulli.entropy",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.bernoulli.Bernoulli.entropy",
        "api_signature": "entropy()",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.beta.Beta.entropy",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.beta.Beta.entropy",
        "api_signature": "entropy()",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.binomial.Binomial.entropy",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.binomial.Binomial.entropy",
        "api_signature": "entropy()",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.categorical.Categorical.entropy",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.categorical.Categorical.entropy",
        "api_signature": "entropy()",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.cauchy.Cauchy.entropy",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.cauchy.Cauchy.entropy",
        "api_signature": "entropy()",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.continuous_bernoulli.ContinuousBernoulli.entropy",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.continuous_bernoulli.ContinuousBernoulli.entropy",
        "api_signature": "entropy()",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.dirichlet.Dirichlet.entropy",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.dirichlet.Dirichlet.entropy",
        "api_signature": "entropy()",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.distribution.Distribution.entropy",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.distribution.Distribution.entropy",
        "api_signature": "entropy()",
        "api_description": "Returns entropy of distribution, batched over batch_shape.",
        "return_value": "Tensor of shape batch_shape.\n",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.exp_family.ExponentialFamily.entropy",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.exp_family.ExponentialFamily.entropy",
        "api_signature": "entropy()",
        "api_description": "Method to compute the entropy using Bregman divergence of the log normalizer.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.exponential.Exponential.entropy",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.exponential.Exponential.entropy",
        "api_signature": "entropy()",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.gamma.Gamma.entropy",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.gamma.Gamma.entropy",
        "api_signature": "entropy()",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.geometric.Geometric.entropy",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.geometric.Geometric.entropy",
        "api_signature": "entropy()",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.gumbel.Gumbel.entropy",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.gumbel.Gumbel.entropy",
        "api_signature": "entropy()",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.half_cauchy.HalfCauchy.entropy",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.half_cauchy.HalfCauchy.entropy",
        "api_signature": "entropy()",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.half_normal.HalfNormal.entropy",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.half_normal.HalfNormal.entropy",
        "api_signature": "entropy()",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.independent.Independent.entropy",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.independent.Independent.entropy",
        "api_signature": "entropy()",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.inverse_gamma.InverseGamma.entropy",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.inverse_gamma.InverseGamma.entropy",
        "api_signature": "entropy()",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.kumaraswamy.Kumaraswamy.entropy",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.kumaraswamy.Kumaraswamy.entropy",
        "api_signature": "entropy()",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.laplace.Laplace.entropy",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.laplace.Laplace.entropy",
        "api_signature": "entropy()",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.log_normal.LogNormal.entropy",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.log_normal.LogNormal.entropy",
        "api_signature": "entropy()",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.entropy",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.entropy",
        "api_signature": "entropy()",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.multinomial.Multinomial.entropy",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.multinomial.Multinomial.entropy",
        "api_signature": "entropy()",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.multivariate_normal.MultivariateNormal.entropy",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.multivariate_normal.MultivariateNormal.entropy",
        "api_signature": "entropy()",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.normal.Normal.entropy",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.normal.Normal.entropy",
        "api_signature": "entropy()",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.one_hot_categorical.OneHotCategorical.entropy",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.one_hot_categorical.OneHotCategorical.entropy",
        "api_signature": "entropy()",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.pareto.Pareto.entropy",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.pareto.Pareto.entropy",
        "api_signature": "entropy()",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.studentT.StudentT.entropy",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.studentT.StudentT.entropy",
        "api_signature": "entropy()",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.uniform.Uniform.entropy",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.uniform.Uniform.entropy",
        "api_signature": "entropy()",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.weibull.Weibull.entropy",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.weibull.Weibull.entropy",
        "api_signature": "entropy()",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.wishart.Wishart.entropy",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.wishart.Wishart.entropy",
        "api_signature": "entropy()",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.bernoulli.Bernoulli.enumerate_support",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.bernoulli.Bernoulli.enumerate_support",
        "api_signature": "enumerate_support(expand=True)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.binomial.Binomial.enumerate_support",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.binomial.Binomial.enumerate_support",
        "api_signature": "enumerate_support(expand=True)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.categorical.Categorical.enumerate_support",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.categorical.Categorical.enumerate_support",
        "api_signature": "enumerate_support(expand=True)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.distribution.Distribution.enumerate_support",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.distribution.Distribution.enumerate_support",
        "api_signature": "enumerate_support(expand=True)",
        "api_description": "Returns tensor containing all values supported by a discrete\ndistribution. The result will enumerate over dimension 0, so the shape\nof the result will be (cardinality,) + batch_shape + event_shape\n(where event_shape = () for univariate distributions).",
        "return_value": "Tensor iterating over dimension 0.\n",
        "parameters": "expand (bool) – whether to expand the support over the\nbatch dims to match the distribution’s batch_shape.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.independent.Independent.enumerate_support",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.independent.Independent.enumerate_support",
        "api_signature": "enumerate_support(expand=True)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.one_hot_categorical.OneHotCategorical.enumerate_support",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.one_hot_categorical.OneHotCategorical.enumerate_support",
        "api_signature": "enumerate_support(expand=True)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "envvar-PYTORCH_JIT",
        "api_url": "https://pytorch.org/docs/stable/jit.html#envvar-PYTORCH_JIT",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "envvar-PYTORCH_JIT",
        "api_url": "https://pytorch.org/docs/stable/jit.html#envvar-PYTORCH_JIT",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.eq",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.eq.html#torch.eq",
        "api_signature": "torch.eq(input, other, *, out=None)",
        "api_description": "Computes element-wise equality",
        "return_value": "A boolean tensor that is True where input is equal to other and False elsewhere\n",
        "parameters": "input (Tensor) – the tensor to compare\nother (Tensor or float) – the tensor or value to compare\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.eq",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.eq.html#torch.Tensor.eq",
        "api_signature": "Tensor.eq(other)",
        "api_description": "See torch.eq()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.eq_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.eq_.html#torch.Tensor.eq_",
        "api_signature": "Tensor.eq_(other)",
        "api_description": "In-place version of eq()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.equal",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.equal.html#torch.equal",
        "api_signature": "torch.equal(input, other)",
        "api_description": "True if two tensors have the same size and elements, False otherwise.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.equal",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.equal.html#torch.Tensor.equal",
        "api_signature": "Tensor.equal(other)",
        "api_description": "See torch.equal()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.symbolic_shapes.EqualityConstraint",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.fx.experimental.symbolic_shapes.EqualityConstraint.html#torch.fx.experimental.symbolic_shapes.EqualityConstraint",
        "api_signature": "torch.fx.experimental.symbolic_shapes.EqualityConstraint(warn_only, source_pairs, derived_equalities, phantom_symbols)",
        "api_description": "Represent and decide various kinds of equality constraints between input sources.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.Graph.erase_node",
        "api_url": "https://pytorch.org/docs/stable/fx.html#torch.fx.Graph.erase_node",
        "api_signature": "erase_node(to_erase)",
        "api_description": "Erases a Node from the Graph. Throws an exception if\nthere are still users of that node in the Graph.",
        "return_value": "",
        "parameters": "to_erase (Node) – The Node to erase from the Graph.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.autograd.profiler.KinetoStepTracker.erase_step_count",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.autograd.profiler.KinetoStepTracker.html#torch.autograd.profiler.KinetoStepTracker.erase_step_count",
        "api_signature": "erase_step_count(requester)",
        "api_description": "Remove a given requester.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.erf",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.erf.html#torch.erf",
        "api_signature": "torch.erf(input, *, out=None)",
        "api_description": "Alias for torch.special.erf().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.special.erf",
        "api_url": "https://pytorch.org/docs/stable/special.html#torch.special.erf",
        "api_signature": "torch.special.erf(input, *, out=None)",
        "api_description": "Computes the error function of input. The error function is defined as follows:",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.erf",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.erf.html#torch.Tensor.erf",
        "api_signature": "Tensor.erf()",
        "api_description": "See torch.erf()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.erf_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.erf_.html#torch.Tensor.erf_",
        "api_signature": "Tensor.erf_()",
        "api_description": "In-place version of erf()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.erfc",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.erfc.html#torch.erfc",
        "api_signature": "torch.erfc(input, *, out=None)",
        "api_description": "Alias for torch.special.erfc().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.special.erfc",
        "api_url": "https://pytorch.org/docs/stable/special.html#torch.special.erfc",
        "api_signature": "torch.special.erfc(input, *, out=None)",
        "api_description": "Computes the complementary error function of input.\nThe complementary error function is defined as follows:",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.erfc",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.erfc.html#torch.Tensor.erfc",
        "api_signature": "Tensor.erfc()",
        "api_description": "See torch.erfc()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.erfc_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.erfc_.html#torch.Tensor.erfc_",
        "api_signature": "Tensor.erfc_()",
        "api_description": "In-place version of erfc()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.special.erfcx",
        "api_url": "https://pytorch.org/docs/stable/special.html#torch.special.erfcx",
        "api_signature": "torch.special.erfcx(input, *, out=None)",
        "api_description": "Computes the scaled complementary error function for each element of input.\nThe scaled complementary error function is defined as follows:",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.erfinv",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.erfinv.html#torch.erfinv",
        "api_signature": "torch.erfinv(input, *, out=None)",
        "api_description": "Alias for torch.special.erfinv().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.special.erfinv",
        "api_url": "https://pytorch.org/docs/stable/special.html#torch.special.erfinv",
        "api_signature": "torch.special.erfinv(input, *, out=None)",
        "api_description": "Computes the inverse error function of input.\nThe inverse error function is defined in the range (−1,1)(-1, 1)(−1,1) as:",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.erfinv",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.erfinv.html#torch.Tensor.erfinv",
        "api_signature": "Tensor.erfinv()",
        "api_description": "See torch.erfinv()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.erfinv_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.erfinv_.html#torch.Tensor.erfinv_",
        "api_signature": "Tensor.erfinv_()",
        "api_description": "In-place version of erfinv()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.multiprocessing.errors.ErrorHandler",
        "api_url": "https://pytorch.org/docs/stable/elastic/errors.html#torch.distributed.elastic.multiprocessing.errors.ErrorHandler",
        "api_signature": null,
        "api_description": "Write the provided exception object along with some other metadata about\nthe error in a structured way in JSON format to an error file specified by the\nenvironment variable: TORCHELASTIC_ERROR_FILE. If this environment\nvariable is not set, then simply logs the contents of what would have been\nwritten to the error file.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.onnx.verification.GraphInfo.essential_node_count",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.onnx.verification.GraphInfo.html#torch.onnx.verification.GraphInfo.essential_node_count",
        "api_signature": "essential_node_count()",
        "api_description": "Return the number of nodes in the subgraph excluding those in _EXCLUDED_NODE_KINDS.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.onnx.verification.GraphInfo.essential_node_kinds",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.onnx.verification.GraphInfo.html#torch.onnx.verification.GraphInfo.essential_node_kinds",
        "api_signature": "essential_node_kinds()",
        "api_description": "Return the set of node kinds in the subgraph excluding those in _EXCLUDED_NODE_KINDS.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.rendezvous.etcd_rendezvous_backend.EtcdRendezvousBackend",
        "api_url": "https://pytorch.org/docs/stable/elastic/rendezvous.html#torch.distributed.elastic.rendezvous.etcd_rendezvous_backend.EtcdRendezvousBackend",
        "api_signature": "torch.distributed.elastic.rendezvous.etcd_rendezvous_backend.EtcdRendezvousBackend(client, run_id, key_prefix=None, ttl=None)",
        "api_description": "Represents an etcd-based rendezvous backend.",
        "return_value": "",
        "parameters": "client (Client) – The etcd.Client instance to use to communicate with etcd.\nrun_id (str) – The run id of the rendezvous.\nkey_prefix (Optional[str]) – The path under which to store the rendezvous state in etcd.\nttl (Optional[int]) – The TTL of the rendezvous state. If not specified, defaults to two hours.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.rendezvous.etcd_rendezvous.EtcdRendezvousHandler",
        "api_url": "https://pytorch.org/docs/stable/elastic/rendezvous.html#torch.distributed.elastic.rendezvous.etcd_rendezvous.EtcdRendezvousHandler",
        "api_signature": "torch.distributed.elastic.rendezvous.etcd_rendezvous.EtcdRendezvousHandler(rdzv_impl)",
        "api_description": "Implements a\ntorch.distributed.elastic.rendezvous.RendezvousHandler interface\nbacked by\ntorch.distributed.elastic.rendezvous.etcd_rendezvous.EtcdRendezvous.\nEtcdRendezvousHandler uses a URL to configure the type of rendezvous to\nuse and to pass implementation specific configurations to the rendezvous\nmodule. The basic etcd rendezvous configuration URL looks like the following",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.rendezvous.etcd_server.EtcdServer",
        "api_url": "https://pytorch.org/docs/stable/elastic/rendezvous.html#torch.distributed.elastic.rendezvous.etcd_server.EtcdServer",
        "api_signature": "torch.distributed.elastic.rendezvous.etcd_server.EtcdServer(data_dir=None)",
        "api_description": "Note",
        "return_value": "",
        "parameters": "etcd_binary_path – path of etcd server binary (see above for fallback path)",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.rendezvous.etcd_store.EtcdStore",
        "api_url": "https://pytorch.org/docs/stable/elastic/rendezvous.html#torch.distributed.elastic.rendezvous.etcd_store.EtcdStore",
        "api_signature": "torch.distributed.elastic.rendezvous.etcd_store.EtcdStore(etcd_client, etcd_store_prefix, timeout=None)",
        "api_description": "Implement a c10 Store interface by piggybacking on the rendezvous etcd instance.",
        "return_value": "the new (incremented) value\nvalue (bytes)\n",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.jit.ScriptModule.eval",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.jit.ScriptModule.html#torch.jit.ScriptModule.eval",
        "api_signature": "eval()",
        "api_description": "Set the module in evaluation mode.",
        "return_value": "self\n",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.Module.eval",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.eval",
        "api_signature": "eval()",
        "api_description": "Set the module in evaluation mode.",
        "return_value": "self\n",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.symbolic_shapes.ShapeEnv.evaluate_expr",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.fx.experimental.symbolic_shapes.ShapeEnv.html#torch.fx.experimental.symbolic_shapes.ShapeEnv.evaluate_expr",
        "api_signature": "evaluate_expr(orig_expr, hint=None, fx_node=None, expect_rational=True, size_oblivious=False, *, forcing_spec=False)",
        "api_description": "Given an expression, evaluates it, adding guards if necessary",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.symbolic_shapes.ShapeEnv.evaluate_guards_expression",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.fx.experimental.symbolic_shapes.ShapeEnv.html#torch.fx.experimental.symbolic_shapes.ShapeEnv.evaluate_guards_expression",
        "api_signature": "evaluate_guards_expression(code, args)",
        "api_description": "Expected to be used with produce_guards_expression(). Evaluates an expression\ngenerated by produce_guards_expression for the given concrete args.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.symbolic_shapes.ShapeEnv.evaluate_guards_for_args",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.fx.experimental.symbolic_shapes.ShapeEnv.html#torch.fx.experimental.symbolic_shapes.ShapeEnv.evaluate_guards_for_args",
        "api_signature": "evaluate_guards_for_args(placeholders, args, *, ignore_static=True)",
        "api_description": "Generate guards for a graph’s placeholder values and evaluate the guards with args",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda.Event",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.cuda.Event.html#torch.cuda.Event",
        "api_signature": "torch.cuda.Event(enable_timing=False, blocking=False, interprocess=False)",
        "api_description": "Wrapper around a CUDA event.",
        "return_value": "A boolean indicating if all work currently captured by event has\ncompleted.\n",
        "parameters": "enable_timing (bool, optional) – indicates if the event should measure time\n(default: False)\nblocking (bool, optional) – if True, wait() will be blocking (default: False)\ninterprocess (bool) – if True, the event can be shared between processes\n(default: False)",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.events.api.Event",
        "api_url": "https://pytorch.org/docs/stable/elastic/events.html#torch.distributed.elastic.events.api.Event",
        "api_signature": "torch.distributed.elastic.events.api.Event(name, source, timestamp=0, metadata=<factory>)",
        "api_description": "The class represents the generic event that occurs during the torchelastic job execution.",
        "return_value": "",
        "parameters": "name (str) – event name.\nsource (EventSource) – the event producer, e.g. agent or worker\ntimestamp (int) – timestamp in milliseconds when event occurred.\nmetadata (Dict[str, Optional[Union[str, int, float, bool]]]) – additional data that is associated with the event.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.monitor.Event",
        "api_url": "https://pytorch.org/docs/stable/monitor.html#torch.monitor.Event",
        "api_signature": null,
        "api_description": "Event represents a specific typed event to be logged. This can represent\nhigh-level data points such as loss or accuracy per epoch or more\nlow-level aggregations such as through the Stats provided through this\nlibrary.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.mps.event.Event",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.mps.event.Event.html#torch.mps.event.Event",
        "api_signature": "torch.mps.event.Event(enable_timing=False)",
        "api_description": "Wrapper around an MPS event.",
        "return_value": "",
        "parameters": "enable_timing (bool, optional) – indicates if the event should measure time\n(default: False)",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.xpu.Event",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.xpu.Event.html#torch.xpu.Event",
        "api_signature": "torch.xpu.Event(enable_timing=False)",
        "api_description": "Wrapper around a XPU event.",
        "return_value": "A boolean indicating if all work currently captured by event has\ncompleted.\n",
        "parameters": "enable_timing (bool, optional) – indicates if the event should measure time\n(default: False)",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.distribution.Distribution.event_shape",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.distribution.Distribution.event_shape",
        "api_signature": null,
        "api_description": "Returns the shape of a single sample (without batching).",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.monitor.EventHandlerHandle",
        "api_url": "https://pytorch.org/docs/stable/monitor.html#torch.monitor.EventHandlerHandle",
        "api_signature": null,
        "api_description": "EventHandlerHandle is a wrapper type returned by\nregister_event_handler used to unregister the handler via\nunregister_event_handler. This cannot be directly initialized.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.events.api.EventMetadataValue",
        "api_url": "https://pytorch.org/docs/stable/elastic/events.html#torch.distributed.elastic.events.api.EventMetadataValue",
        "api_signature": null,
        "api_description": "alias of Optional[Union[str, int, float, bool]]",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.profiler._KinetoProfile.events",
        "api_url": "https://pytorch.org/docs/stable/profiler.html#torch.profiler._KinetoProfile.events",
        "api_signature": "events()",
        "api_description": "Returns the list of unaggregated profiler events,\nto be used in the trace callback or after the profiling is finished",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.events.api.EventSource",
        "api_url": "https://pytorch.org/docs/stable/elastic/events.html#torch.distributed.elastic.events.api.EventSource",
        "api_signature": "torch.distributed.elastic.events.api.EventSource(value)",
        "api_description": "Known identifiers of the event producers.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.autograd.forward_ad.exit_dual_level",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.autograd.forward_ad.exit_dual_level.html#torch.autograd.forward_ad.exit_dual_level",
        "api_signature": "torch.autograd.forward_ad.exit_dual_level(*, level=None)",
        "api_description": "Exit a forward grad level.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.exp",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.exp.html#torch.exp",
        "api_signature": "torch.exp(input, *, out=None)",
        "api_description": "Returns a new tensor with the exponential of the elements\nof the input tensor input.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.exp",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.exp.html#torch.Tensor.exp",
        "api_signature": "Tensor.exp()",
        "api_description": "See torch.exp()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.exp2",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.exp2.html#torch.exp2",
        "api_signature": "torch.exp2(input, *, out=None)",
        "api_description": "Alias for torch.special.exp2().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.special.exp2",
        "api_url": "https://pytorch.org/docs/stable/special.html#torch.special.exp2",
        "api_signature": "torch.special.exp2(input, *, out=None)",
        "api_description": "Computes the base two exponential function of input.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.exp_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.exp_.html#torch.Tensor.exp_",
        "api_signature": "Tensor.exp_()",
        "api_description": "In-place version of exp()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.bernoulli.Bernoulli.expand",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.bernoulli.Bernoulli.expand",
        "api_signature": "expand(batch_shape, _instance=None)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.beta.Beta.expand",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.beta.Beta.expand",
        "api_signature": "expand(batch_shape, _instance=None)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.binomial.Binomial.expand",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.binomial.Binomial.expand",
        "api_signature": "expand(batch_shape, _instance=None)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.categorical.Categorical.expand",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.categorical.Categorical.expand",
        "api_signature": "expand(batch_shape, _instance=None)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.cauchy.Cauchy.expand",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.cauchy.Cauchy.expand",
        "api_signature": "expand(batch_shape, _instance=None)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.chi2.Chi2.expand",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.chi2.Chi2.expand",
        "api_signature": "expand(batch_shape, _instance=None)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.continuous_bernoulli.ContinuousBernoulli.expand",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.continuous_bernoulli.ContinuousBernoulli.expand",
        "api_signature": "expand(batch_shape, _instance=None)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.dirichlet.Dirichlet.expand",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.dirichlet.Dirichlet.expand",
        "api_signature": "expand(batch_shape, _instance=None)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.distribution.Distribution.expand",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.distribution.Distribution.expand",
        "api_signature": "expand(batch_shape, _instance=None)",
        "api_description": "Returns a new distribution instance (or populates an existing instance\nprovided by a derived class) with batch dimensions expanded to\nbatch_shape. This method calls expand on\nthe distribution’s parameters. As such, this does not allocate new\nmemory for the expanded distribution instance. Additionally,\nthis does not repeat any args checking or parameter broadcasting in\n__init__.py, when an instance is first created.",
        "return_value": "New distribution instance with batch dimensions expanded to\nbatch_size.\n",
        "parameters": "batch_shape (torch.Size) – the desired expanded size.\n_instance – new instance provided by subclasses that\nneed to override .expand.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.exponential.Exponential.expand",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.exponential.Exponential.expand",
        "api_signature": "expand(batch_shape, _instance=None)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.fishersnedecor.FisherSnedecor.expand",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.fishersnedecor.FisherSnedecor.expand",
        "api_signature": "expand(batch_shape, _instance=None)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.gamma.Gamma.expand",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.gamma.Gamma.expand",
        "api_signature": "expand(batch_shape, _instance=None)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.geometric.Geometric.expand",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.geometric.Geometric.expand",
        "api_signature": "expand(batch_shape, _instance=None)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.gumbel.Gumbel.expand",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.gumbel.Gumbel.expand",
        "api_signature": "expand(batch_shape, _instance=None)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.half_cauchy.HalfCauchy.expand",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.half_cauchy.HalfCauchy.expand",
        "api_signature": "expand(batch_shape, _instance=None)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.half_normal.HalfNormal.expand",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.half_normal.HalfNormal.expand",
        "api_signature": "expand(batch_shape, _instance=None)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.independent.Independent.expand",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.independent.Independent.expand",
        "api_signature": "expand(batch_shape, _instance=None)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.inverse_gamma.InverseGamma.expand",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.inverse_gamma.InverseGamma.expand",
        "api_signature": "expand(batch_shape, _instance=None)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.kumaraswamy.Kumaraswamy.expand",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.kumaraswamy.Kumaraswamy.expand",
        "api_signature": "expand(batch_shape, _instance=None)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.laplace.Laplace.expand",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.laplace.Laplace.expand",
        "api_signature": "expand(batch_shape, _instance=None)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.lkj_cholesky.LKJCholesky.expand",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.lkj_cholesky.LKJCholesky.expand",
        "api_signature": "expand(batch_shape, _instance=None)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.log_normal.LogNormal.expand",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.log_normal.LogNormal.expand",
        "api_signature": "expand(batch_shape, _instance=None)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.expand",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.expand",
        "api_signature": "expand(batch_shape, _instance=None)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.mixture_same_family.MixtureSameFamily.expand",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.mixture_same_family.MixtureSameFamily.expand",
        "api_signature": "expand(batch_shape, _instance=None)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.multinomial.Multinomial.expand",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.multinomial.Multinomial.expand",
        "api_signature": "expand(batch_shape, _instance=None)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.multivariate_normal.MultivariateNormal.expand",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.multivariate_normal.MultivariateNormal.expand",
        "api_signature": "expand(batch_shape, _instance=None)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.negative_binomial.NegativeBinomial.expand",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.negative_binomial.NegativeBinomial.expand",
        "api_signature": "expand(batch_shape, _instance=None)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.normal.Normal.expand",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.normal.Normal.expand",
        "api_signature": "expand(batch_shape, _instance=None)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.one_hot_categorical.OneHotCategorical.expand",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.one_hot_categorical.OneHotCategorical.expand",
        "api_signature": "expand(batch_shape, _instance=None)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.pareto.Pareto.expand",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.pareto.Pareto.expand",
        "api_signature": "expand(batch_shape, _instance=None)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.poisson.Poisson.expand",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.poisson.Poisson.expand",
        "api_signature": "expand(batch_shape, _instance=None)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.expand",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.expand",
        "api_signature": "expand(batch_shape, _instance=None)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.relaxed_bernoulli.RelaxedBernoulli.expand",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.relaxed_bernoulli.RelaxedBernoulli.expand",
        "api_signature": "expand(batch_shape, _instance=None)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.expand",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.expand",
        "api_signature": "expand(batch_shape, _instance=None)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.studentT.StudentT.expand",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.studentT.StudentT.expand",
        "api_signature": "expand(batch_shape, _instance=None)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.transformed_distribution.TransformedDistribution.expand",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.transformed_distribution.TransformedDistribution.expand",
        "api_signature": "expand(batch_shape, _instance=None)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.uniform.Uniform.expand",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.uniform.Uniform.expand",
        "api_signature": "expand(batch_shape, _instance=None)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.von_mises.VonMises.expand",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.von_mises.VonMises.expand",
        "api_signature": "expand(batch_shape)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.weibull.Weibull.expand",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.weibull.Weibull.expand",
        "api_signature": "expand(batch_shape, _instance=None)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.wishart.Wishart.expand",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.wishart.Wishart.expand",
        "api_signature": "expand(batch_shape, _instance=None)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.expand",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.expand.html#torch.Tensor.expand",
        "api_signature": "Tensor.expand(*sizes)",
        "api_description": "Returns a new view of the self tensor with singleton dimensions expanded\nto a larger size.",
        "return_value": "",
        "parameters": "*sizes (torch.Size or int...) – the desired expanded size",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.expand_as",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.expand_as.html#torch.Tensor.expand_as",
        "api_signature": "Tensor.expand_as(other)",
        "api_description": "Expand this tensor to the same size as other.\nself.expand_as(other) is equivalent to self.expand(other.size()).",
        "return_value": "",
        "parameters": "other (torch.Tensor) – The result tensor has the same size\nas other.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.timer.expires",
        "api_url": "https://pytorch.org/docs/stable/elastic/timer.html#torch.distributed.elastic.timer.expires",
        "api_signature": "torch.distributed.elastic.timer.expires(after, scope=None, client=None)",
        "api_description": "Acquires a countdown timer that expires in after seconds from now,\nunless the code-block that it wraps is finished within the timeframe.\nWhen the timer expires, this worker is eligible to be reaped. The\nexact meaning of “reaped” depends on the client implementation. In\nmost cases, reaping means to terminate the worker process.\nNote that the worker is NOT guaranteed to be reaped at exactly\ntime.now() + after, but rather the worker is “eligible” for being\nreaped and the TimerServer that the client talks to will ultimately\nmake the decision when and how to reap the workers with expired timers.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.special.expit",
        "api_url": "https://pytorch.org/docs/stable/special.html#torch.special.expit",
        "api_signature": "torch.special.expit(input, *, out=None)",
        "api_description": "Computes the expit (also known as the logistic sigmoid function) of the elements of input.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.expm1",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.expm1.html#torch.expm1",
        "api_signature": "torch.expm1(input, *, out=None)",
        "api_description": "Alias for torch.special.expm1().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.special.expm1",
        "api_url": "https://pytorch.org/docs/stable/special.html#torch.special.expm1",
        "api_signature": "torch.special.expm1(input, *, out=None)",
        "api_description": "Computes the exponential of the elements minus 1\nof input.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.expm1",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.expm1.html#torch.Tensor.expm1",
        "api_signature": "Tensor.expm1()",
        "api_description": "See torch.expm1()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.expm1_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.expm1_.html#torch.Tensor.expm1_",
        "api_signature": "Tensor.expm1_()",
        "api_description": "In-place version of expm1()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.exponential.Exponential",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.exponential.Exponential",
        "api_signature": "torch.distributions.exponential.Exponential(rate, validate_args=None)",
        "api_description": "Bases: ExponentialFamily",
        "return_value": "",
        "parameters": "rate (float or Tensor) – rate = 1 / scale of the distribution",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.signal.windows.exponential",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.signal.windows.exponential.html#torch.signal.windows.exponential",
        "api_signature": "torch.signal.windows.exponential(M, *, center=None, tau=1.0, sym=True, dtype=None, layout=torch.strided, device=None, requires_grad=False)",
        "api_description": "Computes a window with an exponential waveform.\nAlso known as Poisson window.",
        "return_value": "",
        "parameters": "M (int) – the length of the window.\nIn other words, the number of points of the returned window.\ncenter (float, optional) – where the center of the window will be located.\nDefault: M / 2 if sym is False, else (M - 1) / 2.\ntau (float, optional) – the decay value.\nTau is generally associated with a percentage, that means, that the value should\nvary within the interval (0, 100]. If tau is 100, it is considered the uniform window.\nDefault: 1.0.\nsym (bool, optional) – If False, returns a periodic window suitable for use in spectral analysis.\nIf True, returns a symmetric window suitable for use in filter design. Default: True.\ndtype (torch.dtype, optional) – the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_dtype()).\nlayout (torch.layout, optional) – the desired layout of returned Tensor.\nDefault: torch.strided.\ndevice (torch.device, optional) – the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_device()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types.\nrequires_grad (bool, optional) – If autograd should record operations on the\nreturned tensor. Default: False.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.exponential_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.exponential_.html#torch.Tensor.exponential_",
        "api_signature": "Tensor.exponential_(lambd=1, *, generator=None)",
        "api_description": "Fills self tensor with elements drawn from the PDF (probability density function):",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.exp_family.ExponentialFamily",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.exp_family.ExponentialFamily",
        "api_signature": "torch.distributions.exp_family.ExponentialFamily(batch_shape=torch.Size([])",
        "api_description": "Bases: Distribution",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.lr_scheduler.ExponentialLR",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.ExponentialLR.html#torch.optim.lr_scheduler.ExponentialLR",
        "api_signature": "torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma, last_epoch=-1, verbose='deprecated')",
        "api_description": "Decays the learning rate of each parameter group by gamma every epoch.\nWhen last_epoch=-1, sets initial lr as lr.",
        "return_value": "",
        "parameters": "optimizer (Optimizer) – Wrapped optimizer.\ngamma (float) – Multiplicative factor of learning rate decay.\nlast_epoch (int) – The index of last epoch. Default: -1.\nverbose (bool) – If True, prints a message to stdout for\neach update. Default: False.\nDeprecated since version 2.2: verbose is deprecated. Please use get_last_lr() to access the\nlearning rate.\nstate_dict (dict) – scheduler state. Should be an object returned\nfrom a call to state_dict().",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.export.export",
        "api_url": "https://pytorch.org/docs/stable/export.html#torch.export.export",
        "api_signature": "torch.export.export(mod, args, kwargs=None, *, dynamic_shapes=None, strict=True, preserve_module_call_signature=()",
        "api_description": "export() takes an arbitrary Python callable (an nn.Module, a function or\na method) along with example inputs, and produces a traced graph representing\nonly the Tensor computation of the function in an Ahead-of-Time (AOT) fashion,\nwhich can subsequently be executed with different inputs or serialized.  The\ntraced graph (1) produces normalized operators in the functional ATen operator set\n(as well as any user-specified custom operators), (2) has eliminated all Python control\nflow and data structures (with certain exceptions), and (3) records the set of\nshape constraints needed to show that this normalization and control-flow elimination\nis sound for future inputs.",
        "return_value": "An ExportedProgram containing the traced callable.\n",
        "parameters": "mod (Module) – We will trace the forward method of this module.\nargs (Tuple[Any, ...]) – Example positional inputs.\nkwargs (Optional[Dict[str, Any]]) – Optional example keyword inputs.\ndynamic_shapes (Optional[Union[Dict[str, Any], Tuple[Any], List[Any]]]) – An optional argument where the type should either be:\n1) a dict from argument names of f to their dynamic shape specifications,\n2) a tuple that specifies dynamic shape specifications for each input in original order.\nIf you are specifying dynamism on keyword args, you will need to pass them in the order that\nis defined in the original function signature.\nThe dynamic shape of a tensor argument can be specified as either\n(1) a dict from dynamic dimension indices to Dim() types, where it is\nnot required to include static dimension indices in this dict, but when they are,\nthey should be mapped to None; or (2) a tuple / list of Dim() types or None,\nwhere the Dim() types correspond to dynamic dimensions, and static dimensions\nare denoted by None. Arguments that are dicts or tuples / lists of tensors are\nrecursively specified by using mappings or sequences of contained specifications.\nstrict (bool) – When enabled (default), the export function will trace the program through\nTorchDynamo which will ensure the soundness of the resulting graph. Otherwise, the\nexported program will not validate the implicit assumptions baked into the graph and\nmay cause behavior divergence between the original model and the exported one. This is\nuseful when users need to workaround bugs in the tracer, or simply want incrementally\nenable safety in their models. Note that this does not affect the resulting IR spec\nto be different and the model will be serialized in the same way regardless of what value\nis passed here.\nWARNING: This option is experimental and use this at your own risk.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.jit.export",
        "api_url": "https://pytorch.org/docs/stable/jit.html#torch.jit.export",
        "api_signature": "torch.jit.export(fn)",
        "api_description": "This decorator indicates that a method on an nn.Module is used as an entry point into a\nScriptModule and should be compiled.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.onnx.export",
        "api_url": "https://pytorch.org/docs/stable/onnx_torchscript.html#torch.onnx.export",
        "api_signature": "torch.onnx.export(model, args, f, export_params=True, verbose=False, training=<TrainingMode.EVAL: 0>, input_names=None, output_names=None, operator_export_type=<OperatorExportTypes.ONNX: 0>, opset_version=None, do_constant_folding=True, dynamic_axes=None, keep_initializers_as_inputs=None, custom_opsets=None, export_modules_as_functions=False, autograd_inlining=True)",
        "api_description": "Exports a model into ONNX format.",
        "return_value": "",
        "parameters": "model (torch.nn.Module, torch.jit.ScriptModule or torch.jit.ScriptFunction) – the model to be exported.\nargs (tuple or torch.Tensor) – args can be structured either as:\nONLY A TUPLE OF ARGUMENTS:\nargs = (x, y, z)\nThe tuple should contain model inputs such that model(*args) is a valid\ninvocation of the model. Any non-Tensor arguments will be hard-coded into the\nexported model; any Tensor arguments will become inputs of the exported model,\nin the order they occur in the tuple.\nA TENSOR:\nargs = torch.Tensor([1])\nThis is equivalent to a 1-ary tuple of that Tensor.\nA TUPLE OF ARGUMENTS ENDING WITH A DICTIONARY OF NAMED ARGUMENTS:\nargs = (\nx,\n{\n\"y\": input_y,\n\"z\": input_z\n}\n)\nAll but the last element of the tuple will be passed as non-keyword arguments,\nand named arguments will be set from the last element. If a named argument is\nnot present in the dictionary, it is assigned the default value, or None if a\ndefault value is not provided.\nNote\nIf a dictionary is the last element of the args tuple, it will be\ninterpreted as containing named arguments. In order to pass a dict as the\nlast non-keyword arg, provide an empty dict as the last element of the args\ntuple. For example, instead of:\ntorch.onnx.export(\nmodel,\n(\nx,\n# WRONG: will be interpreted as named arguments\n{y: z}\n),\n\"test.onnx.pb\"\n)\nWrite:\ntorch.onnx.export(\nmodel,\n(\nx,\n{y: z},\n{}\n),\n\"test.onnx.pb\"\n)\nf (Union[str, BytesIO]) – a file-like object (such that f.fileno() returns a file descriptor)\nor a string containing a file name.  A binary protocol buffer will be written\nto this file.\nexport_params (bool, default True) – if True, all parameters will\nbe exported. Set this to False if you want to export an untrained model.\nIn this case, the exported model will first take all of its parameters\nas arguments, with the ordering as specified by model.state_dict().values()\nverbose (bool, default False) – if True, prints a description of the\nmodel being exported to stdout. In addition, the final ONNX graph will include the\nfield doc_string` from the exported model which mentions the source code locations\nfor model. If True, ONNX exporter logging will be turned on.\ntraining (enum, default TrainingMode.EVAL) –\nTrainingMode.EVAL: export the model in inference mode.\nTrainingMode.PRESERVE: export the model in inference mode if model.training isFalse and in training mode if model.training is True.\nTrainingMode.TRAINING: export the model in training mode. Disables optimizationswhich might interfere with training.\ninput_names (list of str, default empty list) – names to assign to the\ninput nodes of the graph, in order.\noutput_names (list of str, default empty list) – names to assign to the\noutput nodes of the graph, in order.\noperator_export_type (enum, default OperatorExportTypes.ONNX) –\nOperatorExportTypes.ONNX: Export all ops as regular ONNX ops(in the default opset domain).\nOperatorExportTypes.ONNX_FALLTHROUGH: Try to convert all opsto standard ONNX ops in the default opset domain. If unable to do so\n(e.g. because support has not been added to convert a particular torch op to ONNX),\nfall back to exporting the op into a custom opset domain without conversion. Applies\nto custom ops\nas well as ATen ops. For the exported model to be usable, the runtime must support\nthese non-standard ops.\nWarning\nModels exported this way are probably runnable only by Caffe2.\nThis may be useful if the numeric differences in implementations of operators are\ncommon on untrained models).\nOperatorExportTypes.ONNX_ATEN_FALLBACK: Try to export each ATen op(in the TorchScript namespace “aten”) as a regular ONNX op. If we are unable to do so\n(e.g. because support has not been added to convert a particular torch op to ONNX),\nfall back to exporting an ATen op. See documentation on OperatorExportTypes.ONNX_ATEN for\ncontext.\nFor example:\ngraph(%0 : Float):\n%3 : int = prim::Constant[value=0]()\n# conversion unsupported\n%4 : Float = aten::triu(%0, %3)\n# conversion supported\n%5 : Float = aten::mul(%4, %0)\nreturn (%5)\nAssuming aten::triu is not supported in ONNX, this will be exported as:\ngraph(%0 : Float):\n%1 : Long() = onnx::Constant[value={0}]()\n# not converted\n%2 : Float = aten::ATen[operator=\"triu\"](%0, %1)\n# converted\n%3 : Float = onnx::Mul(%2, %0)\nreturn (%3)\nCaffe2-specific behavior will be enabled, including special support\nfor ops are produced by the modules described in\nQuantization.\nWarning\nModels exported this way are probably runnable only by Caffe2.\nopset_version (int, default 17) – The version of the\ndefault (ai.onnx) opset\nto target. Must be >= 7 and <= 17.\ndo_constant_folding (bool, default True) – Apply the constant-folding optimization.\nConstant-folding will replace some of the ops that have all constant inputs\nwith pre-computed constant nodes.\ndynamic_axes (dict[string, dict[int, string]] or dict[string, list(int)], default empty dict) – By default the exported model will have the shapes of all input and output tensors\nset to exactly match those given in args. To specify axes of tensors as\ndynamic (i.e. known only at run-time), set dynamic_axes to a dict with schema:\nKEY (str): an input or output name. Each name must also be provided in input_names oroutput_names.\nVALUE (dict or list): If a dict, keys are axis indices and values are axis names. If alist, each element is an axis index.\nFor example:\nclass SumModule(torch.nn.Module):\ndef forward(self, x):\nreturn torch.sum(x, dim=1)\ntorch.onnx.export(\nSumModule(),\n(torch.ones(2, 2),),\n\"onnx.pb\",\ninput_names=[\"x\"],\noutput_names=[\"sum\"]\n)\nProduces:\ninput {\nname: \"x\"\n...\nshape {\ndim {\ndim_value: 2  # axis 0\n}\ndim {\ndim_value: 2  # axis 1\n...\noutput {\nname: \"sum\"\n...\nshape {\ndim {\ndim_value: 2  # axis 0\n...\nWhile:\ntorch.onnx.export(\nSumModule(),\n(torch.ones(2, 2),),\n\"onnx.pb\",\ninput_names=[\"x\"],\noutput_names=[\"sum\"],\ndynamic_axes={\n# dict value: manually named axes\n\"x\": {0: \"my_custom_axis_name\"},\n# list value: automatic names\n\"sum\": [0],\n}\n)\nProduces:\ninput {\nname: \"x\"\n...\nshape {\ndim {\ndim_param: \"my_custom_axis_name\"  # axis 0\n}\ndim {\ndim_value: 2  # axis 1\n...\noutput {\nname: \"sum\"\n...\nshape {\ndim {\ndim_param: \"sum_dynamic_axes_1\"  # axis 0\n...\nkeep_initializers_as_inputs (bool, default None) – If True, all the\ninitializers (typically corresponding to parameters) in the\nexported graph will also be added as inputs to the graph. If False,\nthen initializers are not added as inputs to the graph, and only\nthe non-parameter inputs are added as inputs.\nThis may allow for better optimizations (e.g. constant folding) by\nbackends/runtimes.\nIf True, deduplicate_initializers pass will not be executed. This means\ninitializers with duplicated values will not be deduplicated and\nwill be treated as distinct inputs to the graph. This allows different\ninput initializers to be supplied at the runtime following export.\nIf opset_version < 9, initializers MUST be part of graph\ninputs and this argument will be ignored and the behavior will be\nequivalent to setting this argument to True.\nIf None, then the behavior is chosen automatically as follows:\nIf operator_export_type=OperatorExportTypes.ONNX, the behavior is equivalentto setting this argument to False.\nElse, the behavior is equivalent to setting this argument to True.\ncustom_opsets (dict[str, int], default empty dict) – A dict with schema:\nKEY (str): opset domain name\nVALUE (int): opset version\nIf a custom opset is referenced by model but not mentioned in this dictionary,\nthe opset version is set to 1. Only custom opset domain name and version should be\nindicated through this argument.\nexport_modules_as_functions (bool or set of type of nn.Module, default False) – Flag to enable\nexporting all nn.Module forward calls as local functions in ONNX. Or a set to indicate the\nparticular types of modules to export as local functions in ONNX.\nThis feature requires opset_version >= 15, otherwise the export will fail. This is because\nopset_version < 15 implies IR version < 8, which means no local function support.\nModule variables will be exported as function attributes. There are two categories of function\nattributes.\n1. Annotated attributes: class variables that have type annotations via\nPEP 526-style\nwill be exported as attributes.\nAnnotated attributes are not used inside the subgraph of ONNX local function because\nto determine whether or not to replace the function with a particular fused kernel.\n2. Inferred attributes: variables that are used by operators inside the module. Attribute names\nwill have prefix “inferred::”. This is to differentiate from predefined attributes retrieved from\npython module annotations. Inferred attributes are used inside the subgraph of ONNX local function.\nFalse (default): export nn.Module forward calls as fine grained nodes.\nTrue: export all nn.Module forward calls as local function nodes.\nSet of type of nn.Module: export nn.Module forward calls as local function nodes,only if the type of the nn.Module is found in the set.\nautograd_inlining (bool, default True) – Flag used to control whether to inline autograd functions.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.autograd.profiler.profile.export_chrome_trace",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.autograd.profiler.profile.export_chrome_trace.html#torch.autograd.profiler.profile.export_chrome_trace",
        "api_signature": "profile.export_chrome_trace(path)",
        "api_description": "Export an EventList as a Chrome tracing tools file.",
        "return_value": "",
        "parameters": "path (str) – Path where the trace will be written.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.profiler._KinetoProfile.export_chrome_trace",
        "api_url": "https://pytorch.org/docs/stable/profiler.html#torch.profiler._KinetoProfile.export_chrome_trace",
        "api_signature": "export_chrome_trace(path)",
        "api_description": "Exports the collected trace in Chrome JSON format.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.profiler._KinetoProfile.export_memory_timeline",
        "api_url": "https://pytorch.org/docs/stable/profiler.html#torch.profiler._KinetoProfile.export_memory_timeline",
        "api_signature": "export_memory_timeline(path, device=None)",
        "api_description": "Export memory event information from the profiler collected\ntree for a given device, and export a timeline plot. There are 3\nexportable files using export_memory_timeline, each controlled by the\npath’s suffix.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.onnx.verification.GraphInfo.export_repro",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.onnx.verification.GraphInfo.html#torch.onnx.verification.GraphInfo.export_repro",
        "api_signature": "export_repro(repro_dir=None, name=None)",
        "api_description": "Export the subgraph to ONNX along with the input/output data for repro.",
        "return_value": "The path to the exported repro directory.\n",
        "parameters": "repro_dir (Optional[str]) – The directory to export the repro files to. Defaults to current\nworking directory if None.\nname (Optional[str]) – An optional name for the test case folder: “test_{name}”.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.profiler._KinetoProfile.export_stacks",
        "api_url": "https://pytorch.org/docs/stable/profiler.html#torch.profiler._KinetoProfile.export_stacks",
        "api_signature": "export_stacks(path, metric='self_cpu_time_total')",
        "api_description": "Save stack traces in a file in a format suitable for visualization.",
        "return_value": "",
        "parameters": "path (str) – save stacks file to this location;\nmetric (str) – metric to use: “self_cpu_time_total” or “self_cuda_time_total”",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.onnx.export_to_pretty_string",
        "api_url": "https://pytorch.org/docs/stable/onnx_torchscript.html#torch.onnx.export_to_pretty_string",
        "api_signature": "torch.onnx.export_to_pretty_string(model, args, export_params=True, verbose=False, training=<TrainingMode.EVAL: 0>, input_names=None, output_names=None, operator_export_type=<OperatorExportTypes.ONNX: 0>, export_type=None, google_printer=False, opset_version=None, keep_initializers_as_inputs=None, custom_opsets=None, add_node_names=True, do_constant_folding=True, dynamic_axes=None)",
        "api_description": "Similar to export(), but returns a text representation of the ONNX\nmodel. Only differences in args listed below. All other args are the same\nas export().",
        "return_value": "A UTF-8 str containing a human-readable representation of the ONNX model.\n",
        "parameters": "add_node_names (bool, default True) – Whether or not to set\nNodeProto.name. This makes no difference unless\ngoogle_printer=True.\ngoogle_printer (bool, default False) – If False, will return a custom,\ncompact representation of the model. If True will return the\nprotobuf’s Message::DebugString(), which is more verbose.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.export.ExportBackwardSignature",
        "api_url": "https://pytorch.org/docs/stable/export.html#torch.export.ExportBackwardSignature",
        "api_signature": "torch.export.ExportBackwardSignature(gradients_to_parameters: Dict[str, str], gradients_to_user_inputs: Dict[str, str], loss_output: str)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.export.ExportedProgram",
        "api_url": "https://pytorch.org/docs/stable/export.html#torch.export.ExportedProgram",
        "api_signature": "torch.export.ExportedProgram(root, graph, graph_signature, state_dict, range_constraints, module_call_graph, example_inputs=None, verifier=None, tensor_constants=None, constants=None)",
        "api_description": "Package of a program from export(). It contains\nan torch.fx.Graph that represents Tensor computation, a state_dict containing\ntensor values of all lifted parameters and buffers, and various metadata.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.export.ExportGraphSignature",
        "api_url": "https://pytorch.org/docs/stable/export.html#torch.export.ExportGraphSignature",
        "api_signature": "torch.export.ExportGraphSignature(input_specs, output_specs)",
        "api_description": "ExportGraphSignature models the input/output signature of Export Graph,\nwhich is a fx.Graph with stronger invariants gurantees.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.export.graph_signature.ExportGraphSignature",
        "api_url": "https://pytorch.org/docs/stable/export.html#torch.export.graph_signature.ExportGraphSignature",
        "api_signature": "torch.export.graph_signature.ExportGraphSignature(input_specs, output_specs)",
        "api_description": "ExportGraphSignature models the input/output signature of Export Graph,\nwhich is a fx.Graph with stronger invariants gurantees.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.onnx.ExportOptions",
        "api_url": "https://pytorch.org/docs/stable/onnx_dynamo.html#torch.onnx.ExportOptions",
        "api_signature": "torch.onnx.ExportOptions(*, dynamic_shapes=None, op_level_debug=None, fake_context=None, onnx_registry=None, diagnostic_options=None)",
        "api_description": "Options to influence the TorchDynamo ONNX exporter.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.transforms.ExpTransform",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.transforms.ExpTransform",
        "api_signature": "torch.distributions.transforms.ExpTransform(cache_size=0)",
        "api_description": "Transform via the mapping y=exp⁡(x)y = \\exp(x)y=exp(x).",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.ModuleList.extend",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.ModuleList.html#torch.nn.ModuleList.extend",
        "api_signature": "extend(modules)",
        "api_description": "Append modules from a Python iterable to the end of the list.",
        "return_value": "",
        "parameters": "modules (iterable) – iterable of modules to append",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.ParameterList.extend",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.ParameterList.html#torch.nn.ParameterList.extend",
        "api_signature": "extend(values)",
        "api_description": "Append values from a Python iterable to the end of the list.",
        "return_value": "",
        "parameters": "values (iterable) – iterable of values to append",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.ns._numeric_suite_fx.extend_logger_results_with_comparison",
        "api_url": "https://pytorch.org/docs/stable/torch.ao.ns._numeric_suite_fx.html#torch.ao.ns._numeric_suite_fx.extend_logger_results_with_comparison",
        "api_signature": "torch.ao.ns._numeric_suite_fx.extend_logger_results_with_comparison(results, model_name_1, model_name_2, comparison_fn, comparison_name)",
        "api_description": "Compares the logged values from model_name_2 against the corresponding\nvalues in model_name_1, using comparison_fn. Records the result\nin model_name_2’s results under comparison_name. Modifies results inplace.",
        "return_value": "",
        "parameters": "results (Dict[str, Dict[str, Dict[str, List[Dict[str, Any]]]]]) – the result data structure from extract_logger_info or\nextract_shadow_logger_info.\nmodel_name_1 (str) – string name of model 1\nmodel_name_2 (str) – string name of model 2\ncomparison_fn (Callable[[Tensor, Tensor], Tensor]) – function to compare two Tensors\ncomparison_name (str) – string name of model to use for\nlayer names in the output",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.package.PackageExporter.extern",
        "api_url": "https://pytorch.org/docs/stable/package.html#torch.package.PackageExporter.extern",
        "api_signature": "extern(include, *, exclude=()",
        "api_description": "Include module in the list of external modules the package can import.\nThis will prevent dependency discovery from saving\nit in the package. The importer will load an external module directly from the standard import system.\nCode for extern modules must also exist in the process loading the package.",
        "return_value": "",
        "parameters": "include (Union[List[str], str]) – A string e.g. \"my_package.my_subpackage\", or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as\ndescribed in mock().\nexclude (Union[List[str], str]) – An optional pattern that excludes some patterns that match the\ninclude string.\nallow_empty (bool) – An optional flag that specifies whether the extern modules specified by this call\nto the extern method must be matched to some module during packaging. If an extern module glob\npattern is added with allow_empty=False, and close() is called (either explicitly or via\n__exit__) before any modules match that pattern, an exception is thrown. If allow_empty=True,\nno such exception is thrown.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda.ExternalStream",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.cuda.ExternalStream.html#torch.cuda.ExternalStream",
        "api_signature": "torch.cuda.ExternalStream(stream_ptr, device=None, **kwargs)",
        "api_description": "Wrapper around an externally allocated CUDA stream.",
        "return_value": "A boolean indicating if all kernels in this stream are completed.\nRecorded event.\n",
        "parameters": "stream_ptr (int) – Integer representation of the cudaStream_t value.\nallocated externally.\ndevice (torch.device or int, optional) – the device where the stream\nwas originally allocated. if device is specified incorrectly,\nsubsequent launches using this stream may fail.\nevent (torch.cuda.Event, optional) – event to record. If not given, a new one\nwill be allocated.\nevent (torch.cuda.Event) – an event to wait for.\nstream (Stream) – a stream to synchronize.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.package.PackageExporter.externed_modules",
        "api_url": "https://pytorch.org/docs/stable/package.html#torch.package.PackageExporter.externed_modules",
        "api_signature": "externed_modules()",
        "api_description": "Return all modules that are currently externed.",
        "return_value": "A list containing the names of modules which will be\nexterned in this package.\n",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.fake_quantize.FixedQParamsFakeQuantize.extra_repr",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.quantization.fake_quantize.FixedQParamsFakeQuantize.html#torch.ao.quantization.fake_quantize.FixedQParamsFakeQuantize.extra_repr",
        "api_signature": "extra_repr()",
        "api_description": "Define a string representation of the object’s attributes.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.jit.ScriptModule.extra_repr",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.jit.ScriptModule.html#torch.jit.ScriptModule.extra_repr",
        "api_signature": "extra_repr()",
        "api_description": "Set the extra representation of the module.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.Module.extra_repr",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.extra_repr",
        "api_signature": "extra_repr()",
        "api_description": "Set the extra representation of the module.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.ns._numeric_suite_fx.extract_logger_info",
        "api_url": "https://pytorch.org/docs/stable/torch.ao.ns._numeric_suite_fx.html#torch.ao.ns._numeric_suite_fx.extract_logger_info",
        "api_signature": "torch.ao.ns._numeric_suite_fx.extract_logger_info(model_a, model_b, logger_cls, model_name_to_use_for_layer_names)",
        "api_description": "Traverse all loggers in model_a and model_b, and extract the logged\ninformation.",
        "return_value": "NSResultsType, containing the logged comparisons\n",
        "parameters": "model_a (Module) – model A\nmodel_b (Module) – model B\nlogger_cls (Callable) – class of Logger to use\nmodel_name_to_use_for_layer_names (str) – string name of model to use for\nlayer names in the output",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.ns._numeric_suite_fx.extract_results_n_shadows_model",
        "api_url": "https://pytorch.org/docs/stable/torch.ao.ns._numeric_suite_fx.html#torch.ao.ns._numeric_suite_fx.extract_results_n_shadows_model",
        "api_signature": "torch.ao.ns._numeric_suite_fx.extract_results_n_shadows_model(model)",
        "api_description": "Extracts logger results from model.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.ns._numeric_suite_fx.extract_shadow_logger_info",
        "api_url": "https://pytorch.org/docs/stable/torch.ao.ns._numeric_suite_fx.html#torch.ao.ns._numeric_suite_fx.extract_shadow_logger_info",
        "api_signature": "torch.ao.ns._numeric_suite_fx.extract_shadow_logger_info(model_a_shadows_b, logger_cls, model_name_to_use_for_layer_names)",
        "api_description": "Traverse all loggers in a shadow model, and extract the logged\ninformation.",
        "return_value": "NSResultsType, containing the logged comparisons\n",
        "parameters": "model_a_shadows_b (Module) – shadow model\nlogger_cls (Callable) – class of Logger to use\nmodel_name_to_use_for_layer_names (str) – string name of model to use for\nlayer names in the output",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.ns._numeric_suite_fx.extract_weights",
        "api_url": "https://pytorch.org/docs/stable/torch.ao.ns._numeric_suite_fx.html#torch.ao.ns._numeric_suite_fx.extract_weights",
        "api_signature": "torch.ao.ns._numeric_suite_fx.extract_weights(model_name_a, model_a, model_name_b, model_b, base_name_to_sets_of_related_ops=None, unmatchable_types_map=None, op_to_type_to_weight_extraction_fn=None)",
        "api_description": "Extract weights from model A and model B, and return a comparison.",
        "return_value": "NSResultsType, containing the weight comparisons\n",
        "parameters": "model_name_a (str) – string name of model A to use in results\nmodel_a (Module) – model A\nmodel_name_b (str) – string name of model B to use in results\nmodel_b (Module) – model B\nbase_name_to_sets_of_related_ops (Optional[Dict[str, Set[Union[Callable, str]]]]) – optional override of subgraph base nodes, subject to change\nunmatchable_types_map (Optional[Dict[str, Set[Union[Callable, str]]]]) – optional override of unmatchable types, subject to change\nop_to_type_to_weight_extraction_fn (Optional[Dict[str, Dict[Callable, Callable]]]) – optional override of function which extracts weight\nfrom a type, subject to change",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.eye",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.eye.html#torch.eye",
        "api_signature": "torch.eye(n, m=None, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False)",
        "api_description": "Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.",
        "return_value": "A 2-D tensor with ones on the diagonal and zeros elsewhere\n",
        "parameters": "n (int) – the number of rows\nm (int, optional) – the number of columns with default being n\nout (Tensor, optional) – the output tensor.\ndtype (torch.dtype, optional) – the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_dtype()).\nlayout (torch.layout, optional) – the desired layout of returned Tensor.\nDefault: torch.strided.\ndevice (torch.device, optional) – the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_device()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types.\nrequires_grad (bool, optional) – If autograd should record operations on the\nreturned tensor. Default: False.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.init.eye_",
        "api_url": "https://pytorch.org/docs/stable/nn.init.html#torch.nn.init.eye_",
        "api_signature": "torch.nn.init.eye_(tensor)",
        "api_description": "Fill the 2-dimensional input Tensor with the identity matrix.",
        "return_value": "",
        "parameters": "tensor – a 2-dimensional torch.Tensor",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.onnx.ONNXProgram.fake_context",
        "api_url": "https://pytorch.org/docs/stable/onnx_dynamo.html#torch.onnx.ONNXProgram.fake_context",
        "api_signature": null,
        "api_description": "The fake context associated with the export.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fake_quantize_per_channel_affine",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.fake_quantize_per_channel_affine.html#torch.fake_quantize_per_channel_affine",
        "api_signature": "torch.fake_quantize_per_channel_affine(input, scale, zero_point, axis, quant_min, quant_max)",
        "api_description": "Returns a new tensor with the data in input fake quantized per channel using scale,\nzero_point, quant_min and quant_max, across the channel specified by axis.",
        "return_value": "A newly fake_quantized per channel torch.float32 tensor\n",
        "parameters": "input (Tensor) – the input value(s), in torch.float32\nscale (Tensor) – quantization scale, per channel in torch.float32\nzero_point (Tensor) – quantization zero_point, per channel in torch.int32 or torch.half or torch.float32\naxis (int32) – channel axis\nquant_min (int64) – lower bound of the quantized domain\nquant_max (int64) – upper bound of the quantized domain",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fake_quantize_per_tensor_affine",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.fake_quantize_per_tensor_affine.html#torch.fake_quantize_per_tensor_affine",
        "api_signature": "torch.fake_quantize_per_tensor_affine(input, scale, zero_point, quant_min, quant_max)",
        "api_description": "Returns a new tensor with the data in input fake quantized using scale,\nzero_point, quant_min and quant_max.",
        "return_value": "A newly fake_quantized torch.float32 tensor\n",
        "parameters": "input (Tensor) – the input value(s), torch.float32 tensor\nscale (double scalar or float32 Tensor) – quantization scale\nzero_point (int64 scalar or int32 Tensor) – quantization zero_point\nquant_min (int64) – lower bound of the quantized domain\nquant_max (int64) – upper bound of the quantized domain",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.fake_quantize.FakeQuantize",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.quantization.fake_quantize.FakeQuantize.html#torch.ao.quantization.fake_quantize.FakeQuantize",
        "api_signature": "torch.ao.quantization.fake_quantize.FakeQuantize(observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=None, quant_max=None, is_dynamic=False, **observer_kwargs)",
        "api_description": "Simulate the quantize and dequantize operations in training time.",
        "return_value": "",
        "parameters": "observer (module) – Module for observing statistics on input tensors and calculating scale\nand zero-point.\nobserver_kwargs (optional) – Arguments for the observer module",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.fake_quantize.FakeQuantizeBase",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.quantization.fake_quantize.FakeQuantizeBase.html#torch.ao.quantization.fake_quantize.FakeQuantizeBase",
        "api_signature": null,
        "api_description": "Base fake quantize module.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.library.fallthrough_kernel",
        "api_url": "https://pytorch.org/docs/stable/library.html#torch.library.fallthrough_kernel",
        "api_signature": "torch.library.fallthrough_kernel()",
        "api_description": "A dummy function to pass to Library.impl in order to register a fallthrough.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.quasirandom.SobolEngine.fast_forward",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.quasirandom.SobolEngine.html#torch.quasirandom.SobolEngine.fast_forward",
        "api_signature": "fast_forward(n)",
        "api_description": "Function to fast-forward the state of the SobolEngine by\nn steps. This is equivalent to drawing n samples\nwithout using the samples.",
        "return_value": "",
        "parameters": "n (Int) – The number of steps to fast-forward by.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.functional.feature_alpha_dropout",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.functional.feature_alpha_dropout.html#torch.nn.functional.feature_alpha_dropout",
        "api_signature": "torch.nn.functional.feature_alpha_dropout(input, p=0.5, training=False, inplace=False)",
        "api_description": "Randomly masks out entire channels (a channel is a feature map).",
        "return_value": "",
        "parameters": "p (float) – dropout probability of a channel to be zeroed. Default: 0.5\ntraining (bool) – apply dropout if is True. Default: True\ninplace (bool) – If set to True, will do this operation in-place. Default: False",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.FeatureAlphaDropout",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.FeatureAlphaDropout.html#torch.nn.FeatureAlphaDropout",
        "api_signature": "torch.nn.FeatureAlphaDropout(p=0.5, inplace=False)",
        "api_description": "Randomly masks out entire channels.",
        "return_value": "",
        "parameters": "p (float, optional) – probability of an element to be zeroed. Default: 0.5\ninplace (bool, optional) – If set to True, will do this operation\nin-place",
        "input_shape": "\nInput: (N,C,D,H,W)(N, C, D, H, W)(N,C,D,H,W) or (C,D,H,W)(C, D, H, W)(C,D,H,W).\nOutput: (N,C,D,H,W)(N, C, D, H, W)(N,C,D,H,W) or (C,D,H,W)(C, D, H, W)(C,D,H,W) (same shape as input).\n\n",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.Interpreter.fetch_args_kwargs_from_env",
        "api_url": "https://pytorch.org/docs/stable/fx.html#torch.fx.Interpreter.fetch_args_kwargs_from_env",
        "api_signature": "fetch_args_kwargs_from_env(n)",
        "api_description": "Fetch the concrete values of args and kwargs of node n\nfrom the current execution environment.",
        "return_value": "args and kwargs with concrete values for n.\n",
        "parameters": "n (Node) – The node for which args and kwargs should be fetched.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.Interpreter.fetch_attr",
        "api_url": "https://pytorch.org/docs/stable/fx.html#torch.fx.Interpreter.fetch_attr",
        "api_signature": "fetch_attr(target)",
        "api_description": "Fetch an attribute from the Module hierarchy of self.module.",
        "return_value": "The value of the attribute.\n",
        "parameters": "target (str) – The fully-qualified name of the attribute to fetch",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fft.fft",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.fft.fft.html#torch.fft.fft",
        "api_signature": "torch.fft.fft(input, n=None, dim=-1, norm=None, *, out=None)",
        "api_description": "Computes the one dimensional discrete Fourier transform of input.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor\nn (int, optional) – Signal length. If given, the input will either be zero-padded\nor trimmed to this length before computing the FFT.\ndim (int, optional) – The dimension along which to take the one dimensional FFT.\nnorm (str, optional) – Normalization mode. For the forward transform\n(fft()), these correspond to:\n\"forward\" - normalize by 1/n\n\"backward\" - no normalization\n\"ortho\" - normalize by 1/sqrt(n) (making the FFT orthonormal)\nCalling the backward transform (ifft()) with the same\nnormalization mode will apply an overall normalization of 1/n between\nthe two transforms. This is required to make ifft()\nthe exact inverse.\nDefault is \"backward\" (no normalization).\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fft.fft2",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.fft.fft2.html#torch.fft.fft2",
        "api_signature": "torch.fft.fft2(input, s=None, dim=(-2, -1)",
        "api_description": "Computes the 2 dimensional discrete Fourier transform of input.\nEquivalent to fftn() but FFTs only the last two dimensions by default.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor\ns (Tuple[int], optional) – Signal size in the transformed dimensions.\nIf given, each dimension dim[i] will either be zero-padded or\ntrimmed to the length s[i] before computing the FFT.\nIf a length -1 is specified, no padding is done in that dimension.\nDefault: s = [input.size(d) for d in dim]\ndim (Tuple[int], optional) – Dimensions to be transformed.\nDefault: last two dimensions.\nnorm (str, optional) – Normalization mode. For the forward transform\n(fft2()), these correspond to:\n\"forward\" - normalize by 1/n\n\"backward\" - no normalization\n\"ortho\" - normalize by 1/sqrt(n) (making the FFT orthonormal)\nWhere n = prod(s) is the logical FFT size.\nCalling the backward transform (ifft2()) with the same\nnormalization mode will apply an overall normalization of 1/n\nbetween the two transforms. This is required to make\nifft2() the exact inverse.\nDefault is \"backward\" (no normalization).\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fft.fftfreq",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.fft.fftfreq.html#torch.fft.fftfreq",
        "api_signature": "torch.fft.fftfreq(n, d=1.0, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False)",
        "api_description": "Computes the discrete Fourier Transform sample frequencies for a signal of size n.",
        "return_value": "",
        "parameters": "n (int) – the FFT length\nd (float, optional) – The sampling length scale.\nThe spacing between individual samples of the FFT input.\nThe default assumes unit spacing, dividing that result by the actual\nspacing gives the result in physical frequency units.\nout (Tensor, optional) – the output tensor.\ndtype (torch.dtype, optional) – the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_dtype()).\nlayout (torch.layout, optional) – the desired layout of returned Tensor.\nDefault: torch.strided.\ndevice (torch.device, optional) – the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_device()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types.\nrequires_grad (bool, optional) – If autograd should record operations on the\nreturned tensor. Default: False.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fft.fftn",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.fft.fftn.html#torch.fft.fftn",
        "api_signature": "torch.fft.fftn(input, s=None, dim=None, norm=None, *, out=None)",
        "api_description": "Computes the N dimensional discrete Fourier transform of input.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor\ns (Tuple[int], optional) – Signal size in the transformed dimensions.\nIf given, each dimension dim[i] will either be zero-padded or\ntrimmed to the length s[i] before computing the FFT.\nIf a length -1 is specified, no padding is done in that dimension.\nDefault: s = [input.size(d) for d in dim]\ndim (Tuple[int], optional) – Dimensions to be transformed.\nDefault: all dimensions, or the last len(s) dimensions if s is given.\nnorm (str, optional) – Normalization mode. For the forward transform\n(fftn()), these correspond to:\n\"forward\" - normalize by 1/n\n\"backward\" - no normalization\n\"ortho\" - normalize by 1/sqrt(n) (making the FFT orthonormal)\nWhere n = prod(s) is the logical FFT size.\nCalling the backward transform (ifftn()) with the same\nnormalization mode will apply an overall normalization of 1/n\nbetween the two transforms. This is required to make\nifftn() the exact inverse.\nDefault is \"backward\" (no normalization).\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fft.fftshift",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.fft.fftshift.html#torch.fft.fftshift",
        "api_signature": "torch.fft.fftshift(input, dim=None)",
        "api_description": "Reorders n-dimensional FFT data, as provided by fftn(), to have\nnegative frequency terms first.",
        "return_value": "",
        "parameters": "input (Tensor) – the tensor in FFT order\ndim (int, Tuple[int], optional) – The dimensions to rearrange.\nOnly dimensions specified here will be rearranged, any other dimensions\nwill be left in their original order.\nDefault: All dimensions of input.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.package.PackageImporter.file_structure",
        "api_url": "https://pytorch.org/docs/stable/package.html#torch.package.PackageImporter.file_structure",
        "api_signature": "file_structure(*, include='**', exclude=()",
        "api_description": "Returns a file structure representation of package’s zipfile.",
        "return_value": "Directory\n",
        "parameters": "include (Union[List[str], str]) – An optional string e.g. \"my_package.my_subpackage\", or optional list of strings\nfor the names of the files to be included in the zipfile representation. This can also be\na glob-style pattern, as described in PackageExporter.mock()\nexclude (Union[List[str], str]) – An optional pattern that excludes files whose name match the pattern.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.TypedStorage.filename",
        "api_url": "https://pytorch.org/docs/stable/storage.html#torch.TypedStorage.filename",
        "api_signature": null,
        "api_description": "Returns the file name associated with this storage if the storage was memory mapped from a file.\nor None if the storage was not created by memory mapping a file.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.UntypedStorage.filename",
        "api_url": "https://pytorch.org/docs/stable/storage.html#torch.UntypedStorage.filename",
        "api_signature": null,
        "api_description": "Returns the file name associated with this storage if the storage was memory mapped from a file.\nor None if the storage was not created by memory mapping a file.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.FileStore",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#torch.distributed.FileStore",
        "api_signature": null,
        "api_description": "A store implementation that uses a file to store the underlying key-value pairs.",
        "return_value": "",
        "parameters": "file_name (str) – path of the file in which to store the key-value pairs\nworld_size (int, optional) – The total number of processes using the store. Default is -1 (a negative value indicates a non-fixed number of store users).",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import torch.distributed as dist\n>>> store1 = dist.FileStore(\"/tmp/filestore\", 2)\n>>> store2 = dist.FileStore(\"/tmp/filestore\", 2)\n>>> # Use any of the store methods from either the client or server after initialization\n>>> store1.set(\"first_key\", \"first_value\")\n>>> store2.get(\"first_key\")\n\n\n"
    },
    {
        "api_name": "torch.distributed.checkpoint.filesystem.FileSystemReader",
        "api_url": "https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.filesystem.FileSystemReader",
        "api_signature": "torch.distributed.checkpoint.filesystem.FileSystemReader(path)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.checkpoint.filesystem.FileSystemWriter",
        "api_url": "https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.filesystem.FileSystemWriter",
        "api_signature": "torch.distributed.checkpoint.filesystem.FileSystemWriter(path, single_file_per_rank=True, sync_files=True, thread_count=1, per_thread_copy_ahead=10000000)",
        "api_description": "Basic implementation of StorageWriter using file IO.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.timer.FileTimerClient",
        "api_url": "https://pytorch.org/docs/stable/elastic/timer.html#torch.distributed.elastic.timer.FileTimerClient",
        "api_signature": "torch.distributed.elastic.timer.FileTimerClient(file_path, signal=Signals.SIGKILL)",
        "api_description": "Client side of FileTimerServer. This client is meant to be used\non the same host that the FileTimerServer is running on and uses\npid to uniquely identify a worker.\nThis client uses a named_pipe to send timer requests to the\nFileTimerServer. This client is a producer while the\nFileTimerServer is a consumer. Multiple clients can work with\nthe same FileTimerServer.",
        "return_value": "",
        "parameters": "file_path (str) – str, the path of a FIFO special file. FileTimerServer\nmust have created it by calling os.mkfifo().\nsignal – signal, the signal to use to kill the process. Using a\nnegative or zero signal will not kill the process.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.timer.FileTimerServer",
        "api_url": "https://pytorch.org/docs/stable/elastic/timer.html#torch.distributed.elastic.timer.FileTimerServer",
        "api_signature": "torch.distributed.elastic.timer.FileTimerServer(file_path, max_interval=10, daemon=True, log_event=None)",
        "api_description": "Server that works with FileTimerClient. Clients are expected to be\nrunning on the same host as the process that is running this server.\nEach host in the job is expected to start its own timer server locally\nand each server instance manages timers for local workers (running on\nprocesses on the same host).",
        "return_value": "",
        "parameters": "file_path (str) – str, the path of a FIFO special file to be created.\nmax_interval (float) – float, max interval in seconds for each watchdog loop.\ndaemon (bool) – bool, running the watchdog thread in daemon mode or not.\nA daemon thread will not block a process to stop.\nlog_event (Optional[Callable[[str, Optional[FileTimerRequest]], None]]) – Callable[[Dict[str, str]], None], an optional callback for\nlogging the events in JSON format.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.fill_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.fill_.html#torch.Tensor.fill_",
        "api_signature": "Tensor.fill_(value)",
        "api_description": "Fills self tensor with the specified value.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.TypedStorage.fill_",
        "api_url": "https://pytorch.org/docs/stable/storage.html#torch.TypedStorage.fill_",
        "api_signature": "fill_(value)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.UntypedStorage.fill_",
        "api_url": "https://pytorch.org/docs/stable/storage.html#torch.UntypedStorage.fill_",
        "api_signature": "fill_()",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.fill_diagonal_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.fill_diagonal_.html#torch.Tensor.fill_diagonal_",
        "api_signature": "Tensor.fill_diagonal_(fill_value, wrap=False)",
        "api_description": "Fill the main diagonal of a tensor that has at least 2-dimensions.\nWhen dims>2, all dimensions of input must be of equal length.\nThis function modifies the input tensor in-place, and returns the input tensor.",
        "return_value": "",
        "parameters": "fill_value (Scalar) – the fill value\nwrap (bool) – the diagonal ‘wrapped’ after N columns for tall matrices.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.deterministic.fill_uninitialized_memory",
        "api_url": "https://pytorch.org/docs/stable/deterministic.html#torch.utils.deterministic.fill_uninitialized_memory",
        "api_signature": null,
        "api_description": "A bool that, if True, causes uninitialized memory to be filled with\na known value when torch.use_deterministic_algorithms() is set to\nTrue. Floating point and complex values are set to NaN, and integer\nvalues are set to the maximum value.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.benchmark.FunctionCounts.filter",
        "api_url": "https://pytorch.org/docs/stable/benchmark_utils.html#torch.utils.benchmark.FunctionCounts.filter",
        "api_signature": "filter(filter_fn)",
        "api_description": "Keep only the elements where filter_fn applied to function name returns True.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.onnx.verification.find_mismatch",
        "api_url": "https://pytorch.org/docs/stable/onnx_torchscript.html#torch.onnx.verification.find_mismatch",
        "api_signature": "torch.onnx.verification.find_mismatch(model, input_args, do_constant_folding=True, training=<TrainingMode.EVAL: 0>, opset_version=None, keep_initializers_as_inputs=True, verbose=False, options=None)",
        "api_description": "Find all mismatches between the original model and the exported model.",
        "return_value": "A GraphInfo object that contains the mismatch information.\n",
        "parameters": "model (Union[Module, ScriptModule]) – The model to be exported.\ninput_args (Tuple[Any, ...]) – The input arguments to the model.\ndo_constant_folding (bool) – Same as do_constant_folding in torch.onnx.export().\ntraining (TrainingMode) – Same as training in torch.onnx.export().\nopset_version (Optional[int]) – Same as opset_version in torch.onnx.export().\nkeep_initializers_as_inputs (bool) – Same as keep_initializers_as_inputs in torch.onnx.export().\nverbose (bool) – Same as verbose in torch.onnx.export().\noptions (Optional[VerificationOptions]) – The options for the mismatch verification.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.onnx.verification.GraphInfo.find_mismatch",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.onnx.verification.GraphInfo.html#torch.onnx.verification.GraphInfo.find_mismatch",
        "api_signature": "find_mismatch(options=None)",
        "api_description": "Find all mismatches between the TorchScript IR graph and the exported onnx model.",
        "return_value": "",
        "parameters": "options (Optional[VerificationOptions]) – The verification options.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.onnx.verification.GraphInfo.find_partition",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.onnx.verification.GraphInfo.html#torch.onnx.verification.GraphInfo.find_partition",
        "api_signature": "find_partition(id)",
        "api_description": "Find the GraphInfo object with the given id.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.checkpoint.StorageWriter.finish",
        "api_url": "https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.StorageWriter.finish",
        "api_signature": "finish(metadata, results)",
        "api_description": "Write the metadata and marks the current checkpoint as successful.",
        "return_value": "None\n",
        "parameters": "metadata (Metadata) – metadata for the new checkpoint\nresults (List[List[WriteResult]]) – A list of WriteResults from all ranks.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.checkpoint.LoadPlanner.finish_plan",
        "api_url": "https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.LoadPlanner.finish_plan",
        "api_signature": "finish_plan(central_plan)",
        "api_description": "Accept the plan from coordinator and return final LoadPlan.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.checkpoint.SavePlanner.finish_plan",
        "api_url": "https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.SavePlanner.finish_plan",
        "api_signature": "finish_plan(new_plan)",
        "api_description": "Merge the plan created by create_local_plan and the result of create_global_plan.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.fishersnedecor.FisherSnedecor",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.fishersnedecor.FisherSnedecor",
        "api_signature": "torch.distributions.fishersnedecor.FisherSnedecor(df1, df2, validate_args=None)",
        "api_description": "Bases: Distribution",
        "return_value": "",
        "parameters": "df1 (float or Tensor) – degrees of freedom parameter 1\ndf2 (float or Tensor) – degrees of freedom parameter 2",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fix",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.fix.html#torch.fix",
        "api_signature": "torch.fix(input, *, out=None)",
        "api_description": "Alias for torch.trunc()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.fix",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.fix.html#torch.Tensor.fix",
        "api_signature": "Tensor.fix()",
        "api_description": "See torch.fix().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.fix_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.fix_.html#torch.Tensor.fix_",
        "api_signature": "Tensor.fix_()",
        "api_description": "In-place version of fix()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.fake_quantize.FixedQParamsFakeQuantize",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.quantization.fake_quantize.FixedQParamsFakeQuantize.html#torch.ao.quantization.fake_quantize.FixedQParamsFakeQuantize",
        "api_signature": "torch.ao.quantization.fake_quantize.FixedQParamsFakeQuantize(observer)",
        "api_description": "Simulate quantize and dequantize in training time.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.backends.nnpack.flags",
        "api_url": "https://pytorch.org/docs/stable/backends.html#torch.backends.nnpack.flags",
        "api_signature": "torch.backends.nnpack.flags(enabled=False)",
        "api_description": "Context manager for setting if nnpack is enabled globally",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.backends.cuda.flash_sdp_enabled",
        "api_url": "https://pytorch.org/docs/stable/backends.html#torch.backends.cuda.flash_sdp_enabled",
        "api_signature": "torch.backends.cuda.flash_sdp_enabled()",
        "api_description": "Warning",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.export.unflatten.FlatArgsAdapter",
        "api_url": "https://pytorch.org/docs/stable/export.html#torch.export.unflatten.FlatArgsAdapter",
        "api_signature": null,
        "api_description": "Adapts input arguments with input_spec to align target_spec.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.Flatten",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.Flatten.html#torch.nn.Flatten",
        "api_signature": "torch.nn.Flatten(start_dim=1, end_dim=-1)",
        "api_description": "Flattens a contiguous range of dims into a tensor.",
        "return_value": "",
        "parameters": "start_dim (int) – first dim to flatten (default = 1).\nend_dim (int) – last dim to flatten (default = -1).",
        "input_shape": "\nInput: (∗,Sstart,...,Si,...,Send,∗)(*, S_{\\text{start}},..., S_{i}, ..., S_{\\text{end}}, *)(∗,Sstart​,...,Si​,...,Send​,∗),’\nwhere SiS_{i}Si​ is the size at dimension iii and ∗*∗ means any\nnumber of dimensions including none.\nOutput: (∗,∏i=startendSi,∗)(*, \\prod_{i=\\text{start}}^{\\text{end}} S_{i}, *)(∗,∏i=startend​Si​,∗).\n\n",
        "notes": "",
        "code_example": ">>> input = torch.randn(32, 1, 5, 5)\n>>> # With default parameters\n>>> m = nn.Flatten()\n>>> output = m(input)\n>>> output.size()\ntorch.Size([32, 25])\n>>> # With non-default parameters\n>>> m = nn.Flatten(0, 2)\n>>> output = m(input)\n>>> output.size()\ntorch.Size([160, 5])\n\n\n"
    },
    {
        "api_name": "torch.flatten",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.flatten.html#torch.flatten",
        "api_signature": "torch.flatten(input, start_dim=0, end_dim=-1)",
        "api_description": "Flattens input by reshaping it into a one-dimensional tensor. If start_dim or end_dim\nare passed, only dimensions starting with start_dim and ending with end_dim are flattened.\nThe order of elements in input is unchanged.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.\nstart_dim (int) – the first dim to flatten\nend_dim (int) – the last dim to flatten",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.flatten",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.flatten.html#torch.Tensor.flatten",
        "api_signature": "Tensor.flatten(start_dim=0, end_dim=-1)",
        "api_description": "See torch.flatten()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.RNNBase.flatten_parameters",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.RNNBase.html#torch.nn.RNNBase.flatten_parameters",
        "api_signature": "flatten_parameters()",
        "api_description": "Reset parameter data pointer so that they can use faster code paths.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.fsdp.FullyShardedDataParallel.flatten_sharded_optim_state_dict",
        "api_url": "https://pytorch.org/docs/stable/fsdp.html#torch.distributed.fsdp.FullyShardedDataParallel.flatten_sharded_optim_state_dict",
        "api_signature": "flatten_sharded_optim_state_dict(sharded_optim_state_dict, model, optim)",
        "api_description": "Flatten a sharded optimizer state-dict.",
        "return_value": "Refer to shard_full_optim_state_dict().\n",
        "parameters": "sharded_optim_state_dict (Dict[str, Any]) – Optimizer state dict\ncorresponding to the unflattened parameters and holding the\nsharded optimizer state.\nmodel (torch.nn.Module) – Refer to shard_full_optim_state_dict().\noptim (torch.optim.Optimizer) – Optimizer for model ‘s\nparameters.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.flip",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.flip.html#torch.flip",
        "api_signature": "torch.flip(input, dims)",
        "api_description": "Reverse the order of an n-D tensor along given axis in dims.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.\ndims (a list or tuple) – axis to flip on",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.flip",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.flip.html#torch.Tensor.flip",
        "api_signature": "Tensor.flip(dims)",
        "api_description": "See torch.flip()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fliplr",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.fliplr.html#torch.fliplr",
        "api_signature": "torch.fliplr(input)",
        "api_description": "Flip tensor in the left/right direction, returning a new tensor.",
        "return_value": "",
        "parameters": "input (Tensor) – Must be at least 2-dimensional.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.fliplr",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.fliplr.html#torch.Tensor.fliplr",
        "api_signature": "Tensor.fliplr()",
        "api_description": "See torch.fliplr()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.flipud",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.flipud.html#torch.flipud",
        "api_signature": "torch.flipud(input)",
        "api_description": "Flip tensor in the up/down direction, returning a new tensor.",
        "return_value": "",
        "parameters": "input (Tensor) – Must be at least 1-dimensional.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.flipud",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.flipud.html#torch.Tensor.flipud",
        "api_signature": "Tensor.flipud()",
        "api_description": "See torch.flipud()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.jit.ScriptModule.float",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.jit.ScriptModule.html#torch.jit.ScriptModule.float",
        "api_signature": "float()",
        "api_description": "Casts all floating point parameters and buffers to float datatype.",
        "return_value": "self\n",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.Module.float",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.float",
        "api_signature": "float()",
        "api_description": "Casts all floating point parameters and buffers to float datatype.",
        "return_value": "self\n",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.float",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.float.html#torch.Tensor.float",
        "api_signature": "Tensor.float(memory_format=torch.preserve_format)",
        "api_description": "self.float() is equivalent to self.to(torch.float32). See to().",
        "return_value": "",
        "parameters": "memory_format (torch.memory_format, optional) – the desired memory format of\nreturned Tensor. Default: torch.preserve_format.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.TypedStorage.float",
        "api_url": "https://pytorch.org/docs/stable/storage.html#torch.TypedStorage.float",
        "api_signature": "float()",
        "api_description": "Casts this storage to float type.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.UntypedStorage.float",
        "api_url": "https://pytorch.org/docs/stable/storage.html#torch.UntypedStorage.float",
        "api_signature": "float()",
        "api_description": "Casts this storage to float type.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.qconfig.float16_dynamic_qconfig",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.quantization.qconfig.float16_dynamic_qconfig.html#torch.ao.quantization.qconfig.float16_dynamic_qconfig",
        "api_signature": null,
        "api_description": "Dynamic qconfig with weights quantized to torch.float16.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.qconfig.float16_static_qconfig",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.quantization.qconfig.float16_static_qconfig.html#torch.ao.quantization.qconfig.float16_static_qconfig",
        "api_signature": null,
        "api_description": "Dynamic qconfig with both activations and weights quantized to torch.float16.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.TypedStorage.float8_e4m3fn",
        "api_url": "https://pytorch.org/docs/stable/storage.html#torch.TypedStorage.float8_e4m3fn",
        "api_signature": "float8_e4m3fn()",
        "api_description": "Casts this storage to float8_e4m3fn type",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.UntypedStorage.float8_e4m3fn",
        "api_url": "https://pytorch.org/docs/stable/storage.html#torch.UntypedStorage.float8_e4m3fn",
        "api_signature": "float8_e4m3fn()",
        "api_description": "Casts this storage to float8_e4m3fn type",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.TypedStorage.float8_e4m3fnuz",
        "api_url": "https://pytorch.org/docs/stable/storage.html#torch.TypedStorage.float8_e4m3fnuz",
        "api_signature": "float8_e4m3fnuz()",
        "api_description": "Casts this storage to float8_e4m3fnuz type",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.UntypedStorage.float8_e4m3fnuz",
        "api_url": "https://pytorch.org/docs/stable/storage.html#torch.UntypedStorage.float8_e4m3fnuz",
        "api_signature": "float8_e4m3fnuz()",
        "api_description": "Casts this storage to float8_e4m3fnuz type",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.TypedStorage.float8_e5m2",
        "api_url": "https://pytorch.org/docs/stable/storage.html#torch.TypedStorage.float8_e5m2",
        "api_signature": "float8_e5m2()",
        "api_description": "Casts this storage to float8_e5m2 type",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.UntypedStorage.float8_e5m2",
        "api_url": "https://pytorch.org/docs/stable/storage.html#torch.UntypedStorage.float8_e5m2",
        "api_signature": "float8_e5m2()",
        "api_description": "Casts this storage to float8_e5m2 type",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.TypedStorage.float8_e5m2fnuz",
        "api_url": "https://pytorch.org/docs/stable/storage.html#torch.TypedStorage.float8_e5m2fnuz",
        "api_signature": "float8_e5m2fnuz()",
        "api_description": "Casts this storage to float8_e5m2fnuz type",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.UntypedStorage.float8_e5m2fnuz",
        "api_url": "https://pytorch.org/docs/stable/storage.html#torch.UntypedStorage.float8_e5m2fnuz",
        "api_signature": "float8_e5m2fnuz()",
        "api_description": "Casts this storage to float8_e5m2fnuz type",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.float_power",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.float_power.html#torch.float_power",
        "api_signature": "torch.float_power(input, exponent, *, out=None)",
        "api_description": "Raises input to the power of exponent, elementwise, in double precision.\nIf neither input is complex returns a torch.float64 tensor,\nand if one or more inputs is complex returns a torch.complex128 tensor.",
        "return_value": "",
        "parameters": "input (Tensor or Number) – the base value(s)\nexponent (Tensor or Number) – the exponent value(s)\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.float_power",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.float_power.html#torch.Tensor.float_power",
        "api_signature": "Tensor.float_power(exponent)",
        "api_description": "See torch.float_power()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.float_power_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.float_power_.html#torch.Tensor.float_power_",
        "api_signature": "Tensor.float_power_(exponent)",
        "api_description": "In-place version of float_power()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.qconfig.float_qparams_weight_only_qconfig",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.quantization.qconfig.float_qparams_weight_only_qconfig.html#torch.ao.quantization.qconfig.float_qparams_weight_only_qconfig",
        "api_signature": null,
        "api_description": "Dynamic qconfig with weights quantized with a floating point zero_point.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.quantized.FloatFunctional",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.nn.quantized.FloatFunctional.html#torch.ao.nn.quantized.FloatFunctional",
        "api_signature": null,
        "api_description": "State collector class for float operations.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.FloatStorage",
        "api_url": "https://pytorch.org/docs/stable/storage.html#torch.FloatStorage",
        "api_signature": "torch.FloatStorage(*args, wrap_storage=None, dtype=None, device=None, _internal=False)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.floor",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.floor.html#torch.floor",
        "api_signature": "torch.floor(input, *, out=None)",
        "api_description": "Returns a new tensor with the floor of the elements of input,\nthe largest integer less than or equal to each element.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.floor",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.floor.html#torch.Tensor.floor",
        "api_signature": "Tensor.floor()",
        "api_description": "See torch.floor()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.floor_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.floor_.html#torch.Tensor.floor_",
        "api_signature": "Tensor.floor_()",
        "api_description": "In-place version of floor()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.floor_divide",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.floor_divide.html#torch.floor_divide",
        "api_signature": "torch.floor_divide(input, other, *, out=None)",
        "api_description": "Note",
        "return_value": "",
        "parameters": "input (Tensor or Number) – the dividend\nother (Tensor or Number) – the divisor\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.floor_divide",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.floor_divide.html#torch.Tensor.floor_divide",
        "api_signature": "Tensor.floor_divide(value)",
        "api_description": "See torch.floor_divide()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.floor_divide_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.floor_divide_.html#torch.Tensor.floor_divide_",
        "api_signature": "Tensor.floor_divide_(value)",
        "api_description": "In-place version of floor_divide()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.tensorboard.writer.SummaryWriter.flush",
        "api_url": "https://pytorch.org/docs/stable/tensorboard.html#torch.utils.tensorboard.writer.SummaryWriter.flush",
        "api_signature": "flush()",
        "api_description": "Flushes the event file to disk.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fmax",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.fmax.html#torch.fmax",
        "api_signature": "torch.fmax(input, other, *, out=None)",
        "api_description": "Computes the element-wise maximum of input and other.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.\nother (Tensor) – the second input tensor\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.fmax",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.fmax.html#torch.Tensor.fmax",
        "api_signature": "Tensor.fmax(other)",
        "api_description": "See torch.fmax()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fmin",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.fmin.html#torch.fmin",
        "api_signature": "torch.fmin(input, other, *, out=None)",
        "api_description": "Computes the element-wise minimum of input and other.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.\nother (Tensor) – the second input tensor\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.fmin",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.fmin.html#torch.Tensor.fmin",
        "api_signature": "Tensor.fmin(other)",
        "api_description": "See torch.fmin()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fmod",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.fmod.html#torch.fmod",
        "api_signature": "torch.fmod(input, other, *, out=None)",
        "api_description": "Applies C++’s std::fmod entrywise.\nThe result has the same sign as the dividend input and its absolute value\nis less than that of other.",
        "return_value": "",
        "parameters": "input (Tensor) – the dividend\nother (Tensor or Scalar) – the divisor\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.fmod",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.fmod.html#torch.Tensor.fmod",
        "api_signature": "Tensor.fmod(divisor)",
        "api_description": "See torch.fmod()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.fmod_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.fmod_.html#torch.Tensor.fmod_",
        "api_signature": "Tensor.fmod_(divisor)",
        "api_description": "In-place version of fmod()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.Fold",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.Fold.html#torch.nn.Fold",
        "api_signature": "torch.nn.Fold(output_size, kernel_size, dilation=1, padding=0, stride=1)",
        "api_description": "Combines an array of sliding local blocks into a large containing tensor.",
        "return_value": "",
        "parameters": "output_size (int or tuple) – the shape of the spatial dimensions of the\noutput (i.e., output.sizes()[2:])\nkernel_size (int or tuple) – the size of the sliding blocks\ndilation (int or tuple, optional) – a parameter that controls the\nstride of elements within the\nneighborhood. Default: 1\npadding (int or tuple, optional) – implicit zero padding to be added on\nboth sides of input. Default: 0\nstride (int or tuple) – the stride of the sliding blocks in the input\nspatial dimensions. Default: 1",
        "input_shape": "\nInput: (N,C×∏(kernel_size),L)(N, C \\times \\prod(\\text{kernel\\_size}), L)(N,C×∏(kernel_size),L) or (C×∏(kernel_size),L)(C \\times \\prod(\\text{kernel\\_size}), L)(C×∏(kernel_size),L)\nOutput: (N,C,output_size[0],output_size[1],… )(N, C, \\text{output\\_size}[0], \\text{output\\_size}[1], \\dots)(N,C,output_size[0],output_size[1],…)\nor (C,output_size[0],output_size[1],… )(C, \\text{output\\_size}[0], \\text{output\\_size}[1], \\dots)(C,output_size[0],output_size[1],…) as described above\n\n",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.functional.fold",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.functional.fold.html#torch.nn.functional.fold",
        "api_signature": "torch.nn.functional.fold(input, output_size, kernel_size, dilation=1, padding=0, stride=1)",
        "api_description": "Combine an array of sliding local blocks into a large containing tensor.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.symbolic_shapes.DimConstraints.forced_specializations",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.fx.experimental.symbolic_shapes.DimConstraints.html#torch.fx.experimental.symbolic_shapes.DimConstraints.forced_specializations",
        "api_signature": "forced_specializations()",
        "api_description": "Returns a dictionary of the names of symbols to their specialized value",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.jit.fork",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.jit.fork.html#torch.jit.fork",
        "api_signature": "torch.jit.fork(func, *args, **kwargs)",
        "api_description": "Create an asynchronous task executing func and a reference to the value of the result of this execution.",
        "return_value": "a reference to the execution of func. The value T\ncan only be accessed by forcing completion of func through torch.jit.wait.\n",
        "parameters": "func (callable or torch.nn.Module) – A Python function or torch.nn.Module\nthat will be invoked. If executed in TorchScript, it will execute asynchronously,\notherwise it will not. Traced invocations of fork will be captured in the IR.\n*args – arguments to invoke func with.\n**kwargs – arguments to invoke func with.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.random.fork_rng",
        "api_url": "https://pytorch.org/docs/stable/random.html#torch.random.fork_rng",
        "api_signature": "torch.random.fork_rng(devices=None, enabled=True, _caller='fork_rng', _devices_kw='devices', device_type='cuda')",
        "api_description": "Forks the RNG, so that when you return, the RNG is reset\nto the state that it was previously in.",
        "return_value": "",
        "parameters": "devices (iterable of Device IDs) – devices for which to fork\nthe RNG. CPU RNG state is always forked. By default, fork_rng() operates\non all devices, but will emit a warning if your machine has a lot\nof devices, since this function will run very slowly in that case.\nIf you explicitly specify devices, this warning will be suppressed\nenabled (bool) – if False, the RNG is not forked.  This is a convenience\nargument for easily disabling the context manager without having\nto delete it and unindent your Python code under it.\ndevice_type (str) – device type str, default is cuda. As for custom device,\nsee details in [Note: support the custom device with privateuse1]",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.symbolic_shapes.ShapeEnv.format_guards",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.fx.experimental.symbolic_shapes.ShapeEnv.html#torch.fx.experimental.symbolic_shapes.ShapeEnv.format_guards",
        "api_signature": "format_guards(verbose=False)",
        "api_description": "Format this shape env’s guard expressions with optional traceback info if verbose",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.Node.format_node",
        "api_url": "https://pytorch.org/docs/stable/fx.html#torch.fx.Node.format_node",
        "api_signature": "format_node(placeholder_names=None, maybe_return_typename=None)",
        "api_description": "Return a descriptive string representation of self.",
        "return_value": "\nIf 1) we’re using format_node as an internal helperin the __str__ method of Graph, and 2) self\nis a placeholder Node, return None. Otherwise,\nreturn a  descriptive string representation of the\ncurrent Node.\n\n\n\n",
        "parameters": "placeholder_names (Optional[List[str]]) – A list that will store formatted strings\nrepresenting the placeholders in the generated\nforward function. Internal use only.\nmaybe_return_typename (Optional[List[str]]) – A single-element list that will store\na formatted string representing the output of the\ngenerated forward function. Internal use only.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.quantizable.MultiheadAttention.forward",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.nn.quantizable.MultiheadAttention.html#torch.ao.nn.quantizable.MultiheadAttention.forward",
        "api_signature": "forward(query, key, value, key_padding_mask=None, need_weights=True, attn_mask=None, average_attn_weights=True, is_causal=False)",
        "api_description": "Please, refer to forward() for more\ninformation",
        "return_value": "",
        "parameters": "query (Tensor) – map a query and a set of key-value pairs to an output.\nSee “Attention Is All You Need” for more details.\nkey (Tensor) – map a query and a set of key-value pairs to an output.\nSee “Attention Is All You Need” for more details.\nvalue (Tensor) – map a query and a set of key-value pairs to an output.\nSee “Attention Is All You Need” for more details.\nkey_padding_mask (Optional[Tensor]) – if provided, specified padding elements in the key will\nbe ignored by the attention. When given a binary mask and a value is True,\nthe corresponding value on the attention layer will be ignored.\nneed_weights (bool) – output attn_output_weights.\nattn_mask (Optional[Tensor]) – 2D or 3D mask that prevents attention to certain positions. A 2D mask will be broadcasted for all\nthe batches while a 3D mask allows to specify a different mask for the entries of each batch.",
        "input_shape": "\nInputs:\nquery: (L,N,E)(L, N, E)(L,N,E) where L is the target sequence length, N is the batch size, E is\nthe embedding dimension. (N,L,E)(N, L, E)(N,L,E) if batch_first is True.\nkey: (S,N,E)(S, N, E)(S,N,E), where S is the source sequence length, N is the batch size, E is\nthe embedding dimension. (N,S,E)(N, S, E)(N,S,E) if batch_first is True.\nvalue: (S,N,E)(S, N, E)(S,N,E) where S is the source sequence length, N is the batch size, E is\nthe embedding dimension. (N,S,E)(N, S, E)(N,S,E) if batch_first is True.\nkey_padding_mask: (N,S)(N, S)(N,S) where N is the batch size, S is the source sequence length.\nIf a BoolTensor is provided, the positions with the\nvalue of True will be ignored while the position with the value of False will be unchanged.\nattn_mask: 2D mask (L,S)(L, S)(L,S) where L is the target sequence length, S is the source sequence length.\n3D mask (N∗numheads,L,S)(N*num_heads, L, S)(N∗numh​eads,L,S) where N is the batch size, L is the target sequence length,\nS is the source sequence length. attn_mask ensure that position i is allowed to attend the unmasked\npositions. If a BoolTensor is provided, positions with True\nis not allowed to attend while False values will be unchanged. If a FloatTensor\nis provided, it will be added to the attention weight.\nis_causal: If specified, applies a causal mask as attention mask. Mutually exclusive with providing attn_mask.\nDefault: False.\naverage_attn_weights: If true, indicates that the returned attn_weights should be averaged across\nheads. Otherwise, attn_weights are provided separately per head. Note that this flag only has an\neffect when need_weights=True.. Default: True (i.e. average weights across heads)\nOutputs:\nattn_output: (L,N,E)(L, N, E)(L,N,E) where L is the target sequence length, N is the batch size,\nE is the embedding dimension. (N,L,E)(N, L, E)(N,L,E) if batch_first is True.\nattn_output_weights: If average_attn_weights=True, returns attention weights averaged\nacross heads of shape (N,L,S)(N, L, S)(N,L,S), where N is the batch size, L is the target sequence length,\nS is the source sequence length. If average_attn_weights=False, returns attention weights per\nhead of shape (N,numheads,L,S)(N, num_heads, L, S)(N,numh​eads,L,S).\n\n",
        "notes": "Please, refer to forward() for more\ninformation\n",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.ns._numeric_suite.Logger.forward",
        "api_url": "https://pytorch.org/docs/stable/torch.ao.ns._numeric_suite.html#torch.ao.ns._numeric_suite.Logger.forward",
        "api_signature": "forward(x)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.ns._numeric_suite.OutputLogger.forward",
        "api_url": "https://pytorch.org/docs/stable/torch.ao.ns._numeric_suite.html#torch.ao.ns._numeric_suite.OutputLogger.forward",
        "api_signature": "forward(x)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.ns._numeric_suite.Shadow.forward",
        "api_url": "https://pytorch.org/docs/stable/torch.ao.ns._numeric_suite.html#torch.ao.ns._numeric_suite.Shadow.forward",
        "api_signature": "forward(*x)",
        "api_description": "Tensor",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.ns._numeric_suite.ShadowLogger.forward",
        "api_url": "https://pytorch.org/docs/stable/torch.ao.ns._numeric_suite.html#torch.ao.ns._numeric_suite.ShadowLogger.forward",
        "api_signature": "forward(x, y)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.ns._numeric_suite_fx.OutputComparisonLogger.forward",
        "api_url": "https://pytorch.org/docs/stable/torch.ao.ns._numeric_suite_fx.html#torch.ao.ns._numeric_suite_fx.OutputComparisonLogger.forward",
        "api_signature": "forward(x, x_ref)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.ns._numeric_suite_fx.OutputLogger.forward",
        "api_url": "https://pytorch.org/docs/stable/torch.ao.ns._numeric_suite_fx.html#torch.ao.ns._numeric_suite_fx.OutputLogger.forward",
        "api_signature": "forward(x)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.observer.MinMaxObserver.forward",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.quantization.observer.MinMaxObserver.html#torch.ao.quantization.observer.MinMaxObserver.forward",
        "api_signature": "forward(x_orig)",
        "api_description": "Records the running minimum and maximum of x.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.autograd.Function.forward",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.autograd.Function.forward.html#torch.autograd.Function.forward",
        "api_signature": "Function.forward(ctx, *args, **kwargs)",
        "api_description": "Define the forward of the custom autograd Function.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.autograd.function.InplaceFunction.forward",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.autograd.function.InplaceFunction.html#torch.autograd.function.InplaceFunction.forward",
        "api_signature": "forward(ctx, *args, **kwargs)",
        "api_description": "Define the forward of the custom autograd Function.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.autograd.function.NestedIOFunction.forward",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.autograd.function.NestedIOFunction.html#torch.autograd.function.NestedIOFunction.forward",
        "api_signature": "forward(*args)",
        "api_description": "Shared forward utility.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.fsdp.FullyShardedDataParallel.forward",
        "api_url": "https://pytorch.org/docs/stable/fsdp.html#torch.distributed.fsdp.FullyShardedDataParallel.forward",
        "api_signature": "forward(*args, **kwargs)",
        "api_description": "Run the forward pass for the wrapped module, inserting FSDP-specific pre- and post-forward sharding logic.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.pipeline.sync.Pipe.forward",
        "api_url": "https://pytorch.org/docs/stable/pipeline.html#torch.distributed.pipeline.sync.Pipe.forward",
        "api_signature": "forward(*inputs)",
        "api_description": "Processes a single input mini-batch through the pipe and returns an\nRRef pointing to the output.\nPipe is a fairly transparent module wrapper. It doesn’t\nmodify the input and output signature of the underlying module. But\nthere’s type restriction. Input and output have to contain at least one\ntensor. This restriction is applied at partition boundaries too.",
        "return_value": "RRef to the output of the mini-batch\n",
        "parameters": "inputs – input mini-batch",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.EmbeddingBag.forward",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.EmbeddingBag.html#torch.nn.EmbeddingBag.forward",
        "api_signature": "forward(input, offsets=None, per_sample_weights=None)",
        "api_description": "Forward pass of EmbeddingBag.",
        "return_value": "Tensor output shape of (B, embedding_dim).\n",
        "parameters": "input (Tensor) – Tensor containing bags of indices into the embedding matrix.\noffsets (Tensor, optional) – Only used when input is 1D. offsets determines\nthe starting index position of each bag (sequence) in input.\nper_sample_weights (Tensor, optional) – a tensor of float / double weights, or None\nto indicate all weights should be taken to be 1. If specified, per_sample_weights\nmust have exactly the same shape as input and is treated as having the same\noffsets, if those are not None. Only supported for mode='sum'.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.Module.forward",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.forward",
        "api_signature": "forward(*input)",
        "api_description": "Define the computation performed at every call.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.MultiheadAttention.forward",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html#torch.nn.MultiheadAttention.forward",
        "api_signature": "forward(query, key, value, key_padding_mask=None, need_weights=True, attn_mask=None, average_attn_weights=True, is_causal=False)",
        "api_description": "Compute attention outputs using query, key, and value embeddings.",
        "return_value": "",
        "parameters": "query (Tensor) – Query embeddings of shape (L,Eq)(L, E_q)(L,Eq​) for unbatched input, (L,N,Eq)(L, N, E_q)(L,N,Eq​) when batch_first=False\nor (N,L,Eq)(N, L, E_q)(N,L,Eq​) when batch_first=True, where LLL is the target sequence length,\nNNN is the batch size, and EqE_qEq​ is the query embedding dimension embed_dim.\nQueries are compared against key-value pairs to produce the output.\nSee “Attention Is All You Need” for more details.\nkey (Tensor) – Key embeddings of shape (S,Ek)(S, E_k)(S,Ek​) for unbatched input, (S,N,Ek)(S, N, E_k)(S,N,Ek​) when batch_first=False\nor (N,S,Ek)(N, S, E_k)(N,S,Ek​) when batch_first=True, where SSS is the source sequence length,\nNNN is the batch size, and EkE_kEk​ is the key embedding dimension kdim.\nSee “Attention Is All You Need” for more details.\nvalue (Tensor) – Value embeddings of shape (S,Ev)(S, E_v)(S,Ev​) for unbatched input, (S,N,Ev)(S, N, E_v)(S,N,Ev​) when\nbatch_first=False or (N,S,Ev)(N, S, E_v)(N,S,Ev​) when batch_first=True, where SSS is the source\nsequence length, NNN is the batch size, and EvE_vEv​ is the value embedding dimension vdim.\nSee “Attention Is All You Need” for more details.\nkey_padding_mask (Optional[Tensor]) – If specified, a mask of shape (N,S)(N, S)(N,S) indicating which elements within key\nto ignore for the purpose of attention (i.e. treat as “padding”). For unbatched query, shape should be (S)(S)(S).\nBinary and float masks are supported.\nFor a binary mask, a True value indicates that the corresponding key value will be ignored for\nthe purpose of attention. For a float mask, it will be directly added to the corresponding key value.\nneed_weights (bool) – If specified, returns attn_output_weights in addition to attn_outputs.\nSet need_weights=False to use the optimized scaled_dot_product_attention\nand achieve the best performance for MHA.\nDefault: True.\nattn_mask (Optional[Tensor]) – If specified, a 2D or 3D mask preventing attention to certain positions. Must be of shape\n(L,S)(L, S)(L,S) or (N⋅num_heads,L,S)(N\\cdot\\text{num\\_heads}, L, S)(N⋅num_heads,L,S), where NNN is the batch size,\nLLL is the target sequence length, and SSS is the source sequence length. A 2D mask will be\nbroadcasted across the batch while a 3D mask allows for a different mask for each entry in the batch.\nBinary and float masks are supported. For a binary mask, a True value indicates that the\ncorresponding position is not allowed to attend. For a float mask, the mask values will be added to\nthe attention weight.\nIf both attn_mask and key_padding_mask are supplied, their types should match.\naverage_attn_weights (bool) – If true, indicates that the returned attn_weights should be averaged across\nheads. Otherwise, attn_weights are provided separately per head. Note that this flag only has an\neffect when need_weights=True. Default: True (i.e. average weights across heads)\nis_causal (bool) – If specified, applies a causal mask as attention mask.\nDefault: False.\nWarning:\nis_causal provides a hint that attn_mask is the\ncausal mask. Providing incorrect hints can result in\nincorrect execution, including forward and backward\ncompatibility.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.Transformer.forward",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html#torch.nn.Transformer.forward",
        "api_signature": "forward(src, tgt, src_mask=None, tgt_mask=None, memory_mask=None, src_key_padding_mask=None, tgt_key_padding_mask=None, memory_key_padding_mask=None, src_is_causal=None, tgt_is_causal=None, memory_is_causal=False)",
        "api_description": "Take in and process masked source/target sequences.",
        "return_value": "",
        "parameters": "src (Tensor) – the sequence to the encoder (required).\ntgt (Tensor) – the sequence to the decoder (required).\nsrc_mask (Optional[Tensor]) – the additive mask for the src sequence (optional).\ntgt_mask (Optional[Tensor]) – the additive mask for the tgt sequence (optional).\nmemory_mask (Optional[Tensor]) – the additive mask for the encoder output (optional).\nsrc_key_padding_mask (Optional[Tensor]) – the Tensor mask for src keys per batch (optional).\ntgt_key_padding_mask (Optional[Tensor]) – the Tensor mask for tgt keys per batch (optional).\nmemory_key_padding_mask (Optional[Tensor]) – the Tensor mask for memory keys per batch (optional).\nsrc_is_causal (Optional[bool]) – If specified, applies a causal mask as src_mask.\nDefault: None; try to detect a causal mask.\nWarning:\nsrc_is_causal provides a hint that src_mask is\nthe causal mask. Providing incorrect hints can result in\nincorrect execution, including forward and backward\ncompatibility.\ntgt_is_causal (Optional[bool]) – If specified, applies a causal mask as tgt_mask.\nDefault: None; try to detect a causal mask.\nWarning:\ntgt_is_causal provides a hint that tgt_mask is\nthe causal mask. Providing incorrect hints can result in\nincorrect execution, including forward and backward\ncompatibility.\nmemory_is_causal (bool) – If specified, applies a causal mask as\nmemory_mask.\nDefault: False.\nWarning:\nmemory_is_causal provides a hint that\nmemory_mask is the causal mask. Providing incorrect\nhints can result in incorrect execution, including\nforward and backward compatibility.",
        "input_shape": "\nsrc: (S,E)(S, E)(S,E) for unbatched input, (S,N,E)(S, N, E)(S,N,E) if batch_first=False or\n(N, S, E) if batch_first=True.\ntgt: (T,E)(T, E)(T,E) for unbatched input, (T,N,E)(T, N, E)(T,N,E) if batch_first=False or\n(N, T, E) if batch_first=True.\nsrc_mask: (S,S)(S, S)(S,S) or (N⋅num_heads,S,S)(N\\cdot\\text{num\\_heads}, S, S)(N⋅num_heads,S,S).\ntgt_mask: (T,T)(T, T)(T,T) or (N⋅num_heads,T,T)(N\\cdot\\text{num\\_heads}, T, T)(N⋅num_heads,T,T).\nmemory_mask: (T,S)(T, S)(T,S).\nsrc_key_padding_mask: (S)(S)(S) for unbatched input otherwise (N,S)(N, S)(N,S).\ntgt_key_padding_mask: (T)(T)(T) for unbatched input otherwise (N,T)(N, T)(N,T).\nmemory_key_padding_mask: (S)(S)(S) for unbatched input otherwise (N,S)(N, S)(N,S).\n\nNote: [src/tgt/memory]_mask ensures that position iii is allowed to attend the unmasked\npositions. If a BoolTensor is provided, positions with True\nare not allowed to attend while False values will be unchanged. If a FloatTensor\nis provided, it will be added to the attention weight.\n[src/tgt/memory]_key_padding_mask provides specified elements in the key to be ignored by\nthe attention. If a BoolTensor is provided, the positions with the\nvalue of True will be ignored while the position with the value of False will be unchanged.\n\noutput: (T,E)(T, E)(T,E) for unbatched input, (T,N,E)(T, N, E)(T,N,E) if batch_first=False or\n(N, T, E) if batch_first=True.\n\nNote: Due to the multi-head attention architecture in the transformer model,\nthe output sequence length of a transformer is same as the input sequence\n(i.e. target) length of the decoder.\nwhere SSS is the source sequence length, TTT is the target sequence length, NNN is the\nbatch size, EEE is the feature number\n",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.TransformerDecoder.forward",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.TransformerDecoder.html#torch.nn.TransformerDecoder.forward",
        "api_signature": "forward(tgt, memory, tgt_mask=None, memory_mask=None, tgt_key_padding_mask=None, memory_key_padding_mask=None, tgt_is_causal=None, memory_is_causal=False)",
        "api_description": "Pass the inputs (and mask) through the decoder layer in turn.",
        "return_value": "",
        "parameters": "tgt (Tensor) – the sequence to the decoder (required).\nmemory (Tensor) – the sequence from the last layer of the encoder (required).\ntgt_mask (Optional[Tensor]) – the mask for the tgt sequence (optional).\nmemory_mask (Optional[Tensor]) – the mask for the memory sequence (optional).\ntgt_key_padding_mask (Optional[Tensor]) – the mask for the tgt keys per batch (optional).\nmemory_key_padding_mask (Optional[Tensor]) – the mask for the memory keys per batch (optional).\ntgt_is_causal (Optional[bool]) – If specified, applies a causal mask as tgt mask.\nDefault: None; try to detect a causal mask.\nWarning:\ntgt_is_causal provides a hint that tgt_mask is\nthe causal mask. Providing incorrect hints can result in\nincorrect execution, including forward and backward\ncompatibility.\nmemory_is_causal (bool) – If specified, applies a causal mask as\nmemory mask.\nDefault: False.\nWarning:\nmemory_is_causal provides a hint that\nmemory_mask is the causal mask. Providing incorrect\nhints can result in incorrect execution, including\nforward and backward compatibility.",
        "input_shape": "see the docs in Transformer.\n",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.TransformerDecoderLayer.forward",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.TransformerDecoderLayer.html#torch.nn.TransformerDecoderLayer.forward",
        "api_signature": "forward(tgt, memory, tgt_mask=None, memory_mask=None, tgt_key_padding_mask=None, memory_key_padding_mask=None, tgt_is_causal=False, memory_is_causal=False)",
        "api_description": "Pass the inputs (and mask) through the decoder layer.",
        "return_value": "",
        "parameters": "tgt (Tensor) – the sequence to the decoder layer (required).\nmemory (Tensor) – the sequence from the last layer of the encoder (required).\ntgt_mask (Optional[Tensor]) – the mask for the tgt sequence (optional).\nmemory_mask (Optional[Tensor]) – the mask for the memory sequence (optional).\ntgt_key_padding_mask (Optional[Tensor]) – the mask for the tgt keys per batch (optional).\nmemory_key_padding_mask (Optional[Tensor]) – the mask for the memory keys per batch (optional).\ntgt_is_causal (bool) – If specified, applies a causal mask as tgt mask.\nDefault: False.\nWarning:\ntgt_is_causal provides a hint that tgt_mask is\nthe causal mask. Providing incorrect hints can result in\nincorrect execution, including forward and backward\ncompatibility.\nmemory_is_causal (bool) – If specified, applies a causal mask as\nmemory mask.\nDefault: False.\nWarning:\nmemory_is_causal provides a hint that\nmemory_mask is the causal mask. Providing incorrect\nhints can result in incorrect execution, including\nforward and backward compatibility.",
        "input_shape": "see the docs in Transformer.\n",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.TransformerEncoder.forward",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoder.html#torch.nn.TransformerEncoder.forward",
        "api_signature": "forward(src, mask=None, src_key_padding_mask=None, is_causal=None)",
        "api_description": "Pass the input through the encoder layers in turn.",
        "return_value": "",
        "parameters": "src (Tensor) – the sequence to the encoder (required).\nmask (Optional[Tensor]) – the mask for the src sequence (optional).\nsrc_key_padding_mask (Optional[Tensor]) – the mask for the src keys per batch (optional).\nis_causal (Optional[bool]) – If specified, applies a causal mask as mask.\nDefault: None; try to detect a causal mask.\nWarning:\nis_causal provides a hint that mask is the\ncausal mask. Providing incorrect hints can result in\nincorrect execution, including forward and backward\ncompatibility.",
        "input_shape": "see the docs in Transformer.\n",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.TransformerEncoderLayer.forward",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoderLayer.html#torch.nn.TransformerEncoderLayer.forward",
        "api_signature": "forward(src, src_mask=None, src_key_padding_mask=None, is_causal=False)",
        "api_description": "Pass the input through the encoder layer.",
        "return_value": "",
        "parameters": "src (Tensor) – the sequence to the encoder layer (required).\nsrc_mask (Optional[Tensor]) – the mask for the src sequence (optional).\nsrc_key_padding_mask (Optional[Tensor]) – the mask for the src keys per batch (optional).\nis_causal (bool) – If specified, applies a causal mask as src mask.\nDefault: False.\nWarning:\nis_causal provides a hint that src_mask is the\ncausal mask. Providing incorrect hints can result in\nincorrect execution, including forward and backward\ncompatibility.",
        "input_shape": "see the docs in Transformer.\n",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.autograd.function.NestedIOFunction.forward_extended",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.autograd.function.NestedIOFunction.html#torch.autograd.function.NestedIOFunction.forward_extended",
        "api_signature": "forward_extended(*input)",
        "api_description": "User defined forward.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.transforms.Transform.forward_shape",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.transforms.Transform.forward_shape",
        "api_signature": "forward_shape(shape)",
        "api_description": "Infers the shape of the forward computation, given the input shape.\nDefaults to preserving shape.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.algorithms.ddp_comm_hooks.default_hooks.fp16_compress_hook",
        "api_url": "https://pytorch.org/docs/stable/ddp_comm_hooks.html#torch.distributed.algorithms.ddp_comm_hooks.default_hooks.fp16_compress_hook",
        "api_signature": "torch.distributed.algorithms.ddp_comm_hooks.default_hooks.fp16_compress_hook(process_group, bucket)",
        "api_description": "Compress by casting GradBucket to torch.float16 divided by process group size.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> ddp_model.register_comm_hook(process_group, fp16_compress_hook)\n\n\n"
    },
    {
        "api_name": "torch.distributed.algorithms.ddp_comm_hooks.default_hooks.fp16_compress_wrapper",
        "api_url": "https://pytorch.org/docs/stable/ddp_comm_hooks.html#torch.distributed.algorithms.ddp_comm_hooks.default_hooks.fp16_compress_wrapper",
        "api_signature": "torch.distributed.algorithms.ddp_comm_hooks.default_hooks.fp16_compress_wrapper(hook)",
        "api_description": "Cast input tensor to torch.float16, cast result of hook back to input dtype.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> state = PowerSGDState(process_group=process_group, matrix_approximation_rank=1, start_powerSGD_iter=10)\n>>> ddp_model.register_comm_hook(state, fp16_compress_wrapper(powerSGD_hook))\n\n\n"
    },
    {
        "api_name": "torch.frac",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.frac.html#torch.frac",
        "api_signature": "torch.frac(input, *, out=None)",
        "api_description": "Computes the fractional portion of each element in input.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.frac",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.frac.html#torch.Tensor.frac",
        "api_signature": "Tensor.frac()",
        "api_description": "See torch.frac()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.frac_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.frac_.html#torch.Tensor.frac_",
        "api_signature": "Tensor.frac_()",
        "api_description": "In-place version of frac()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.functional.fractional_max_pool2d",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.functional.fractional_max_pool2d.html#torch.nn.functional.fractional_max_pool2d",
        "api_signature": "torch.nn.functional.fractional_max_pool2d(input, kernel_size, output_size=None, output_ratio=None, return_indices=False, _random_samples=None)",
        "api_description": "Applies 2D fractional max pooling over an input signal composed of several input planes.",
        "return_value": "",
        "parameters": "kernel_size – the size of the window to take a max over.\nCan be a single number kkk (for a square kernel of k×kk \\times kk×k)\nor a tuple (kH, kW)\noutput_size – the target output size of the image of the form oH×oWoH \\times oWoH×oW.\nCan be a tuple (oH, oW) or a single number oHoHoH for a square image oH×oHoH \\times oHoH×oH\noutput_ratio – If one wants to have an output size as a ratio of the input size, this option can be given.\nThis has to be a number or tuple in the range (0, 1)\nreturn_indices – if True, will return the indices along with the outputs.\nUseful to pass to max_unpool2d().",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> input = torch.randn(20, 16, 50, 32)\n>>> # pool of square window of size=3, and target output size 13x12\n>>> F.fractional_max_pool2d(input, 3, output_size=(13, 12))\n>>> # pool of square window and target output size being half of input image size\n>>> F.fractional_max_pool2d(input, 3, output_ratio=(0.5, 0.5))\n\n\n"
    },
    {
        "api_name": "torch.nn.functional.fractional_max_pool3d",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.functional.fractional_max_pool3d.html#torch.nn.functional.fractional_max_pool3d",
        "api_signature": "torch.nn.functional.fractional_max_pool3d(input, kernel_size, output_size=None, output_ratio=None, return_indices=False, _random_samples=None)",
        "api_description": "Applies 3D fractional max pooling over an input signal composed of several input planes.",
        "return_value": "",
        "parameters": "kernel_size – the size of the window to take a max over.\nCan be a single number kkk (for a square kernel of k×k×kk \\times k \\times kk×k×k)\nor a tuple (kT, kH, kW)\noutput_size – the target output size of the form oT×oH×oWoT \\times oH \\times oWoT×oH×oW.\nCan be a tuple (oT, oH, oW) or a single number oHoHoH for a cubic output\noH×oH×oHoH \\times oH \\times oHoH×oH×oH\noutput_ratio – If one wants to have an output size as a ratio of the input size, this option can be given.\nThis has to be a number or tuple in the range (0, 1)\nreturn_indices – if True, will return the indices along with the outputs.\nUseful to pass to max_unpool3d().",
        "input_shape": "\nInput: (N,C,Tin,Hin,Win)(N, C, T_{in}, H_{in}, W_{in})(N,C,Tin​,Hin​,Win​) or (C,Tin,Hin,Win)(C, T_{in}, H_{in}, W_{in})(C,Tin​,Hin​,Win​).\nOutput: (N,C,Tout,Hout,Wout)(N, C, T_{out}, H_{out}, W_{out})(N,C,Tout​,Hout​,Wout​) or (C,Tout,Hout,Wout)(C, T_{out}, H_{out}, W_{out})(C,Tout​,Hout​,Wout​), where\n(Tout,Hout,Wout)=output_size(T_{out}, H_{out}, W_{out})=\\text{output\\_size}(Tout​,Hout​,Wout​)=output_size or\n(Tout,Hout,Wout)=output_ratio×(Tin,Hin,Win)(T_{out}, H_{out}, W_{out})=\\text{output\\_ratio} \\times (T_{in}, H_{in}, W_{in})(Tout​,Hout​,Wout​)=output_ratio×(Tin​,Hin​,Win​)\n\n",
        "notes": "",
        "code_example": ">>> input = torch.randn(20, 16, 50, 32, 16)\n>>> # pool of cubic window of size=3, and target output size 13x12x11\n>>> F.fractional_max_pool3d(input, 3, output_size=(13, 12, 11))\n>>> # pool of cubic window and target output size being half of input size\n>>> F.fractional_max_pool3d(input, 3, output_ratio=(0.5, 0.5, 0.5))\n\n\n"
    },
    {
        "api_name": "torch.nn.FractionalMaxPool2d",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.FractionalMaxPool2d.html#torch.nn.FractionalMaxPool2d",
        "api_signature": "torch.nn.FractionalMaxPool2d(kernel_size, output_size=None, output_ratio=None, return_indices=False, _random_samples=None)",
        "api_description": "Applies a 2D fractional max pooling over an input signal composed of several input planes.",
        "return_value": "",
        "parameters": "kernel_size (Union[int, Tuple[int, int]]) – the size of the window to take a max over.\nCan be a single number k (for a square kernel of k x k) or a tuple (kh, kw)\noutput_size (Union[int, Tuple[int, int]]) – the target output size of the image of the form oH x oW.\nCan be a tuple (oH, oW) or a single number oH for a square image oH x oH.\nNote that we must have kH+oH−1<=HinkH + oH - 1 <= H_{in}kH+oH−1<=Hin​ and kW+oW−1<=WinkW + oW - 1 <= W_{in}kW+oW−1<=Win​\noutput_ratio (Union[float, Tuple[float, float]]) – If one wants to have an output size as a ratio of the input size, this option can be given.\nThis has to be a number or tuple in the range (0, 1).\nNote that we must have kH+(output_ratio_H∗Hin)−1<=HinkH + (output\\_ratio\\_H * H_{in}) - 1 <= H_{in}kH+(output_ratio_H∗Hin​)−1<=Hin​\nand kW+(output_ratio_W∗Win)−1<=WinkW + (output\\_ratio\\_W * W_{in}) - 1 <= W_{in}kW+(output_ratio_W∗Win​)−1<=Win​\nreturn_indices (bool) – if True, will return the indices along with the outputs.\nUseful to pass to nn.MaxUnpool2d(). Default: False",
        "input_shape": "\nInput: (N,C,Hin,Win)(N, C, H_{in}, W_{in})(N,C,Hin​,Win​) or (C,Hin,Win)(C, H_{in}, W_{in})(C,Hin​,Win​).\nOutput: (N,C,Hout,Wout)(N, C, H_{out}, W_{out})(N,C,Hout​,Wout​) or (C,Hout,Wout)(C, H_{out}, W_{out})(C,Hout​,Wout​), where\n(Hout,Wout)=output_size(H_{out}, W_{out})=\\text{output\\_size}(Hout​,Wout​)=output_size or\n(Hout,Wout)=output_ratio×(Hin,Win)(H_{out}, W_{out})=\\text{output\\_ratio} \\times (H_{in}, W_{in})(Hout​,Wout​)=output_ratio×(Hin​,Win​).\n\n",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.FractionalMaxPool3d",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.FractionalMaxPool3d.html#torch.nn.FractionalMaxPool3d",
        "api_signature": "torch.nn.FractionalMaxPool3d(kernel_size, output_size=None, output_ratio=None, return_indices=False, _random_samples=None)",
        "api_description": "Applies a 3D fractional max pooling over an input signal composed of several input planes.",
        "return_value": "",
        "parameters": "kernel_size (Union[int, Tuple[int, int, int]]) – the size of the window to take a max over.\nCan be a single number k (for a square kernel of k x k x k) or a tuple (kt x kh x kw)\noutput_size (Union[int, Tuple[int, int, int]]) – the target output size of the image of the form oT x oH x oW.\nCan be a tuple (oT, oH, oW) or a single number oH for a square image oH x oH x oH\noutput_ratio (Union[float, Tuple[float, float, float]]) – If one wants to have an output size as a ratio of the input size, this option can be given.\nThis has to be a number or tuple in the range (0, 1)\nreturn_indices (bool) – if True, will return the indices along with the outputs.\nUseful to pass to nn.MaxUnpool3d(). Default: False",
        "input_shape": "\nInput: (N,C,Tin,Hin,Win)(N, C, T_{in}, H_{in}, W_{in})(N,C,Tin​,Hin​,Win​) or (C,Tin,Hin,Win)(C, T_{in}, H_{in}, W_{in})(C,Tin​,Hin​,Win​).\nOutput: (N,C,Tout,Hout,Wout)(N, C, T_{out}, H_{out}, W_{out})(N,C,Tout​,Hout​,Wout​) or (C,Tout,Hout,Wout)(C, T_{out}, H_{out}, W_{out})(C,Tout​,Hout​,Wout​), where\n(Tout,Hout,Wout)=output_size(T_{out}, H_{out}, W_{out})=\\text{output\\_size}(Tout​,Hout​,Wout​)=output_size or\n(Tout,Hout,Wout)=output_ratio×(Tin,Hin,Win)(T_{out}, H_{out}, W_{out})=\\text{output\\_ratio} \\times (T_{in}, H_{in}, W_{in})(Tout​,Hout​,Wout​)=output_ratio×(Tin​,Hin​,Win​)\n\n",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.jit.freeze",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.jit.freeze.html#torch.jit.freeze",
        "api_signature": "torch.jit.freeze(mod, preserved_attrs=None, optimize_numerics=True)",
        "api_description": "Freeze ScriptModule, inline submodules, and attributes as constants.",
        "return_value": "Frozen ScriptModule.\n",
        "parameters": "mod (ScriptModule) – a module to be frozen\npreserved_attrs (Optional[List[str]]) – a list of attributes to preserve in addition to the forward method.\nAttributes modified in preserved methods will also be preserved.\noptimize_numerics (bool) – If True, a set of optimization passes will be run that does not strictly\npreserve numerics. Full details of optimization can be found at torch.jit.run_frozen_optimizations.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.symbolic_shapes.ShapeEnv.freeze",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.fx.experimental.symbolic_shapes.ShapeEnv.html#torch.fx.experimental.symbolic_shapes.ShapeEnv.freeze",
        "api_signature": "freeze()",
        "api_description": "Freeze this ShapeEnv to stop accumulating guards",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.intrinsic.qat.freeze_bn_stats",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.nn.intrinsic.qat.freeze_bn_stats.html#torch.ao.nn.intrinsic.qat.freeze_bn_stats",
        "api_signature": "torch.ao.nn.intrinsic.qat.freeze_bn_stats(mod)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.frexp",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.frexp.html#torch.frexp",
        "api_signature": "torch.frexp(input, *, out=None)",
        "api_description": "Decomposes input into mantissa and exponent tensors\nsuch that input=mantissa×2exponent\\text{input} = \\text{mantissa} \\times 2^{\\text{exponent}}input=mantissa×2exponent.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor\nout (tuple, optional) – the output tensors",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.frexp",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.frexp.html#torch.Tensor.frexp",
        "api_signature": "Tensor.frexp(input)",
        "api_description": "See torch.frexp()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.rendezvous.dynamic_rendezvous.DynamicRendezvousHandler.from_backend",
        "api_url": "https://pytorch.org/docs/stable/elastic/rendezvous.html#torch.distributed.elastic.rendezvous.dynamic_rendezvous.DynamicRendezvousHandler.from_backend",
        "api_signature": "from_backend(run_id, store, backend, min_nodes, max_nodes, local_addr=None, timeout=None)",
        "api_description": "Create a new DynamicRendezvousHandler.",
        "return_value": "",
        "parameters": "run_id (str) – The run id of the rendezvous.\nstore (Store) – The C10d store to return as part of the rendezvous.\nbackend (RendezvousBackend) – The backend to use to hold the rendezvous state.\nmin_nodes (int) – The minimum number of nodes to admit to the rendezvous.\nmax_nodes (int) – The maximum number of nodes to admit to the rendezvous.\nlocal_addr (Optional[str]) – The local node address.\ntimeout (Optional[RendezvousTimeout]) – The timeout configuration of the rendezvous.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.TypedStorage.from_buffer",
        "api_url": "https://pytorch.org/docs/stable/storage.html#torch.TypedStorage.from_buffer",
        "api_signature": "from_buffer(*args, **kwargs)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.UntypedStorage.from_buffer",
        "api_url": "https://pytorch.org/docs/stable/storage.html#torch.UntypedStorage.from_buffer",
        "api_signature": "from_buffer()",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.backend_config.BackendConfig.from_dict",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.quantization.backend_config.BackendConfig.html#torch.ao.quantization.backend_config.BackendConfig.from_dict",
        "api_signature": "from_dict(backend_config_dict)",
        "api_description": "Create a BackendConfig from a dictionary with the following items:",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.backend_config.BackendPatternConfig.from_dict",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.quantization.backend_config.BackendPatternConfig.html#torch.ao.quantization.backend_config.BackendPatternConfig.from_dict",
        "api_signature": "from_dict(backend_pattern_config_dict)",
        "api_description": "Create a BackendPatternConfig from a dictionary with the following items:",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.backend_config.DTypeConfig.from_dict",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.quantization.backend_config.DTypeConfig.html#torch.ao.quantization.backend_config.DTypeConfig.from_dict",
        "api_signature": "from_dict(dtype_config_dict)",
        "api_description": "“input_dtype”: torch.dtype or DTypeWithConstraints\n“output_dtype”: torch.dtype or DTypeWithConstraints\n“weight_dtype”: torch.dtype or DTypeWithConstraints\n“bias_type”: torch.dtype\n“is_dynamic”: bool",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.fx.custom_config.ConvertCustomConfig.from_dict",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.quantization.fx.custom_config.ConvertCustomConfig.html#torch.ao.quantization.fx.custom_config.ConvertCustomConfig.from_dict",
        "api_signature": "from_dict(convert_custom_config_dict)",
        "api_description": "Create a ConvertCustomConfig from a dictionary with the following items:",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.fx.custom_config.FuseCustomConfig.from_dict",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.quantization.fx.custom_config.FuseCustomConfig.html#torch.ao.quantization.fx.custom_config.FuseCustomConfig.from_dict",
        "api_signature": "from_dict(fuse_custom_config_dict)",
        "api_description": "Create a ConvertCustomConfig from a dictionary with the following items:",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.fx.custom_config.PrepareCustomConfig.from_dict",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.quantization.fx.custom_config.PrepareCustomConfig.html#torch.ao.quantization.fx.custom_config.PrepareCustomConfig.from_dict",
        "api_signature": "from_dict(prepare_custom_config_dict)",
        "api_description": "Create a PrepareCustomConfig from a dictionary with the following items:",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.qconfig_mapping.QConfigMapping.from_dict",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.quantization.qconfig_mapping.QConfigMapping.html#torch.ao.quantization.qconfig_mapping.QConfigMapping.from_dict",
        "api_signature": "from_dict(qconfig_dict)",
        "api_description": "Create a QConfigMapping from a dictionary with the following keys (all optional):",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.from_dlpack",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.from_dlpack.html#torch.from_dlpack",
        "api_signature": "torch.from_dlpack(ext_tensor)",
        "api_description": "Converts a tensor from an external library into a torch.Tensor.",
        "return_value": "",
        "parameters": "ext_tensor (object with __dlpack__ attribute, or a DLPack capsule) – The tensor or DLPack capsule to convert.\nIf ext_tensor is a tensor (or ndarray) object, it must support\nthe __dlpack__ protocol (i.e., have a ext_tensor.__dlpack__\nmethod). Otherwise ext_tensor may be a DLPack capsule, which is\nan opaque PyCapsule instance, typically produced by a\nto_dlpack function or method.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.dlpack.from_dlpack",
        "api_url": "https://pytorch.org/docs/stable/dlpack.html#torch.utils.dlpack.from_dlpack",
        "api_signature": "torch.utils.dlpack.from_dlpack(ext_tensor)",
        "api_description": "Converts a tensor from an external library into a torch.Tensor.",
        "return_value": "",
        "parameters": "ext_tensor (object with __dlpack__ attribute, or a DLPack capsule) – The tensor or DLPack capsule to convert.\nIf ext_tensor is a tensor (or ndarray) object, it must support\nthe __dlpack__ protocol (i.e., have a ext_tensor.__dlpack__\nmethod). Otherwise ext_tensor may be a DLPack capsule, which is\nan opaque PyCapsule instance, typically produced by a\nto_dlpack function or method.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.onnx.JitScalarType.from_dtype",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.onnx.JitScalarType.html#torch.onnx.JitScalarType.from_dtype",
        "api_signature": "from_dtype(dtype)",
        "api_description": "Convert a torch dtype to JitScalarType.",
        "return_value": "JitScalarType\n",
        "parameters": "dtype (Optional[dtype]) – A torch.dtype to create a JitScalarType from",
        "input_shape": "",
        "notes": "A “RuntimeError: INTERNAL ASSERT FAILED at “../aten/src/ATen/core/jit_type_base.h” can\nbe raised in several scenarios where shape info is not present.\nInstead use from_value API which is safer.\n",
        "code_example": ""
    },
    {
        "api_name": "torch.from_file",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.from_file.html#torch.from_file",
        "api_signature": "torch.from_file(filename, shared=None, size=0, *, dtype=None, layout=None, device=None, pin_memory=False)",
        "api_description": "Creates a CPU tensor with a storage backed by a memory-mapped file.",
        "return_value": "",
        "parameters": "filename (str) – file name to map\nshared (bool) – whether to share memory (whether MAP_SHARED or MAP_PRIVATE is passed to the\nunderlying mmap(2) call)\nsize (int) – number of elements in the tensor\ndtype (torch.dtype, optional) – the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_dtype()).\nlayout (torch.layout, optional) – the desired layout of returned Tensor.\nDefault: torch.strided.\ndevice (torch.device, optional) – the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_device()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types.\npin_memory (bool, optional) – If set, returned tensor would be allocated in\nthe pinned memory. Works only for CPU tensors. Default: False.",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> t = torch.randn(2, 5, dtype=torch.float64)\n>>> t.numpy().tofile('storage.pt')\n>>> t_mapped = torch.from_file('storage.pt', shared=False, size=10, dtype=torch.float64)\n\n\n"
    },
    {
        "api_name": "torch.TypedStorage.from_file",
        "api_url": "https://pytorch.org/docs/stable/storage.html#torch.TypedStorage.from_file",
        "api_signature": "from_file(filename, shared=False, size=0)",
        "api_description": "Creates a CPU storage backed by a memory-mapped file.",
        "return_value": "",
        "parameters": "filename (str) – file name to map\nshared (bool) – whether to share memory (whether MAP_SHARED or MAP_PRIVATE is passed to the\nunderlying mmap(2) call)\nsize (int) – number of elements in the storage",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.UntypedStorage.from_file",
        "api_url": "https://pytorch.org/docs/stable/storage.html#torch.UntypedStorage.from_file",
        "api_signature": "from_file(filename, shared=False, size=0)",
        "api_description": "Creates a CPU storage backed by a memory-mapped file.",
        "return_value": "",
        "parameters": "filename (str) – file name to map\nshared (bool) – whether to share memory (whether MAP_SHARED or MAP_PRIVATE is passed to the\nunderlying mmap(2) call)\nsize (int) – number of elements in the storage",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.qat.Linear.from_float",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.nn.qat.Linear.html#torch.ao.nn.qat.Linear.from_float",
        "api_signature": "from_float(mod)",
        "api_description": "Create a qat module from a float module or qparams_dict\nArgs: mod a float module, either produced by torch.ao.quantization utilities\nor directly from user",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.quantized.Conv1d.from_float",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.nn.quantized.Conv1d.html#torch.ao.nn.quantized.Conv1d.from_float",
        "api_signature": "from_float(mod)",
        "api_description": "Creates a quantized module from a float module or qparams_dict.",
        "return_value": "",
        "parameters": "mod (Module) – a float module, either produced by torch.ao.quantization\nutilities or provided by the user",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.quantized.Conv2d.from_float",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.nn.quantized.Conv2d.html#torch.ao.nn.quantized.Conv2d.from_float",
        "api_signature": "from_float(mod)",
        "api_description": "Creates a quantized module from a float module or qparams_dict.",
        "return_value": "",
        "parameters": "mod (Module) – a float module, either produced by torch.ao.quantization\nutilities or provided by the user",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.quantized.Conv3d.from_float",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.nn.quantized.Conv3d.html#torch.ao.nn.quantized.Conv3d.from_float",
        "api_signature": "from_float(mod)",
        "api_description": "Creates a quantized module from a float module or qparams_dict.",
        "return_value": "",
        "parameters": "mod (Module) – a float module, either produced by torch.ao.quantization\nutilities or provided by the user",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.quantized.dynamic.Linear.from_float",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.nn.quantized.dynamic.Linear.html#torch.ao.nn.quantized.dynamic.Linear.from_float",
        "api_signature": "from_float(mod)",
        "api_description": "Create a dynamic quantized module from a float module or qparams_dict",
        "return_value": "",
        "parameters": "mod (Module) – a float module, either produced by torch.ao.quantization\nutilities or provided by the user",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.quantized.Embedding.from_float",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.nn.quantized.Embedding.html#torch.ao.nn.quantized.Embedding.from_float",
        "api_signature": "from_float(mod)",
        "api_description": "Create a quantized embedding module from a float module",
        "return_value": "",
        "parameters": "mod (Module) – a float module, either produced by torch.ao.quantization\nutilities or provided by user",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.quantized.EmbeddingBag.from_float",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.nn.quantized.EmbeddingBag.html#torch.ao.nn.quantized.EmbeddingBag.from_float",
        "api_signature": "from_float(mod)",
        "api_description": "Create a quantized embedding_bag module from a float module",
        "return_value": "",
        "parameters": "mod (Module) – a float module, either produced by torch.ao.quantization\nutilities or provided by user",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.quantized.Linear.from_float",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.nn.quantized.Linear.html#torch.ao.nn.quantized.Linear.from_float",
        "api_signature": "from_float(mod)",
        "api_description": "Create a quantized module from an observed float module",
        "return_value": "",
        "parameters": "mod (Module) – a float module, either produced by torch.ao.quantization\nutilities or provided by the user",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda.Event.from_ipc_handle",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.cuda.Event.html#torch.cuda.Event.from_ipc_handle",
        "api_signature": "from_ipc_handle(device, handle)",
        "api_description": "Reconstruct an event from an IPC handle on the given device.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.from_numpy",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.from_numpy.html#torch.from_numpy",
        "api_signature": "torch.from_numpy(ndarray)",
        "api_description": "Creates a Tensor from a numpy.ndarray.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.Embedding.from_pretrained",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html#torch.nn.Embedding.from_pretrained",
        "api_signature": "from_pretrained(embeddings, freeze=True, padding_idx=None, max_norm=None, norm_type=2.0, scale_grad_by_freq=False, sparse=False)",
        "api_description": "Create Embedding instance from given 2-dimensional FloatTensor.",
        "return_value": "",
        "parameters": "embeddings (Tensor) – FloatTensor containing weights for the Embedding.\nFirst dimension is being passed to Embedding as num_embeddings, second as embedding_dim.\nfreeze (bool, optional) – If True, the tensor does not get updated in the learning process.\nEquivalent to embedding.weight.requires_grad = False. Default: True\npadding_idx (int, optional) – If specified, the entries at padding_idx do not contribute to the gradient;\ntherefore, the embedding vector at padding_idx is not updated during training,\ni.e. it remains as a fixed “pad”.\nmax_norm (float, optional) – See module initialization documentation.\nnorm_type (float, optional) – See module initialization documentation. Default 2.\nscale_grad_by_freq (bool, optional) – See module initialization documentation. Default False.\nsparse (bool, optional) – See module initialization documentation.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.EmbeddingBag.from_pretrained",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.EmbeddingBag.html#torch.nn.EmbeddingBag.from_pretrained",
        "api_signature": "from_pretrained(embeddings, freeze=True, max_norm=None, norm_type=2.0, scale_grad_by_freq=False, mode='mean', sparse=False, include_last_offset=False, padding_idx=None)",
        "api_description": "Create EmbeddingBag instance from given 2-dimensional FloatTensor.",
        "return_value": "",
        "parameters": "embeddings (Tensor) – FloatTensor containing weights for the EmbeddingBag.\nFirst dimension is being passed to EmbeddingBag as ‘num_embeddings’, second as ‘embedding_dim’.\nfreeze (bool, optional) – If True, the tensor does not get updated in the learning process.\nEquivalent to embeddingbag.weight.requires_grad = False. Default: True\nmax_norm (float, optional) – See module initialization documentation. Default: None\nnorm_type (float, optional) – See module initialization documentation. Default 2.\nscale_grad_by_freq (bool, optional) – See module initialization documentation. Default False.\nmode (str, optional) – See module initialization documentation. Default: \"mean\"\nsparse (bool, optional) – See module initialization documentation. Default: False.\ninclude_last_offset (bool, optional) – See module initialization documentation. Default: False.\npadding_idx (int, optional) – See module initialization documentation. Default: None.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.quantized.dynamic.Linear.from_reference",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.nn.quantized.dynamic.Linear.html#torch.ao.nn.quantized.dynamic.Linear.from_reference",
        "api_signature": "from_reference(ref_qlinear)",
        "api_description": "Create a (fbgemm/qnnpack) dynamic quantized module from a reference quantized\nmodule\n:param ref_qlinear: a reference quantized  module, either produced by\n:type ref_qlinear: Module\n:param torch.ao.quantization functions or provided by the user:",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.quantized.Linear.from_reference",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.nn.quantized.Linear.html#torch.ao.nn.quantized.Linear.from_reference",
        "api_signature": "from_reference(ref_qlinear, output_scale, output_zero_point)",
        "api_description": "Create a (fbgemm/qnnpack) quantized module from a reference quantized module",
        "return_value": "",
        "parameters": "ref_qlinear (Module) – a reference quantized linear module, either produced by torch.ao.quantization\nutilities or provided by the user\noutput_scale (float) – scale for output Tensor\noutput_zero_point (int) – zero point for output Tensor",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.onnx.JitScalarType.from_value",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.onnx.JitScalarType.html#torch.onnx.JitScalarType.from_value",
        "api_signature": "from_value(value, default=None)",
        "api_description": "Create a JitScalarType from an value’s scalar type.",
        "return_value": "JitScalarType.\n",
        "parameters": "value (Union[None, Value, Tensor]) – An object to fetch scalar type from.\ndefault – The JitScalarType to return if a valid scalar cannot be fetched from value",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.frombuffer",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.frombuffer.html#torch.frombuffer",
        "api_signature": "torch.frombuffer(buffer, *, dtype, count=-1, offset=0, requires_grad=False)",
        "api_description": "Creates a 1-dimensional Tensor from an object that implements\nthe Python buffer protocol.",
        "return_value": "",
        "parameters": "buffer (object) – a Python object that exposes the buffer interface.\ndtype (torch.dtype) – the desired data type of returned tensor.\ncount (int, optional) – the number of desired elements to be read.\nIf negative, all the elements (until the end of the buffer) will be\nread. Default: -1.\noffset (int, optional) – the number of bytes to skip at the start of\nthe buffer. Default: 0.\nrequires_grad (bool, optional) – If autograd should record operations on the\nreturned tensor. Default: False.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.autograd.profiler_util.StringTable.fromkeys",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.autograd.profiler_util.StringTable.html#torch.autograd.profiler_util.StringTable.fromkeys",
        "api_signature": "fromkeys(value=None, /)",
        "api_description": "Create a new dictionary with keys from iterable and values set to value.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.ParameterDict.fromkeys",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.ParameterDict.html#torch.nn.ParameterDict.fromkeys",
        "api_signature": "fromkeys(keys, default=None)",
        "api_description": "Return a new ParameterDict with the keys provided.",
        "return_value": "",
        "parameters": "keys (iterable, string) – keys to make the new ParameterDict from\ndefault (Parameter, optional) – value to set for all keys",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.fsdp.FullyShardedDataParallel.fsdp_modules",
        "api_url": "https://pytorch.org/docs/stable/fsdp.html#torch.distributed.fsdp.FullyShardedDataParallel.fsdp_modules",
        "api_signature": "fsdp_modules(module, root_only=False)",
        "api_description": "Return all nested FSDP instances.",
        "return_value": "FSDP modules that are nested in\nthe input module.\n",
        "parameters": "module (torch.nn.Module) – Root module, which may or may not be an\nFSDP module.\nroot_only (bool) – Whether to return only FSDP root modules.\n(Default: False)",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.checkpoint.fsspec.FsspecReader",
        "api_url": "https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.fsspec.FsspecReader",
        "api_signature": "torch.distributed.checkpoint.fsspec.FsspecReader(path)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.checkpoint.fsspec.FsspecWriter",
        "api_url": "https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.fsspec.FsspecWriter",
        "api_signature": "torch.distributed.checkpoint.fsspec.FsspecWriter(path, single_file_per_rank=True, sync_files=True, thread_count=1, per_thread_copy_ahead=10000000)",
        "api_description": "Basic implementation of StorageWriter using FFspec.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.full",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.full.html#torch.full",
        "api_signature": "torch.full(size, fill_value, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False)",
        "api_description": "Creates a tensor of size size filled with fill_value. The\ntensor’s dtype is inferred from fill_value.",
        "return_value": "",
        "parameters": "size (int...) – a list, tuple, or torch.Size of integers defining the\nshape of the output tensor.\nfill_value (Scalar) – the value to fill the output tensor with.\nout (Tensor, optional) – the output tensor.\ndtype (torch.dtype, optional) – the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_dtype()).\nlayout (torch.layout, optional) – the desired layout of returned Tensor.\nDefault: torch.strided.\ndevice (torch.device, optional) – the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_device()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types.\nrequires_grad (bool, optional) – If autograd should record operations on the\nreturned tensor. Default: False.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.full_like",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.full_like.html#torch.full_like",
        "api_signature": "torch.full_like(input, fill_value, \\*, dtype=None, layout=torch.strided, device=None, requires_grad=False, memory_format=torch.preserve_format)",
        "api_description": "Returns a tensor with the same size as input filled with fill_value.\ntorch.full_like(input, fill_value) is equivalent to\ntorch.full(input.size(), fill_value, dtype=input.dtype, layout=input.layout, device=input.device).",
        "return_value": "",
        "parameters": "input (Tensor) – the size of input will determine size of the output tensor.\nfill_value – the number to fill the output tensor with.\ndtype (torch.dtype, optional) – the desired data type of returned Tensor.\nDefault: if None, defaults to the dtype of input.\nlayout (torch.layout, optional) – the desired layout of returned tensor.\nDefault: if None, defaults to the layout of input.\ndevice (torch.device, optional) – the desired device of returned tensor.\nDefault: if None, defaults to the device of input.\nrequires_grad (bool, optional) – If autograd should record operations on the\nreturned tensor. Default: False.\nmemory_format (torch.memory_format, optional) – the desired memory format of\nreturned Tensor. Default: torch.preserve_format.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.fsdp.FullyShardedDataParallel.full_optim_state_dict",
        "api_url": "https://pytorch.org/docs/stable/fsdp.html#torch.distributed.fsdp.FullyShardedDataParallel.full_optim_state_dict",
        "api_signature": "full_optim_state_dict(model, optim, optim_input=None, rank0_only=True, group=None)",
        "api_description": "Return the full optimizer state-dict.",
        "return_value": "A dict containing the optimizer state for\nmodel ‘s original unflattened parameters and including keys\n“state” and “param_groups” following the convention of\ntorch.optim.Optimizer.state_dict(). If rank0_only=True,\nthen nonzero ranks return an empty dict.\n",
        "parameters": "model (torch.nn.Module) – Root module (which may or may not be a\nFullyShardedDataParallel instance) whose parameters\nwere passed into the optimizer optim.\noptim (torch.optim.Optimizer) – Optimizer for model ‘s\nparameters.\noptim_input (Optional[Union[List[Dict[str, Any]], Iterable[torch.nn.Parameter]]]) – Input passed into the optimizer optim representing either a\nlist of parameter groups or an iterable of parameters;\nif None, then this method assumes the input was\nmodel.parameters(). This argument is deprecated, and there\nis no need to pass it in anymore. (Default: None)\nrank0_only (bool) – If True, saves the populated dict\nonly on rank 0; if False, saves it on all ranks. (Default:\nTrue)\ngroup (dist.ProcessGroup) – Model’s process group or None if using\nthe default process group. (Default: None)",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.fsdp.FullOptimStateDictConfig",
        "api_url": "https://pytorch.org/docs/stable/fsdp.html#torch.distributed.fsdp.FullOptimStateDictConfig",
        "api_signature": "torch.distributed.fsdp.FullOptimStateDictConfig(offload_to_cpu=True, rank0_only=False)",
        "api_description": "rank0_only (bool) – If True, then only rank 0 saves the full state\ndict, and nonzero ranks save an empty dict. If False, then all\nranks save the full state dict. (Default: False)",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.fsdp.FullStateDictConfig",
        "api_url": "https://pytorch.org/docs/stable/fsdp.html#torch.distributed.fsdp.FullStateDictConfig",
        "api_signature": "torch.distributed.fsdp.FullStateDictConfig(offload_to_cpu=False, rank0_only=False)",
        "api_description": "FullStateDictConfig is a config class meant to be used with\nStateDictType.FULL_STATE_DICT. We recommend enabling both\noffload_to_cpu=True and rank0_only=True when saving full state\ndicts to save GPU memory and CPU memory, respectively. This config class\nis meant to be used via the state_dict_type() context manager as\nfollows:",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.fsdp.FullyShardedDataParallel",
        "api_url": "https://pytorch.org/docs/stable/fsdp.html#torch.distributed.fsdp.FullyShardedDataParallel",
        "api_signature": "torch.distributed.fsdp.FullyShardedDataParallel(module, process_group=None, sharding_strategy=None, cpu_offload=None, auto_wrap_policy=None, backward_prefetch=BackwardPrefetch.BACKWARD_PRE, mixed_precision=None, ignored_modules=None, param_init_fn=None, device_id=None, sync_module_states=False, forward_prefetch=False, limit_all_gathers=True, use_orig_params=False, ignored_states=None, device_mesh=None)",
        "api_description": "A wrapper for sharding module parameters across data parallel workers.",
        "return_value": "self\nTotal norm of the parameters (viewed as a single vector).\nRefer to shard_full_optim_state_dict().\nFSDP modules that are nested in\nthe input module.\nA dict containing the optimizer state for\nmodel ‘s original unflattened parameters and including keys\n“state” and “param_groups” following the convention of\ntorch.optim.Optimizer.state_dict(). If rank0_only=True,\nthen nonzero ranks return an empty dict.\nA StateDictSettings containing the state_dict_type and\nstate_dict / optim_state_dict configs that are currently set.\nA dict containing the optimizer state for\nmodel. The sharding of the optimizer state is based on\nstate_dict_type.\nThe optimizer state dict re-keyed using the\nparameter keys specified by optim_state_key_type.\nThe full optimizer state dict now remapped to\nflattened parameters instead of unflattened parameters and\nrestricted to only include this rank’s part of the optimizer state.\nA StateDictSettings that include the previous state_dict type and\nconfiguration for the module.\nThe full optimizer state dict now remapped to\nflattened parameters instead of unflattened parameters and\nrestricted to only include this rank’s part of the optimizer state.\n",
        "parameters": "module (nn.Module) – This is the module to be wrapped with FSDP.\nprocess_group (Optional[Union[ProcessGroup, Tuple[ProcessGroup, ProcessGroup]]]) – This is the process group over which the model is sharded and thus\nthe one used for FSDP’s all-gather and reduce-scatter collective\ncommunications. If None, then FSDP uses the default process\ngroup. For hybrid sharding strategies such as\nShardingStrategy.HYBRID_SHARD, users can pass in a tuple of\nprocess groups, representing the groups over which to shard and\nreplicate, respectively. If None, then FSDP constructs process\ngroups for the user to shard intra-node and replicate inter-node.\n(Default: None)\nsharding_strategy (Optional[ShardingStrategy]) – This configures the sharding strategy, which may trade off memory\nsaving and communication overhead. See ShardingStrategy\nfor details. (Default: FULL_SHARD)\ncpu_offload (Optional[CPUOffload]) – This configures CPU offloading. If this is set to None, then\nno CPU offloading happens. See CPUOffload for details.\n(Default: None)\nauto_wrap_policy (Optional[Union[Callable[[nn.Module, bool, int], bool], ModuleWrapPolicy, CustomPolicy]]) – This specifies a policy to apply FSDP to submodules of module,\nwhich is needed for communication and computation overlap and thus\naffects performance. If None, then FSDP only applies to\nmodule, and users should manually apply FSDP to parent modules\nthemselves (proceeding bottom-up). For convenience, this accepts\nModuleWrapPolicy directly, which allows users to specify the\nmodule classes to wrap (e.g. the transformer block). Otherwise,\nthis should be a callable that takes in three arguments\nmodule: nn.Module, recurse: bool, and\nnonwrapped_numel: int and should return a bool specifying\nwhether the passed-in module should have FSDP applied if\nrecurse=False or if the traversal should continue into the\nmodule’s subtree if recurse=True. Users may add additional\narguments to the callable. The size_based_auto_wrap_policy in\ntorch.distributed.fsdp.wrap.py gives an example callable that\napplies FSDP to a module if the parameters in its subtree exceed\n100M numel. We recommend printing the model after applying FSDP\nand adjusting as needed.\nExample:\n>>> def custom_auto_wrap_policy(\n>>>     module: nn.Module,\n>>>     recurse: bool,\n>>>     nonwrapped_numel: int,\n>>>     # Additional custom arguments\n>>>     min_num_params: int = int(1e8),\n>>> ) -> bool:\n>>>     return nonwrapped_numel >= min_num_params\n>>> # Configure a custom `min_num_params`\n>>> my_auto_wrap_policy = functools.partial(custom_auto_wrap_policy, min_num_params=int(1e5))\nbackward_prefetch (Optional[BackwardPrefetch]) – This configures explicit backward prefetching of all-gathers. If\nNone, then FSDP does not backward prefetch, and there is no\ncommunication and computation overlap in the backward pass. See\nBackwardPrefetch for details. (Default: BACKWARD_PRE)\nmixed_precision (Optional[MixedPrecision]) – This configures native mixed precision for FSDP. If this is set to\nNone, then no mixed precision is used. Otherwise, parameter,\nbuffer, and gradient reduction dtypes can be set. See\nMixedPrecision for details. (Default: None)\nignored_modules (Optional[Iterable[torch.nn.Module]]) – Modules whose\nown parameters and child modules’ parameters and buffers are\nignored by this instance. None of the modules directly in\nignored_modules should be FullyShardedDataParallel\ninstances, and any child modules that are already-constructed\nFullyShardedDataParallel instances will not be ignored if\nthey are nested under this instance. This argument may be used to\navoid sharding specific parameters at module granularity when using an\nauto_wrap_policy or if parameters’ sharding is not managed by\nFSDP. (Default: None)\nparam_init_fn (Optional[Callable[[nn.Module], None]]) – A Callable[torch.nn.Module] -> None that\nspecifies how modules that are currently on the meta device should\nbe initialized onto an actual device. As of v1.12, FSDP detects\nmodules with parameters or buffers on meta device via is_meta\nand either applies param_init_fn if specified or calls\nnn.Module.reset_parameters() otherwise. For both cases, the\nimplementation should only initialize the parameters/buffers of\nthe module, not those of its submodules. This is to avoid\nre-initialization. In addition, FSDP also supports deferred\ndeferred_init() API, where the deferred modules are initialized\nby calling param_init_fn if specified or torchdistX’s default\nmaterialize_module() otherwise. If param_init_fn is\nspecified, then it is applied to all meta-device modules, meaning\nthat it should probably case on the module type. FSDP calls the\ninitialization function before parameter flattening and sharding.\nExample:\n>>> module = MyModule(device=\"meta\")\n>>> def my_init_fn(module: nn.Module):\n>>>     # E.g. initialize depending on the module type\n>>>     ...\n>>> fsdp_model = FSDP(module, param_init_fn=my_init_fn, auto_wrap_policy=size_based_auto_wrap_policy)\n>>> print(next(fsdp_model.parameters()).device) # current CUDA device\n>>> # With torchdistX\n>>> module = deferred_init.deferred_init(MyModule, device=\"cuda\")\n>>> # Will initialize via deferred_init.materialize_module().\n>>> fsdp_model = FSDP(module, auto_wrap_policy=size_based_auto_wrap_policy)\ndevice_id (Optional[Union[int, torch.device]]) – An int or\ntorch.device giving the CUDA device on which FSDP\ninitialization takes place, including the module initialization\nif needed and the parameter sharding. This should be specified to\nimprove initialization speed if module is on CPU. If the\ndefault CUDA device was set (e.g. via torch.cuda.set_device),\nthen the user may pass torch.cuda.current_device to this.\n(Default: None)\nsync_module_states (bool) – If True, then each FSDP module will\nbroadcast module parameters and buffers from rank 0 to ensure that\nthey are replicated across ranks (adding communication overhead to\nthis constructor). This can help load state_dict checkpoints\nvia load_state_dict in a memory efficient way. See\nFullStateDictConfig for an example of this. (Default:\nFalse)\nforward_prefetch (bool) – If True, then FSDP explicitly prefetches\nthe next forward-pass all-gather before the current forward\ncomputation. This is only useful for CPU-bound workloads, in which\ncase issuing the next all-gather earlier may improve overlap. This\nshould only be used for static-graph models since the prefetching\nfollows the first iteration’s execution order. (Default: False)\nlimit_all_gathers (bool) – If True, then FSDP explicitly\nsynchronizes the CPU thread to ensure GPU memory usage from only\ntwo consecutive FSDP instances (the current instance running\ncomputation and the next instance whose all-gather is prefetched).\nIf False, then FSDP allows the CPU thread to issue all-gathers\nwithout any extra synchronization. (Default: True) We often\nrefer to this feature as the “rate limiter”. This flag should only\nbe set to False for specific CPU-bound workloads with low\nmemory pressure in which case the CPU thread can aggressively issue\nall kernels without concern for the GPU memory usage.\nuse_orig_params (bool) – Setting this to True has FSDP use\nmodule ‘s original parameters. FSDP exposes those original\nparameters to the user via nn.Module.named_parameters()\ninstead of FSDP’s internal FlatParameter s. This means\nthat the optimizer step runs on the original parameters, enabling\nper-original-parameter hyperparameters. FSDP preserves the original\nparameter variables and manipulates their data between unsharded\nand sharded forms, where they are always views into the underlying\nunsharded or sharded FlatParameter, respectively. With the\ncurrent algorithm, the sharded form is always 1D, losing the\noriginal tensor structure. An original parameter may have all,\nsome, or none of its data present for a given rank. In the none\ncase, its data will be like a size-0 empty tensor. Users should not\nauthor programs relying on what data is present for a given\noriginal parameter in its sharded form. True is required to\nuse torch.compile(). Setting this to False exposes FSDP’s\ninternal FlatParameter s to the user via\nnn.Module.named_parameters(). (Default: False)\nignored_states (Optional[Iterable[torch.nn.Parameter]], Optional[Iterable[torch.nn.Module]]) – Ignored parameters or modules that will not be managed by this FSDP\ninstance, meaning that the parameters are not sharded and their\ngradients are not reduced across ranks. This argument unifies with\nthe existing ignored_modules argument, and we may deprecate\nignored_modules soon. For backward compatibility, we keep both\nignored_states and ignored_modules`, but FSDP only allows one\nof them to be specified as not None.\nfn (Module -> None) – function to be applied to each submodule\nmax_norm (float or int) – max norm of the gradients\nnorm_type (float or int) – type of the used p-norm. Can be 'inf'\nfor infinity norm.\nsharded_optim_state_dict (Dict[str, Any]) – Optimizer state dict\ncorresponding to the unflattened parameters and holding the\nsharded optimizer state.\nmodel (torch.nn.Module) – Refer to shard_full_optim_state_dict().\noptim (torch.optim.Optimizer) – Optimizer for model ‘s\nparameters.\nmodule (torch.nn.Module) – Root module, which may or may not be an\nFSDP module.\nroot_only (bool) – Whether to return only FSDP root modules.\n(Default: False)\nmodel (torch.nn.Module) – Root module (which may or may not be a\nFullyShardedDataParallel instance) whose parameters\nwere passed into the optimizer optim.\noptim (torch.optim.Optimizer) – Optimizer for model ‘s\nparameters.\noptim_input (Optional[Union[List[Dict[str, Any]], Iterable[torch.nn.Parameter]]]) – Input passed into the optimizer optim representing either a\nlist of parameter groups or an iterable of parameters;\nif None, then this method assumes the input was\nmodel.parameters(). This argument is deprecated, and there\nis no need to pass it in anymore. (Default: None)\nrank0_only (bool) – If True, saves the populated dict\nonly on rank 0; if False, saves it on all ranks. (Default:\nTrue)\ngroup (dist.ProcessGroup) – Model’s process group or None if using\nthe default process group. (Default: None)\nmodel (torch.nn.Module) – Root module (which may or may not be a\nFullyShardedDataParallel instance) whose parameters\nwere passed into the optimizer optim.\noptim (torch.optim.Optimizer) – Optimizer for model ‘s\nparameters.\noptim_state_dict (Dict[str, Any]) – the target optimizer state_dict to\ntransform. If the value is None, optim.state_dict() will be used. (\nDefault: None)\ngroup (dist.ProcessGroup) – Model’s process group across which parameters\nare sharded or None if using the default process group. (\nDefault: None)\nmodel (torch.nn.Module) – Root module (which may or may not be a\nFullyShardedDataParallel instance) whose parameters\nwere passed into the optimizer optim.\noptim (torch.optim.Optimizer) – Optimizer for model ‘s\nparameters.\noptim_state_dict (Dict[str, Any]) – The optimizer states to be loaded.\nis_named_optimizer (bool) – Is this optimizer a NamedOptimizer or\nKeyedOptimizer. Only set to True if optim is TorchRec’s\nKeyedOptimizer or torch.distributed’s NamedOptimizer.\nload_directly (bool) – If this is set to True, this API will also\ncall optim.load_state_dict(result) before returning the result.\nOtherwise, users are responsible to call optim.load_state_dict()\n(Default: False)\ngroup (dist.ProcessGroup) – Model’s process group across which parameters\nare sharded or None if using the default process group. (\nDefault: None)\nstate (object) – Passed to the hook to maintain any state information during the training process.\nExamples include error feedback in gradient compression,\npeers to communicate with next in GossipGrad, etc.\nIt is locally stored by each worker\nand shared by all the gradient tensors on the worker.\nhook (Callable) – Callable, which has one of the following signatures:\n1) hook: Callable[torch.Tensor] -> None:\nThis function takes in a Python tensor, which represents\nthe full, flattened, unsharded gradient with respect to all variables\ncorresponding to the model this FSDP unit is wrapping\n(that are not wrapped by other FSDP sub-units).\nIt then performs all necessary processing and returns None;\n2) hook: Callable[torch.Tensor, torch.Tensor] -> None:\nThis function takes in two Python tensors, the first one represents\nthe full, flattened, unsharded gradient with respect to all variables\ncorresponding to the model this FSDP unit is wrapping\n(that are not wrapped by other FSDP sub-units). The latter\nrepresents a pre-sized tensor to store a chunk of a sharded gradient after\nreduction.\nIn both cases, callable performs all necessary processing and returns None.\nCallables with signature 1 are expected to handle gradient communication for a NO_SHARD case.\nCallables with signature 2 are expected to handle gradient communication for sharded cases.\nfull_optim_state_dict (Optional[Dict[str, Any]]) – Optimizer state\ndict corresponding to the unflattened parameters and holding\nthe full non-sharded optimizer state if on rank 0; the argument\nis ignored on nonzero ranks.\nmodel (torch.nn.Module) – Root module (which may or may not be a\nFullyShardedDataParallel instance) whose parameters\ncorrespond to the optimizer state in full_optim_state_dict.\noptim_input (Optional[Union[List[Dict[str, Any]], Iterable[torch.nn.Parameter]]]) – Input passed into the optimizer representing either a\nlist of parameter groups or an iterable of parameters;\nif None, then this method assumes the input was\nmodel.parameters(). This argument is deprecated, and there\nis no need to pass it in anymore. (Default: None)\noptim (Optional[torch.optim.Optimizer]) – Optimizer that will load\nthe state dict returned by this method. This is the preferred\nargument to use over optim_input. (Default: None)\ngroup (dist.ProcessGroup) – Model’s process group or None if\nusing the default process group. (Default: None)\nmodule (torch.nn.Module) – Root module.\nstate_dict_type (StateDictType) – the desired state_dict_type to set.\nstate_dict_config (Optional[StateDictConfig]) – the configuration for the\ntarget state_dict_type.\noptim_state_dict_config (Optional[OptimStateDictConfig]) – the configuration\nfor the optimizer state dict.\nfull_optim_state_dict (Dict[str, Any]) – Optimizer state dict\ncorresponding to the unflattened parameters and holding the\nfull non-sharded optimizer state.\nmodel (torch.nn.Module) – Root module (which may or may not be a\nFullyShardedDataParallel instance) whose parameters\ncorrespond to the optimizer state in full_optim_state_dict.\noptim_input (Optional[Union[List[Dict[str, Any]], Iterable[torch.nn.Parameter]]]) – Input passed into the optimizer representing either a\nlist of parameter groups or an iterable of parameters;\nif None, then this method assumes the input was\nmodel.parameters(). This argument is deprecated, and there\nis no need to pass it in anymore. (Default: None)\noptim (Optional[torch.optim.Optimizer]) – Optimizer that will load\nthe state dict returned by this method. This is the preferred\nargument to use over optim_input. (Default: None)\nmodule (torch.nn.Module) – Root module.\nstate_dict_type (StateDictType) – the desired state_dict_type to set.\nstate_dict_config (Optional[StateDictConfig]) – the model state_dict\nconfiguration for the target state_dict_type.\noptim_state_dict_config (Optional[OptimStateDictConfig]) – the optimizer\nstate_dict configuration for the target state_dict_type.\nrecurse (bool, Optional) – recursively summon all params for nested\nFSDP instances (default: True).\nwriteback (bool, Optional) – if False, modifications to params are\ndiscarded after the context manager exits;\ndisabling this can be slightly more efficient (default: True)\nrank0_only (bool, Optional) – if True, full parameters are\nmaterialized on only global rank 0. This means that within the\ncontext, only rank 0 will have full parameters and the other\nranks will have sharded parameters. Note that setting\nrank0_only=True with writeback=True is not supported,\nas model parameter shapes will be different across ranks\nwithin the context, and writing to them can lead to\ninconsistency across ranks when the context is exited.\noffload_to_cpu (bool, Optional) – If True, full parameters are\noffloaded to CPU. Note that this offloading currently only\noccurs if the parameter is sharded (which is only not the case\nfor world_size = 1 or NO_SHARD config). It is recommended\nto use offload_to_cpu with rank0_only=True to avoid\nredundant copies of model parameters being offloaded to the same CPU memory.\nwith_grads (bool, Optional) – If True, gradients are also\nunsharded with the parameters. Currently, this is only\nsupported when passing use_orig_params=True to the FSDP\nconstructor and offload_to_cpu=False to this method.\n(Default: False)",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.autograd.Function",
        "api_url": "https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function",
        "api_signature": "torch.autograd.Function(*args, **kwargs)",
        "api_description": "Base class to create custom autograd.Function.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.func.functional_call",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.func.functional_call.html#torch.func.functional_call",
        "api_signature": "torch.func.functional_call(module, parameter_and_buffer_dicts, args, kwargs=None, *, tie_weights=True, strict=False)",
        "api_description": "Performs a functional call on the module by replacing the module parameters\nand buffers with the provided ones.",
        "return_value": "the result of calling module.\n",
        "parameters": "module (torch.nn.Module) – the module to call\nparameters_and_buffer_dicts (Dict[str, Tensor] or tuple of Dict[str, Tensor]) – the parameters that will be used in\nthe module call. If given a tuple of dictionaries, they must have distinct keys so that all dictionaries can\nbe used together\nargs (Any or tuple) – arguments to be passed to the module call. If not a tuple, considered a single argument.\nkwargs (dict) – keyword arguments to be passed to the module call\ntie_weights (bool, optional) – If True, then parameters and buffers tied in the original model will be treated as\ntied in the reparameterized version. Therefore, if True and different values are passed for the tied\nparameters and buffers, it will error. If False, it will not respect the originally tied parameters and\nbuffers unless the values passed for both weights are the same. Default: True.\nstrict (bool, optional) – If True, then the parameters and buffers passed in must match the parameters and\nbuffers in the original module. Therefore, if True and there are any missing or unexpected keys, it will\nerror. Default: False.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.utils.stateless.functional_call",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.utils.stateless.functional_call.html#torch.nn.utils.stateless.functional_call",
        "api_signature": "torch.nn.utils.stateless.functional_call(module, parameters_and_buffers, args, kwargs=None, *, tie_weights=True, strict=False)",
        "api_description": "Perform a functional call on the module by replacing the module parameters and buffers with the provided ones.",
        "return_value": "the result of calling module.\n",
        "parameters": "module (torch.nn.Module) – the module to call\nparameters_and_buffers (dict of str and Tensor) – the parameters that will be used in\nthe module call.\nargs (Any or tuple) – arguments to be passed to the module call. If not a tuple, considered a single argument.\nkwargs (dict) – keyword arguments to be passed to the module call\ntie_weights (bool, optional) – If True, then parameters and buffers tied in the original model will be treated as\ntied in the reparamaterized version. Therefore, if True and different values are passed for the tied\nparameters and buffers, it will error. If False, it will not respect the originally tied parameters and\nbuffers unless the values passed for both weights are the same. Default: True.\nstrict (bool, optional) – If True, then the parameters and buffers passed in must match the parameters and\nbuffers in the original module. Therefore, if True and there are any missing or unexpected keys, it will\nerror. Default: False.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.func.functionalize",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.func.functionalize.html#torch.func.functionalize",
        "api_signature": "torch.func.functionalize(func, *, remove='mutations')",
        "api_description": "functionalize is a transform that can be used to remove (intermediate)\nmutations and aliasing from a function, while preserving the function’s\nsemantics.",
        "return_value": "Returns a new “functionalized” function. It takes the same inputs as\nfunc, and has the same behavior, but any mutations\n(and optionally aliasing) performed on intermediate tensors\nin the function will be removed.\n",
        "parameters": "func (Callable) – A Python function that takes one or more arguments.\nremove (str) – An optional string argument, that takes on either\nthe value ‘mutations’ or ‘mutations_and_views’.\nIf ‘mutations’ is passed in then all mutating operators\nwill be replaced with their non-mutating equivalents.\nIf ‘mutations_and_views’ is passed in, then additionally, all aliasing\noperators will be replaced with their non-aliasing equivalents.\nDefault: ‘mutations’.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.benchmark.FunctionCounts",
        "api_url": "https://pytorch.org/docs/stable/benchmark_utils.html#torch.utils.benchmark.FunctionCounts",
        "api_signature": "torch.utils.benchmark.FunctionCounts(_data, inclusive, truncate_rows=True, _linewidth=None)",
        "api_description": "Container for manipulating Callgrind results.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.utils.fuse_conv_bn_eval",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.utils.fuse_conv_bn_eval.html#torch.nn.utils.fuse_conv_bn_eval",
        "api_signature": "torch.nn.utils.fuse_conv_bn_eval(conv, bn, transpose=False)",
        "api_description": "Fuse a convolutional module and a BatchNorm module into a single, new convolutional module.",
        "return_value": "The fused convolutional module.\n",
        "parameters": "conv (torch.nn.modules.conv._ConvNd) – A convolutional module.\nbn (torch.nn.modules.batchnorm._BatchNorm) – A BatchNorm module.\ntranspose (bool, optional) – If True, transpose the convolutional weight. Defaults to False.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.utils.fuse_conv_bn_weights",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.utils.fuse_conv_bn_weights.html#torch.nn.utils.fuse_conv_bn_weights",
        "api_signature": "torch.nn.utils.fuse_conv_bn_weights(conv_w, conv_b, bn_rm, bn_rv, bn_eps, bn_w, bn_b, transpose=False)",
        "api_description": "Fuse convolutional module parameters and BatchNorm module parameters into new convolutional module parameters.",
        "return_value": "Fused convolutional weight and bias.\n",
        "parameters": "conv_w (torch.Tensor) – Convolutional weight.\nconv_b (Optional[torch.Tensor]) – Convolutional bias.\nbn_rm (torch.Tensor) – BatchNorm running mean.\nbn_rv (torch.Tensor) – BatchNorm running variance.\nbn_eps (float) – BatchNorm epsilon.\nbn_w (Optional[torch.Tensor]) – BatchNorm weight.\nbn_b (Optional[torch.Tensor]) – BatchNorm bias.\ntranspose (bool, optional) – If True, transpose the conv weight. Defaults to False.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.quantize_fx.fuse_fx",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.quantization.quantize_fx.fuse_fx.html#torch.ao.quantization.quantize_fx.fuse_fx",
        "api_signature": "torch.ao.quantization.quantize_fx.fuse_fx(model, fuse_custom_config=None, backend_config=None)",
        "api_description": "Fuse modules like conv+bn, conv+bn+relu etc, model must be in eval mode.\nFusion rules are defined in torch.ao.quantization.fx.fusion_pattern.py",
        "return_value": "",
        "parameters": "model (*) – a torch.nn.Module model\nfuse_custom_config (*) – custom configurations for fuse_fx.\nSee FuseCustomConfig for more details",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.utils.fuse_linear_bn_eval",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.utils.fuse_linear_bn_eval.html#torch.nn.utils.fuse_linear_bn_eval",
        "api_signature": "torch.nn.utils.fuse_linear_bn_eval(linear, bn)",
        "api_description": "Fuse a linear module and a BatchNorm module into a single, new linear module.",
        "return_value": "The fused linear module.\n",
        "parameters": "linear (torch.nn.Linear) – A Linear module.\nbn (torch.nn.modules.batchnorm._BatchNorm) – A BatchNorm module.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.utils.fuse_linear_bn_weights",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.utils.fuse_linear_bn_weights.html#torch.nn.utils.fuse_linear_bn_weights",
        "api_signature": "torch.nn.utils.fuse_linear_bn_weights(linear_w, linear_b, bn_rm, bn_rv, bn_eps, bn_w, bn_b)",
        "api_description": "Fuse linear module parameters and BatchNorm module parameters into new linear module parameters.",
        "return_value": "Fused linear weight and bias.\n",
        "parameters": "linear_w (torch.Tensor) – Linear weight.\nlinear_b (Optional[torch.Tensor]) – Linear bias.\nbn_rm (torch.Tensor) – BatchNorm running mean.\nbn_rv (torch.Tensor) – BatchNorm running variance.\nbn_eps (float) – BatchNorm epsilon.\nbn_w (torch.Tensor) – BatchNorm weight.\nbn_b (torch.Tensor) – BatchNorm bias.\ntranspose (bool, optional) – If True, transpose the conv weight. Defaults to False.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.fuse_modules.fuse_modules",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.quantization.fuse_modules.fuse_modules.html#torch.ao.quantization.fuse_modules.fuse_modules",
        "api_signature": "torch.ao.quantization.fuse_modules.fuse_modules(model, modules_to_fuse, inplace=False, fuser_func=<function fuse_known_modules>, fuse_custom_config_dict=None)",
        "api_description": "Fuse a list of modules into a single module.",
        "return_value": "model with fused modules. A new copy is created if inplace=True.\n",
        "parameters": "model – Model containing the modules to be fused\nmodules_to_fuse – list of list of module names to fuse. Can also be a list\nof strings if there is only a single list of modules to fuse.\ninplace – bool specifying if fusion happens in place on the model, by default\na new model is returned\nfuser_func – Function that takes in a list of modules and outputs a list of fused modules\nof the same length. For example,\nfuser_func([convModule, BNModule]) returns the list [ConvBNModule, nn.Identity()]\nDefaults to torch.ao.quantization.fuse_known_modules\nfuse_custom_config_dict – custom configuration for fusion",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.fx.custom_config.FuseCustomConfig",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.quantization.fx.custom_config.FuseCustomConfig.html#torch.ao.quantization.fx.custom_config.FuseCustomConfig",
        "api_signature": null,
        "api_description": "Custom configuration for fuse_fx().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize.html#torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize",
        "api_signature": "torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize(observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255, **observer_kwargs)",
        "api_description": "Define a fused module to observe the tensor.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.futures.Future",
        "api_url": "https://pytorch.org/docs/stable/futures.html#torch.futures.Future",
        "api_signature": "torch.futures.Future(*, devices=None)",
        "api_description": "Wrapper around a torch._C.Future which encapsulates an asynchronous\nexecution of a callable, e.g. rpc_async(). It\nalso exposes a set of APIs to add callback functions and set results.",
        "return_value": "A new Future object that holds the return value of the\ncallback and will be marked as completed when the given\ncallback finishes.\nThe value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this value() method will\nalso throw an error.\nThe value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this wait method will\nalso throw an error.\n",
        "parameters": "callback (Future) – a Callable that takes in one argument,\nwhich is the reference to this Future.\nresult (BaseException) – the exception for this Future.\nresult (object) – the result object of this Future.\ncallback (Callable) – a Callable that takes this Future as\nthe only argument.",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> def callback(fut):\n...     print(\"This will run after the future has finished.\")\n...     print(fut.wait())\n>>> fut = torch.futures.Future()\n>>> fut.add_done_callback(callback)\n>>> fut.set_result(5)\nThis will run after the future has finished.\n5\n\n\n>>> fut = torch.futures.Future()\n>>> fut.set_exception(ValueError(\"foo\"))\n>>> fut.wait()\nTraceback (most recent call last):\n...\nValueError: foo\n\n\n>>> import threading\n>>> import time\n>>> def slow_set_future(fut, value):\n...     time.sleep(0.5)\n...     fut.set_result(value)\n>>> fut = torch.futures.Future()\n>>> t = threading.Thread(\n...     target=slow_set_future,\n...     args=(fut, torch.ones(2) * 3)\n... )\n>>> t.start()\n>>> print(fut.wait())\ntensor([3., 3.])\n>>> t.join()\n\n\n>>> def callback(fut):\n...     print(f\"RPC return value is {fut.wait()}.\")\n>>> fut = torch.futures.Future()\n>>> # The inserted callback will print the return value when\n>>> # receiving the response from \"worker1\"\n>>> cb_fut = fut.then(callback)\n>>> chain_cb_fut = cb_fut.then(\n...     lambda x : print(f\"Chained cb done. {x.wait()}\")\n... )\n>>> fut.set_result(5)\nRPC return value is 5.\nChained cb done. None\n\n\n"
    },
    {
        "api_name": "torch.ao.nn.quantized.FXFloatFunctional",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.nn.quantized.FXFloatFunctional.html#torch.ao.nn.quantized.FXFloatFunctional",
        "api_signature": "torch.ao.nn.quantized.FXFloatFunctional(*args, **kwargs)",
        "api_description": "module to replace FloatFunctional module before FX graph mode quantization,\nsince activation_post_process will be inserted in top level module directly",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.gamma.Gamma",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.gamma.Gamma",
        "api_signature": "torch.distributions.gamma.Gamma(concentration, rate, validate_args=None)",
        "api_description": "Bases: ExponentialFamily",
        "return_value": "",
        "parameters": "concentration (float or Tensor) – shape parameter of the distribution\n(often referred to as alpha)\nrate (float or Tensor) – rate = 1 / scale of the distribution\n(often referred to as beta)",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.special.gammainc",
        "api_url": "https://pytorch.org/docs/stable/special.html#torch.special.gammainc",
        "api_signature": "torch.special.gammainc(input, other, *, out=None)",
        "api_description": "Computes the regularized lower incomplete gamma function:",
        "return_value": "",
        "parameters": "input (Tensor) – the first non-negative input tensor\nother (Tensor) – the second non-negative input tensor\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.special.gammaincc",
        "api_url": "https://pytorch.org/docs/stable/special.html#torch.special.gammaincc",
        "api_signature": "torch.special.gammaincc(input, other, *, out=None)",
        "api_description": "Computes the regularized upper incomplete gamma function:",
        "return_value": "",
        "parameters": "input (Tensor) – the first non-negative input tensor\nother (Tensor) – the second non-negative input tensor\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.special.gammaln",
        "api_url": "https://pytorch.org/docs/stable/special.html#torch.special.gammaln",
        "api_signature": "torch.special.gammaln(input, *, out=None)",
        "api_description": "Computes the natural logarithm of the absolute value of the gamma function on input.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.gather",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.gather.html#torch.gather",
        "api_signature": "torch.gather(input, dim, index, *, sparse_grad=False, out=None)",
        "api_description": "Gathers values along an axis specified by dim.",
        "return_value": "",
        "parameters": "input (Tensor) – the source tensor\ndim (int) – the axis along which to index\nindex (LongTensor) – the indices of elements to gather\nsparse_grad (bool, optional) – If True, gradient w.r.t. input will be a sparse tensor.\nout (Tensor, optional) – the destination tensor",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda.comm.gather",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.cuda.comm.gather.html#torch.cuda.comm.gather",
        "api_signature": "torch.cuda.comm.gather(tensors, dim=0, destination=None, *, out=None)",
        "api_description": "Gathers tensors from multiple GPU devices.",
        "return_value": "\n\nIf destination is specified,a tensor located on destination device, that is a result of\nconcatenating tensors along dim.\n\n\n\n\nIf out is specified,the out tensor, now containing results of concatenating\ntensors along dim.\n\n\n\n\n\n",
        "parameters": "tensors (Iterable[Tensor]) – an iterable of tensors to gather.\nTensor sizes in all dimensions other than dim have to match.\ndim (int, optional) – a dimension along which the tensors will be\nconcatenated. Default: 0.\ndestination (torch.device, str, or int, optional) – the output device.\nCan be CPU or CUDA. Default: the current CUDA device.\nout (Tensor, optional, keyword-only) – the tensor to store gather result.\nIts sizes must match those of tensors, except for dim,\nwhere the size must equal sum(tensor.size(dim) for tensor in tensors).\nCan be on CPU or CUDA.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.gather",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#torch.distributed.gather",
        "api_signature": "torch.distributed.gather(tensor, gather_list=None, dst=0, group=None, async_op=False)",
        "api_description": "Gathers a list of tensors in a single process.",
        "return_value": "Async work handle, if async_op is set to True.\nNone, if not async_op or if not part of the group\n",
        "parameters": "tensor (Tensor) – Input tensor.\ngather_list (list[Tensor], optional) – List of appropriately-sized\ntensors to use for gathered data (default is None, must be specified\non the destination rank)\ndst (int, optional) – Destination rank on global process group (regardless of group argument). (default is 0)\ngroup (ProcessGroup, optional) – The process group to work on. If None,\nthe default process group will be used.\nasync_op (bool, optional) – Whether this op should be an async op",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.gather",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.gather.html#torch.Tensor.gather",
        "api_signature": "Tensor.gather(dim, index)",
        "api_description": "See torch.gather()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.gather_object",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#torch.distributed.gather_object",
        "api_signature": "torch.distributed.gather_object(obj, object_gather_list=None, dst=0, group=None)",
        "api_description": "Gathers picklable objects from the whole group in a single process.",
        "return_value": "None. On the dst rank, object_gather_list will contain the\noutput of the collective.\n",
        "parameters": "obj (Any) – Input object. Must be picklable.\nobject_gather_list (list[Any]) – Output list. On the dst rank, it\nshould be correctly sized as the size of the group for this\ncollective and will contain the output. Must be None on non-dst\nranks. (default is None)\ndst (int, optional) – Destination rank on global process group (regardless of group argument). (default is 0)\ngroup – (ProcessGroup, optional): The process group to work on. If None,\nthe default process group will be used. Default is None.",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> # Note: Process group initialization omitted on each rank.\n>>> import torch.distributed as dist\n>>> # Assumes world_size of 3.\n>>> gather_objects = [\"foo\", 12, {1: 2}] # any picklable object\n>>> output = [None for _ in gather_objects]\n>>> dist.gather_object(\n...     gather_objects[dist.get_rank()],\n...     output if dist.get_rank() == 0 else None,\n...     dst=0\n... )\n>>> # On rank 0\n>>> output\n['foo', 12, {1: 2}]\n\n\n"
    },
    {
        "api_name": "torch.signal.windows.gaussian",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.signal.windows.gaussian.html#torch.signal.windows.gaussian",
        "api_signature": "torch.signal.windows.gaussian(M, *, std=1.0, sym=True, dtype=None, layout=torch.strided, device=None, requires_grad=False)",
        "api_description": "Computes a window with a gaussian waveform.",
        "return_value": "",
        "parameters": "M (int) – the length of the window.\nIn other words, the number of points of the returned window.\nstd (float, optional) – the standard deviation of the gaussian. It controls how narrow or wide the window is.\nDefault: 1.0.\nsym (bool, optional) – If False, returns a periodic window suitable for use in spectral analysis.\nIf True, returns a symmetric window suitable for use in filter design. Default: True.\ndtype (torch.dtype, optional) – the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_dtype()).\nlayout (torch.layout, optional) – the desired layout of returned Tensor.\nDefault: torch.strided.\ndevice (torch.device, optional) – the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_device()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types.\nrequires_grad (bool, optional) – If autograd should record operations on the\nreturned tensor. Default: False.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.functional.gaussian_nll_loss",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.functional.gaussian_nll_loss.html#torch.nn.functional.gaussian_nll_loss",
        "api_signature": "torch.nn.functional.gaussian_nll_loss(input, target, var, full=False, eps=1e-06, reduction='mean')",
        "api_description": "Gaussian negative log likelihood loss.",
        "return_value": "",
        "parameters": "input (Tensor) – expectation of the Gaussian distribution.\ntarget (Tensor) – sample from the Gaussian distribution.\nvar (Tensor) – tensor of positive variance(s), one for each of the expectations\nin the input (heteroscedastic), or a single one (homoscedastic).\nfull (bool, optional) – include the constant term in the loss calculation. Default: False.\neps (float, optional) – value added to var, for stability. Default: 1e-6.\nreduction (str, optional) – specifies the reduction to apply to the output:\n'none' | 'mean' | 'sum'. 'none': no reduction will be applied,\n'mean': the output is the average of all batch member losses,\n'sum': the output is the sum of all batch member losses.\nDefault: 'mean'.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.GaussianNLLLoss",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.GaussianNLLLoss.html#torch.nn.GaussianNLLLoss",
        "api_signature": "torch.nn.GaussianNLLLoss(*, full=False, eps=1e-06, reduction='mean')",
        "api_description": "Gaussian negative log likelihood loss.",
        "return_value": "",
        "parameters": "full (bool, optional) – include the constant term in the loss\ncalculation. Default: False.\neps (float, optional) – value used to clamp var (see note below), for\nstability. Default: 1e-6.\nreduction (str, optional) – specifies the reduction to apply to the\noutput:'none' | 'mean' | 'sum'. 'none': no reduction\nwill be applied, 'mean': the output is the average of all batch\nmember losses, 'sum': the output is the sum of all batch member\nlosses. Default: 'mean'.",
        "input_shape": "\nInput: (N,∗)(N, *)(N,∗) or (∗)(*)(∗) where ∗*∗ means any number of additional\ndimensions\nTarget: (N,∗)(N, *)(N,∗) or (∗)(*)(∗), same shape as the input, or same shape as the input\nbut with one dimension equal to 1 (to allow for broadcasting)\nVar: (N,∗)(N, *)(N,∗) or (∗)(*)(∗), same shape as the input, or same shape as the input but\nwith one dimension equal to 1, or same shape as the input but with one fewer\ndimension (to allow for broadcasting)\nOutput: scalar if reduction is 'mean' (default) or\n'sum'. If reduction is 'none', then (N,∗)(N, *)(N,∗), same\nshape as the input\n\n",
        "notes": "",
        "code_example": ">>> loss = nn.GaussianNLLLoss()\n>>> input = torch.randn(5, 2, requires_grad=True)\n>>> target = torch.randn(5, 2)\n>>> var = torch.ones(5, 2, requires_grad=True)  # heteroscedastic\n>>> output = loss(input, target, var)\n>>> output.backward()\n\n\n>>> loss = nn.GaussianNLLLoss()\n>>> input = torch.randn(5, 2, requires_grad=True)\n>>> target = torch.randn(5, 2)\n>>> var = torch.ones(5, 1, requires_grad=True)  # homoscedastic\n>>> output = loss(input, target, var)\n>>> output.backward()\n\n\n"
    },
    {
        "api_name": "torch.gcd",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.gcd.html#torch.gcd",
        "api_signature": "torch.gcd(input, other, *, out=None)",
        "api_description": "Computes the element-wise greatest common divisor (GCD) of input and other.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.\nother (Tensor) – the second input tensor\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.gcd",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.gcd.html#torch.Tensor.gcd",
        "api_signature": "Tensor.gcd(other)",
        "api_description": "See torch.gcd()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.gcd_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.gcd_.html#torch.Tensor.gcd_",
        "api_signature": "Tensor.gcd_(other)",
        "api_description": "In-place version of gcd()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ge",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ge.html#torch.ge",
        "api_signature": "torch.ge(input, other, *, out=None)",
        "api_description": "Computes input≥other\\text{input} \\geq \\text{other}input≥other element-wise.",
        "return_value": "A boolean tensor that is True where input is greater than or equal to other and False elsewhere\n",
        "parameters": "input (Tensor) – the tensor to compare\nother (Tensor or float) – the tensor or value to compare\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.ge",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.ge.html#torch.Tensor.ge",
        "api_signature": "Tensor.ge(other)",
        "api_description": "See torch.ge().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.ge_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.ge_.html#torch.Tensor.ge_",
        "api_signature": "Tensor.ge_(other)",
        "api_description": "In-place version of ge().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.GELU",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.GELU.html#torch.nn.GELU",
        "api_signature": "torch.nn.GELU(approximate='none')",
        "api_description": "Applies the Gaussian Error Linear Units function.",
        "return_value": "",
        "parameters": "approximate (str, optional) – the gelu approximation algorithm to use:\n'none' | 'tanh'. Default: 'none'",
        "input_shape": "\nInput: (∗)(*)(∗), where ∗*∗ means any number of dimensions.\nOutput: (∗)(*)(∗), same shape as the input.\n\n",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.functional.gelu",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.functional.gelu.html#torch.nn.functional.gelu",
        "api_signature": "torch.nn.functional.gelu(input, approximate='none')",
        "api_description": "When the approximate argument is ‘none’, it applies element-wise the function\nGELU(x)=x∗Φ(x)\\text{GELU}(x) = x * \\Phi(x)GELU(x)=x∗Φ(x)",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.signal.windows.general_cosine",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.signal.windows.general_cosine.html#torch.signal.windows.general_cosine",
        "api_signature": "torch.signal.windows.general_cosine(M, *, a, sym=True, dtype=None, layout=torch.strided, device=None, requires_grad=False)",
        "api_description": "Computes the general cosine window.",
        "return_value": "",
        "parameters": "M (int) – the length of the window.\nIn other words, the number of points of the returned window.\na (Iterable) – the coefficients associated to each of the cosine functions.\nsym (bool, optional) – If False, returns a periodic window suitable for use in spectral analysis.\nIf True, returns a symmetric window suitable for use in filter design. Default: True.\ndtype (torch.dtype, optional) – the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_dtype()).\nlayout (torch.layout, optional) – the desired layout of returned Tensor.\nDefault: torch.strided.\ndevice (torch.device, optional) – the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_device()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types.\nrequires_grad (bool, optional) – If autograd should record operations on the\nreturned tensor. Default: False.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.signal.windows.general_hamming",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.signal.windows.general_hamming.html#torch.signal.windows.general_hamming",
        "api_signature": "torch.signal.windows.general_hamming(M, *, alpha=0.54, sym=True, dtype=None, layout=torch.strided, device=None, requires_grad=False)",
        "api_description": "Computes the general Hamming window.",
        "return_value": "",
        "parameters": "M (int) – the length of the window.\nIn other words, the number of points of the returned window.\nalpha (float, optional) – the window coefficient. Default: 0.54.\nsym (bool, optional) – If False, returns a periodic window suitable for use in spectral analysis.\nIf True, returns a symmetric window suitable for use in filter design. Default: True.\ndtype (torch.dtype, optional) – the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_dtype()).\nlayout (torch.layout, optional) – the desired layout of returned Tensor.\nDefault: torch.strided.\ndevice (torch.device, optional) – the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_device()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types.\nrequires_grad (bool, optional) – If autograd should record operations on the\nreturned tensor. Default: False.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.generate_methods_for_privateuse1_backend",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.utils.generate_methods_for_privateuse1_backend.html#torch.utils.generate_methods_for_privateuse1_backend",
        "api_signature": "torch.utils.generate_methods_for_privateuse1_backend(for_tensor=True, for_module=True, for_storage=False, unsupported_dtype=None)",
        "api_description": "Automatically generate attributes and methods for the custom backend after rename privateuse1 backend.",
        "return_value": "",
        "parameters": "for_tensor (bool) – whether register related methods for torch.Tensor class.\nfor_module (bool) – whether register related methods for torch.nn.Module class.\nfor_storage (bool) – whether register related methods for torch.Storage class.\nunsupported_dtype (List[torch.dtype]) – takes effect only when the storage method needs to be generated,\nindicating that the storage does not support the torch.dtype type.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.Transformer.generate_square_subsequent_mask",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html#torch.nn.Transformer.generate_square_subsequent_mask",
        "api_signature": "generate_square_subsequent_mask(sz, device=None, dtype=None)",
        "api_description": "Generate a square causal mask for the sequence.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Generator",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator",
        "api_signature": "torch.Generator(device='cpu')",
        "api_description": "Creates and returns a generator object that manages the state of the algorithm which\nproduces pseudo random numbers. Used as a keyword argument in many In-place random sampling\nfunctions.",
        "return_value": "An torch.Generator object.\nA torch.ByteTensor which contains all the necessary bits\nto restore a Generator to a specific point in time.\nAn torch.Generator object.\n",
        "parameters": "device (torch.device, optional) – the desired device for the generator.\nseed (int) – The desired seed. Value must be within the inclusive range\n[-0x8000_0000_0000_0000, 0xffff_ffff_ffff_ffff]. Otherwise, a RuntimeError\nis raised. Negative inputs are remapped to positive values with the formula\n0xffff_ffff_ffff_ffff + seed.\nnew_state (torch.ByteTensor) – The desired state.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.geometric.Geometric",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.geometric.Geometric",
        "api_signature": "torch.distributions.geometric.Geometric(probs=None, logits=None, validate_args=None)",
        "api_description": "Bases: Distribution",
        "return_value": "",
        "parameters": "probs (Number, Tensor) – the probability of sampling 1. Must be in range (0, 1]\nlogits (Number, Tensor) – the log-odds of sampling 1.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.geometric_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.geometric_.html#torch.Tensor.geometric_",
        "api_signature": "Tensor.geometric_(p, *, generator=None)",
        "api_description": "Fills self tensor with elements drawn from the geometric distribution:",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.geqrf",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.geqrf.html#torch.geqrf",
        "api_signature": "torch.geqrf(input, *, out=None)",
        "api_description": "This is a low-level function for calling LAPACK’s geqrf directly. This function\nreturns a namedtuple (a, tau) as defined in LAPACK documentation for geqrf .",
        "return_value": "",
        "parameters": "input (Tensor) – the input matrix\nout (tuple, optional) – the output tuple of (Tensor, Tensor). Ignored if None. Default: None.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.geqrf",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.geqrf.html#torch.Tensor.geqrf",
        "api_signature": "Tensor.geqrf()",
        "api_description": "See torch.geqrf()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ger",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ger.html#torch.ger",
        "api_signature": "torch.ger(input, vec2, *, out=None)",
        "api_description": "Alias of torch.outer().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.ger",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.ger.html#torch.Tensor.ger",
        "api_signature": "Tensor.ger(vec2)",
        "api_description": "See torch.ger()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.Store.get",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#torch.distributed.Store.get",
        "api_signature": "torch.distributed.Store.get(self: torch._C._distributed_c10d.Store, arg0: str)",
        "api_description": "Retrieves the value associated with the given key in the store. If key is not\npresent in the store, the function will wait for timeout, which is defined\nwhen initializing the store, before throwing an exception.",
        "return_value": "Value associated with key if key is in the store.\n",
        "parameters": "key (str) – The function will return the value associated with this key.",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n>>> store.set(\"first_key\", \"first_value\")\n>>> # Should return \"first_value\"\n>>> store.get(\"first_key\")\n\n\n"
    },
    {
        "api_name": "torch.autograd.profiler_util.StringTable.get",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.autograd.profiler_util.StringTable.html#torch.autograd.profiler_util.StringTable.get",
        "api_signature": "get(key, default=None, /)",
        "api_description": "Return the value for key if key is in the dictionary, else default.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.rendezvous.etcd_store.EtcdStore.get",
        "api_url": "https://pytorch.org/docs/stable/elastic/rendezvous.html#torch.distributed.elastic.rendezvous.etcd_store.EtcdStore.get",
        "api_signature": "get(key)",
        "api_description": "Get a value by key, possibly doing a blocking wait.",
        "return_value": "value (bytes)\n",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.rendezvous.RendezvousParameters.get",
        "api_url": "https://pytorch.org/docs/stable/elastic/rendezvous.html#torch.distributed.elastic.rendezvous.RendezvousParameters.get",
        "api_signature": "get(key, default=None)",
        "api_description": "Return the value for key if key exists, else default.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.monitor.Stat.get",
        "api_url": "https://pytorch.org/docs/stable/monitor.html#torch.monitor.Stat.get",
        "api_signature": "get(self: torch._C._monitor.Stat)",
        "api_description": "Returns the current value of the stat, primarily for testing\npurposes. If the stat has logged and no additional values have been\nadded this will be zero.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.ParameterDict.get",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.ParameterDict.html#torch.nn.ParameterDict.get",
        "api_signature": "get(key, default=None)",
        "api_description": "Return the parameter associated with key if present. Otherwise return default if provided, None if not.",
        "return_value": "",
        "parameters": "key (str) – key to get from the ParameterDict\ndefault (Parameter, optional) – value to return if key not present",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.multiprocessing.get_all_sharing_strategies",
        "api_url": "https://pytorch.org/docs/stable/multiprocessing.html#torch.multiprocessing.get_all_sharing_strategies",
        "api_signature": "torch.multiprocessing.get_all_sharing_strategies()",
        "api_description": "Return a set of sharing strategies supported on a current system.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda.get_allocator_backend",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.cuda.get_allocator_backend.html#torch.cuda.get_allocator_backend",
        "api_signature": "torch.cuda.get_allocator_backend()",
        "api_description": "Return a string describing the active allocator backend as set by\nPYTORCH_CUDA_ALLOC_CONF. Currently available backends are\nnative (PyTorch’s native caching allocator) and cudaMallocAsync`\n(CUDA’s built-in asynchronous allocator).",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda.get_arch_list",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.cuda.get_arch_list.html#torch.cuda.get_arch_list",
        "api_signature": "torch.cuda.get_arch_list()",
        "api_description": "Return list CUDA architectures this library was compiled for.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.rendezvous.RendezvousParameters.get_as_bool",
        "api_url": "https://pytorch.org/docs/stable/elastic/rendezvous.html#torch.distributed.elastic.rendezvous.RendezvousParameters.get_as_bool",
        "api_signature": "get_as_bool(key, default=None)",
        "api_description": "Return the value for key as a bool.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.rendezvous.RendezvousParameters.get_as_int",
        "api_url": "https://pytorch.org/docs/stable/elastic/rendezvous.html#torch.distributed.elastic.rendezvous.RendezvousParameters.get_as_int",
        "api_signature": "get_as_int(key, default=None)",
        "api_description": "Return the value for key as an int.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.Graph.get_attr",
        "api_url": "https://pytorch.org/docs/stable/fx.html#torch.fx.Graph.get_attr",
        "api_signature": "get_attr(qualified_name, type_expr=None)",
        "api_description": "Insert a get_attr node into the Graph. A get_attr Node represents the\nfetch of an attribute from the Module hierarchy.",
        "return_value": "The newly-created and inserted get_attr node.\n",
        "parameters": "qualified_name (str) – the fully-qualified name of the attribute to be retrieved.\nFor example, if the traced Module has a submodule named foo, which has a\nsubmodule named bar, which has an attribute named baz, the qualified\nname foo.bar.baz should be passed as qualified_name.\ntype_expr (Optional[Any]) – an optional type annotation representing the\nPython type the output of this node will have.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.Interpreter.get_attr",
        "api_url": "https://pytorch.org/docs/stable/fx.html#torch.fx.Interpreter.get_attr",
        "api_signature": "get_attr(target, args, kwargs)",
        "api_description": "Execute a get_attr node. Will retrieve an attribute\nvalue from the Module hierarchy of self.module.",
        "return_value": "The value of the attribute that was retrieved\n",
        "parameters": "target (Target) – The call target for this node. See\nNode for\ndetails on semantics\nargs (Tuple) – Tuple of positional args for this invocation\nkwargs (Dict) – Dict of keyword arguments for this invocation",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.Transformer.get_attr",
        "api_url": "https://pytorch.org/docs/stable/fx.html#torch.fx.Transformer.get_attr",
        "api_signature": "get_attr(target, args, kwargs)",
        "api_description": "Execute a get_attr node. In Transformer, this is\noverridden to insert a new get_attr node into the output\ngraph.",
        "return_value": "",
        "parameters": "target (Target) – The call target for this node. See\nNode for\ndetails on semantics\nargs (Tuple) – Tuple of positional args for this invocation\nkwargs (Dict) – Dict of keyword arguments for this invocation",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.get_backend",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#torch.distributed.get_backend",
        "api_signature": "torch.distributed.get_backend(group=None)",
        "api_description": "Return the backend of the given process group.",
        "return_value": "The backend of the given process group as a lower case string.\n",
        "parameters": "group (ProcessGroup, optional) – The process group to work on. The\ndefault is the general main process group. If another specific group\nis specified, the calling process must be part of group.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.rendezvous.RendezvousHandler.get_backend",
        "api_url": "https://pytorch.org/docs/stable/elastic/rendezvous.html#torch.distributed.elastic.rendezvous.RendezvousHandler.get_backend",
        "api_signature": "get_backend()",
        "api_description": "Return the name of the rendezvous backend.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.jit.ScriptModule.get_buffer",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.jit.ScriptModule.html#torch.jit.ScriptModule.get_buffer",
        "api_signature": "get_buffer(target)",
        "api_description": "Return the buffer given by target if it exists, otherwise throw an error.",
        "return_value": "The buffer referenced by target\n",
        "parameters": "target (str) – The fully-qualified string name of the buffer\nto look for. (See get_submodule for how to specify a\nfully-qualified string.)",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.Module.get_buffer",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.get_buffer",
        "api_signature": "get_buffer(target)",
        "api_description": "Return the buffer given by target if it exists, otherwise throw an error.",
        "return_value": "The buffer referenced by target\n",
        "parameters": "target (str) – The fully-qualified string name of the buffer\nto look for. (See get_submodule for how to specify a\nfully-qualified string.)",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.cpp_extension.get_compiler_abi_compatibility_and_version",
        "api_url": "https://pytorch.org/docs/stable/cpp_extension.html#torch.utils.cpp_extension.get_compiler_abi_compatibility_and_version",
        "api_signature": "torch.utils.cpp_extension.get_compiler_abi_compatibility_and_version(compiler)",
        "api_description": "Determine if the given compiler is ABI-compatible with PyTorch alongside its version.",
        "return_value": "A tuple that contains a boolean that defines if the compiler is (likely) ABI-incompatible with PyTorch,\nfollowed by a TorchVersion string that contains the compiler version separated by dots.\n",
        "parameters": "compiler (str) – The compiler executable name to check (e.g. g++).\nMust be executable in a shell process.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.get_cpp_backtrace",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.utils.get_cpp_backtrace.html#torch.utils.get_cpp_backtrace",
        "api_signature": "torch.utils.get_cpp_backtrace(frames_to_skip=0, maximum_number_of_frames=64)",
        "api_description": "Return a string containing the C++ stack trace of the current thread.",
        "return_value": "",
        "parameters": "frames_to_skip (int) – the number of frames to skip from the top of the stack\nmaximum_number_of_frames (int) – the maximum number of frames to return",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.backends.cpu.get_cpu_capability",
        "api_url": "https://pytorch.org/docs/stable/backends.html#torch.backends.cpu.get_cpu_capability",
        "api_signature": "torch.backends.cpu.get_cpu_capability()",
        "api_description": "Return cpu capability as a string value.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.library.get_ctx",
        "api_url": "https://pytorch.org/docs/stable/library.html#torch.library.get_ctx",
        "api_signature": "torch.library.get_ctx()",
        "api_description": "get_ctx() returns the current AbstractImplCtx object.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.jit.ScriptFunction.get_debug_state",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.jit.ScriptFunction.html#torch.jit.ScriptFunction.get_debug_state",
        "api_signature": "get_debug_state(self: torch._C.ScriptFunction)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.get_default_device",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.get_default_device.html#torch.get_default_device",
        "api_signature": "torch.get_default_device()",
        "api_description": "Gets the default torch.Tensor to be allocated on device",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.get_default_dtype",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.get_default_dtype.html#torch.get_default_dtype",
        "api_signature": "torch.get_default_dtype()",
        "api_description": "Get the current default floating point torch.dtype.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.serialization.get_default_load_endianness",
        "api_url": "https://pytorch.org/docs/stable/notes/serialization.html#torch.serialization.get_default_load_endianness",
        "api_signature": "torch.serialization.get_default_load_endianness()",
        "api_description": "Get fallback byte order for loading files",
        "return_value": "Optional[LoadEndianness]\n",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.qconfig_mapping.get_default_qat_qconfig_mapping",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.quantization.qconfig_mapping.get_default_qat_qconfig_mapping.html#torch.ao.quantization.qconfig_mapping.get_default_qat_qconfig_mapping",
        "api_signature": "torch.ao.quantization.qconfig_mapping.get_default_qat_qconfig_mapping(backend='x86', version=1)",
        "api_description": "Return the default QConfigMapping for quantization aware training.",
        "return_value": "",
        "parameters": "backend (*) – the quantization backend for the default qconfig mapping, should be\none of [“x86” (default), “fbgemm”, “qnnpack”, “onednn”]\nversion (*) – the version for the default qconfig mapping",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.qconfig_mapping.get_default_qconfig_mapping",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.quantization.qconfig_mapping.get_default_qconfig_mapping.html#torch.ao.quantization.qconfig_mapping.get_default_qconfig_mapping",
        "api_signature": "torch.ao.quantization.qconfig_mapping.get_default_qconfig_mapping(backend='x86', version=0)",
        "api_description": "Return the default QConfigMapping for post training quantization.",
        "return_value": "",
        "parameters": "backend (*) – the quantization backend for the default qconfig mapping, should be\none of [“x86” (default), “fbgemm”, “qnnpack”, “onednn”]\nversion (*) – the version for the default qconfig mapping",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.get_deterministic_debug_mode",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.get_deterministic_debug_mode.html#torch.get_deterministic_debug_mode",
        "api_signature": "torch.get_deterministic_debug_mode()",
        "api_description": "Returns the current value of the debug mode for deterministic\noperations. Refer to torch.set_deterministic_debug_mode()\ndocumentation for more details.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.get_device",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.get_device.html#torch.Tensor.get_device",
        "api_signature": "Tensor.get_device()",
        "api_description": "For CUDA tensors, this function returns the device ordinal of the GPU on which the tensor resides.\nFor CPU tensors, this function returns -1.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.TypedStorage.get_device",
        "api_url": "https://pytorch.org/docs/stable/storage.html#torch.TypedStorage.get_device",
        "api_signature": "get_device()",
        "api_description": "int",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.UntypedStorage.get_device",
        "api_url": "https://pytorch.org/docs/stable/storage.html#torch.UntypedStorage.get_device",
        "api_signature": "get_device()",
        "api_description": "int",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda.get_device_capability",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.cuda.get_device_capability.html#torch.cuda.get_device_capability",
        "api_signature": "torch.cuda.get_device_capability(device=None)",
        "api_description": "Get the cuda capability of a device.",
        "return_value": "the major and minor cuda capability of the device\n",
        "parameters": "device (torch.device or int, optional) – device for which to return the\ndevice capability. This function is a no-op if this argument is\na negative integer. It uses the current device, given by\ncurrent_device(), if device is None\n(default).",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.xpu.get_device_capability",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.xpu.get_device_capability.html#torch.xpu.get_device_capability",
        "api_signature": "torch.xpu.get_device_capability(device=None)",
        "api_description": "Get the xpu capability of a device.",
        "return_value": "the xpu capability dictionary of the device\n",
        "parameters": "device (torch.device or int or str, optional) – device for which to\nreturn the device capability. This function is a no-op if this\nargument is a negative integer. It uses the current device, given by\ncurrent_device(), if device is None\n(default).",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda.get_device_name",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.cuda.get_device_name.html#torch.cuda.get_device_name",
        "api_signature": "torch.cuda.get_device_name(device=None)",
        "api_description": "Get the name of a device.",
        "return_value": "the name of the device\n",
        "parameters": "device (torch.device or int, optional) – device for which to return the\nname. This function is a no-op if this argument is a negative\ninteger. It uses the current device, given by current_device(),\nif device is None (default).",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.xpu.get_device_name",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.xpu.get_device_name.html#torch.xpu.get_device_name",
        "api_signature": "torch.xpu.get_device_name(device=None)",
        "api_description": "Get the name of a device.",
        "return_value": "the name of the device\n",
        "parameters": "device (torch.device or int or str, optional) – device for which to\nreturn the name. This function is a no-op if this argument is a\nnegative integer. It uses the current device, given by current_device(),\nif device is None (default).",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda.get_device_properties",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.cuda.get_device_properties.html#torch.cuda.get_device_properties",
        "api_signature": "torch.cuda.get_device_properties(device)",
        "api_description": "Get the properties of a device.",
        "return_value": "the properties of the device\n",
        "parameters": "device (torch.device or int or str) – device for which to return the\nproperties of the device.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.xpu.get_device_properties",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.xpu.get_device_properties.html#torch.xpu.get_device_properties",
        "api_signature": "torch.xpu.get_device_properties(device=None)",
        "api_description": "Get the properties of a device.",
        "return_value": "the properties of the device\n",
        "parameters": "device (torch.device or int or str) – device for which to return the\nproperties of the device.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.hub.get_dir",
        "api_url": "https://pytorch.org/docs/stable/hub.html#torch.hub.get_dir",
        "api_signature": "torch.hub.get_dir()",
        "api_description": "Get the Torch Hub cache directory used for storing downloaded models & weights.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.agent.server.WorkerSpec.get_entrypoint_name",
        "api_url": "https://pytorch.org/docs/stable/elastic/agent.html#torch.distributed.elastic.agent.server.WorkerSpec.get_entrypoint_name",
        "api_signature": "get_entrypoint_name()",
        "api_description": "Get the entry point name.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.timer.TimerServer.get_expired_timers",
        "api_url": "https://pytorch.org/docs/stable/elastic/timer.html#torch.distributed.elastic.timer.TimerServer.get_expired_timers",
        "api_signature": "get_expired_timers(deadline)",
        "api_description": "Returns all expired timers for each worker_id. An expired timer\nis a timer for which the expiration_time is less than or equal to\nthe provided deadline.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.jit.ScriptModule.get_extra_state",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.jit.ScriptModule.html#torch.jit.ScriptModule.get_extra_state",
        "api_signature": "get_extra_state()",
        "api_description": "Return any extra state to include in the module’s state_dict.",
        "return_value": "Any extra state to store in the module’s state_dict\n",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.Module.get_extra_state",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.get_extra_state",
        "api_signature": "get_extra_state()",
        "api_description": "Return any extra state to include in the module’s state_dict.",
        "return_value": "Any extra state to store in the module’s state_dict\n",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.backends.mha.get_fastpath_enabled",
        "api_url": "https://pytorch.org/docs/stable/backends.html#torch.backends.mha.get_fastpath_enabled",
        "api_signature": "torch.backends.mha.get_fastpath_enabled()",
        "api_description": "Returns whether fast path for TransformerEncoder and MultiHeadAttention\nis enabled, or True if jit is scripting.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.get_float32_matmul_precision",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.get_float32_matmul_precision.html#torch.get_float32_matmul_precision",
        "api_signature": "torch.get_float32_matmul_precision()",
        "api_description": "Returns the current value of float32 matrix multiplication precision. Refer to\ntorch.set_float32_matmul_precision() documentation for more details.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda.get_gencode_flags",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.cuda.get_gencode_flags.html#torch.cuda.get_gencode_flags",
        "api_signature": "torch.cuda.get_gencode_flags()",
        "api_description": "Return NVCC gencode flags this library was compiled with.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.get_global_rank",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#torch.distributed.get_global_rank",
        "api_signature": "torch.distributed.get_global_rank(group, group_rank)",
        "api_description": "Translate a group rank into a global rank.",
        "return_value": "Global rank of group_rank relative to group\n",
        "parameters": "group (ProcessGroup) – ProcessGroup to find the global rank from.\ngroup_rank (int) – Group rank to query.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.autograd.graph.get_gradient_edge",
        "api_url": "https://pytorch.org/docs/stable/autograd.html#torch.autograd.graph.get_gradient_edge",
        "api_signature": "torch.autograd.graph.get_gradient_edge(tensor)",
        "api_description": "Get the gradient edge for computing the gradient of the given Tensor.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.autograd.get_gradients",
        "api_url": "https://pytorch.org/docs/stable/rpc.html#torch.distributed.autograd.get_gradients",
        "api_signature": "torch.distributed.autograd.get_gradients(context_id: int)",
        "api_description": "Retrieves a map from Tensor to the appropriate gradient for that Tensor\naccumulated in the provided context corresponding to the given context_id\nas part of the distributed autograd backward pass.",
        "return_value": "A map where the key is the Tensor and the value is the associated gradient\nfor that Tensor.\n",
        "parameters": "context_id (int) – The autograd context id for which we should retrieve the\ngradients.",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import torch.distributed.autograd as dist_autograd\n>>> with dist_autograd.context() as context_id:\n>>>     t1 = torch.rand((3, 3), requires_grad=True)\n>>>     t2 = torch.rand((3, 3), requires_grad=True)\n>>>     loss = t1 + t2\n>>>     dist_autograd.backward(context_id, [loss.sum()])\n>>>     grads = dist_autograd.get_gradients(context_id)\n>>>     print(grads[t1])\n>>>     print(grads[t2])\n\n\n"
    },
    {
        "api_name": "torch.distributed.get_group_rank",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#torch.distributed.get_group_rank",
        "api_signature": "torch.distributed.get_group_rank(group, global_rank)",
        "api_description": "Translate a global rank into a group rank.",
        "return_value": "Group rank of global_rank relative to group\n",
        "parameters": "group (ProcessGroup) – ProcessGroup to find the relative rank.\nglobal_rank (int) – Global rank to query.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.overrides.get_ignored_functions",
        "api_url": "https://pytorch.org/docs/stable/torch.overrides.html#torch.overrides.get_ignored_functions",
        "api_signature": "torch.overrides.get_ignored_functions()",
        "api_description": "Return public functions that cannot be overridden by __torch_function__.",
        "return_value": "A tuple of functions that are publicly available in the torch API but cannot\nbe overridden with __torch_function__. Mostly this is because none of the\narguments of these functions are tensors or tensor-likes.\n",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.lr_scheduler.ChainedScheduler.get_last_lr",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.ChainedScheduler.html#torch.optim.lr_scheduler.ChainedScheduler.get_last_lr",
        "api_signature": "get_last_lr()",
        "api_description": "Return last computed learning rate by current scheduler.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.lr_scheduler.ConstantLR.get_last_lr",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.ConstantLR.html#torch.optim.lr_scheduler.ConstantLR.get_last_lr",
        "api_signature": "get_last_lr()",
        "api_description": "Return last computed learning rate by current scheduler.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.lr_scheduler.CosineAnnealingLR.get_last_lr",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.CosineAnnealingLR.html#torch.optim.lr_scheduler.CosineAnnealingLR.get_last_lr",
        "api_signature": "get_last_lr()",
        "api_description": "Return last computed learning rate by current scheduler.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.lr_scheduler.CosineAnnealingWarmRestarts.get_last_lr",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.CosineAnnealingWarmRestarts.html#torch.optim.lr_scheduler.CosineAnnealingWarmRestarts.get_last_lr",
        "api_signature": "get_last_lr()",
        "api_description": "Return last computed learning rate by current scheduler.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.lr_scheduler.CyclicLR.get_last_lr",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.CyclicLR.html#torch.optim.lr_scheduler.CyclicLR.get_last_lr",
        "api_signature": "get_last_lr()",
        "api_description": "Return last computed learning rate by current scheduler.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.lr_scheduler.ExponentialLR.get_last_lr",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.ExponentialLR.html#torch.optim.lr_scheduler.ExponentialLR.get_last_lr",
        "api_signature": "get_last_lr()",
        "api_description": "Return last computed learning rate by current scheduler.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.lr_scheduler.LambdaLR.get_last_lr",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.LambdaLR.html#torch.optim.lr_scheduler.LambdaLR.get_last_lr",
        "api_signature": "get_last_lr()",
        "api_description": "Return last computed learning rate by current scheduler.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.lr_scheduler.LinearLR.get_last_lr",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.LinearLR.html#torch.optim.lr_scheduler.LinearLR.get_last_lr",
        "api_signature": "get_last_lr()",
        "api_description": "Return last computed learning rate by current scheduler.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.lr_scheduler.MultiplicativeLR.get_last_lr",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.MultiplicativeLR.html#torch.optim.lr_scheduler.MultiplicativeLR.get_last_lr",
        "api_signature": "get_last_lr()",
        "api_description": "Return last computed learning rate by current scheduler.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.lr_scheduler.MultiStepLR.get_last_lr",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.MultiStepLR.html#torch.optim.lr_scheduler.MultiStepLR.get_last_lr",
        "api_signature": "get_last_lr()",
        "api_description": "Return last computed learning rate by current scheduler.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.lr_scheduler.OneCycleLR.get_last_lr",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.OneCycleLR.html#torch.optim.lr_scheduler.OneCycleLR.get_last_lr",
        "api_signature": "get_last_lr()",
        "api_description": "Return last computed learning rate by current scheduler.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.lr_scheduler.PolynomialLR.get_last_lr",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.PolynomialLR.html#torch.optim.lr_scheduler.PolynomialLR.get_last_lr",
        "api_signature": "get_last_lr()",
        "api_description": "Return last computed learning rate by current scheduler.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.lr_scheduler.ReduceLROnPlateau.get_last_lr",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.ReduceLROnPlateau.html#torch.optim.lr_scheduler.ReduceLROnPlateau.get_last_lr",
        "api_signature": "get_last_lr()",
        "api_description": "Return last computed learning rate by current scheduler.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.lr_scheduler.SequentialLR.get_last_lr",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.SequentialLR.html#torch.optim.lr_scheduler.SequentialLR.get_last_lr",
        "api_signature": "get_last_lr()",
        "api_description": "Return last computed learning rate by current scheduler.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.lr_scheduler.StepLR.get_last_lr",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.StepLR.html#torch.optim.lr_scheduler.StepLR.get_last_lr",
        "api_signature": "get_last_lr()",
        "api_description": "Return last computed learning rate by current scheduler.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.ns._numeric_suite.get_logger_dict",
        "api_url": "https://pytorch.org/docs/stable/torch.ao.ns._numeric_suite.html#torch.ao.ns._numeric_suite.get_logger_dict",
        "api_signature": "torch.ao.ns._numeric_suite.get_logger_dict(mod, prefix='')",
        "api_description": "Traverse the modules and save all logger stats into target dict.\nThis is mainly used for quantization accuracy debug.",
        "return_value": "the dictionary used to save all logger stats\n",
        "parameters": "mod (Module) – module we want to save all logger stats\nprefix (str) – prefix for the current module",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.events.get_logging_handler",
        "api_url": "https://pytorch.org/docs/stable/elastic/events.html#torch.distributed.elastic.events.get_logging_handler",
        "api_signature": "torch.distributed.elastic.events.get_logging_handler(destination='null')",
        "api_description": "Handler",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.lr_scheduler.CyclicLR.get_lr",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.CyclicLR.html#torch.optim.lr_scheduler.CyclicLR.get_lr",
        "api_signature": "get_lr()",
        "api_description": "Calculates the learning rate at batch index. This function treats\nself.last_epoch as the last batch index.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.ns._numeric_suite.get_matching_activations",
        "api_url": "https://pytorch.org/docs/stable/torch.ao.ns._numeric_suite.html#torch.ao.ns._numeric_suite.get_matching_activations",
        "api_signature": "torch.ao.ns._numeric_suite.get_matching_activations(float_module, q_module)",
        "api_description": "Find the matching activation between float and quantized modules.",
        "return_value": "dict with key corresponding to quantized module names and each\nentry being a dictionary with two keys ‘float’ and ‘quantized’, containing\nthe matching float and quantized activations\n",
        "parameters": "float_module (Module) – float module used to generate the q_module\nq_module (Module) – module quantized from float_module",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.checkpoint.state_dict.get_model_state_dict",
        "api_url": "https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_model_state_dict",
        "api_signature": "torch.distributed.checkpoint.state_dict.get_model_state_dict(model, *, submodules=None, options=None)",
        "api_description": "Return the model state_dict of model.",
        "return_value": "The state_dict for model.\n",
        "parameters": "model (nn.Module) – the nn.Module to the model.\nsubmodules (Optional[Set[Module]]) – Optional[Set[nn.Module]]: only return the model parameters\nthat belong to the submodules.\noptions (StateDictOptions) – the options to control how\nmodel state_dict and optimizer state_dict should be returned. See\nStateDictOptions for the details.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.nn.api.remote_module.RemoteModule.get_module_rref",
        "api_url": "https://pytorch.org/docs/stable/rpc.html#torch.distributed.nn.api.remote_module.RemoteModule.get_module_rref",
        "api_signature": "get_module_rref()",
        "api_description": "Return an RRef (RRef[nn.Module]) pointing to the remote module.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.symbolic_shapes.ShapeEnv.get_nontrivial_guards",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.fx.experimental.symbolic_shapes.ShapeEnv.html#torch.fx.experimental.symbolic_shapes.ShapeEnv.get_nontrivial_guards",
        "api_signature": "get_nontrivial_guards()",
        "api_description": "Returns a list of guard expressions that aren’t statically known (i.e. not trivial)",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.get_num_interop_threads",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.get_num_interop_threads.html#torch.get_num_interop_threads",
        "api_signature": "torch.get_num_interop_threads()",
        "api_description": "Returns the number of threads used for inter-op parallelism on CPU\n(e.g. in JIT interpreter)",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.get_num_threads",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.get_num_threads.html#torch.get_num_threads",
        "api_signature": "torch.get_num_threads()",
        "api_description": "Returns the number of threads used for parallelizing CPU operations",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.observer.get_observer_state_dict",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.quantization.observer.get_observer_state_dict.html#torch.ao.quantization.observer.get_observer_state_dict",
        "api_signature": "torch.ao.quantization.observer.get_observer_state_dict(mod)",
        "api_description": "Returns the state dict corresponding to the observer stats.\nTraverse the model state_dict and extract out the stats.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.onnx.OnnxRegistry.get_op_functions",
        "api_url": "https://pytorch.org/docs/stable/onnx_dynamo.html#torch.onnx.OnnxRegistry.get_op_functions",
        "api_signature": "get_op_functions(namespace, op_name, overload=None)",
        "api_description": "Returns a list of ONNXFunctions for the given op: torch.ops.<namespace>.<op_name>.<overload>.",
        "return_value": "A list of ONNXFunctions corresponding to the given name, or None if\nthe name is not in the registry.\n",
        "parameters": "namespace (str) – The namespace of the operator to get.\nop_name (str) – The name of the operator to get.\noverload (Optional[str]) – The overload of the operator to get. If it’s default overload,\nleave it to None.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.backends.opt_einsum.get_opt_einsum",
        "api_url": "https://pytorch.org/docs/stable/backends.html#torch.backends.opt_einsum.get_opt_einsum",
        "api_signature": "torch.backends.opt_einsum.get_opt_einsum()",
        "api_description": "Return the opt_einsum package if opt_einsum is currently available, else None.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.checkpoint.state_dict.get_optimizer_state_dict",
        "api_url": "https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_optimizer_state_dict",
        "api_signature": "torch.distributed.checkpoint.state_dict.get_optimizer_state_dict(model, optimizers, *, submodules=None, options=None)",
        "api_description": "Return the combined state_dict for optimizers.",
        "return_value": "The state_dict for optimizers.\n",
        "parameters": "model (nn.Module) – the nn.Module to the model.\noptimizers (Union[None, Optimizer, Iterable[Optimizer]]) – The optimizers that are used to optimize model.\nsubmodules (Optional[Set[Module]]) – Optional[Set[nn.Module]]: only return the model parameters\nthat belong to the submodules.\noptions (StateDictOptions) – the options to control how\nmodel state_dict and optimizer state_dict should be returned. See\nStateDictOptions for the details.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.overrides.get_overridable_functions",
        "api_url": "https://pytorch.org/docs/stable/torch.overrides.html#torch.overrides.get_overridable_functions",
        "api_signature": "torch.overrides.get_overridable_functions()",
        "api_description": "List functions that are overridable via __torch_function__",
        "return_value": "A dictionary that maps namespaces that contain overridable functions\nto functions in that namespace that can be overridden.\n",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.__future__.get_overwrite_module_params_on_conversion",
        "api_url": "https://pytorch.org/docs/stable/future_mod.html#torch.__future__.get_overwrite_module_params_on_conversion",
        "api_signature": "torch.__future__.get_overwrite_module_params_on_conversion()",
        "api_description": "Returns whether to assign new tensors to the parameters instead of changing the\nexisting parameters in-place when converting an torch.nn.Module. Defaults to False.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.jit.ScriptModule.get_parameter",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.jit.ScriptModule.html#torch.jit.ScriptModule.get_parameter",
        "api_signature": "get_parameter(target)",
        "api_description": "Return the parameter given by target if it exists, otherwise throw an error.",
        "return_value": "The Parameter referenced by target\n",
        "parameters": "target (str) – The fully-qualified string name of the Parameter\nto look for. (See get_submodule for how to specify a\nfully-qualified string.)",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.Module.get_parameter",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.get_parameter",
        "api_signature": "get_parameter(target)",
        "api_description": "Return the parameter given by target if it exists, otherwise throw an error.",
        "return_value": "The Parameter referenced by target\n",
        "parameters": "target (str) – The fully-qualified string name of the Parameter\nto look for. (See get_submodule for how to specify a\nfully-qualified string.)",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.get_process_group_ranks",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#torch.distributed.get_process_group_ranks",
        "api_signature": "torch.distributed.get_process_group_ranks(group)",
        "api_description": "Get all ranks associated with group.",
        "return_value": "List of global ranks ordered by group rank.\n",
        "parameters": "group (ProcessGroup) – ProcessGroup to get all ranks from.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.get_rank",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#torch.distributed.get_rank",
        "api_signature": "torch.distributed.get_rank(group=None)",
        "api_description": "Return the rank of the current process in the provided group, default otherwise.",
        "return_value": "The rank of the process group\n-1, if not part of the group\n",
        "parameters": "group (ProcessGroup, optional) – The process group to work on. If None,\nthe default process group will be used.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.package.PackageExporter.get_rdeps",
        "api_url": "https://pytorch.org/docs/stable/package.html#torch.package.PackageExporter.get_rdeps",
        "api_signature": "get_rdeps(module_name)",
        "api_description": "Return a list of all modules which depend on the module module_name.",
        "return_value": "A list containing the names of modules which depend on module_name.\n",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.export.graph_signature.ExportGraphSignature.get_replace_hook",
        "api_url": "https://pytorch.org/docs/stable/export.html#torch.export.graph_signature.ExportGraphSignature.get_replace_hook",
        "api_signature": "get_replace_hook()",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.get_rng_state",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.get_rng_state.html#torch.get_rng_state",
        "api_signature": "torch.get_rng_state()",
        "api_description": "Returns the random number generator state as a torch.ByteTensor.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda.get_rng_state",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.cuda.get_rng_state.html#torch.cuda.get_rng_state",
        "api_signature": "torch.cuda.get_rng_state(device='cuda')",
        "api_description": "Return the random number generator state of the specified GPU as a ByteTensor.",
        "return_value": "",
        "parameters": "device (torch.device or int, optional) – The device to return the RNG state of.\nDefault: 'cuda' (i.e., torch.device('cuda'), the current CUDA device).",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.mps.get_rng_state",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.mps.get_rng_state.html#torch.mps.get_rng_state",
        "api_signature": "torch.mps.get_rng_state()",
        "api_description": "Returns the random number generator state as a ByteTensor.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.random.get_rng_state",
        "api_url": "https://pytorch.org/docs/stable/random.html#torch.random.get_rng_state",
        "api_signature": "torch.random.get_rng_state()",
        "api_description": "Returns the random number generator state as a torch.ByteTensor.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.xpu.get_rng_state",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.xpu.get_rng_state.html#torch.xpu.get_rng_state",
        "api_signature": "torch.xpu.get_rng_state(device='xpu')",
        "api_description": "Return the random number generator state of the specified GPU as a ByteTensor.",
        "return_value": "",
        "parameters": "device (torch.device or int, optional) – The device to return the RNG state of.\nDefault: 'xpu' (i.e., torch.device('xpu'), the current XPU device).",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda.get_rng_state_all",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.cuda.get_rng_state_all.html#torch.cuda.get_rng_state_all",
        "api_signature": "torch.cuda.get_rng_state_all()",
        "api_description": "Return a list of ByteTensor representing the random number states of all devices.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.xpu.get_rng_state_all",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.xpu.get_rng_state_all.html#torch.xpu.get_rng_state_all",
        "api_signature": "torch.xpu.get_rng_state_all()",
        "api_description": "Return a list of ByteTensor representing the random number states of all devices.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.rendezvous.RendezvousHandler.get_run_id",
        "api_url": "https://pytorch.org/docs/stable/elastic/rendezvous.html#torch.distributed.elastic.rendezvous.RendezvousHandler.get_run_id",
        "api_signature": "get_run_id()",
        "api_description": "Return the run id of the rendezvous.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.multiprocessing.get_sharing_strategy",
        "api_url": "https://pytorch.org/docs/stable/multiprocessing.html#torch.multiprocessing.get_sharing_strategy",
        "api_signature": "torch.multiprocessing.get_sharing_strategy()",
        "api_description": "Return the current strategy for sharing CPU tensors.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.rendezvous.c10d_rendezvous_backend.C10dRendezvousBackend.get_state",
        "api_url": "https://pytorch.org/docs/stable/elastic/rendezvous.html#torch.distributed.elastic.rendezvous.c10d_rendezvous_backend.C10dRendezvousBackend.get_state",
        "api_signature": "get_state()",
        "api_description": "See base class.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.rendezvous.dynamic_rendezvous.RendezvousBackend.get_state",
        "api_url": "https://pytorch.org/docs/stable/elastic/rendezvous.html#torch.distributed.elastic.rendezvous.dynamic_rendezvous.RendezvousBackend.get_state",
        "api_signature": "get_state()",
        "api_description": "Get the rendezvous state.",
        "return_value": "A tuple of the encoded rendezvous state and its fencing token or\nNone if no state is found in the backend.\n",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.rendezvous.etcd_rendezvous_backend.EtcdRendezvousBackend.get_state",
        "api_url": "https://pytorch.org/docs/stable/elastic/rendezvous.html#torch.distributed.elastic.rendezvous.etcd_rendezvous_backend.EtcdRendezvousBackend.get_state",
        "api_signature": "get_state()",
        "api_description": "See base class.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Generator.get_state",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator.get_state",
        "api_signature": "get_state()",
        "api_description": "Returns the Generator state as a torch.ByteTensor.",
        "return_value": "A torch.ByteTensor which contains all the necessary bits\nto restore a Generator to a specific point in time.\n",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.checkpoint.state_dict.get_state_dict",
        "api_url": "https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict",
        "api_signature": "torch.distributed.checkpoint.state_dict.get_state_dict(model, optimizers, *, submodules=None, options=None)",
        "api_description": "Return the model state_dict and optimizers state_dict.",
        "return_value": "Tuple that contain model state_dict and optimizer state_dict.\n",
        "parameters": "model (nn.Module) – the nn.Module to the model.\noptimizers (Union[None, Optimizer, Iterable[Optimizer]]) – The optimizers that are used to optimize model.\nsubmodules (Optional[Set[Module]]) – Optional[Set[nn.Module]]: only return the model parameters\nthat belong to the submodules.\noptions (StateDictOptions) – the options to control how\nmodel state_dict and optimizer state_dict should be returned. See\nStateDictOptions for the details.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.fsdp.FullyShardedDataParallel.get_state_dict_type",
        "api_url": "https://pytorch.org/docs/stable/fsdp.html#torch.distributed.fsdp.FullyShardedDataParallel.get_state_dict_type",
        "api_signature": "get_state_dict_type(module)",
        "api_description": "Get the state_dict_type and the corresponding configurations for the FSDP modules rooted at module.",
        "return_value": "A StateDictSettings containing the state_dict_type and\nstate_dict / optim_state_dict configs that are currently set.\n",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.jit.ScriptModule.get_submodule",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.jit.ScriptModule.html#torch.jit.ScriptModule.get_submodule",
        "api_signature": "get_submodule(target)",
        "api_description": "Return the submodule given by target if it exists, otherwise throw an error.",
        "return_value": "The submodule referenced by target\n",
        "parameters": "target (str) – The fully-qualified string name of the submodule\nto look for. (See above example for how to specify a\nfully-qualified string.)",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.Module.get_submodule",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.get_submodule",
        "api_signature": "get_submodule(target)",
        "api_description": "Return the submodule given by target if it exists, otherwise throw an error.",
        "return_value": "The submodule referenced by target\n",
        "parameters": "target (str) – The fully-qualified string name of the submodule\nto look for. (See above example for how to specify a\nfully-qualified string.)",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.multiprocessing.subprocess_handler.handlers.get_subprocess_handler",
        "api_url": "https://pytorch.org/docs/stable/elastic/subprocess_handler.html#torch.distributed.elastic.multiprocessing.subprocess_handler.handlers.get_subprocess_handler",
        "api_signature": "torch.distributed.elastic.multiprocessing.subprocess_handler.handlers.get_subprocess_handler(entrypoint, args, env, stdout, stderr, local_rank_id)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.__future__.get_swap_module_params_on_conversion",
        "api_url": "https://pytorch.org/docs/stable/future_mod.html#torch.__future__.get_swap_module_params_on_conversion",
        "api_signature": "torch.__future__.get_swap_module_params_on_conversion()",
        "api_description": "Returns whether to use swap_tensors() instead of setting .data to\nchange the existing parameters in-place when converting an nn.Module. Defaults to False.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda.get_sync_debug_mode",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.cuda.get_sync_debug_mode.html#torch.cuda.get_sync_debug_mode",
        "api_signature": "torch.cuda.get_sync_debug_mode()",
        "api_description": "Return current value of debug mode for cuda synchronizing operations.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.overrides.get_testing_overrides",
        "api_url": "https://pytorch.org/docs/stable/torch.overrides.html#torch.overrides.get_testing_overrides",
        "api_signature": "torch.overrides.get_testing_overrides()",
        "api_description": "Return a dict containing dummy overrides for all overridable functions",
        "return_value": "A dictionary that maps overridable functions in the PyTorch API to\nlambda functions that have the same signature as the real function\nand unconditionally return -1. These lambda functions are useful\nfor testing API coverage for a type that defines __torch_function__.\n",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.package.PackageExporter.get_unique_id",
        "api_url": "https://pytorch.org/docs/stable/package.html#torch.package.PackageExporter.get_unique_id",
        "api_signature": "get_unique_id()",
        "api_description": "Get an id. This id is guaranteed to only be handed out once for this package.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.agent.server.ElasticAgent.get_worker_group",
        "api_url": "https://pytorch.org/docs/stable/elastic/agent.html#torch.distributed.elastic.agent.server.ElasticAgent.get_worker_group",
        "api_signature": "get_worker_group(role='default')",
        "api_description": "Return the WorkerGroup for the given role.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.rpc.get_worker_info",
        "api_url": "https://pytorch.org/docs/stable/rpc.html#torch.distributed.rpc.get_worker_info",
        "api_signature": "torch.distributed.rpc.get_worker_info(worker_name=None)",
        "api_description": "Get WorkerInfo of a given worker name.\nUse this WorkerInfo to avoid passing an\nexpensive string on every invocation.",
        "return_value": "WorkerInfo instance for the given\nworker_name or WorkerInfo of the\ncurrent worker if worker_name is None.\n",
        "parameters": "worker_name (str) – the string name of a worker. If None, return the\nthe id of the current worker. (default None)",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.data.get_worker_info",
        "api_url": "https://pytorch.org/docs/stable/data.html#torch.utils.data.get_worker_info",
        "api_signature": "torch.utils.data.get_worker_info()",
        "api_description": "Returns the information about the current\nDataLoader iterator worker process.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.get_world_size",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#torch.distributed.get_world_size",
        "api_signature": "torch.distributed.get_world_size(group=None)",
        "api_description": "Return the number of processes in the current process group.",
        "return_value": "The world size of the process group\n-1, if not part of the group\n",
        "parameters": "group (ProcessGroup, optional) – The process group to work on. If None,\nthe default process group will be used.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.Tracer.getattr",
        "api_url": "https://pytorch.org/docs/stable/fx.html#torch.fx.Tracer.getattr",
        "api_signature": "getattr(attr, attr_val, parameter_proxy_cache)",
        "api_description": "Method that specifies the behavior of this Tracer when we call getattr\non a call to an nn.Module instance.",
        "return_value": "The return value from the getattr call.\n",
        "parameters": "attr (str) – The name of the attribute being queried\nattr_val (Any) – The value of the attribute\nparameter_proxy_cache (Dict[str, Any]) – A cache of attr names to proxies",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.utils.prune.global_unstructured",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.global_unstructured.html#torch.nn.utils.prune.global_unstructured",
        "api_signature": "torch.nn.utils.prune.global_unstructured(parameters, pruning_method, importance_scores=None, **kwargs)",
        "api_description": "Globally prunes tensors corresponding to all parameters in parameters by applying the specified pruning_method.",
        "return_value": "",
        "parameters": "parameters (Iterable of (module, name) tuples) – parameters of\nthe model to prune in a global fashion, i.e. by aggregating all\nweights prior to deciding which ones to prune. module must be of\ntype nn.Module, and name must be a string.\npruning_method (function) – a valid pruning function from this module,\nor a custom one implemented by the user that satisfies the\nimplementation guidelines and has PRUNING_TYPE='unstructured'.\nimportance_scores (dict) – a dictionary mapping (module, name) tuples to\nthe corresponding parameter’s importance scores tensor. The tensor\nshould be the same shape as the parameter, and is used for computing\nmask for pruning.\nIf unspecified or None, the parameter will be used in place of its\nimportance scores.\nkwargs – other keyword arguments such as:\namount (int or float): quantity of parameters to prune across the\nspecified parameters.\nIf float, should be between 0.0 and 1.0 and represent the\nfraction of parameters to prune. If int, it represents the\nabsolute number of parameters to prune.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.GLU",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.GLU.html#torch.nn.GLU",
        "api_signature": "torch.nn.GLU(dim=-1)",
        "api_description": "Applies the gated linear unit function.",
        "return_value": "",
        "parameters": "dim (int) – the dimension on which to split the input. Default: -1",
        "input_shape": "\nInput: (∗1,N,∗2)(\\ast_1, N, \\ast_2)(∗1​,N,∗2​) where * means, any number of additional\ndimensions\nOutput: (∗1,M,∗2)(\\ast_1, M, \\ast_2)(∗1​,M,∗2​) where M=N/2M=N/2M=N/2\n\n",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.functional.glu",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.functional.glu.html#torch.nn.functional.glu",
        "api_signature": "torch.nn.functional.glu(input, dim=-1)",
        "api_description": "The gated linear unit. Computes:",
        "return_value": "",
        "parameters": "input (Tensor) – input tensor\ndim (int) – dimension on which to split the input. Default: -1",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.grad",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.grad.html#torch.Tensor.grad",
        "api_signature": null,
        "api_description": "This attribute is None by default and becomes a Tensor the first time a call to\nbackward() computes gradients for self.\nThe attribute will then contain the gradients computed and future calls to\nbackward() will accumulate (add) gradients into it.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.autograd.grad",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.autograd.grad.html#torch.autograd.grad",
        "api_signature": "torch.autograd.grad(outputs, inputs, grad_outputs=None, retain_graph=None, create_graph=False, only_inputs=True, allow_unused=None, is_grads_batched=False, materialize_grads=False)",
        "api_description": "Computes and returns the sum of gradients of outputs with respect to\nthe inputs.",
        "return_value": "",
        "parameters": "outputs (sequence of Tensor) – outputs of the differentiated function.\ninputs (sequence of Tensor or GradientEdge) – Inputs w.r.t. which the gradient will be\nreturned (and not accumulated into .grad).\ngrad_outputs (sequence of Tensor) – The “vector” in the vector-Jacobian product.\nUsually gradients w.r.t. each output. None values can be specified for scalar\nTensors or ones that don’t require grad. If a None value would be acceptable\nfor all grad_tensors, then this argument is optional. Default: None.\nretain_graph (bool, optional) – If False, the graph used to compute the grad\nwill be freed. Note that in nearly all cases setting this option to True\nis not needed and often can be worked around in a much more efficient\nway. Defaults to the value of create_graph.\ncreate_graph (bool, optional) – If True, graph of the derivative will\nbe constructed, allowing to compute higher order derivative products.\nDefault: False.\nallow_unused (Optional[bool], optional) – If False, specifying inputs\nthat were not used when computing outputs (and therefore their grad is\nalways zero) is an error. Defaults to the value of materialize_grads.\nis_grads_batched (bool, optional) – If True, the first dimension of each\ntensor in grad_outputs will be interpreted as the batch dimension.\nInstead of computing a single vector-Jacobian product, we compute a\nbatch of vector-Jacobian products for each “vector” in the batch.\nWe use the vmap prototype feature as the backend to vectorize calls\nto the autograd engine so that this computation can be performed in a\nsingle call. This should lead to performance improvements when compared\nto manually looping and performing backward multiple times. Note that\ndue to this feature being experimental, there may be performance\ncliffs. Please use torch._C._debug_only_display_vmap_fallback_warnings(True)\nto show any performance warnings and file an issue on github if warnings exist\nfor your use case. Defaults to False.\nmaterialize_grads (bool, optional) – If True, set the gradient for unused inputs\nto zero instead of None. This is useful when computing higher-order derivatives.\nIf materialize_grads is True and allow_unused is False, an error\nwill be raised. Defaults to False.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.func.grad",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.func.grad.html#torch.func.grad",
        "api_signature": "torch.func.grad(func, argnums=0, has_aux=False)",
        "api_description": "grad operator helps computing gradients of func with respect to the\ninput(s) specified by argnums. This operator can be nested to\ncompute higher-order gradients.",
        "return_value": "Function to compute gradients with respect to its inputs. By default, the output of\nthe function is the gradient tensor(s) with respect to the first argument.\nIf specified has_aux equals True, tuple of gradients and output auxiliary objects\nis returned. If argnums is a tuple of integers, a tuple of output gradients with\nrespect to each argnums value is returned.\n",
        "parameters": "func (Callable) – A Python function that takes one or more arguments.\nMust return a single-element Tensor. If specified has_aux equals True,\nfunction can return a tuple of single-element Tensor and other auxiliary objects:\n(output, aux).\nargnums (int or Tuple[int]) – Specifies arguments to compute gradients with respect to.\nargnums can be single integer or tuple of integers. Default: 0.\nhas_aux (bool) – Flag indicating that func returns a tensor and other\nauxiliary objects: (output, aux). Default: False.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.func.grad_and_value",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.func.grad_and_value.html#torch.func.grad_and_value",
        "api_signature": "torch.func.grad_and_value(func, argnums=0, has_aux=False)",
        "api_description": "Returns a function to compute a tuple of the gradient and primal, or\nforward, computation.",
        "return_value": "Function to compute a tuple of gradients with respect to its inputs\nand the forward computation. By default, the output of the function is\na tuple of the gradient tensor(s) with respect to the first argument\nand the primal computation. If specified has_aux equals\nTrue, tuple of gradients and tuple of the forward computation with\noutput auxiliary objects is returned. If argnums is a tuple of\nintegers, a tuple of a tuple of the output gradients with respect to\neach argnums value and the forward computation is returned.\n",
        "parameters": "func (Callable) – A Python function that takes one or more arguments.\nMust return a single-element Tensor. If specified has_aux\nequals True, function can return a tuple of single-element\nTensor and other auxiliary objects: (output, aux).\nargnums (int or Tuple[int]) – Specifies arguments to compute gradients\nwith respect to. argnums can be single integer or tuple of\nintegers. Default: 0.\nhas_aux (bool) – Flag indicating that func returns a tensor and\nother auxiliary objects: (output, aux). Default: False.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.GradBucket",
        "api_url": "https://pytorch.org/docs/stable/ddp_comm_hooks.html#torch.distributed.GradBucket",
        "api_signature": null,
        "api_description": "This class mainly passes a flattened gradient tensor\n(returned by buffer())\nto DDP communication hook.\nThis tensor can be further decomposed into a list of per-parameter tensors within this bucket\n(returned by get_per_parameter_tensors())\nto apply layer-wise operations.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.autograd.gradcheck.gradcheck",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.autograd.gradcheck.gradcheck.html#torch.autograd.gradcheck.gradcheck",
        "api_signature": "torch.autograd.gradcheck.gradcheck(func, inputs, *, eps=1e-06, atol=1e-05, rtol=0.001, raise_exception=True, nondet_tol=0.0, check_undefined_grad=True, check_grad_dtypes=False, check_batched_grad=False, check_batched_forward_grad=False, check_forward_ad=False, check_backward_ad=True, fast_mode=False, masked=None)",
        "api_description": "Check gradients computed via small finite differences against analytical\ngradients wrt tensors in inputs that are of floating point or complex type\nand with requires_grad=True.",
        "return_value": "True if all differences satisfy allclose condition\n",
        "parameters": "func (function) – a Python function that takes Tensor inputs and returns\na Tensor or a tuple of Tensors\ninputs (tuple of Tensor or Tensor) – inputs to the function\neps (float, optional) – perturbation for finite differences\natol (float, optional) – absolute tolerance\nrtol (float, optional) – relative tolerance\nraise_exception (bool, optional) – indicating whether to raise an exception if\nthe check fails. The exception gives more information about the\nexact nature of the failure. This is helpful when debugging gradchecks.\nnondet_tol (float, optional) – tolerance for non-determinism. When running\nidentical inputs through the differentiation, the results must either match\nexactly (default, 0.0) or be within this tolerance.\ncheck_undefined_grad (bool, optional) – if True, check if undefined output grads\nare supported and treated as zeros, for Tensor outputs.\ncheck_batched_grad (bool, optional) – if True, check if we can compute\nbatched gradients using prototype vmap support. Defaults to False.\ncheck_batched_forward_grad (bool, optional) – if True, checks if we can compute\nbatched forward gradients using forward ad and prototype vmap support. Defaults to False.\ncheck_forward_ad (bool, optional) – if True, check that the gradients computed with forward\nmode AD match the numerical ones. Defaults to False.\ncheck_backward_ad (bool, optional) – if False, do not perform any checks that rely on\nbackward mode AD to be implemented. Defaults to True.\nfast_mode (bool, optional) – Fast mode for gradcheck and gradgradcheck is currently only\nimplemented for R to R functions. If none of the inputs and outputs are complex\na faster implementation of gradcheck that no longer computes the entire jacobian\nis run; otherwise, we fall back to the slow implementation.\nmasked (bool, optional) – if True, the gradients of unspecified elements of\nsparse tensors are ignored. Defaults to False.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.autograd.gradcheck.GradcheckError",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.autograd.gradcheck.GradcheckError.html#torch.autograd.gradcheck.GradcheckError",
        "api_signature": null,
        "api_description": "Error raised by gradcheck() and gradgradcheck().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.autograd.gradcheck.gradgradcheck",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.autograd.gradcheck.gradgradcheck.html#torch.autograd.gradcheck.gradgradcheck",
        "api_signature": "torch.autograd.gradcheck.gradgradcheck(func, inputs, grad_outputs=None, *, eps=1e-06, atol=1e-05, rtol=0.001, gen_non_contig_grad_outputs=False, raise_exception=True, nondet_tol=0.0, check_undefined_grad=True, check_grad_dtypes=False, check_batched_grad=False, check_fwd_over_rev=False, check_rev_over_rev=True, fast_mode=False, masked=False)",
        "api_description": "Check gradients of gradients computed via small finite differences\nagainst analytical gradients wrt tensors in inputs and\ngrad_outputs that are of floating point or complex type and with\nrequires_grad=True.",
        "return_value": "True if all differences satisfy allclose condition\n",
        "parameters": "func (function) – a Python function that takes Tensor inputs and returns\na Tensor or a tuple of Tensors\ninputs (tuple of Tensor or Tensor) – inputs to the function\ngrad_outputs (tuple of Tensor or Tensor, optional) – The gradients with\nrespect to the function’s outputs.\neps (float, optional) – perturbation for finite differences\natol (float, optional) – absolute tolerance\nrtol (float, optional) – relative tolerance\ngen_non_contig_grad_outputs (bool, optional) – if grad_outputs is\nNone and gen_non_contig_grad_outputs is True, the\nrandomly generated gradient outputs are made to be noncontiguous\nraise_exception (bool, optional) – indicating whether to raise an exception if\nthe check fails. The exception gives more information about the\nexact nature of the failure. This is helpful when debugging gradchecks.\nnondet_tol (float, optional) – tolerance for non-determinism. When running\nidentical inputs through the differentiation, the results must either match\nexactly (default, 0.0) or be within this tolerance. Note that a small amount\nof nondeterminism in the gradient will lead to larger inaccuracies in\nthe second derivative.\ncheck_undefined_grad (bool, optional) – if True, check if undefined output grads\nare supported and treated as zeros\ncheck_batched_grad (bool, optional) – if True, check if we can compute\nbatched gradients using prototype vmap support. Defaults to False.\nfast_mode (bool, optional) – if True, run a faster implementation of gradgradcheck that\nno longer computes the entire jacobian.\nmasked (bool, optional) – if True, the gradients of unspecified elements of\nsparse tensors are ignored (default, False).",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.gradient",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.gradient.html#torch.gradient",
        "api_signature": "torch.gradient(input, *, spacing=1, dim=None, edge_order=1)",
        "api_description": "Estimates the gradient of a function g:Rn→Rg : \\mathbb{R}^n \\rightarrow \\mathbb{R}g:Rn→R in\none or more dimensions using the second-order accurate central differences method and\neither first or second order estimates at the boundaries.",
        "return_value": "",
        "parameters": "input (Tensor) – the tensor that represents the values of the function\nspacing (scalar, list of scalar, list of Tensor, optional) – spacing can be used to modify\nhow the input tensor’s indices relate to sample coordinates. If spacing is a scalar then\nthe indices are multiplied by the scalar to produce the coordinates. For example, if spacing=2 the\nindices (1, 2, 3) become coordinates (2, 4, 6). If spacing is a list of scalars then the corresponding\nindices are multiplied. For example, if spacing=(2, -1, 3) the indices (1, 2, 3) become coordinates (2, -2, 9).\nFinally, if spacing is a list of one-dimensional tensors then each tensor specifies the coordinates for\nthe corresponding dimension. For example, if the indices are (1, 2, 3) and the tensors are (t0, t1, t2), then\nthe coordinates are (t0[1], t1[2], t2[3])\ndim (int, list of int, optional) – the dimension or dimensions to approximate the gradient over.  By default\nthe partial  gradient in every dimension is computed. Note that when dim is  specified the elements of\nthe spacing argument must correspond with the specified dims.”\nedge_order (int, optional) – 1 or 2, for first-order or\nsecond-order\nestimation of the boundary (“edge”) values, respectively.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.autograd.graph.GradientEdge",
        "api_url": "https://pytorch.org/docs/stable/autograd.html#torch.autograd.graph.GradientEdge",
        "api_signature": "torch.autograd.graph.GradientEdge(node, output_nr)",
        "api_description": "Object representing a given gradient edge within the autograd graph.\nTo get the gradient edge where a given Tensor gradient will be computed,\nyou can do edge = autograd.graph.get_gradient_edge(tensor).",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.GradBucket.gradients",
        "api_url": "https://pytorch.org/docs/stable/ddp_comm_hooks.html#torch.distributed.GradBucket.gradients",
        "api_signature": "torch.distributed.GradBucket.gradients(self: torch._C._distributed_c10d.GradBucket)",
        "api_description": "A list of torch.Tensor. Each tensor in the list corresponds to a gradient.",
        "return_value": "A list of torch.Tensor. Each tensor in the list corresponds to a gradient.\n",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda.amp.GradScaler",
        "api_url": "https://pytorch.org/docs/stable/amp.html#torch.cuda.amp.GradScaler",
        "api_signature": "torch.cuda.amp.GradScaler(init_scale=65536.0, growth_factor=2.0, backoff_factor=0.5, growth_interval=2000, enabled=True)",
        "api_description": "See torch.amp.GradScaler.\ntorch.cuda.amp.GradScaler(args...) is equivalent to torch.amp.GradScaler(\"cuda\", args...)",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda.graph",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.cuda.graph.html#torch.cuda.graph",
        "api_signature": "torch.cuda.graph(cuda_graph, pool=None, stream=None, capture_error_mode='global')",
        "api_description": "Context-manager that captures CUDA work into a torch.cuda.CUDAGraph object for later replay.",
        "return_value": "",
        "parameters": "cuda_graph (torch.cuda.CUDAGraph) – Graph object used for capture.\npool (optional) – Opaque token (returned by a call to graph_pool_handle() or\nother_Graph_instance.pool()) hinting this graph’s capture\nmay share memory from the specified pool. See Graph memory management.\nstream (torch.cuda.Stream, optional) – If supplied, will be set as the current stream in the context.\nIf not supplied, graph sets its own internal side stream as the current stream in the context.\ncapture_error_mode (str, optional) – specifies the cudaStreamCaptureMode for the graph capture stream.\nCan be “global”, “thread_local” or “relaxed”. During cuda graph capture, some actions, such as cudaMalloc,\nmay be unsafe. “global” will error on actions in other threads, “thread_local” will only error for\nactions in the current thread, and “relaxed” will not error on actions. Do NOT change this setting\nunless you’re familiar with cudaStreamCaptureMode",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.Graph",
        "api_url": "https://pytorch.org/docs/stable/fx.html#torch.fx.Graph",
        "api_signature": "torch.fx.Graph(owning_module=None, tracer_cls=None, tracer_extras=None)",
        "api_description": "Graph is the main data structure used in the FX Intermediate Representation.\nIt consists of a series of Node s, each representing callsites (or other\nsyntactic constructs). The list of Node s, taken together, constitute a\nvalid Python function.",
        "return_value": "The newly created and inserted call_function node.\nThe newly created and inserted call_method node.\nThe newly-created and inserted call_module node.\nThe newly-created and inserted node.\nWhether the graph was changed as a result of the pass.\nThe newly-created and inserted get_attr node.\nThe value in self that is now equivalent to the output value in g,\nif g had an output node. None otherwise.\nA resource manager that will restore the insert point on __exit__.\nA resource manager that will restore the insert point on __exit__.\nA doubly-linked list of Nodes. Note that reversed can be called on\nthis list to switch iteration order.\na context manager that when used in a with statement, to automatically\nrestore the previously registered code transformer.\nsrc: the Python source code representing the object\nglobals: a dictionary of global names in src -> the objects that they reference.\n",
        "parameters": "operator, Python function, or member of the builtins or operator\nnamespaces.\nargs (Optional[Tuple[Argument, ...]]) – The positional arguments to be passed\nto the called function.\nkwargs (Optional[Dict[str, Argument]]) – The keyword arguments to be passed\nto the called function\ntype_expr (Optional[Any]) – an optional type annotation representing the\nPython type the output of this node will have.\nmethod_name (str) – The name of the method to apply to the self argument.\nFor example, if args[0] is a Node representing a Tensor,\nthen to call relu() on that Tensor, pass relu to method_name.\nargs (Optional[Tuple[Argument, ...]]) – The positional arguments to be passed\nto the called method. Note that this should include a self argument.\nkwargs (Optional[Dict[str, Argument]]) – The keyword arguments to be passed\nto the called method\ntype_expr (Optional[Any]) – an optional type annotation representing the\nPython type the output of this node will have.\nmodule_name (str) – The qualified name of the Module in the Module\nhierarchy to be called. For example, if the traced Module has a\nsubmodule named foo, which has a submodule named bar, the\nqualified name foo.bar should be passed as module_name to\ncall that module.\nargs (Optional[Tuple[Argument, ...]]) – The positional arguments to be passed\nto the called method. Note that this should not include a self argument.\nkwargs (Optional[Dict[str, Argument]]) – The keyword arguments to be passed\nto the called method\ntype_expr (Optional[Any]) – an optional type annotation representing the\nPython type the output of this node will have.\nop (str) – the opcode for this Node. One of ‘call_function’, ‘call_method’, ‘get_attr’,\n‘call_module’, ‘placeholder’, or ‘output’. The semantics of these opcodes are\ndescribed in the Graph docstring.\nargs (Optional[Tuple[Argument, ...]]) – is a tuple of arguments to this node.\nkwargs (Optional[Dict[str, Argument]]) – the kwargs of this Node\nname (Optional[str]) – an optional string name for the Node.\nThis will influence the name of the value assigned to in the\nPython generated code.\ntype_expr (Optional[Any]) – an optional type annotation representing the\nPython type the output of this node will have.\nto_erase (Node) – The Node to erase from the Graph.\nqualified_name (str) – the fully-qualified name of the attribute to be retrieved.\nFor example, if the traced Module has a submodule named foo, which has a\nsubmodule named bar, which has an attribute named baz, the qualified\nname foo.bar.baz should be passed as qualified_name.\ntype_expr (Optional[Any]) – an optional type annotation representing the\nPython type the output of this node will have.\ng (Graph) – The source graph from which to copy Nodes.\nval_map (Dict[Node, Node]) – a dictionary that will be populated with a mapping\nfrom nodes in g to nodes in self. Note that val_map can be passed\nin with values in it already to override copying of certain values.\nnode (Node) – The node to copy into self.\narg_transform (Callable[[Node], Argument]) – A function that transforms\nNode arguments in node’s args and kwargs into the\nequivalent argument in self. In the simplest case, this should\nretrieve a value out of a table mapping Nodes in the original\ngraph to self.\nresult (Argument) – The value to be returned.\ntype_expr (Optional[Any]) – an optional type annotation representing the\nPython type the output of this node will have.\nname (str) – A name for the input value. This corresponds to the name\nof the positional argument to the function this Graph represents.\ntype_expr (Optional[Any]) – an optional type annotation representing the\nPython type the output of this node will have. This is needed in some\ncases for proper code generation (e.g. when the function is used\nsubsequently in TorchScript compilation).\ndefault_value (Any) – The default value this function argument should take\non. NOTE: to allow for None as a default value, inspect.Signature.empty\nshould be passed as this argument to specify that the parameter does _not_\nhave a default value.\nroot_module (str) – The name of the root module on which to look-up\nqualified name targets. This is usually ‘self’.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.GraphModule.graph",
        "api_url": "https://pytorch.org/docs/stable/fx.html#torch.fx.GraphModule.graph",
        "api_signature": null,
        "api_description": "Return the Graph underlying this GraphModule",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.jit.ScriptModule.graph",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.jit.ScriptModule.html#torch.jit.ScriptModule.graph",
        "api_signature": null,
        "api_description": "Return a string representation of the internal graph for the forward method.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.Graph.graph_copy",
        "api_url": "https://pytorch.org/docs/stable/fx.html#torch.fx.Graph.graph_copy",
        "api_signature": "graph_copy(g, val_map, return_output_node=False)",
        "api_description": "Copy all nodes from a given graph into self.",
        "return_value": "The value in self that is now equivalent to the output value in g,\nif g had an output node. None otherwise.\n",
        "parameters": "g (Graph) – The source graph from which to copy Nodes.\nval_map (Dict[Node, Node]) – a dictionary that will be populated with a mapping\nfrom nodes in g to nodes in self. Note that val_map can be passed\nin with values in it already to override copying of certain values.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda.graph_pool_handle",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.cuda.graph_pool_handle.html#torch.cuda.graph_pool_handle",
        "api_signature": "torch.cuda.graph_pool_handle()",
        "api_description": "Return an opaque token representing the id of a graph memory pool.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.onnx.verification.GraphInfo",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.onnx.verification.GraphInfo.html#torch.onnx.verification.GraphInfo",
        "api_signature": "torch.onnx.verification.GraphInfo(graph, input_args, params_dict, export_options=<factory>, id='', _EXCLUDED_NODE_KINDS=frozenset({'aten::ScalarImplicit', 'prim::Constant', 'prim::ListConstruct'})",
        "api_description": "GraphInfo contains validation information of a TorchScript graph and its converted ONNX graph.",
        "return_value": "The path to the exported repro directory.\nThe AssertionError raised during the verification. Returns None if no\nerror is raised.\nonnx_graph: The exported ONNX graph in TorchScript IR format.\nonnx_outs: The outputs from running exported ONNX model under the onnx\nbackend in options.\npt_outs: The outputs from running the TorchScript IR graph.\n",
        "parameters": "repro_dir (Optional[str]) – The directory to export the repro files to. Defaults to current\nworking directory if None.\nname (Optional[str]) – An optional name for the test case folder: “test_{name}”.\noptions (Optional[VerificationOptions]) – The verification options.\ngraph (bool) – If True, print the ATen JIT graph and ONNX graph.\noptions (VerificationOptions) – The verification options.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.GraphModule",
        "api_url": "https://pytorch.org/docs/stable/fx.html#torch.fx.GraphModule",
        "api_signature": "torch.fx.GraphModule(*args, **kwargs)",
        "api_description": "GraphModule is an nn.Module generated from an fx.Graph. Graphmodule has a\ngraph attribute, as well as code and forward attributes generated\nfrom that graph.",
        "return_value": "\nWhether or not the submodule could be inserted. Forthis method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute)\n\n\n\n\nWhether or not the target string referenced asubmodule we want to delete. A return value of False\nmeans that the target was not a valid reference to\na submodule.\n\n\n\n",
        "parameters": "root (Union[torch.nn.Module, Dict[str, Any]) – root can either be an nn.Module instance or a Dict mapping strings to any attribute type.\nIn the case that root is a Module, any references to Module-based objects (via qualified\nname) in the Graph’s Nodes’ target field will be copied over from the respective place\nwithin root’s Module hierarchy into the GraphModule’s module hierarchy.\nIn the case that root is a dict, the qualified name found in a Node’s target will be\nlooked up directly in the dict’s keys. The object mapped to by the Dict will be copied\nover into the appropriate place within the GraphModule’s module hierarchy.\ngraph (Graph) – graph contains the nodes this GraphModule should use for code generation\nclass_name (str) – name denotes the name of this GraphModule for debugging purposes. If it’s unset, all\nerror messages will report as originating from GraphModule. It may be helpful to set this\nto root’s original name or a name that makes sense within the context of your transform.\ntarget (str) – The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.)\nm (Module) – The submodule itself; the actual object we want to\ninstall in the current Module\ntarget (str) – The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.)",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.greater",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.greater.html#torch.greater",
        "api_signature": "torch.greater(input, other, *, out=None)",
        "api_description": "Alias for torch.gt().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.greater",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.greater.html#torch.Tensor.greater",
        "api_signature": "Tensor.greater(other)",
        "api_description": "See torch.greater().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.greater_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.greater_.html#torch.Tensor.greater_",
        "api_signature": "Tensor.greater_(other)",
        "api_description": "In-place version of greater().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.greater_equal",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.greater_equal.html#torch.greater_equal",
        "api_signature": "torch.greater_equal(input, other, *, out=None)",
        "api_description": "Alias for torch.ge().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.greater_equal",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.greater_equal.html#torch.Tensor.greater_equal",
        "api_signature": "Tensor.greater_equal(other)",
        "api_description": "See torch.greater_equal().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.greater_equal_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.greater_equal_.html#torch.Tensor.greater_equal_",
        "api_signature": "Tensor.greater_equal_(other)",
        "api_description": "In-place version of greater_equal().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.constraints.greater_than",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.constraints.greater_than",
        "api_signature": null,
        "api_description": "alias of _GreaterThan",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.constraints.greater_than_eq",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.constraints.greater_than_eq",
        "api_signature": null,
        "api_description": "alias of _GreaterThanEq",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.functional.grid_sample",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.functional.grid_sample.html#torch.nn.functional.grid_sample",
        "api_signature": "torch.nn.functional.grid_sample(input, grid, mode='bilinear', padding_mode='zeros', align_corners=None)",
        "api_description": "Compute grid sample.",
        "return_value": "output Tensor\n",
        "parameters": "input (Tensor) – input of shape (N,C,Hin,Win)(N, C, H_\\text{in}, W_\\text{in})(N,C,Hin​,Win​) (4-D case)\nor (N,C,Din,Hin,Win)(N, C, D_\\text{in}, H_\\text{in}, W_\\text{in})(N,C,Din​,Hin​,Win​) (5-D case)\ngrid (Tensor) – flow-field of shape (N,Hout,Wout,2)(N, H_\\text{out}, W_\\text{out}, 2)(N,Hout​,Wout​,2) (4-D case)\nor (N,Dout,Hout,Wout,3)(N, D_\\text{out}, H_\\text{out}, W_\\text{out}, 3)(N,Dout​,Hout​,Wout​,3) (5-D case)\nmode (str) – interpolation mode to calculate output values\n'bilinear' | 'nearest' | 'bicubic'. Default: 'bilinear'\nNote: mode='bicubic' supports only 4-D input.\nWhen mode='bilinear' and the input is 5-D, the interpolation mode\nused internally will actually be trilinear. However, when the input is 4-D,\nthe interpolation mode will legitimately be bilinear.\npadding_mode (str) – padding mode for outside grid values\n'zeros' | 'border' | 'reflection'. Default: 'zeros'\nalign_corners (bool, optional) – Geometrically, we consider the pixels of the\ninput  as squares rather than points.\nIf set to True, the extrema (-1 and 1) are considered as referring\nto the center points of the input’s corner pixels. If set to False, they\nare instead considered as referring to the corner points of the input’s corner\npixels, making the sampling more resolution agnostic.\nThis option parallels the align_corners option in\ninterpolate(), and so whichever option is used here\nshould also be used there to resize the input image before grid sampling.\nDefault: False",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.functional.group_norm",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.functional.group_norm.html#torch.nn.functional.group_norm",
        "api_signature": "torch.nn.functional.group_norm(input, num_groups, weight=None, bias=None, eps=1e-05)",
        "api_description": "Apply Group Normalization for last certain number of dimensions.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.quantized.GroupNorm",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.nn.quantized.GroupNorm.html#torch.ao.nn.quantized.GroupNorm",
        "api_signature": "torch.ao.nn.quantized.GroupNorm(num_groups, num_channels, weight, bias, scale, zero_point, eps=1e-05, affine=True, device=None, dtype=None)",
        "api_description": "This is the quantized version of GroupNorm.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.GroupNorm",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.GroupNorm.html#torch.nn.GroupNorm",
        "api_signature": "torch.nn.GroupNorm(num_groups, num_channels, eps=1e-05, affine=True, device=None, dtype=None)",
        "api_description": "Applies Group Normalization over a mini-batch of inputs.",
        "return_value": "",
        "parameters": "num_groups (int) – number of groups to separate the channels into\nnum_channels (int) – number of channels expected in input\neps (float) – a value added to the denominator for numerical stability. Default: 1e-5\naffine (bool) – a boolean value that when set to True, this module\nhas learnable per-channel affine parameters initialized to ones (for weights)\nand zeros (for biases). Default: True.",
        "input_shape": "\nInput: (N,C,∗)(N, C, *)(N,C,∗) where C=num_channelsC=\\text{num\\_channels}C=num_channels\nOutput: (N,C,∗)(N, C, *)(N,C,∗) (same shape as input)\n\n",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.quantized.dynamic.GRU",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.nn.quantized.dynamic.GRU.html#torch.ao.nn.quantized.dynamic.GRU",
        "api_signature": "torch.ao.nn.quantized.dynamic.GRU(*args, **kwargs)",
        "api_description": "Applies a multi-layer gated recurrent unit (GRU) RNN to an input sequence.",
        "return_value": "",
        "parameters": "input_size – The number of expected features in the input x\nhidden_size – The number of features in the hidden state h\nnum_layers – Number of recurrent layers. E.g., setting num_layers=2\nwould mean stacking two GRUs together to form a stacked GRU,\nwith the second GRU taking in outputs of the first GRU and\ncomputing the final results. Default: 1\nbias – If False, then the layer does not use bias weights b_ih and b_hh.\nDefault: True\nbatch_first – If True, then the input and output tensors are provided\nas (batch, seq, feature). Default: False\ndropout – If non-zero, introduces a Dropout layer on the outputs of each\nGRU layer except the last layer, with dropout probability equal to\ndropout. Default: 0\nbidirectional – If True, becomes a bidirectional GRU. Default: False",
        "input_shape": "\nInput1: (L,N,Hin)(L, N, H_{in})(L,N,Hin​) tensor containing input features where\nHin=input_sizeH_{in}=\\text{input\\_size}Hin​=input_size and L represents a sequence length.\nInput2: (S,N,Hout)(S, N, H_{out})(S,N,Hout​) tensor\ncontaining the initial hidden state for each element in the batch.\nHout=hidden_sizeH_{out}=\\text{hidden\\_size}Hout​=hidden_size\nDefaults to zero if not provided. where S=num_layers∗num_directionsS=\\text{num\\_layers} * \\text{num\\_directions}S=num_layers∗num_directions\nIf the RNN is bidirectional, num_directions should be 2, else it should be 1.\nOutput1: (L,N,Hall)(L, N, H_{all})(L,N,Hall​) where Hall=num_directions∗hidden_sizeH_{all}=\\text{num\\_directions} * \\text{hidden\\_size}Hall​=num_directions∗hidden_size\nOutput2: (S,N,Hout)(S, N, H_{out})(S,N,Hout​) tensor containing the next hidden state\nfor each element in the batch\n\n",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.GRU",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.GRU.html#torch.nn.GRU",
        "api_signature": "torch.nn.GRU(input_size, hidden_size, num_layers=1, bias=True, batch_first=False, dropout=0.0, bidirectional=False, device=None, dtype=None)",
        "api_description": "Apply a multi-layer gated recurrent unit (GRU) RNN to an input sequence.\nFor each element in the input sequence, each layer computes the following\nfunction:",
        "return_value": "",
        "parameters": "input_size – The number of expected features in the input x\nhidden_size – The number of features in the hidden state h\nnum_layers – Number of recurrent layers. E.g., setting num_layers=2\nwould mean stacking two GRUs together to form a stacked GRU,\nwith the second GRU taking in outputs of the first GRU and\ncomputing the final results. Default: 1\nbias – If False, then the layer does not use bias weights b_ih and b_hh.\nDefault: True\nbatch_first – If True, then the input and output tensors are provided\nas (batch, seq, feature) instead of (seq, batch, feature).\nNote that this does not apply to hidden or cell states. See the\nInputs/Outputs sections below for details.  Default: False\ndropout – If non-zero, introduces a Dropout layer on the outputs of each\nGRU layer except the last layer, with dropout probability equal to\ndropout. Default: 0\nbidirectional – If True, becomes a bidirectional GRU. Default: False",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.quantized.dynamic.GRUCell",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.nn.quantized.dynamic.GRUCell.html#torch.ao.nn.quantized.dynamic.GRUCell",
        "api_signature": "torch.ao.nn.quantized.dynamic.GRUCell(input_size, hidden_size, bias=True, dtype=torch.qint8)",
        "api_description": "A gated recurrent unit (GRU) cell",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.GRUCell",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.GRUCell.html#torch.nn.GRUCell",
        "api_signature": "torch.nn.GRUCell(input_size, hidden_size, bias=True, device=None, dtype=None)",
        "api_description": "A gated recurrent unit (GRU) cell.",
        "return_value": "",
        "parameters": "input_size (int) – The number of expected features in the input x\nhidden_size (int) – The number of features in the hidden state h\nbias (bool) – If False, then the layer does not use bias weights b_ih and\nb_hh. Default: True",
        "input_shape": "\ninput: (N,Hin)(N, H_{in})(N,Hin​) or (Hin)(H_{in})(Hin​) tensor containing input features where\nHinH_{in}Hin​ = input_size.\nhidden: (N,Hout)(N, H_{out})(N,Hout​) or (Hout)(H_{out})(Hout​) tensor containing the initial hidden\nstate where HoutH_{out}Hout​ = hidden_size. Defaults to zero if not provided.\noutput: (N,Hout)(N, H_{out})(N,Hout​) or (Hout)(H_{out})(Hout​) tensor containing the next hidden state.\n\n",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.gt",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.gt.html#torch.gt",
        "api_signature": "torch.gt(input, other, *, out=None)",
        "api_description": "Computes input>other\\text{input} > \\text{other}input>other element-wise.",
        "return_value": "A boolean tensor that is True where input is greater than other and False elsewhere\n",
        "parameters": "input (Tensor) – the tensor to compare\nother (Tensor or float) – the tensor or value to compare\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.gt",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.gt.html#torch.Tensor.gt",
        "api_signature": "Tensor.gt(other)",
        "api_description": "See torch.gt().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.gt_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.gt_.html#torch.Tensor.gt_",
        "api_signature": "Tensor.gt_(other)",
        "api_description": "In-place version of gt().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.symbolic_shapes.guard_size_oblivious",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.fx.experimental.symbolic_shapes.guard_size_oblivious.html#torch.fx.experimental.symbolic_shapes.guard_size_oblivious",
        "api_signature": "torch.fx.experimental.symbolic_shapes.guard_size_oblivious(expr)",
        "api_description": "Perform a guard on a symbolic boolean expression in a size oblivious way.\nThis is typically used when a non-oblivious test would result in a guard\non a data dependent value of which we don’t know the value of at compile time.\nWhen a guard is tested this way, we may diverge in behavior from how regular\nPyTorch semantics would treat it.  For more information, see\nhttps://github.com/pytorch/pytorch/pull/118579",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.gumbel.Gumbel",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.gumbel.Gumbel",
        "api_signature": "torch.distributions.gumbel.Gumbel(loc, scale, validate_args=None)",
        "api_description": "Bases: TransformedDistribution",
        "return_value": "",
        "parameters": "loc (float or Tensor) – Location parameter of the distribution\nscale (float or Tensor) – Scale parameter of the distribution",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.functional.gumbel_softmax",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.functional.gumbel_softmax.html#torch.nn.functional.gumbel_softmax",
        "api_signature": "torch.nn.functional.gumbel_softmax(logits, tau=1, hard=False, eps=1e-10, dim=-1)",
        "api_description": "Sample from the Gumbel-Softmax distribution (Link 1  Link 2) and optionally discretize.",
        "return_value": "Sampled tensor of same shape as logits from the Gumbel-Softmax distribution.\nIf hard=True, the returned samples will be one-hot, otherwise they will\nbe probability distributions that sum to 1 across dim.\n",
        "parameters": "logits (Tensor) – […, num_features] unnormalized log probabilities\ntau (float) – non-negative scalar temperature\nhard (bool) – if True, the returned samples will be discretized as one-hot vectors,\nbut will be differentiated as if it is the soft sample in autograd\ndim (int) – A dimension along which softmax will be computed. Default: -1.",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> logits = torch.randn(20, 32)\n>>> # Sample soft categorical using reparametrization trick:\n>>> F.gumbel_softmax(logits, tau=1, hard=False)\n>>> # Sample hard categorical using \"Straight-through\" trick:\n>>> F.gumbel_softmax(logits, tau=1, hard=True)\n\n\n"
    },
    {
        "api_name": "torch.Tensor.H",
        "api_url": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor.H",
        "api_signature": null,
        "api_description": "Returns a view of a matrix (2-D tensor) conjugated and transposed.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.jit.ScriptModule.half",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.jit.ScriptModule.html#torch.jit.ScriptModule.half",
        "api_signature": "half()",
        "api_description": "Casts all floating point parameters and buffers to half datatype.",
        "return_value": "self\n",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.Module.half",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.half",
        "api_signature": "half()",
        "api_description": "Casts all floating point parameters and buffers to half datatype.",
        "return_value": "self\n",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.half",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.half.html#torch.Tensor.half",
        "api_signature": "Tensor.half(memory_format=torch.preserve_format)",
        "api_description": "self.half() is equivalent to self.to(torch.float16). See to().",
        "return_value": "",
        "parameters": "memory_format (torch.memory_format, optional) – the desired memory format of\nreturned Tensor. Default: torch.preserve_format.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.TypedStorage.half",
        "api_url": "https://pytorch.org/docs/stable/storage.html#torch.TypedStorage.half",
        "api_signature": "half()",
        "api_description": "Casts this storage to half type.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.UntypedStorage.half",
        "api_url": "https://pytorch.org/docs/stable/storage.html#torch.UntypedStorage.half",
        "api_signature": "half()",
        "api_description": "Casts this storage to half type.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.constraints.half_open_interval",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.constraints.half_open_interval",
        "api_signature": null,
        "api_description": "alias of _HalfOpenInterval",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.half_cauchy.HalfCauchy",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.half_cauchy.HalfCauchy",
        "api_signature": "torch.distributions.half_cauchy.HalfCauchy(scale, validate_args=None)",
        "api_description": "Bases: TransformedDistribution",
        "return_value": "",
        "parameters": "scale (float or Tensor) – scale of the full Cauchy distribution",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.half_normal.HalfNormal",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.half_normal.HalfNormal",
        "api_signature": "torch.distributions.half_normal.HalfNormal(scale, validate_args=None)",
        "api_description": "Bases: TransformedDistribution",
        "return_value": "",
        "parameters": "scale (float or Tensor) – scale of the full Normal distribution",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.HalfStorage",
        "api_url": "https://pytorch.org/docs/stable/storage.html#torch.HalfStorage",
        "api_signature": "torch.HalfStorage(*args, wrap_storage=None, dtype=None, device=None, _internal=False)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.signal.windows.hamming",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.signal.windows.hamming.html#torch.signal.windows.hamming",
        "api_signature": "torch.signal.windows.hamming(M, *, sym=True, dtype=None, layout=torch.strided, device=None, requires_grad=False)",
        "api_description": "Computes the Hamming window.",
        "return_value": "",
        "parameters": "M (int) – the length of the window.\nIn other words, the number of points of the returned window.\nsym (bool, optional) – If False, returns a periodic window suitable for use in spectral analysis.\nIf True, returns a symmetric window suitable for use in filter design. Default: True.\nalpha (float, optional) – The coefficient α\\alphaα in the equation above.\nbeta (float, optional) – The coefficient β\\betaβ in the equation above.\ndtype (torch.dtype, optional) – the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_dtype()).\nlayout (torch.layout, optional) – the desired layout of returned Tensor.\nDefault: torch.strided.\ndevice (torch.device, optional) – the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_device()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types.\nrequires_grad (bool, optional) – If autograd should record operations on the\nreturned tensor. Default: False.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.hamming_window",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.hamming_window.html#torch.hamming_window",
        "api_signature": "torch.hamming_window(window_length, periodic=True, alpha=0.54, beta=0.46, *, dtype=None, layout=torch.strided, device=None, requires_grad=False)",
        "api_description": "Hamming window function.",
        "return_value": "A 1-D tensor of size (window_length,)(\\text{window\\_length},)(window_length,) containing the window.\n",
        "parameters": "window_length (int) – the size of returned window\nperiodic (bool, optional) – If True, returns a window to be used as periodic\nfunction. If False, return a symmetric window.\nalpha (float, optional) – The coefficient α\\alphaα in the equation above\nbeta (float, optional) – The coefficient β\\betaβ in the equation above\ndtype (torch.dtype, optional) – the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_dtype()). Only floating point types are supported.\nlayout (torch.layout, optional) – the desired layout of returned window tensor. Only\ntorch.strided (dense layout) is supported.\ndevice (torch.device, optional) – the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_device()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types.\nrequires_grad (bool, optional) – If autograd should record operations on the\nreturned tensor. Default: False.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.overrides.handle_torch_function",
        "api_url": "https://pytorch.org/docs/stable/torch.overrides.html#torch.overrides.handle_torch_function",
        "api_signature": "torch.overrides.handle_torch_function(public_api, relevant_args, *args, **kwargs)",
        "api_description": "Implement a function with checks for __torch_function__ overrides.",
        "return_value": "Result from calling implementation or an __torch_function__\nmethod, as appropriate.\n",
        "parameters": "public_api (function) – Function exposed by the public torch API originally called like\npublic_api(*args, **kwargs) on which arguments are now being\nchecked.\nrelevant_args (iterable) – Iterable of arguments to check for __torch_function__ methods.\nargs (tuple) – Arbitrary positional arguments originally passed into public_api.\nkwargs (tuple) – Arbitrary keyword arguments originally passed into public_api.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.signal.windows.hann",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.signal.windows.hann.html#torch.signal.windows.hann",
        "api_signature": "torch.signal.windows.hann(M, *, sym=True, dtype=None, layout=torch.strided, device=None, requires_grad=False)",
        "api_description": "Computes the Hann window.",
        "return_value": "",
        "parameters": "M (int) – the length of the window.\nIn other words, the number of points of the returned window.\nsym (bool, optional) – If False, returns a periodic window suitable for use in spectral analysis.\nIf True, returns a symmetric window suitable for use in filter design. Default: True.\ndtype (torch.dtype, optional) – the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_dtype()).\nlayout (torch.layout, optional) – the desired layout of returned Tensor.\nDefault: torch.strided.\ndevice (torch.device, optional) – the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_device()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types.\nrequires_grad (bool, optional) – If autograd should record operations on the\nreturned tensor. Default: False.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.hann_window",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.hann_window.html#torch.hann_window",
        "api_signature": "torch.hann_window(window_length, periodic=True, *, dtype=None, layout=torch.strided, device=None, requires_grad=False)",
        "api_description": "Hann window function.",
        "return_value": "A 1-D tensor of size (window_length,)(\\text{window\\_length},)(window_length,) containing the window\n",
        "parameters": "window_length (int) – the size of returned window\nperiodic (bool, optional) – If True, returns a window to be used as periodic\nfunction. If False, return a symmetric window.\ndtype (torch.dtype, optional) – the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_dtype()). Only floating point types are supported.\nlayout (torch.layout, optional) – the desired layout of returned window tensor. Only\ntorch.strided (dense layout) is supported.\ndevice (torch.device, optional) – the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_device()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types.\nrequires_grad (bool, optional) – If autograd should record operations on the\nreturned tensor. Default: False.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.Hardshrink",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.Hardshrink.html#torch.nn.Hardshrink",
        "api_signature": "torch.nn.Hardshrink(lambd=0.5)",
        "api_description": "Applies the Hard Shrinkage (Hardshrink) function element-wise.",
        "return_value": "",
        "parameters": "lambd (float) – the λ\\lambdaλ value for the Hardshrink formulation. Default: 0.5",
        "input_shape": "\nInput: (∗)(*)(∗), where ∗*∗ means any number of dimensions.\nOutput: (∗)(*)(∗), same shape as the input.\n\n",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.functional.hardshrink",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.functional.hardshrink.html#torch.nn.functional.hardshrink",
        "api_signature": "torch.nn.functional.hardshrink(input, lambd=0.5)",
        "api_description": "Applies the hard shrinkage function element-wise",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.hardshrink",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.hardshrink.html#torch.Tensor.hardshrink",
        "api_signature": "Tensor.hardshrink(lambd=0.5)",
        "api_description": "See torch.nn.functional.hardshrink()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.quantized.functional.hardsigmoid",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.nn.quantized.functional.hardsigmoid.html#torch.ao.nn.quantized.functional.hardsigmoid",
        "api_signature": "torch.ao.nn.quantized.functional.hardsigmoid(input, inplace=False)",
        "api_description": "This is the quantized version of hardsigmoid().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.Hardsigmoid",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.Hardsigmoid.html#torch.nn.Hardsigmoid",
        "api_signature": "torch.nn.Hardsigmoid(inplace=False)",
        "api_description": "Applies the Hardsigmoid function element-wise.",
        "return_value": "",
        "parameters": "inplace (bool) – can optionally do the operation in-place. Default: False",
        "input_shape": "\nInput: (∗)(*)(∗), where ∗*∗ means any number of dimensions.\nOutput: (∗)(*)(∗), same shape as the input.\n\n",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.functional.hardsigmoid",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.functional.hardsigmoid.html#torch.nn.functional.hardsigmoid",
        "api_signature": "torch.nn.functional.hardsigmoid(input, inplace=False)",
        "api_description": "Apply the Hardsigmoid function element-wise.",
        "return_value": "",
        "parameters": "inplace (bool) – If set to True, will do this operation in-place. Default: False",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.quantized.Hardswish",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.nn.quantized.Hardswish.html#torch.ao.nn.quantized.Hardswish",
        "api_signature": "torch.ao.nn.quantized.Hardswish(scale, zero_point, device=None, dtype=None)",
        "api_description": "This is the quantized version of Hardswish.",
        "return_value": "",
        "parameters": "scale – quantization scale of the output tensor\nzero_point – quantization zero point of the output tensor",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.quantized.functional.hardswish",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.nn.quantized.functional.hardswish.html#torch.ao.nn.quantized.functional.hardswish",
        "api_signature": "torch.ao.nn.quantized.functional.hardswish(input, scale, zero_point)",
        "api_description": "This is the quantized version of hardswish().",
        "return_value": "",
        "parameters": "input (Tensor) – quantized input\nscale (float) – quantization scale of the output tensor\nzero_point (int) – quantization zero point of the output tensor",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.Hardswish",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.Hardswish.html#torch.nn.Hardswish",
        "api_signature": "torch.nn.Hardswish(inplace=False)",
        "api_description": "Applies the Hardswish function, element-wise.",
        "return_value": "",
        "parameters": "inplace (bool) – can optionally do the operation in-place. Default: False",
        "input_shape": "\nInput: (∗)(*)(∗), where ∗*∗ means any number of dimensions.\nOutput: (∗)(*)(∗), same shape as the input.\n\n",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.functional.hardswish",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.functional.hardswish.html#torch.nn.functional.hardswish",
        "api_signature": "torch.nn.functional.hardswish(input, inplace=False)",
        "api_description": "Apply hardswish function, element-wise.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.quantized.functional.hardtanh",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.nn.quantized.functional.hardtanh.html#torch.ao.nn.quantized.functional.hardtanh",
        "api_signature": "torch.ao.nn.quantized.functional.hardtanh(input, min_val=-1.0, max_val=1.0, inplace=False)",
        "api_description": "This is the quantized version of hardtanh().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.Hardtanh",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.Hardtanh.html#torch.nn.Hardtanh",
        "api_signature": "torch.nn.Hardtanh(min_val=-1.0, max_val=1.0, inplace=False, min_value=None, max_value=None)",
        "api_description": "Applies the HardTanh function element-wise.",
        "return_value": "",
        "parameters": "min_val (float) – minimum value of the linear region range. Default: -1\nmax_val (float) – maximum value of the linear region range. Default: 1\ninplace (bool) – can optionally do the operation in-place. Default: False",
        "input_shape": "\nInput: (∗)(*)(∗), where ∗*∗ means any number of dimensions.\nOutput: (∗)(*)(∗), same shape as the input.\n\n",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.functional.hardtanh",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.functional.hardtanh.html#torch.nn.functional.hardtanh",
        "api_signature": "torch.nn.functional.hardtanh(input, min_val=-1., max_val=1., inplace=False)",
        "api_description": "Applies the HardTanh function element-wise. See Hardtanh for more\ndetails.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.functional.hardtanh_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.functional.hardtanh_.html#torch.nn.functional.hardtanh_",
        "api_signature": "torch.nn.functional.hardtanh_(input, min_val=-1., max_val=1.)",
        "api_description": "In-place version of hardtanh().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.bernoulli.Bernoulli.has_enumerate_support",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.bernoulli.Bernoulli.has_enumerate_support",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.binomial.Binomial.has_enumerate_support",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.binomial.Binomial.has_enumerate_support",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.categorical.Categorical.has_enumerate_support",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.categorical.Categorical.has_enumerate_support",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.independent.Independent.has_enumerate_support",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.independent.Independent.has_enumerate_support",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.one_hot_categorical.OneHotCategorical.has_enumerate_support",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.one_hot_categorical.OneHotCategorical.has_enumerate_support",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.package.Directory.has_file",
        "api_url": "https://pytorch.org/docs/stable/package.html#torch.package.Directory.has_file",
        "api_signature": "has_file(filename)",
        "api_description": "Checks if a file is present in a Directory.",
        "return_value": "If a Directory contains the specified file.\n",
        "parameters": "filename (str) – Path of file to search for.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.symbolic_shapes.has_free_symbols",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.fx.experimental.symbolic_shapes.has_free_symbols.html#torch.fx.experimental.symbolic_shapes.has_free_symbols",
        "api_signature": "torch.fx.experimental.symbolic_shapes.has_free_symbols(val)",
        "api_description": "Faster version of bool(free_symbols(val))",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.onnx.verification.GraphInfo.has_mismatch",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.onnx.verification.GraphInfo.html#torch.onnx.verification.GraphInfo.has_mismatch",
        "api_signature": "has_mismatch()",
        "api_description": "Return True if the subgraph has output mismatch between torch and ONNX.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.beta.Beta.has_rsample",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.beta.Beta.has_rsample",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.cauchy.Cauchy.has_rsample",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.cauchy.Cauchy.has_rsample",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.continuous_bernoulli.ContinuousBernoulli.has_rsample",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.continuous_bernoulli.ContinuousBernoulli.has_rsample",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.dirichlet.Dirichlet.has_rsample",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.dirichlet.Dirichlet.has_rsample",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.exponential.Exponential.has_rsample",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.exponential.Exponential.has_rsample",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.fishersnedecor.FisherSnedecor.has_rsample",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.fishersnedecor.FisherSnedecor.has_rsample",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.gamma.Gamma.has_rsample",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.gamma.Gamma.has_rsample",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.half_cauchy.HalfCauchy.has_rsample",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.half_cauchy.HalfCauchy.has_rsample",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.half_normal.HalfNormal.has_rsample",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.half_normal.HalfNormal.has_rsample",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.independent.Independent.has_rsample",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.independent.Independent.has_rsample",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.inverse_gamma.InverseGamma.has_rsample",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.inverse_gamma.InverseGamma.has_rsample",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.kumaraswamy.Kumaraswamy.has_rsample",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.kumaraswamy.Kumaraswamy.has_rsample",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.laplace.Laplace.has_rsample",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.laplace.Laplace.has_rsample",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.log_normal.LogNormal.has_rsample",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.log_normal.LogNormal.has_rsample",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.has_rsample",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.has_rsample",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.mixture_same_family.MixtureSameFamily.has_rsample",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.mixture_same_family.MixtureSameFamily.has_rsample",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.multivariate_normal.MultivariateNormal.has_rsample",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.multivariate_normal.MultivariateNormal.has_rsample",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.normal.Normal.has_rsample",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.normal.Normal.has_rsample",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.relaxed_bernoulli.RelaxedBernoulli.has_rsample",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.relaxed_bernoulli.RelaxedBernoulli.has_rsample",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.has_rsample",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.has_rsample",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.studentT.StudentT.has_rsample",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.studentT.StudentT.has_rsample",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.transformed_distribution.TransformedDistribution.has_rsample",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.transformed_distribution.TransformedDistribution.has_rsample",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.uniform.Uniform.has_rsample",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.uniform.Uniform.has_rsample",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.von_mises.VonMises.has_rsample",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.von_mises.VonMises.has_rsample",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.wishart.Wishart.has_rsample",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.wishart.Wishart.has_rsample",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.overrides.has_torch_function",
        "api_url": "https://pytorch.org/docs/stable/torch.overrides.html#torch.overrides.has_torch_function",
        "api_signature": "torch.overrides.has_torch_function()",
        "api_description": "Check for __torch_function__ implementations in the elements of an iterable\nor if a __torch_function__ mode is enabled.  Considers exact Tensor s\nand Parameter s non-dispatchable.  Use this to guard a call to\nhandle_torch_function(); don’t use it to test if something\nis Tensor-like, use is_tensor_like() instead.\n:param relevant_args: Iterable or arguments to check for __torch_function__ methods.\n:type relevant_args: iterable",
        "return_value": "True if any of the elements of relevant_args have __torch_function__\nimplementations, False otherwise.\n",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.modules.lazy.LazyModuleMixin.has_uninitialized_params",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.modules.lazy.LazyModuleMixin.html#torch.nn.modules.lazy.LazyModuleMixin.has_uninitialized_params",
        "api_signature": "has_uninitialized_params()",
        "api_description": "Check if a module has parameters that are not initialized.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.HashStore",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#torch.distributed.HashStore",
        "api_signature": null,
        "api_description": "A thread-safe store implementation based on an underlying hashmap. This store can be used\nwithin the same process (for example, by other threads), but cannot be used across processes.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import torch.distributed as dist\n>>> store = dist.HashStore()\n>>> # store can be used from other threads\n>>> # Use any of the store methods after initialization\n>>> store.set(\"first_key\", \"first_value\")\n\n\n"
    },
    {
        "api_name": "torch.distributed.elastic.rendezvous.dynamic_rendezvous.RendezvousTimeout.heartbeat",
        "api_url": "https://pytorch.org/docs/stable/elastic/rendezvous.html#torch.distributed.elastic.rendezvous.dynamic_rendezvous.RendezvousTimeout.heartbeat",
        "api_signature": null,
        "api_description": "Get the keep-alive heartbeat timeout.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.heaviside",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.heaviside.html#torch.heaviside",
        "api_signature": "torch.heaviside(input, values, *, out=None)",
        "api_description": "Computes the Heaviside step function for each element in input.\nThe Heaviside step function is defined as:",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.\nvalues (Tensor) – The values to use where input is zero.\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.heaviside",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.heaviside.html#torch.Tensor.heaviside",
        "api_signature": "Tensor.heaviside(values)",
        "api_description": "See torch.heaviside()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.hub.help",
        "api_url": "https://pytorch.org/docs/stable/hub.html#torch.hub.help",
        "api_signature": "torch.hub.help(github, model, force_reload=False, skip_validation=False, trust_repo=None)",
        "api_description": "Show the docstring of entrypoint model.",
        "return_value": "",
        "parameters": "github (str) – a string with format <repo_owner/repo_name[:ref]> with an optional\nref (a tag or a branch). If ref is not specified, the default branch is assumed\nto be main if it exists, and otherwise master.\nmodel (str) – a string of entrypoint name defined in repo’s hubconf.py\nforce_reload (bool, optional) – whether to discard the existing cache and force a fresh download.\nDefault is False.\nskip_validation (bool, optional) – if False, torchhub will check that the ref\nspecified by the github argument properly belongs to the repo owner. This will make\nrequests to the GitHub API; you can specify a non-default GitHub token by setting the\nGITHUB_TOKEN environment variable. Default is False.\ntrust_repo (bool, str or None) – \"check\", True, False or None.\nThis parameter was introduced in v1.12 and helps ensuring that users\nonly run code from repos that they trust.\nIf False, a prompt will ask the user whether the repo should\nbe trusted.\nIf True, the repo will be added to the trusted list and loaded\nwithout requiring explicit confirmation.\nIf \"check\", the repo will be checked against the list of\ntrusted repos in the cache. If it is not present in that list, the\nbehaviour will fall back onto the trust_repo=False option.\nIf None: this will raise a warning, inviting the user to set\ntrust_repo to either False, True or \"check\". This\nis only present for backward compatibility and will be removed in\nv2.0.\nDefault is None and will eventually change to \"check\" in v2.0.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.autograd.functional.hessian",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.autograd.functional.hessian.html#torch.autograd.functional.hessian",
        "api_signature": "torch.autograd.functional.hessian(func, inputs, create_graph=False, strict=False, vectorize=False, outer_jacobian_strategy='reverse-mode')",
        "api_description": "Compute the Hessian of a given scalar function.",
        "return_value": "if there is a single input,\nthis will be a single Tensor containing the Hessian for the input.\nIf it is a tuple, then the Hessian will be a tuple of tuples where\nHessian[i][j] will contain the Hessian of the ith input\nand jth input with size the sum of the size of the ith input plus\nthe size of the jth input. Hessian[i][j] will have the same\ndtype and device as the corresponding ith input.\n",
        "parameters": "func (function) – a Python function that takes Tensor inputs and returns\na Tensor with a single element.\ninputs (tuple of Tensors or Tensor) – inputs to the function func.\ncreate_graph (bool, optional) – If True, the Hessian will be computed in\na differentiable manner. Note that when strict is False, the result can not\nrequire gradients or be disconnected from the inputs.\nDefaults to False.\nstrict (bool, optional) – If True, an error will be raised when we detect that there exists an input\nsuch that all the outputs are independent of it. If False, we return a Tensor of zeros as the\nhessian for said inputs, which is the expected mathematical value.\nDefaults to False.\nvectorize (bool, optional) – This feature is experimental.\nPlease consider using torch.func.hessian()\ninstead if you are looking for something less experimental and more performant.\nWhen computing the hessian, usually we invoke\nautograd.grad once per row of the hessian. If this flag is\nTrue, we use the vmap prototype feature as the backend to\nvectorize calls to autograd.grad so we only invoke it once\ninstead of once per row. This should lead to performance\nimprovements in many use cases, however, due to this feature\nbeing incomplete, there may be performance cliffs. Please\nuse torch._C._debug_only_display_vmap_fallback_warnings(True)\nto show any performance warnings and file us issues if\nwarnings exist for your use case. Defaults to False.\nouter_jacobian_strategy (str, optional) – The Hessian is computed by\ncomputing the Jacobian of a Jacobian. The inner Jacobian is always\ncomputed in reverse-mode AD. Setting strategy to \"forward-mode\"\nor \"reverse-mode\" determines whether the outer Jacobian will be\ncomputed with forward or reverse mode AD. Currently, computing the outer\nJacobian in \"forward-mode\" requires vectorized=True. Defaults\nto \"reverse-mode\".",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.func.hessian",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.func.hessian.html#torch.func.hessian",
        "api_signature": "torch.func.hessian(func, argnums=0)",
        "api_description": "Computes the Hessian of func with respect to the arg(s) at index\nargnum via a forward-over-reverse strategy.",
        "return_value": "Returns a function that takes in the same inputs as func and\nreturns the Hessian of func with respect to the arg(s) at\nargnums.\n",
        "parameters": "func (function) – A Python function that takes one or more arguments,\none of which must be a Tensor, and returns one or more Tensors\nargnums (int or Tuple[int]) – Optional, integer or tuple of integers,\nsaying which arguments to get the Hessian with respect to.\nDefault: 0.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fft.hfft",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.fft.hfft.html#torch.fft.hfft",
        "api_signature": "torch.fft.hfft(input, n=None, dim=-1, norm=None, *, out=None)",
        "api_description": "Computes the one dimensional discrete Fourier transform of a Hermitian\nsymmetric input signal.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor representing a half-Hermitian signal\nn (int, optional) – Output signal length. This determines the length of the\nreal output. If given, the input will either be zero-padded or trimmed to this\nlength before computing the Hermitian FFT.\nDefaults to even output: n=2*(input.size(dim) - 1).\ndim (int, optional) – The dimension along which to take the one dimensional Hermitian FFT.\nnorm (str, optional) – Normalization mode. For the forward transform\n(hfft()), these correspond to:\n\"forward\" - normalize by 1/n\n\"backward\" - no normalization\n\"ortho\" - normalize by 1/sqrt(n) (making the Hermitian FFT orthonormal)\nCalling the backward transform (ihfft()) with the same\nnormalization mode will apply an overall normalization of 1/n between\nthe two transforms. This is required to make ihfft()\nthe exact inverse.\nDefault is \"backward\" (no normalization).\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fft.hfft2",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.fft.hfft2.html#torch.fft.hfft2",
        "api_signature": "torch.fft.hfft2(input, s=None, dim=(-2, -1)",
        "api_description": "Computes the 2-dimensional discrete Fourier transform of a Hermitian symmetric\ninput signal. Equivalent to hfftn() but only\ntransforms the last two dimensions by default.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor\ns (Tuple[int], optional) – Signal size in the transformed dimensions.\nIf given, each dimension dim[i] will either be zero-padded or\ntrimmed to the length s[i] before computing the Hermitian FFT.\nIf a length -1 is specified, no padding is done in that dimension.\nDefaults to even output in the last dimension:\ns[-1] = 2*(input.size(dim[-1]) - 1).\ndim (Tuple[int], optional) – Dimensions to be transformed.\nThe last dimension must be the half-Hermitian compressed dimension.\nDefault: last two dimensions.\nnorm (str, optional) – Normalization mode. For the forward transform\n(hfft2()), these correspond to:\n\"forward\" - normalize by 1/n\n\"backward\" - no normalization\n\"ortho\" - normalize by 1/sqrt(n) (making the Hermitian FFT orthonormal)\nWhere n = prod(s) is the logical FFT size.\nCalling the backward transform (ihfft2()) with the same\nnormalization mode will apply an overall normalization of 1/n between\nthe two transforms. This is required to make ihfft2()\nthe exact inverse.\nDefault is \"backward\" (no normalization).\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fft.hfftn",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.fft.hfftn.html#torch.fft.hfftn",
        "api_signature": "torch.fft.hfftn(input, s=None, dim=None, norm=None, *, out=None)",
        "api_description": "Computes the n-dimensional discrete Fourier transform of a Hermitian symmetric\ninput signal.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor\ns (Tuple[int], optional) – Signal size in the transformed dimensions.\nIf given, each dimension dim[i] will either be zero-padded or\ntrimmed to the length s[i] before computing the real FFT.\nIf a length -1 is specified, no padding is done in that dimension.\nDefaults to even output in the last dimension:\ns[-1] = 2*(input.size(dim[-1]) - 1).\ndim (Tuple[int], optional) – Dimensions to be transformed.\nThe last dimension must be the half-Hermitian compressed dimension.\nDefault: all dimensions, or the last len(s) dimensions if s is given.\nnorm (str, optional) – Normalization mode. For the forward transform\n(hfftn()), these correspond to:\n\"forward\" - normalize by 1/n\n\"backward\" - no normalization\n\"ortho\" - normalize by 1/sqrt(n) (making the Hermitian FFT orthonormal)\nWhere n = prod(s) is the logical FFT size.\nCalling the backward transform (ihfftn()) with the same\nnormalization mode will apply an overall normalization of 1/n between\nthe two transforms. This is required to make ihfftn()\nthe exact inverse.\nDefault is \"backward\" (no normalization).\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.functional.hinge_embedding_loss",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.functional.hinge_embedding_loss.html#torch.nn.functional.hinge_embedding_loss",
        "api_signature": "torch.nn.functional.hinge_embedding_loss(input, target, margin=1.0, size_average=None, reduce=None, reduction='mean')",
        "api_description": "See HingeEmbeddingLoss for details.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.HingeEmbeddingLoss",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.HingeEmbeddingLoss.html#torch.nn.HingeEmbeddingLoss",
        "api_signature": "torch.nn.HingeEmbeddingLoss(margin=1.0, size_average=None, reduce=None, reduction='mean')",
        "api_description": "Measures the loss given an input tensor xxx and a labels tensor yyy\n(containing 1 or -1).\nThis is usually used for measuring whether two inputs are similar or\ndissimilar, e.g. using the L1 pairwise distance as xxx, and is typically\nused for learning nonlinear embeddings or semi-supervised learning.",
        "return_value": "",
        "parameters": "margin (float, optional) – Has a default value of 1.\nsize_average (bool, optional) – Deprecated (see reduction). By default,\nthe losses are averaged over each loss element in the batch. Note that for\nsome losses, there are multiple elements per sample. If the field size_average\nis set to False, the losses are instead summed for each minibatch. Ignored\nwhen reduce is False. Default: True\nreduce (bool, optional) – Deprecated (see reduction). By default, the\nlosses are averaged or summed over observations for each minibatch depending\non size_average. When reduce is False, returns a loss per\nbatch element instead and ignores size_average. Default: True\nreduction (str, optional) – Specifies the reduction to apply to the output:\n'none' | 'mean' | 'sum'. 'none': no reduction will be applied,\n'mean': the sum of the output will be divided by the number of\nelements in the output, 'sum': the output will be summed. Note: size_average\nand reduce are in the process of being deprecated, and in the meantime,\nspecifying either of those two args will override reduction. Default: 'mean'",
        "input_shape": "\nInput: (∗)(*)(∗) where ∗*∗ means, any number of dimensions. The sum operation\noperates over all the elements.\nTarget: (∗)(*)(∗), same shape as the input\nOutput: scalar. If reduction is 'none', then same shape as the input\n\n",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.symbolic_shapes.hint_int",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.fx.experimental.symbolic_shapes.hint_int.html#torch.fx.experimental.symbolic_shapes.hint_int",
        "api_signature": "torch.fx.experimental.symbolic_shapes.hint_int(a, fallback=None)",
        "api_description": "Retrieve the hint for an int (based on the underlying real values as observed\nat runtime).  If no hint is available (e.g., because data dependent shapes),\nif fallback is not None, use that instead (otherwise raise an error).",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.histc",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.histc.html#torch.histc",
        "api_signature": "torch.histc(input, bins=100, min=0, max=0, *, out=None)",
        "api_description": "Computes the histogram of a tensor.",
        "return_value": "Histogram represented as a tensor\n",
        "parameters": "input (Tensor) – the input tensor.\nbins (int) – number of histogram bins\nmin (Scalar) – lower end of the range (inclusive)\nmax (Scalar) – upper end of the range (inclusive)\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.histc",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.histc.html#torch.Tensor.histc",
        "api_signature": "Tensor.histc(bins=100, min=0, max=0)",
        "api_description": "See torch.histc()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.histogram",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.histogram.html#torch.histogram",
        "api_signature": "torch.histogram(input, bins, *, range=None, weight=None, density=False, out=None)",
        "api_description": "Computes a histogram of the values in a tensor.",
        "return_value": "1D Tensor containing the values of the histogram.\nbin_edges(Tensor): 1D Tensor containing the edges of the histogram bins.\n",
        "parameters": "input (Tensor) – the input tensor.\nbins – int or 1D Tensor. If int, defines the number of equal-width bins. If tensor,\ndefines the sequence of bin edges including the rightmost edge.\nrange (tuple of float) – Defines the range of the bins.\nweight (Tensor) – If provided, weight should have the same shape as input. Each value in\ninput contributes its associated weight towards its bin’s result.\ndensity (bool) – If False, the result will contain the count (or total weight) in each bin.\nIf True, the result is the value of the probability density function over the bins,\nnormalized such that the integral over the range of the bins is 1.\nout (Tensor, optional) – the output tensor. (tuple, optional): The result tuple of two output tensors (hist, bin_edges).",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.histogram",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.histogram.html#torch.Tensor.histogram",
        "api_signature": "Tensor.histogram(input, bins, *, range=None, weight=None, density=False)",
        "api_description": "See torch.histogram()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.histogramdd",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.histogramdd.html#torch.histogramdd",
        "api_signature": "torch.histogramdd(input, bins, *, range=None, weight=None, density=False, out=None)",
        "api_description": "Computes a multi-dimensional histogram of the values in a tensor.",
        "return_value": "N-dimensional Tensor containing the values of the histogram.\nbin_edges(Tensor[]): sequence of N 1D Tensors containing the bin edges.\n",
        "parameters": "input (Tensor) – the input tensor.\nbins – Tensor[], int[], or int.\nIf Tensor[], defines the sequences of bin edges.\nIf int[], defines the number of equal-width bins in each dimension.\nIf int, defines the number of equal-width bins for all dimensions.\nrange (sequence of float) – Defines the leftmost and rightmost bin edges\nin each dimension.\nweight (Tensor) – By default, each value in the input has weight 1. If a weight\ntensor is passed, each N-dimensional coordinate in input\ncontributes its associated weight towards its bin’s result.\nThe weight tensor should have the same shape as the input\ntensor excluding its innermost dimension N.\ndensity (bool) – If False (default), the result will contain the count (or total weight)\nin each bin. If True, each count (weight) is divided by the total count\n(total weight), then divided by the volume of its associated bin.",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> torch.histogramdd(torch.tensor([[0., 1.], [1., 0.], [2., 0.], [2., 2.]]), bins=[3, 3],\n...                   weight=torch.tensor([1., 2., 4., 8.]))\n    torch.return_types.histogramdd(\n        hist=tensor([[0., 1., 0.],\n                     [2., 0., 0.],\n                     [4., 0., 8.]]),\n        bin_edges=(tensor([0.0000, 0.6667, 1.3333, 2.0000]),\n                   tensor([0.0000, 0.6667, 1.3333, 2.0000])))\n\n\n>>> torch.histogramdd(torch.tensor([[0., 0.], [1., 1.], [2., 2.]]), bins=[2, 2],\n...                   range=[0., 1., 0., 1.], density=True)\n    torch.return_types.histogramdd(\n       hist=tensor([[2., 0.],\n                    [0., 2.]]),\n       bin_edges=(tensor([0.0000, 0.5000, 1.0000]),\n                  tensor([0.0000, 0.5000, 1.0000])))\n\n\n"
    },
    {
        "api_name": "torch.ao.quantization.observer.HistogramObserver",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.quantization.observer.HistogramObserver.html#torch.ao.quantization.observer.HistogramObserver",
        "api_signature": "torch.ao.quantization.observer.HistogramObserver(bins=2048, upsample_rate=128, dtype=torch.quint8, qscheme=torch.per_tensor_affine, reduce_range=False, quant_min=None, quant_max=None, factory_kwargs=None, eps=1.1920928955078125e-07, is_dynamic=False, **kwargs)",
        "api_description": "The module records the running histogram of tensor values along with\nmin/max values. calculate_qparams will calculate scale and zero_point.",
        "return_value": "",
        "parameters": "bins (int) – Number of bins to use for the histogram\nupsample_rate (int) – Factor by which the histograms are upsampled, this is\nused to interpolate histograms with varying ranges across observations\ndtype (dtype) – dtype argument to the quantize node needed to implement the\nreference model spec\nqscheme – Quantization scheme to be used\nreduce_range – Reduces the range of the quantized data type by 1 bit\neps (Tensor) – Epsilon value for float32, Defaults to torch.finfo(torch.float32).eps.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.linalg.householder_product",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.linalg.householder_product.html#torch.linalg.householder_product",
        "api_signature": "torch.linalg.householder_product(A, tau, *, out=None)",
        "api_description": "Computes the first n columns of a product of Householder matrices.",
        "return_value": "",
        "parameters": "A (Tensor) – tensor of shape (*, m, n) where * is zero or more batch dimensions.\ntau (Tensor) – tensor of shape (*, k) where * is zero or more batch dimensions.\nout (Tensor, optional) – output tensor. Ignored if None. Default: None.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.TypedStorage.hpu",
        "api_url": "https://pytorch.org/docs/stable/storage.html#torch.TypedStorage.hpu",
        "api_signature": "hpu(device=None, non_blocking=False, **kwargs)",
        "api_description": "Returns a copy of this object in HPU memory.",
        "return_value": "",
        "parameters": "device (int) – The destination HPU id. Defaults to the current device.\nnon_blocking (bool) – If True and the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect.\n**kwargs – For compatibility, may contain the key async in place of\nthe non_blocking argument.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.UntypedStorage.hpu",
        "api_url": "https://pytorch.org/docs/stable/storage.html#torch.UntypedStorage.hpu",
        "api_signature": "hpu(device=None, non_blocking=False, **kwargs)",
        "api_description": "Returns a copy of this object in HPU memory.",
        "return_value": "",
        "parameters": "device (int) – The destination HPU id. Defaults to the current device.\nnon_blocking (bool) – If True and the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect.\n**kwargs – For compatibility, may contain the key async in place of\nthe non_blocking argument.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.hsplit",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.hsplit.html#torch.hsplit",
        "api_signature": "torch.hsplit(input, indices_or_sections)",
        "api_description": "Splits input, a tensor with one or more dimensions, into multiple tensors\nhorizontally according to indices_or_sections. Each split is a view of\ninput.",
        "return_value": "",
        "parameters": "input (Tensor) – tensor to split.\nindices_or_sections (int or list or tuple of ints) – See argument in torch.tensor_split().",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> t = torch.arange(16.0).reshape(4,4)\n>>> t\ntensor([[ 0.,  1.,  2.,  3.],\n        [ 4.,  5.,  6.,  7.],\n        [ 8.,  9., 10., 11.],\n        [12., 13., 14., 15.]])\n>>> torch.hsplit(t, 2)\n(tensor([[ 0.,  1.],\n         [ 4.,  5.],\n         [ 8.,  9.],\n         [12., 13.]]),\n tensor([[ 2.,  3.],\n         [ 6.,  7.],\n         [10., 11.],\n         [14., 15.]]))\n>>> torch.hsplit(t, [3, 6])\n(tensor([[ 0.,  1.,  2.],\n         [ 4.,  5.,  6.],\n         [ 8.,  9., 10.],\n         [12., 13., 14.]]),\n tensor([[ 3.],\n         [ 7.],\n         [11.],\n         [15.]]),\n tensor([], size=(4, 0)))\n\n\n"
    },
    {
        "api_name": "torch.Tensor.hsplit",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.hsplit.html#torch.Tensor.hsplit",
        "api_signature": "Tensor.hsplit(split_size_or_sections)",
        "api_description": "See torch.hsplit()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.hspmm",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.hspmm.html#torch.hspmm",
        "api_signature": "torch.hspmm(mat1, mat2, *, out=None)",
        "api_description": "Performs a matrix multiplication of a sparse COO matrix mat1 and a strided matrix mat2. The\nresult is a (1 + 1)-dimensional hybrid COO matrix.",
        "return_value": "",
        "parameters": "mat1 (Tensor) – the first sparse matrix to be matrix multiplied\nmat2 (Tensor) – the second strided matrix to be matrix multiplied\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.hstack",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.hstack.html#torch.hstack",
        "api_signature": "torch.hstack(tensors, *, out=None)",
        "api_description": "Stack tensors in sequence horizontally (column wise).",
        "return_value": "",
        "parameters": "tensors (sequence of Tensors) – sequence of tensors to concatenate\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.functional.huber_loss",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.functional.huber_loss.html#torch.nn.functional.huber_loss",
        "api_signature": "torch.nn.functional.huber_loss(input, target, reduction='mean', delta=1.0)",
        "api_description": "Compute the Huber loss.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.HuberLoss",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.HuberLoss.html#torch.nn.HuberLoss",
        "api_signature": "torch.nn.HuberLoss(reduction='mean', delta=1.0)",
        "api_description": "Creates a criterion that uses a squared term if the absolute\nelement-wise error falls below delta and a delta-scaled L1 term otherwise.\nThis loss combines advantages of both L1Loss and MSELoss; the\ndelta-scaled L1 region makes the loss less sensitive to outliers than MSELoss,\nwhile the L2 region provides smoothness over L1Loss near 0. See\nHuber loss for more information.",
        "return_value": "",
        "parameters": "reduction (str, optional) – Specifies the reduction to apply to the output:\n'none' | 'mean' | 'sum'. 'none': no reduction will be applied,\n'mean': the sum of the output will be divided by the number of\nelements in the output, 'sum': the output will be summed. Default: 'mean'\ndelta (float, optional) – Specifies the threshold at which to change between delta-scaled L1 and L2 loss.\nThe value must be positive.  Default: 1.0",
        "input_shape": "\nInput: (∗)(*)(∗) where ∗*∗ means any number of dimensions.\nTarget: (∗)(*)(∗), same shape as the input.\nOutput: scalar. If reduction is 'none', then (∗)(*)(∗), same shape as the input.\n\n",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.autograd.functional.hvp",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.autograd.functional.hvp.html#torch.autograd.functional.hvp",
        "api_signature": "torch.autograd.functional.hvp(func, inputs, v=None, create_graph=False, strict=False)",
        "api_description": "Compute the dot product between the scalar function’s Hessian and a vector v at a specified point.",
        "return_value": "\ntuple with:func_output (tuple of Tensors or Tensor): output of func(inputs)\nhvp (tuple of Tensors or Tensor): result of the dot product with\nthe same shape as the inputs.\n\n\n\n",
        "parameters": "func (function) – a Python function that takes Tensor inputs and returns\na Tensor with a single element.\ninputs (tuple of Tensors or Tensor) – inputs to the function func.\nv (tuple of Tensors or Tensor) – The vector for which the Hessian vector\nproduct is computed. Must be the same size as the input of\nfunc. This argument is optional when func’s input contains\na single element and (if it is not provided) will be set as a\nTensor containing a single 1.\ncreate_graph (bool, optional) – If True, both the output and result will be\ncomputed in a differentiable way. Note that when strict is\nFalse, the result can not require gradients or be disconnected\nfrom the inputs.  Defaults to False.\nstrict (bool, optional) – If True, an error will be raised when we\ndetect that there exists an input such that all the outputs are\nindependent of it. If False, we return a Tensor of zeros as the\nhvp for said inputs, which is the expected mathematical value.\nDefaults to False.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.hypot",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.hypot.html#torch.hypot",
        "api_signature": "torch.hypot(input, other, *, out=None)",
        "api_description": "Given the legs of a right triangle, return its hypotenuse.",
        "return_value": "",
        "parameters": "input (Tensor) – the first input tensor\nother (Tensor) – the second input tensor\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.hypot",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.hypot.html#torch.Tensor.hypot",
        "api_signature": "Tensor.hypot(other)",
        "api_description": "See torch.hypot()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.hypot_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.hypot_.html#torch.Tensor.hypot_",
        "api_signature": "Tensor.hypot_(other)",
        "api_description": "In-place version of hypot()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.i0",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.i0.html#torch.i0",
        "api_signature": "torch.i0(input, *, out=None)",
        "api_description": "Alias for torch.special.i0().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.special.i0",
        "api_url": "https://pytorch.org/docs/stable/special.html#torch.special.i0",
        "api_signature": "torch.special.i0(input, *, out=None)",
        "api_description": "Computes the zeroth order modified Bessel function of the first kind for each element of input.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.i0",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.i0.html#torch.Tensor.i0",
        "api_signature": "Tensor.i0()",
        "api_description": "See torch.i0()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.i0_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.i0_.html#torch.Tensor.i0_",
        "api_signature": "Tensor.i0_()",
        "api_description": "In-place version of i0()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.special.i0e",
        "api_url": "https://pytorch.org/docs/stable/special.html#torch.special.i0e",
        "api_signature": "torch.special.i0e(input, *, out=None)",
        "api_description": "Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> torch.special.i0e(torch.arange(5, dtype=torch.float32))\ntensor([1.0000, 0.4658, 0.3085, 0.2430, 0.2070])\n\n\n"
    },
    {
        "api_name": "torch.special.i1",
        "api_url": "https://pytorch.org/docs/stable/special.html#torch.special.i1",
        "api_signature": "torch.special.i1(input, *, out=None)",
        "api_description": "Computes the first order modified Bessel function of the first kind (as defined below)\nfor each element of input.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> torch.special.i1(torch.arange(5, dtype=torch.float32))\ntensor([0.0000, 0.5652, 1.5906, 3.9534, 9.7595])\n\n\n"
    },
    {
        "api_name": "torch.special.i1e",
        "api_url": "https://pytorch.org/docs/stable/special.html#torch.special.i1e",
        "api_signature": "torch.special.i1e(input, *, out=None)",
        "api_description": "Computes the exponentially scaled first order modified Bessel function of the first kind (as defined below)\nfor each element of input.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> torch.special.i1e(torch.arange(5, dtype=torch.float32))\ntensor([0.0000, 0.2079, 0.2153, 0.1968, 0.1788])\n\n\n"
    },
    {
        "api_name": "torch.distributions.cauchy.Cauchy.icdf",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.cauchy.Cauchy.icdf",
        "api_signature": "icdf(value)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.continuous_bernoulli.ContinuousBernoulli.icdf",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.continuous_bernoulli.ContinuousBernoulli.icdf",
        "api_signature": "icdf(value)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.distribution.Distribution.icdf",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.distribution.Distribution.icdf",
        "api_signature": "icdf(value)",
        "api_description": "Returns the inverse cumulative density/mass function evaluated at\nvalue.",
        "return_value": "",
        "parameters": "value (Tensor) –",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.exponential.Exponential.icdf",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.exponential.Exponential.icdf",
        "api_signature": "icdf(value)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.half_cauchy.HalfCauchy.icdf",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.half_cauchy.HalfCauchy.icdf",
        "api_signature": "icdf(prob)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.half_normal.HalfNormal.icdf",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.half_normal.HalfNormal.icdf",
        "api_signature": "icdf(prob)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.laplace.Laplace.icdf",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.laplace.Laplace.icdf",
        "api_signature": "icdf(value)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.normal.Normal.icdf",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.normal.Normal.icdf",
        "api_signature": "icdf(value)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.transformed_distribution.TransformedDistribution.icdf",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.transformed_distribution.TransformedDistribution.icdf",
        "api_signature": "icdf(value)",
        "api_description": "Computes the inverse cumulative distribution function using\ntransform(s) and computing the score of the base distribution.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.uniform.Uniform.icdf",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.uniform.Uniform.icdf",
        "api_signature": "icdf(value)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.rpc.WorkerInfo.id",
        "api_url": "https://pytorch.org/docs/stable/rpc.html#torch.distributed.rpc.WorkerInfo.id",
        "api_signature": null,
        "api_description": "Globally unique id to identify the worker.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.package.PackageImporter.id",
        "api_url": "https://pytorch.org/docs/stable/package.html#torch.package.PackageImporter.id",
        "api_signature": "id()",
        "api_description": "Returns internal identifier that torch.package uses to distinguish PackageImporter instances.\nLooks like:",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.Identity",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.Identity.html#torch.nn.Identity",
        "api_signature": "torch.nn.Identity(*args, **kwargs)",
        "api_description": "A placeholder identity operator that is argument-insensitive.",
        "return_value": "",
        "parameters": "args (Any) – any argument (unused)\nkwargs (Any) – any keyword argument (unused)",
        "input_shape": "\nInput: (∗)(*)(∗), where ∗*∗ means any number of dimensions.\nOutput: (∗)(*)(∗), same shape as the input.\n\n",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.utils.prune.Identity",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.Identity.html#torch.nn.utils.prune.Identity",
        "api_signature": null,
        "api_description": "Utility pruning method that does not prune any units but generates the pruning parametrization with a mask of ones.",
        "return_value": "pruned version of the input tensor\npruned version of tensor t.\n",
        "parameters": "module (nn.Module) – module containing the tensor to prune\nname (str) – parameter name within module on which pruning\nwill act.\nmodule (nn.Module) – module containing the tensor to prune\nt (torch.Tensor) – tensor to prune (of same dimensions as\ndefault_mask).\nimportance_scores (torch.Tensor) – tensor of importance scores (of\nsame shape as t) used to compute mask for pruning t.\nThe values in this tensor indicate the importance of the\ncorresponding elements in the t that is being pruned.\nIf unspecified or None, the tensor t will be used in its place.\ndefault_mask (torch.Tensor, optional) – mask from previous pruning\niteration, if any. To be considered when determining what\nportion of the tensor that pruning should act on. If None,\ndefault to a mask of ones.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.utils.prune.identity",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.identity.html#torch.nn.utils.prune.identity",
        "api_signature": "torch.nn.utils.prune.identity(module, name)",
        "api_description": "Apply pruning reparametrization without pruning any units.",
        "return_value": "modified (i.e. pruned) version of the input module\n",
        "parameters": "module (nn.Module) – module containing the tensor to prune.\nname (str) – parameter name within module on which pruning\nwill act.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fft.ifft",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.fft.ifft.html#torch.fft.ifft",
        "api_signature": "torch.fft.ifft(input, n=None, dim=-1, norm=None, *, out=None)",
        "api_description": "Computes the one dimensional inverse discrete Fourier transform of input.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor\nn (int, optional) – Signal length. If given, the input will either be zero-padded\nor trimmed to this length before computing the IFFT.\ndim (int, optional) – The dimension along which to take the one dimensional IFFT.\nnorm (str, optional) – Normalization mode. For the backward transform\n(ifft()), these correspond to:\n\"forward\" - no normalization\n\"backward\" - normalize by 1/n\n\"ortho\" - normalize by 1/sqrt(n) (making the IFFT orthonormal)\nCalling the forward transform (fft()) with the same\nnormalization mode will apply an overall normalization of 1/n between\nthe two transforms. This is required to make ifft()\nthe exact inverse.\nDefault is \"backward\" (normalize by 1/n).\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fft.ifft2",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.fft.ifft2.html#torch.fft.ifft2",
        "api_signature": "torch.fft.ifft2(input, s=None, dim=(-2, -1)",
        "api_description": "Computes the 2 dimensional inverse discrete Fourier transform of input.\nEquivalent to ifftn() but IFFTs only the last two dimensions by default.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor\ns (Tuple[int], optional) – Signal size in the transformed dimensions.\nIf given, each dimension dim[i] will either be zero-padded or\ntrimmed to the length s[i] before computing the IFFT.\nIf a length -1 is specified, no padding is done in that dimension.\nDefault: s = [input.size(d) for d in dim]\ndim (Tuple[int], optional) – Dimensions to be transformed.\nDefault: last two dimensions.\nnorm (str, optional) – Normalization mode. For the backward transform\n(ifft2()), these correspond to:\n\"forward\" - no normalization\n\"backward\" - normalize by 1/n\n\"ortho\" - normalize by 1/sqrt(n) (making the IFFT orthonormal)\nWhere n = prod(s) is the logical IFFT size.\nCalling the forward transform (fft2()) with the same\nnormalization mode will apply an overall normalization of 1/n between\nthe two transforms. This is required to make ifft2()\nthe exact inverse.\nDefault is \"backward\" (normalize by 1/n).\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fft.ifftn",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.fft.ifftn.html#torch.fft.ifftn",
        "api_signature": "torch.fft.ifftn(input, s=None, dim=None, norm=None, *, out=None)",
        "api_description": "Computes the N dimensional inverse discrete Fourier transform of input.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor\ns (Tuple[int], optional) – Signal size in the transformed dimensions.\nIf given, each dimension dim[i] will either be zero-padded or\ntrimmed to the length s[i] before computing the IFFT.\nIf a length -1 is specified, no padding is done in that dimension.\nDefault: s = [input.size(d) for d in dim]\ndim (Tuple[int], optional) – Dimensions to be transformed.\nDefault: all dimensions, or the last len(s) dimensions if s is given.\nnorm (str, optional) – Normalization mode. For the backward transform\n(ifftn()), these correspond to:\n\"forward\" - no normalization\n\"backward\" - normalize by 1/n\n\"ortho\" - normalize by 1/sqrt(n) (making the IFFT orthonormal)\nWhere n = prod(s) is the logical IFFT size.\nCalling the forward transform (fftn()) with the same\nnormalization mode will apply an overall normalization of 1/n between\nthe two transforms. This is required to make ifftn()\nthe exact inverse.\nDefault is \"backward\" (normalize by 1/n).\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fft.ifftshift",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.fft.ifftshift.html#torch.fft.ifftshift",
        "api_signature": "torch.fft.ifftshift(input, dim=None)",
        "api_description": "Inverse of fftshift().",
        "return_value": "",
        "parameters": "input (Tensor) – the tensor in FFT order\ndim (int, Tuple[int], optional) – The dimensions to rearrange.\nOnly dimensions specified here will be rearranged, any other dimensions\nwill be left in their original order.\nDefault: All dimensions of input.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.igamma",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.igamma.html#torch.igamma",
        "api_signature": "torch.igamma(input, other, *, out=None)",
        "api_description": "Alias for torch.special.gammainc().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.igamma",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.igamma.html#torch.Tensor.igamma",
        "api_signature": "Tensor.igamma(other)",
        "api_description": "See torch.igamma()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.igamma_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.igamma_.html#torch.Tensor.igamma_",
        "api_signature": "Tensor.igamma_(other)",
        "api_description": "In-place version of igamma()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.igammac",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.igammac.html#torch.igammac",
        "api_signature": "torch.igammac(input, other, *, out=None)",
        "api_description": "Alias for torch.special.gammaincc().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.igammac",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.igammac.html#torch.Tensor.igammac",
        "api_signature": "Tensor.igammac(other)",
        "api_description": "See torch.igammac()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.igammac_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.igammac_.html#torch.Tensor.igammac_",
        "api_signature": "Tensor.igammac_(other)",
        "api_description": "In-place version of igammac()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.jit.ignore",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.jit.ignore.html#torch.jit.ignore",
        "api_signature": "torch.jit.ignore(drop=False, **kwargs)",
        "api_description": "This decorator indicates to the compiler that a function or method should\nbe ignored and left as a Python function. This allows you to leave code in\nyour model that is not yet TorchScript compatible. If called from TorchScript,\nignored functions will dispatch the call to the Python interpreter. Models with ignored\nfunctions cannot be exported; use @torch.jit.unused instead.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fft.ihfft",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.fft.ihfft.html#torch.fft.ihfft",
        "api_signature": "torch.fft.ihfft(input, n=None, dim=-1, norm=None, *, out=None)",
        "api_description": "Computes the inverse of hfft().",
        "return_value": "",
        "parameters": "input (Tensor) – the real input tensor\nn (int, optional) – Signal length. If given, the input will either be zero-padded\nor trimmed to this length before computing the Hermitian IFFT.\ndim (int, optional) – The dimension along which to take the one dimensional Hermitian IFFT.\nnorm (str, optional) – Normalization mode. For the backward transform\n(ihfft()), these correspond to:\n\"forward\" - no normalization\n\"backward\" - normalize by 1/n\n\"ortho\" - normalize by 1/sqrt(n) (making the IFFT orthonormal)\nCalling the forward transform (hfft()) with the same\nnormalization mode will apply an overall normalization of 1/n between\nthe two transforms. This is required to make ihfft()\nthe exact inverse.\nDefault is \"backward\" (normalize by 1/n).\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fft.ihfft2",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.fft.ihfft2.html#torch.fft.ihfft2",
        "api_signature": "torch.fft.ihfft2(input, s=None, dim=(-2, -1)",
        "api_description": "Computes the 2-dimensional inverse discrete Fourier transform of real\ninput. Equivalent to ihfftn() but transforms only the\ntwo last dimensions by default.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor\ns (Tuple[int], optional) – Signal size in the transformed dimensions.\nIf given, each dimension dim[i] will either be zero-padded or\ntrimmed to the length s[i] before computing the Hermitian IFFT.\nIf a length -1 is specified, no padding is done in that dimension.\nDefault: s = [input.size(d) for d in dim]\ndim (Tuple[int], optional) – Dimensions to be transformed.\nDefault: last two dimensions.\nnorm (str, optional) – Normalization mode. For the backward transform\n(ihfft2()), these correspond to:\n\"forward\" - no normalization\n\"backward\" - normalize by 1/n\n\"ortho\" - normalize by 1/sqrt(n) (making the Hermitian IFFT orthonormal)\nWhere n = prod(s) is the logical IFFT size.\nCalling the forward transform (hfft2()) with the same\nnormalization mode will apply an overall normalization of 1/n between\nthe two transforms. This is required to make ihfft2()\nthe exact inverse.\nDefault is \"backward\" (normalize by 1/n).\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fft.ihfftn",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.fft.ihfftn.html#torch.fft.ihfftn",
        "api_signature": "torch.fft.ihfftn(input, s=None, dim=None, norm=None, *, out=None)",
        "api_description": "Computes the N-dimensional inverse discrete Fourier transform of real input.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor\ns (Tuple[int], optional) – Signal size in the transformed dimensions.\nIf given, each dimension dim[i] will either be zero-padded or\ntrimmed to the length s[i] before computing the Hermitian IFFT.\nIf a length -1 is specified, no padding is done in that dimension.\nDefault: s = [input.size(d) for d in dim]\ndim (Tuple[int], optional) – Dimensions to be transformed.\nDefault: all dimensions, or the last len(s) dimensions if s is given.\nnorm (str, optional) – Normalization mode. For the backward transform\n(ihfftn()), these correspond to:\n\"forward\" - no normalization\n\"backward\" - normalize by 1/n\n\"ortho\" - normalize by 1/sqrt(n) (making the Hermitian IFFT orthonormal)\nWhere n = prod(s) is the logical IFFT size.\nCalling the forward transform (hfftn()) with the same\nnormalization mode will apply an overall normalization of 1/n between\nthe two transforms. This is required to make ihfftn()\nthe exact inverse.\nDefault is \"backward\" (normalize by 1/n).\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.imag",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.imag.html#torch.Tensor.imag",
        "api_signature": null,
        "api_description": "Returns a new tensor containing imaginary values of the self tensor.\nThe returned tensor and self share the same underlying storage.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> x=torch.randn(4, dtype=torch.cfloat)\n>>> x\ntensor([(0.3100+0.3553j), (-0.5445-0.7896j), (-1.6492-0.0633j), (-0.0638-0.8119j)])\n>>> x.imag\ntensor([ 0.3553, -0.7896, -0.0633, -0.8119])\n\n\n"
    },
    {
        "api_name": "torch.imag",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.imag.html#torch.imag",
        "api_signature": "torch.imag(input)",
        "api_description": "Returns a new tensor containing imaginary values of the self tensor.\nThe returned tensor and self share the same underlying storage.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.library.impl",
        "api_url": "https://pytorch.org/docs/stable/library.html#torch.library.impl",
        "api_signature": "torch.library.impl(qualname, types, func=None, *, lib=None)",
        "api_description": "",
        "return_value": "",
        "parameters": "qualname (str) – Should be a string that looks like “namespace::operator_name”.\ntypes (str | Sequence[str]) – The device types to register an impl to.\nlib (Optional[Library]) – If provided, the lifetime of this registration\nwill be tied to the lifetime of the Library object.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.library.Library.impl",
        "api_url": "https://pytorch.org/docs/stable/library.html#torch.library.Library.impl",
        "api_signature": "impl(op_name, fn, dispatch_key='')",
        "api_description": "Registers the function implementation for an operator defined in the library.",
        "return_value": "",
        "parameters": "op_name – operator name (along with the overload) or OpOverload object.\nfn – function that’s the operator implementation for the input dispatch key or fallthrough_kernel()\nto register a fallthrough.\ndispatch_key – dispatch key that the input function should be registered for. By default, it uses\nthe dispatch key that the library was created with.",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> my_lib = Library(\"aten\", \"IMPL\")\n>>> def div_cpu(self, other):\n>>>     return self * (1 / other)\n>>> my_lib.impl(\"div.Tensor\", div_cpu, \"CPU\")\n\n\n"
    },
    {
        "api_name": "torch.library.impl_abstract",
        "api_url": "https://pytorch.org/docs/stable/library.html#torch.library.impl_abstract",
        "api_signature": "torch.library.impl_abstract(qualname, func=None, *, lib=None, _stacklevel=1)",
        "api_description": "Register an abstract implementation for this operator.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.package.PackageImporter.import_module",
        "api_url": "https://pytorch.org/docs/stable/package.html#torch.package.PackageImporter.import_module",
        "api_signature": "import_module(name, package=None)",
        "api_description": "Load a module from the package if it hasn’t already been loaded, and then return\nthe module. Modules are loaded locally\nto the importer and will appear in self.modules rather than sys.modules.",
        "return_value": "The (possibly already) loaded module.\n",
        "parameters": "name (str) – Fully qualified name of the module to load.\npackage ([type], optional) – Unused, but present to match the signature of importlib.import_module. Defaults to None.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.autograd.profiler_util.MemRecordsAcc.in_interval",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.autograd.profiler_util.MemRecordsAcc.html#torch.autograd.profiler_util.MemRecordsAcc.in_interval",
        "api_signature": "in_interval(start_us, end_us)",
        "api_description": "Return all records in the given interval",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.cpp_extension.include_paths",
        "api_url": "https://pytorch.org/docs/stable/cpp_extension.html#torch.utils.cpp_extension.include_paths",
        "api_signature": "torch.utils.cpp_extension.include_paths(cuda=False)",
        "api_description": "Get the include paths required to build a C++ or CUDA extension.",
        "return_value": "A list of include path strings.\n",
        "parameters": "cuda (bool) – If True, includes CUDA-specific include paths.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.autograd.profiler.KinetoStepTracker.increment_step",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.autograd.profiler.KinetoStepTracker.html#torch.autograd.profiler.KinetoStepTracker.increment_step",
        "api_signature": "increment_step(requester)",
        "api_description": "Increments the step count for the requester.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.autograd.graph.increment_version",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.autograd.graph.increment_version.html#torch.autograd.graph.increment_version",
        "api_signature": "torch.autograd.graph.increment_version(tensor)",
        "api_description": "Update autograd metadata tracking whether the given Tensor was modified in place.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.independent.Independent",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.independent.Independent",
        "api_signature": "torch.distributions.independent.Independent(base_distribution, reinterpreted_batch_ndims, validate_args=None)",
        "api_description": "Bases: Distribution",
        "return_value": "",
        "parameters": "base_distribution (torch.distributions.distribution.Distribution) – a\nbase distribution\nreinterpreted_batch_ndims (int) – the number of batch dims to\nreinterpret as event dims",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.constraints.independent",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.constraints.independent",
        "api_signature": null,
        "api_description": "alias of _IndependentConstraint",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.transforms.IndependentTransform",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.transforms.IndependentTransform",
        "api_signature": "torch.distributions.transforms.IndependentTransform(base_transform, reinterpreted_batch_ndims, cache_size=0)",
        "api_description": "Wrapper around another transform to treat\nreinterpreted_batch_ndims-many extra of the right most dimensions as\ndependent. This has no effect on the forward or backward transforms, but\ndoes sum out reinterpreted_batch_ndims-many of the rightmost dimensions\nin log_abs_det_jacobian().",
        "return_value": "",
        "parameters": "base_transform (Transform) – A base transform.\nreinterpreted_batch_ndims (int) – The number of extra rightmost\ndimensions to treat as dependent.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.GradBucket.index",
        "api_url": "https://pytorch.org/docs/stable/ddp_comm_hooks.html#torch.distributed.GradBucket.index",
        "api_signature": "torch.distributed.GradBucket.index(self: torch._C._distributed_c10d.GradBucket)",
        "api_description": "Warning",
        "return_value": "The index of a bucket that stores gradients of a few contiguous layers.\nAll the gradients are bucketized.\n",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.autograd.forward_ad.UnpackedDualTensor.index",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.autograd.forward_ad.UnpackedDualTensor.html#torch.autograd.forward_ad.UnpackedDualTensor.index",
        "api_signature": "index(value, start=0, stop=9223372036854775807, /)",
        "api_description": "Return first index of value.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.autograd.profiler_util.Kernel.index",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.autograd.profiler_util.Kernel.html#torch.autograd.profiler_util.Kernel.index",
        "api_signature": "index(value, start=0, stop=9223372036854775807, /)",
        "api_description": "Return first index of value.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.jit.Attribute.index",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.jit.Attribute.html#torch.jit.Attribute.index",
        "api_signature": "index(value, start=0, stop=9223372036854775807, /)",
        "api_description": "Return first index of value.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.utils.rnn.PackedSequence.index",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.PackedSequence.html#torch.nn.utils.rnn.PackedSequence.index",
        "api_signature": "index(value, start=0, stop=9223372036854775807, /)",
        "api_description": "Return first index of value.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.index_add",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.index_add.html#torch.index_add",
        "api_signature": "torch.index_add(input, dim, index, source, *, alpha=1, out=None)",
        "api_description": "See index_add_() for function description.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.index_add",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.index_add.html#torch.Tensor.index_add",
        "api_signature": "Tensor.index_add(dim, index, source, *, alpha=1)",
        "api_description": "Out-of-place version of torch.Tensor.index_add_().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.index_add_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.index_add_.html#torch.Tensor.index_add_",
        "api_signature": "Tensor.index_add_(dim, index, source, *, alpha=1)",
        "api_description": "Accumulate the elements of alpha times source into the self\ntensor by adding to the indices in the order given in index. For example,\nif dim == 0, index[i] == j, and alpha=-1, then the ith row of\nsource is subtracted from the jth row of self.",
        "return_value": "",
        "parameters": "dim (int) – dimension along which to index\nindex (Tensor) – indices of source to select from,\nshould have dtype either torch.int64 or torch.int32\nsource (Tensor) – the tensor containing values to add\nalpha (Number) – the scalar multiplier for source",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.index_copy",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.index_copy.html#torch.index_copy",
        "api_signature": "torch.index_copy(input, dim, index, source, *, out=None)",
        "api_description": "See index_add_() for function description.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.index_copy",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.index_copy.html#torch.Tensor.index_copy",
        "api_signature": "Tensor.index_copy(dim, index, tensor2)",
        "api_description": "Out-of-place version of torch.Tensor.index_copy_().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.index_copy_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.index_copy_.html#torch.Tensor.index_copy_",
        "api_signature": "Tensor.index_copy_(dim, index, tensor)",
        "api_description": "Copies the elements of tensor into the self tensor by selecting\nthe indices in the order given in index. For example, if dim == 0\nand index[i] == j, then the ith row of tensor is copied to the\njth row of self.",
        "return_value": "",
        "parameters": "dim (int) – dimension along which to index\nindex (LongTensor) – indices of tensor to select from\ntensor (Tensor) – the tensor containing values to copy",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.index_fill",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.index_fill.html#torch.Tensor.index_fill",
        "api_signature": "Tensor.index_fill(dim, index, value)",
        "api_description": "Out-of-place version of torch.Tensor.index_fill_().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.index_fill_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.index_fill_.html#torch.Tensor.index_fill_",
        "api_signature": "Tensor.index_fill_(dim, index, value)",
        "api_description": "Fills the elements of the self tensor with value value by\nselecting the indices in the order given in index.",
        "return_value": "",
        "parameters": "dim (int) – dimension along which to index\nindex (LongTensor) – indices of self tensor to fill in\nvalue (float) – the value to fill with",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> x = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=torch.float)\n>>> index = torch.tensor([0, 2])\n>>> x.index_fill_(1, index, -1)\ntensor([[-1.,  2., -1.],\n        [-1.,  5., -1.],\n        [-1.,  8., -1.]])\n\n\n"
    },
    {
        "api_name": "torch.Tensor.index_put",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.index_put.html#torch.Tensor.index_put",
        "api_signature": "Tensor.index_put(indices, values, accumulate=False)",
        "api_description": "Out-place version of index_put_().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.index_put_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.index_put_.html#torch.Tensor.index_put_",
        "api_signature": "Tensor.index_put_(indices, values, accumulate=False)",
        "api_description": "Puts values from the tensor values into the tensor self using\nthe indices specified in indices (which is a tuple of Tensors). The\nexpression tensor.index_put_(indices, values) is equivalent to\ntensor[indices] = values. Returns self.",
        "return_value": "",
        "parameters": "indices (tuple of LongTensor) – tensors used to index into self.\nvalues (Tensor) – tensor of same dtype as self.\naccumulate (bool) – whether to accumulate into self",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.index_reduce",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.index_reduce.html#torch.index_reduce",
        "api_signature": "torch.index_reduce(input, dim, index, source, reduce, *, include_self=True, out=None)",
        "api_description": "See index_reduce_() for function description.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.index_reduce",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.index_reduce.html#torch.Tensor.index_reduce",
        "api_signature": "Tensor.index_reduce()",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.index_reduce_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.index_reduce_.html#torch.Tensor.index_reduce_",
        "api_signature": "Tensor.index_reduce_(dim, index, source, reduce, *, include_self=True)",
        "api_description": "Accumulate the elements of source into the self\ntensor by accumulating to the indices in the order given in index\nusing the reduction given by the reduce argument. For example, if dim == 0,\nindex[i] == j, reduce == prod and include_self == True then the ith\nrow of source is multiplied by the jth row of self. If\ninclude_self=\"True\", the values in the self tensor are included\nin the reduction, otherwise, rows in the self tensor that are accumulated\nto are treated as if they were filled with the reduction identites.",
        "return_value": "",
        "parameters": "dim (int) – dimension along which to index\nindex (Tensor) – indices of source to select from,\nshould have dtype either torch.int64 or torch.int32\nsource (FloatTensor) – the tensor containing values to accumulate\nreduce (str) – the reduction operation to apply\n(\"prod\", \"mean\", \"amax\", \"amin\")\ninclude_self (bool) – whether the elements from the self tensor are\nincluded in the reduction",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.index_select",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.index_select.html#torch.index_select",
        "api_signature": "torch.index_select(input, dim, index, *, out=None)",
        "api_description": "Returns a new tensor which indexes the input tensor along dimension\ndim using the entries in index which is a LongTensor.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.\ndim (int) – the dimension in which we index\nindex (IntTensor or LongTensor) – the 1-D tensor containing the indices to index\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.index_select",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.index_select.html#torch.Tensor.index_select",
        "api_signature": "Tensor.index_select(dim, index)",
        "api_description": "See torch.index_select()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.indices",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.indices.html#torch.Tensor.indices",
        "api_signature": "Tensor.indices()",
        "api_description": "Return the indices tensor of a sparse COO tensor.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.autograd.grad_mode.inference_mode",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.autograd.grad_mode.inference_mode.html#torch.autograd.grad_mode.inference_mode",
        "api_signature": "torch.autograd.grad_mode.inference_mode(mode=True)",
        "api_description": "Context-manager that enables or disables inference mode.",
        "return_value": "",
        "parameters": "mode (bool or function) – Either a boolean flag whether to enable or\ndisable inference mode or a Python function to decorate with\ninference mode enabled",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import torch\n>>> x = torch.ones(1, 2, 3, requires_grad=True)\n>>> with torch.inference_mode():\n...     y = x * x\n>>> y.requires_grad\nFalse\n>>> y._version\nTraceback (most recent call last):\nFile \"<stdin>\", line 1, in <module>\nRuntimeError: Inference tensors do not track version counter.\n>>> @torch.inference_mode()\n... def func(x):\n...     return x * x\n>>> out = func(x)\n>>> out.requires_grad\nFalse\n>>> @torch.inference_mode\n... def doubler(x):\n...     return x * 2\n>>> out = doubler(x)\n>>> out.requires_grad\nFalse\n\n\n"
    },
    {
        "api_name": "torch.cuda.init",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.cuda.init.html#torch.cuda.init",
        "api_signature": "torch.cuda.init()",
        "api_description": "Initialize PyTorch’s CUDA state.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.xpu.init",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.xpu.init.html#torch.xpu.init",
        "api_signature": "torch.xpu.init()",
        "api_description": "Initialize PyTorch’s XPU state.\nThis is a Python API about lazy initialization that avoids initializing\nXPU until the first time it is accessed. Does nothing if the XPU state is\nalready initialized.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.device_mesh.init_device_mesh",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#torch.distributed.device_mesh.init_device_mesh",
        "api_signature": "torch.distributed.device_mesh.init_device_mesh(device_type, mesh_shape, *, mesh_dim_names=None)",
        "api_description": "Initializes a DeviceMesh based on device_type, mesh_shape, and mesh_dim_names parameters.",
        "return_value": "A DeviceMesh object representing the device layout.\n",
        "parameters": "device_type (str) – The device type of the mesh. Currently supports: “cpu”, “cuda/cuda-like”.\nmesh_shape (Tuple[int]) – A tuple defining the dimensions of the multi-dimensional array\ndescribing the layout of devices.\nmesh_dim_names (Tuple[str], optional) – A tuple of mesh dimension names to assign to each dimension\nof the multi-dimensional array describing the layout of devices. Its length must match the length\nof mesh_shape. Each string in mesh_dim_names must be unique.",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> from torch.distributed.device_mesh import init_device_mesh\n>>>\n>>> mesh_1d = init_device_mesh(\"cuda\", mesh_shape=(8,))\n>>> mesh_2d = init_device_mesh(\"cuda\", mesh_shape=(2, 8), mesh_dim_names=(\"dp\", \"tp\"))\n\n\n"
    },
    {
        "api_name": "torch.distributed.rpc.RpcBackendOptions.init_method",
        "api_url": "https://pytorch.org/docs/stable/rpc.html#torch.distributed.rpc.RpcBackendOptions.init_method",
        "api_signature": null,
        "api_description": "URL specifying how to initialize the process group.\nDefault is env://",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.rpc.TensorPipeRpcBackendOptions.init_method",
        "api_url": "https://pytorch.org/docs/stable/rpc.html#torch.distributed.rpc.TensorPipeRpcBackendOptions.init_method",
        "api_signature": null,
        "api_description": "URL specifying how to initialize the process group.\nDefault is env://",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.init_process_group",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#torch.distributed.init_process_group",
        "api_signature": "torch.distributed.init_process_group(backend=None, init_method=None, timeout=None, world_size=-1, rank=-1, store=None, group_name='', pg_options=None, device_id=None)",
        "api_description": "Initialize the default distributed process group.",
        "return_value": "",
        "parameters": "backend (str or Backend, optional) – The backend to use. Depending on\nbuild-time configurations, valid values include mpi, gloo,\nnccl, and ucc. If the backend is not provided, then both a gloo\nand nccl backend will be created, see notes below for how multiple\nbackends are managed. This field can be given as a lowercase string\n(e.g., \"gloo\"), which can also be accessed via\nBackend attributes (e.g., Backend.GLOO). If using\nmultiple processes per machine with nccl backend, each process\nmust have exclusive access to every GPU it uses, as sharing GPUs\nbetween processes can result in deadlocks. ucc backend is\nexperimental.\ninit_method (str, optional) – URL specifying how to initialize the\nprocess group. Default is “env://” if no\ninit_method or store is specified.\nMutually exclusive with store.\nworld_size (int, optional) – Number of processes participating in\nthe job. Required if store is specified.\nrank (int, optional) – Rank of the current process (it should be a\nnumber between 0 and world_size-1).\nRequired if store is specified.\nstore (Store, optional) – Key/value store accessible to all workers, used\nto exchange connection/address information.\nMutually exclusive with init_method.\ntimeout (timedelta, optional) – Timeout for operations executed against\nthe process group. Default value is 10 minutes for NCCL and 30 minutes for other backends.\nThis is the duration after which collectives will be aborted asynchronously and the process will crash.\nThis is done since CUDA execution is async and it is no longer safe to continue executing user code since\nfailed async NCCL operations might result in subsequent CUDA operations running on corrupted data.\nWhen TORCH_NCCL_BLOCKING_WAIT is set, the process will block and wait for this timeout.\ngroup_name (str, optional, deprecated) – Group name. This argument is ignored\npg_options (ProcessGroupOptions, optional) – process group options\nspecifying what additional options need to be passed in during\nthe construction of specific process groups. As of now, the only\noptions we support is ProcessGroupNCCL.Options for the nccl\nbackend, is_high_priority_stream can be specified so that\nthe nccl backend can pick up high priority cuda streams when\nthere’re compute kernels waiting.\ndevice_id (torch.device, optional) – a single, specific device\nto “bind” this process to, allowing for backend-specific\noptimizations.  Currently this has two effects, only under\nNCCL: the communicator is immediately formed (calling\nncclCommInit* immediately rather than the normal lazy\ncall) and sub-groups will use ncclCommSplit when\npossible to avoid unnecessary overhead of group creation. If you\nwant to know NCCL initialization error early, you can also use this\nfield.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.rpc.init_rpc",
        "api_url": "https://pytorch.org/docs/stable/rpc.html#torch.distributed.rpc.init_rpc",
        "api_signature": "torch.distributed.rpc.init_rpc(name, backend=None, rank=-1, world_size=None, rpc_backend_options=None)",
        "api_description": "Initializes RPC primitives such as the local RPC agent\nand distributed autograd, which immediately makes the current\nprocess ready to send and receive RPCs.",
        "return_value": "",
        "parameters": "name (str) – a globally unique name of this node. (e.g.,\nTrainer3, ParameterServer2, Master, Worker1)\nName can only contain number, alphabet, underscore, colon,\nand/or dash, and must be shorter than 128 characters.\nbackend (BackendType, optional) – The type of RPC backend\nimplementation. Supported values is\nBackendType.TENSORPIPE (the default).\nSee Backends for more information.\nrank (int) – a globally unique id/rank of this node.\nworld_size (int) – The number of workers in the group.\nrpc_backend_options (RpcBackendOptions, optional) – The options\npassed to the RpcAgent constructor. It must be an agent-specific\nsubclass of RpcBackendOptions\nand contains agent-specific initialization configurations. By\ndefault, for all agents, it sets the default timeout to 60\nseconds and performs the rendezvous with an underlying process\ngroup initialized using init_method = \"env://\",\nmeaning that environment variables MASTER_ADDR and\nMASTER_PORT need to be set properly. See\nBackends for more information and find which options\nare available.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.autograd.profiler.KinetoStepTracker.init_step_count",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.autograd.profiler.KinetoStepTracker.html#torch.autograd.profiler.KinetoStepTracker.init_step_count",
        "api_signature": "init_step_count(requester)",
        "api_description": "Initialize for a given requester.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.initial_seed",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.initial_seed.html#torch.initial_seed",
        "api_signature": "torch.initial_seed()",
        "api_description": "Returns the initial seed for generating random numbers as a\nPython long.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda.initial_seed",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.cuda.initial_seed.html#torch.cuda.initial_seed",
        "api_signature": "torch.cuda.initial_seed()",
        "api_description": "Return the current random seed of the current GPU.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.random.initial_seed",
        "api_url": "https://pytorch.org/docs/stable/random.html#torch.random.initial_seed",
        "api_signature": "torch.random.initial_seed()",
        "api_description": "Returns the initial seed for generating random numbers as a\nPython long.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.xpu.initial_seed",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.xpu.initial_seed.html#torch.xpu.initial_seed",
        "api_signature": "torch.xpu.initial_seed()",
        "api_description": "Return the current random seed of the current GPU.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Generator.initial_seed",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator.initial_seed",
        "api_signature": "initial_seed()",
        "api_description": "Returns the initial seed for generating random numbers.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.modules.lazy.LazyModuleMixin.initialize_parameters",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.modules.lazy.LazyModuleMixin.html#torch.nn.modules.lazy.LazyModuleMixin.initialize_parameters",
        "api_signature": "initialize_parameters(*args, **kwargs)",
        "api_description": "Initialize parameters according to the input batch properties.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.jit.ScriptModule.inlined_graph",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.jit.ScriptModule.html#torch.jit.ScriptModule.inlined_graph",
        "api_signature": null,
        "api_description": "Return a string representation of the internal graph for the forward method.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.inner",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.inner.html#torch.inner",
        "api_signature": "torch.inner(input, other, *, out=None)",
        "api_description": "Computes the dot product for 1D tensors. For higher dimensions, sums the product\nof elements from input and other along their last dimension.",
        "return_value": "",
        "parameters": "input (Tensor) – First input tensor\nother (Tensor) – Second input tensor\nout (Tensor, optional) – Optional output tensor to write result into. The output\nshape is input.shape[:-1] + other.shape[:-1].",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.inner",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.inner.html#torch.Tensor.inner",
        "api_signature": "Tensor.inner(other)",
        "api_description": "See torch.inner().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.autograd.function.InplaceFunction",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.autograd.function.InplaceFunction.html#torch.autograd.function.InplaceFunction",
        "api_signature": "torch.autograd.function.InplaceFunction(inplace=False)",
        "api_description": "This class is here only for backward compatibility reasons.\nUse Function instead of this for any new use case.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> class Inplace(Function):\n>>>     @staticmethod\n>>>     def forward(ctx, x):\n>>>         x_npy = x.numpy() # x_npy shares storage with x\n>>>         x_npy += 1\n>>>         ctx.mark_dirty(x)\n>>>         return x\n>>>\n>>>     @staticmethod\n>>>     @once_differentiable\n>>>     def backward(ctx, grad_output):\n>>>         return grad_output\n>>>\n>>> a = torch.tensor(1., requires_grad=True, dtype=torch.double).clone()\n>>> b = a * a\n>>> Inplace.apply(a)  # This would lead to wrong gradients!\n>>>                   # but the engine would not know unless we mark_dirty\n>>> b.backward() # RuntimeError: one of the variables needed for gradient\n>>>              # computation has been modified by an inplace operation\n\n\n>>> class Func(Function):\n>>>     @staticmethod\n>>>     def forward(ctx, x: torch.Tensor, y: torch.Tensor, z: int):\n>>>         w = x * z\n>>>         out = x * y + y * z + w * y\n>>>         ctx.save_for_backward(x, y, w, out)\n>>>         ctx.z = z  # z is not a tensor\n>>>         return out\n>>>\n>>>     @staticmethod\n>>>     @once_differentiable\n>>>     def backward(ctx, grad_out):\n>>>         x, y, w, out = ctx.saved_tensors\n>>>         z = ctx.z\n>>>         gx = grad_out * (y + y * z)\n>>>         gy = grad_out * (x + z + w)\n>>>         gz = None\n>>>         return gx, gy, gz\n>>>\n>>> a = torch.tensor(1., requires_grad=True, dtype=torch.double)\n>>> b = torch.tensor(2., requires_grad=True, dtype=torch.double)\n>>> c = 4\n>>> d = Func.apply(a, b, c)\n\n\n>>> class Func(torch.autograd.Function):\n>>>     @staticmethod\n>>>     def forward(ctx, x: torch.Tensor, y: torch.Tensor, z: int):\n>>>         ctx.save_for_backward(x, y)\n>>>         ctx.save_for_forward(x, y)\n>>>         ctx.z = z\n>>>         return x * y * z\n>>>\n>>>     @staticmethod\n>>>     def jvp(ctx, x_t, y_t, _):\n>>>         x, y = ctx.saved_tensors\n>>>         z = ctx.z\n>>>         return z * (y * x_t + x * y_t)\n>>>\n>>>     @staticmethod\n>>>     def vjp(ctx, grad_out):\n>>>         x, y = ctx.saved_tensors\n>>>         z = ctx.z\n>>>         return z * grad_out * y, z * grad_out * x, None\n>>>\n>>>     a = torch.tensor(1., requires_grad=True, dtype=torch.double)\n>>>     t = torch.tensor(1., dtype=torch.double)\n>>>     b = torch.tensor(2., requires_grad=True, dtype=torch.double)\n>>>     c = 4\n>>>\n>>>     with fwAD.dual_level():\n>>>         a_dual = fwAD.make_dual(a, t)\n>>>         d = Func.apply(a_dual, b, c)\n\n\n>>> class SimpleFunc(Function):\n>>>     @staticmethod\n>>>     def forward(ctx, x):\n>>>         return x.clone(), x.clone()\n>>>\n>>>     @staticmethod\n>>>     @once_differentiable\n>>>     def backward(ctx, g1, g2):\n>>>         return g1 + g2  # No check for None necessary\n>>>\n>>> # We modify SimpleFunc to handle non-materialized grad outputs\n>>> class Func(Function):\n>>>     @staticmethod\n>>>     def forward(ctx, x):\n>>>         ctx.set_materialize_grads(False)\n>>>         ctx.save_for_backward(x)\n>>>         return x.clone(), x.clone()\n>>>\n>>>     @staticmethod\n>>>     @once_differentiable\n>>>     def backward(ctx, g1, g2):\n>>>         x, = ctx.saved_tensors\n>>>         grad_input = torch.zeros_like(x)\n>>>         if g1 is not None:  # We must check for None now\n>>>             grad_input += g1\n>>>         if g2 is not None:\n>>>             grad_input += g2\n>>>         return grad_input\n>>>\n>>> a = torch.tensor(1., requires_grad=True)\n>>> b, _ = Func.apply(a)  # induces g2 to be undefined\n\n\n"
    },
    {
        "api_name": "torch.ao.quantization.backend_config.ObservationType.INPUT_OUTPUT_NOT_OBSERVED",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.quantization.backend_config.ObservationType.html#torch.ao.quantization.backend_config.ObservationType.INPUT_OUTPUT_NOT_OBSERVED",
        "api_signature": null,
        "api_description": "this means the input and output are never observed\nexample: x.shape, x.size",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.export.graph_signature.InputKind",
        "api_url": "https://pytorch.org/docs/stable/export.html#torch.export.graph_signature.InputKind",
        "api_signature": "torch.export.graph_signature.InputKind(value)",
        "api_description": "An enumeration.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.export.graph_signature.InputSpec",
        "api_url": "https://pytorch.org/docs/stable/export.html#torch.export.graph_signature.InputSpec",
        "api_signature": "torch.export.graph_signature.InputSpec(kind: torch.export.graph_signature.InputKind, arg: Union[torch.export.graph_signature.TensorArgument, torch.export.graph_signature.SymIntArgument, torch.export.graph_signature.ConstantArgument, torch.export.graph_signature.CustomObjArgument], target: Union[str, NoneType], persistent: Union[bool, NoneType] = None)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.ModuleList.insert",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.ModuleList.html#torch.nn.ModuleList.insert",
        "api_signature": "insert(index, module)",
        "api_description": "Insert a given module before a given index in the list.",
        "return_value": "",
        "parameters": "index (int) – index to insert.\nmodule (nn.Module) – module to insert",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.Node.insert_arg",
        "api_url": "https://pytorch.org/docs/stable/fx.html#torch.fx.Node.insert_arg",
        "api_signature": "insert_arg(idx, arg)",
        "api_description": "Insert an positional argument to the argument list with given index.",
        "return_value": "",
        "parameters": "idx (int) – The index of the element in self.args to be inserted before.\narg (Argument) – The new argument value to insert into args",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.Graph.inserting_after",
        "api_url": "https://pytorch.org/docs/stable/fx.html#torch.fx.Graph.inserting_after",
        "api_signature": "inserting_after(n=None)",
        "api_description": "When used within a ‘with’ statement, this will temporary set the insert point and\nthen restore it when the with statement exits:",
        "return_value": "A resource manager that will restore the insert point on __exit__.\n",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.Graph.inserting_before",
        "api_url": "https://pytorch.org/docs/stable/fx.html#torch.fx.Graph.inserting_before",
        "api_signature": "inserting_before(n=None)",
        "api_description": "When used within a ‘with’ statement, this will temporary set the insert point and\nthen restore it when the with statement exits:",
        "return_value": "A resource manager that will restore the insert point on __exit__.\n",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.functional.instance_norm",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.functional.instance_norm.html#torch.nn.functional.instance_norm",
        "api_signature": "torch.nn.functional.instance_norm(input, running_mean=None, running_var=None, weight=None, bias=None, use_input_stats=True, momentum=0.1, eps=1e-05)",
        "api_description": "Apply Instance Normalization independently for each channel in every data sample within a batch.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.quantized.InstanceNorm1d",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.nn.quantized.InstanceNorm1d.html#torch.ao.nn.quantized.InstanceNorm1d",
        "api_signature": "torch.ao.nn.quantized.InstanceNorm1d(num_features, weight, bias, scale, zero_point, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False, device=None, dtype=None)",
        "api_description": "This is the quantized version of InstanceNorm1d.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.InstanceNorm1d",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.InstanceNorm1d.html#torch.nn.InstanceNorm1d",
        "api_signature": "torch.nn.InstanceNorm1d(num_features, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False, device=None, dtype=None)",
        "api_description": "Applies Instance Normalization.",
        "return_value": "",
        "parameters": "num_features (int) – number of features or channels CCC of the input\neps (float) – a value added to the denominator for numerical stability. Default: 1e-5\nmomentum (float) – the value used for the running_mean and running_var computation. Default: 0.1\naffine (bool) – a boolean value that when set to True, this module has\nlearnable affine parameters, initialized the same way as done for batch normalization.\nDefault: False.\ntrack_running_stats (bool) – a boolean value that when set to True, this\nmodule tracks the running mean and variance, and when set to False,\nthis module does not track such statistics and always uses batch\nstatistics in both training and eval modes. Default: False",
        "input_shape": "\nInput: (N,C,L)(N, C, L)(N,C,L) or (C,L)(C, L)(C,L)\nOutput: (N,C,L)(N, C, L)(N,C,L) or (C,L)(C, L)(C,L) (same shape as input)\n\n",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.quantized.InstanceNorm2d",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.nn.quantized.InstanceNorm2d.html#torch.ao.nn.quantized.InstanceNorm2d",
        "api_signature": "torch.ao.nn.quantized.InstanceNorm2d(num_features, weight, bias, scale, zero_point, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False, device=None, dtype=None)",
        "api_description": "This is the quantized version of InstanceNorm2d.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.InstanceNorm2d",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.InstanceNorm2d.html#torch.nn.InstanceNorm2d",
        "api_signature": "torch.nn.InstanceNorm2d(num_features, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False, device=None, dtype=None)",
        "api_description": "Applies Instance Normalization.",
        "return_value": "",
        "parameters": "num_features (int) – CCC from an expected input of size\n(N,C,H,W)(N, C, H, W)(N,C,H,W) or (C,H,W)(C, H, W)(C,H,W)\neps (float) – a value added to the denominator for numerical stability. Default: 1e-5\nmomentum (float) – the value used for the running_mean and running_var computation. Default: 0.1\naffine (bool) – a boolean value that when set to True, this module has\nlearnable affine parameters, initialized the same way as done for batch normalization.\nDefault: False.\ntrack_running_stats (bool) – a boolean value that when set to True, this\nmodule tracks the running mean and variance, and when set to False,\nthis module does not track such statistics and always uses batch\nstatistics in both training and eval modes. Default: False",
        "input_shape": "\nInput: (N,C,H,W)(N, C, H, W)(N,C,H,W) or (C,H,W)(C, H, W)(C,H,W)\nOutput: (N,C,H,W)(N, C, H, W)(N,C,H,W) or (C,H,W)(C, H, W)(C,H,W) (same shape as input)\n\n",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.quantized.InstanceNorm3d",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.nn.quantized.InstanceNorm3d.html#torch.ao.nn.quantized.InstanceNorm3d",
        "api_signature": "torch.ao.nn.quantized.InstanceNorm3d(num_features, weight, bias, scale, zero_point, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False, device=None, dtype=None)",
        "api_description": "This is the quantized version of InstanceNorm3d.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.InstanceNorm3d",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.InstanceNorm3d.html#torch.nn.InstanceNorm3d",
        "api_signature": "torch.nn.InstanceNorm3d(num_features, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False, device=None, dtype=None)",
        "api_description": "Applies Instance Normalization.",
        "return_value": "",
        "parameters": "num_features (int) – CCC from an expected input of size\n(N,C,D,H,W)(N, C, D, H, W)(N,C,D,H,W) or (C,D,H,W)(C, D, H, W)(C,D,H,W)\neps (float) – a value added to the denominator for numerical stability. Default: 1e-5\nmomentum (float) – the value used for the running_mean and running_var computation. Default: 0.1\naffine (bool) – a boolean value that when set to True, this module has\nlearnable affine parameters, initialized the same way as done for batch normalization.\nDefault: False.\ntrack_running_stats (bool) – a boolean value that when set to True, this\nmodule tracks the running mean and variance, and when set to False,\nthis module does not track such statistics and always uses batch\nstatistics in both training and eval modes. Default: False",
        "input_shape": "\nInput: (N,C,D,H,W)(N, C, D, H, W)(N,C,D,H,W) or (C,D,H,W)(C, D, H, W)(C,D,H,W)\nOutput: (N,C,D,H,W)(N, C, D, H, W)(N,C,D,H,W) or (C,D,H,W)(C, D, H, W)(C,D,H,W) (same shape as input)\n\n",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.int",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.int.html#torch.Tensor.int",
        "api_signature": "Tensor.int(memory_format=torch.preserve_format)",
        "api_description": "self.int() is equivalent to self.to(torch.int32). See to().",
        "return_value": "",
        "parameters": "memory_format (torch.memory_format, optional) – the desired memory format of\nreturned Tensor. Default: torch.preserve_format.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.TypedStorage.int",
        "api_url": "https://pytorch.org/docs/stable/storage.html#torch.TypedStorage.int",
        "api_signature": "int()",
        "api_description": "Casts this storage to int type.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.UntypedStorage.int",
        "api_url": "https://pytorch.org/docs/stable/storage.html#torch.UntypedStorage.int",
        "api_signature": "int()",
        "api_description": "Casts this storage to int type.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.int_repr",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.int_repr.html#torch.Tensor.int_repr",
        "api_signature": "Tensor.int_repr()",
        "api_description": "Given a quantized Tensor,\nself.int_repr() returns a CPU Tensor with uint8_t as data type that stores the\nunderlying uint8_t values of the given Tensor.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.constraints.integer_interval",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.constraints.integer_interval",
        "api_signature": null,
        "api_description": "alias of _IntegerInterval",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.jit.interface",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.jit.interface.html#torch.jit.interface",
        "api_signature": "torch.jit.interface(obj)",
        "api_description": "Decorate to annotate classes or modules of different types.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.package.PackageExporter.intern",
        "api_url": "https://pytorch.org/docs/stable/package.html#torch.package.PackageExporter.intern",
        "api_signature": "intern(include, *, exclude=()",
        "api_description": "Specify modules that should be packaged. A module must match some intern pattern in order to be\nincluded in the package and have its dependencies processed recursively.",
        "return_value": "",
        "parameters": "include (Union[List[str], str]) – A string e.g. “my_package.my_subpackage”, or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as described in mock().\nexclude (Union[List[str], str]) – An optional pattern that excludes some patterns that match the include string.\nallow_empty (bool) – An optional flag that specifies whether the intern modules specified by this call\nto the intern method must be matched to some module during packaging. If an intern module glob\npattern is added with allow_empty=False, and close() is called (either explicitly or via __exit__)\nbefore any modules match that pattern, an exception is thrown. If allow_empty=True, no such exception is thrown.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.package.PackageExporter.interned_modules",
        "api_url": "https://pytorch.org/docs/stable/package.html#torch.package.PackageExporter.interned_modules",
        "api_signature": "interned_modules()",
        "api_description": "Return all modules that are currently interned.",
        "return_value": "A list containing the names of modules which will be\ninterned in this package.\n",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.quantized.functional.interpolate",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.nn.quantized.functional.interpolate.html#torch.ao.nn.quantized.functional.interpolate",
        "api_signature": "torch.ao.nn.quantized.functional.interpolate(input, size=None, scale_factor=None, mode='nearest', align_corners=None)",
        "api_description": "Down/up samples the input to either the given size or the given\nscale_factor",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor\nsize (int or Tuple[int] or Tuple[int, int] or Tuple[int, int, int]) – output spatial size.\nscale_factor (float or Tuple[float]) – multiplier for spatial size. Has to match input size if it is a tuple.\nmode (str) – algorithm used for upsampling:\n'nearest' | 'bilinear'\nalign_corners (bool, optional) – Geometrically, we consider the pixels of the\ninput and output as squares rather than points.\nIf set to True, the input and output tensors are aligned by the\ncenter points of their corner pixels, preserving the values at the corner pixels.\nIf set to False, the input and output tensors are aligned by the corner\npoints of their corner pixels, and the interpolation uses edge value padding\nfor out-of-boundary values, making this operation independent of input size\nwhen scale_factor is kept the same. This only has an effect when mode\nis 'bilinear'.\nDefault: False",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.functional.interpolate",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.functional.interpolate.html#torch.nn.functional.interpolate",
        "api_signature": "torch.nn.functional.interpolate(input, size=None, scale_factor=None, mode='nearest', align_corners=None, recompute_scale_factor=None, antialias=False)",
        "api_description": "Down/up samples the input.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor\nsize (int or Tuple[int] or Tuple[int, int] or Tuple[int, int, int]) – output spatial size.\nscale_factor (float or Tuple[float]) – multiplier for spatial size. If scale_factor is a tuple,\nits length has to match the number of spatial dimensions; input.dim() - 2.\nmode (str) – algorithm used for upsampling:\n'nearest' | 'linear' | 'bilinear' | 'bicubic' |\n'trilinear' | 'area' | 'nearest-exact'. Default: 'nearest'\nalign_corners (bool, optional) – Geometrically, we consider the pixels of the\ninput and output as squares rather than points.\nIf set to True, the input and output tensors are aligned by the\ncenter points of their corner pixels, preserving the values at the corner pixels.\nIf set to False, the input and output tensors are aligned by the corner\npoints of their corner pixels, and the interpolation uses edge value padding\nfor out-of-boundary values, making this operation independent of input size\nwhen scale_factor is kept the same. This only has an effect when mode\nis 'linear', 'bilinear', 'bicubic' or 'trilinear'.\nDefault: False\nrecompute_scale_factor (bool, optional) – recompute the scale_factor for use in the\ninterpolation calculation. If recompute_scale_factor is True, then\nscale_factor must be passed in and scale_factor is used to compute the\noutput size. The computed output size will be used to infer new scales for\nthe interpolation. Note that when scale_factor is floating-point, it may differ\nfrom the recomputed scale_factor due to rounding and precision issues.\nIf recompute_scale_factor is False, then size or scale_factor will\nbe used directly for interpolation. Default: None.\nantialias (bool, optional) – flag to apply anti-aliasing. Default: False. Using anti-alias\noption together with align_corners=False, interpolation result would match Pillow\nresult for downsampling operation. Supported modes: 'bilinear', 'bicubic'.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.Interpreter",
        "api_url": "https://pytorch.org/docs/stable/fx.html#torch.fx.Interpreter",
        "api_signature": "torch.fx.Interpreter(module, garbage_collect_values=True, graph=None)",
        "api_description": "An Interpreter executes an FX graph Node-by-Node. This pattern\ncan be useful for many things, including writing code\ntransformations as well as analysis passes.",
        "return_value": "args and kwargs with concrete values for n.\nThe value of the attribute.\nThe value of the attribute that was retrieved\nThe return value referenced by the output node\nThe argument value that was retrieved.\nThe value returned from executing the Module\nThe result of executing n\n",
        "parameters": "module (torch.nn.Module) – The module to be executed\ngarbage_collect_values (bool) – Whether to delete values after their last\nuse within the Module’s execution. This ensures optimal memory usage during\nexecution. This can be disabled to, for example, examine all of the intermediate\nvalues in the execution by looking at the Interpreter.env attribute.\ngraph (Optional[Graph]) – If passed, the interpreter will execute this\ngraph instead of module.graph, using the provided module\nargument to satisfy any requests for state.\ntarget (Target) – The call target for this node. See\nNode for\ndetails on semantics\nargs (Tuple) – Tuple of positional args for this invocation\nkwargs (Dict) – Dict of keyword arguments for this invocation\ntarget (Target) – The call target for this node. See\nNode for\ndetails on semantics\nargs (Tuple) – Tuple of positional args for this invocation\nkwargs (Dict) – Dict of keyword arguments for this invocation\ntarget (Target) – The call target for this node. See\nNode for\ndetails on semantics\nargs (Tuple) – Tuple of positional args for this invocation\nkwargs (Dict) – Dict of keyword arguments for this invocation\nn (Node) – The node for which args and kwargs should be fetched.\ntarget (str) – The fully-qualified name of the attribute to fetch\ntarget (Target) – The call target for this node. See\nNode for\ndetails on semantics\nargs (Tuple) – Tuple of positional args for this invocation\nkwargs (Dict) – Dict of keyword arguments for this invocation\nargs (Argument) – Data structure within which to look up concrete values\nn (Node) – Node to which args belongs. This is only used for error reporting.\ntarget (Target) – The call target for this node. See\nNode for\ndetails on semantics\nargs (Tuple) – Tuple of positional args for this invocation\nkwargs (Dict) – Dict of keyword arguments for this invocation\ntarget (Target) – The call target for this node. See\nNode for\ndetails on semantics\nargs (Tuple) – Tuple of positional args for this invocation\nkwargs (Dict) – Dict of keyword arguments for this invocation\n*args – The arguments to the Module to run, in positional order\ninitial_env (Optional[Dict[Node, Any]]) – An optional starting environment for execution.\nThis is a dict mapping Node to any value. This can be used, for example, to\npre-populate results for certain Nodes so as to do only partial evaluation within\nthe interpreter.\nenable_io_processing (bool) – If true, we process the inputs and outputs with graph’s process_inputs and\nprocess_outputs function first before using them.\nn (Node) – The Node to execute",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.export.unflatten.InterpreterModule",
        "api_url": "https://pytorch.org/docs/stable/export.html#torch.export.unflatten.InterpreterModule",
        "api_signature": "torch.export.unflatten.InterpreterModule(graph)",
        "api_description": "A module that uses torch.fx.Interpreter to execute instead of the usual\ncodegen that GraphModule uses. This provides better stack trace information\nand makes it easier to debug execution.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.autograd.profiler_util.Interval",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.autograd.profiler_util.Interval.html#torch.autograd.profiler_util.Interval",
        "api_signature": "torch.autograd.profiler_util.Interval(start, end)",
        "api_description": "Returns the length of the interval",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.constraints.interval",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.constraints.interval",
        "api_signature": null,
        "api_description": "alias of _Interval",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.IntStorage",
        "api_url": "https://pytorch.org/docs/stable/storage.html#torch.IntStorage",
        "api_signature": "torch.IntStorage(*args, wrap_storage=None, dtype=None, device=None, _internal=False)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.transforms.Transform.inv",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.transforms.Transform.inv",
        "api_signature": null,
        "api_description": "Returns the inverse Transform of this transform.\nThis should satisfy t.inv.inv is t.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.linalg.inv",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.linalg.inv.html#torch.linalg.inv",
        "api_signature": "torch.linalg.inv(A, *, out=None)",
        "api_description": "Computes the inverse of a square matrix if it exists.\nThrows a RuntimeError if the matrix is not invertible.",
        "return_value": "",
        "parameters": "A (Tensor) – tensor of shape (*, n, n) where * is zero or more batch dimensions\nconsisting of invertible matrices.\nout (Tensor, optional) – output tensor. Ignored if None. Default: None.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.linalg.inv_ex",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.linalg.inv_ex.html#torch.linalg.inv_ex",
        "api_signature": "torch.linalg.inv_ex(A, *, check_errors=False, out=None)",
        "api_description": "Computes the inverse of a square matrix if it is invertible.",
        "return_value": "",
        "parameters": "A (Tensor) – tensor of shape (*, n, n) where * is zero or more batch dimensions\nconsisting of square matrices.\ncheck_errors (bool, optional) – controls whether to check the content of info. Default: False.\nout (tuple, optional) – tuple of two tensors to write the output to. Ignored if None. Default: None.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.onnx.InvalidExportOptionsError",
        "api_url": "https://pytorch.org/docs/stable/onnx_dynamo.html#torch.onnx.InvalidExportOptionsError",
        "api_signature": null,
        "api_description": "Raised when user specified an invalid value for the ExportOptions.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.inverse",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.inverse.html#torch.inverse",
        "api_signature": "torch.inverse(input, *, out=None)",
        "api_description": "Alias for torch.linalg.inv()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.inverse",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.inverse.html#torch.Tensor.inverse",
        "api_signature": "Tensor.inverse()",
        "api_description": "See torch.inverse()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.transforms.Transform.inverse_shape",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.transforms.Transform.inverse_shape",
        "api_signature": "inverse_shape(shape)",
        "api_description": "Infers the shapes of the inverse computation, given the output shape.\nDefaults to preserving shape.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.inverse_gamma.InverseGamma",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.inverse_gamma.InverseGamma",
        "api_signature": "torch.distributions.inverse_gamma.InverseGamma(concentration, rate, validate_args=None)",
        "api_description": "Bases: TransformedDistribution",
        "return_value": "",
        "parameters": "concentration (float or Tensor) – shape parameter of the distribution\n(often referred to as alpha)\nrate (float or Tensor) – rate = 1 / scale of the distribution\n(often referred to as beta)",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda.ipc_collect",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.cuda.ipc_collect.html#torch.cuda.ipc_collect",
        "api_signature": "torch.cuda.ipc_collect()",
        "api_description": "Force collects GPU memory after it has been released by CUDA IPC.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda.Event.ipc_handle",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.cuda.Event.html#torch.cuda.Event.ipc_handle",
        "api_signature": "ipc_handle()",
        "api_description": "Return an IPC handle of this event.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.jit.ScriptModule.ipu",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.jit.ScriptModule.html#torch.jit.ScriptModule.ipu",
        "api_signature": "ipu(device=None)",
        "api_description": "Move all model parameters and buffers to the IPU.",
        "return_value": "self\n",
        "parameters": "device (int, optional) – if specified, all parameters will be\ncopied to that device",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.Module.ipu",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.ipu",
        "api_signature": "ipu(device=None)",
        "api_description": "Move all model parameters and buffers to the IPU.",
        "return_value": "self\n",
        "parameters": "device (int, optional) – if specified, all parameters will be\ncopied to that device",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.irecv",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#torch.distributed.irecv",
        "api_signature": "torch.distributed.irecv(tensor, src=None, group=None, tag=0)",
        "api_description": "Receives a tensor asynchronously.",
        "return_value": "A distributed request object.\nNone, if not part of the group\n",
        "parameters": "tensor (Tensor) – Tensor to fill with received data.\nsrc (int, optional) – Source rank on global process group (regardless of group argument).\nWill receive from any process if unspecified.\ngroup (ProcessGroup, optional) – The process group to work on. If None,\nthe default process group will be used.\ntag (int, optional) – Tag to match recv with remote send",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fft.irfft",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.fft.irfft.html#torch.fft.irfft",
        "api_signature": "torch.fft.irfft(input, n=None, dim=-1, norm=None, *, out=None)",
        "api_description": "Computes the inverse of rfft().",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor representing a half-Hermitian signal\nn (int, optional) – Output signal length. This determines the length of the\noutput signal. If given, the input will either be zero-padded or trimmed to this\nlength before computing the real IFFT.\nDefaults to even output: n=2*(input.size(dim) - 1).\ndim (int, optional) – The dimension along which to take the one dimensional real IFFT.\nnorm (str, optional) – Normalization mode. For the backward transform\n(irfft()), these correspond to:\n\"forward\" - no normalization\n\"backward\" - normalize by 1/n\n\"ortho\" - normalize by 1/sqrt(n) (making the real IFFT orthonormal)\nCalling the forward transform (rfft()) with the same\nnormalization mode will apply an overall normalization of 1/n between\nthe two transforms. This is required to make irfft()\nthe exact inverse.\nDefault is \"backward\" (normalize by 1/n).\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fft.irfft2",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.fft.irfft2.html#torch.fft.irfft2",
        "api_signature": "torch.fft.irfft2(input, s=None, dim=(-2, -1)",
        "api_description": "Computes the inverse of rfft2().\nEquivalent to irfftn() but IFFTs only the last two dimensions by default.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor\ns (Tuple[int], optional) – Signal size in the transformed dimensions.\nIf given, each dimension dim[i] will either be zero-padded or\ntrimmed to the length s[i] before computing the real FFT.\nIf a length -1 is specified, no padding is done in that dimension.\nDefaults to even output in the last dimension:\ns[-1] = 2*(input.size(dim[-1]) - 1).\ndim (Tuple[int], optional) – Dimensions to be transformed.\nThe last dimension must be the half-Hermitian compressed dimension.\nDefault: last two dimensions.\nnorm (str, optional) – Normalization mode. For the backward transform\n(irfft2()), these correspond to:\n\"forward\" - no normalization\n\"backward\" - normalize by 1/n\n\"ortho\" - normalize by 1/sqrt(n) (making the real IFFT orthonormal)\nWhere n = prod(s) is the logical IFFT size.\nCalling the forward transform (rfft2()) with the same\nnormalization mode will apply an overall normalization of 1/n between\nthe two transforms. This is required to make irfft2()\nthe exact inverse.\nDefault is \"backward\" (normalize by 1/n).\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fft.irfftn",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.fft.irfftn.html#torch.fft.irfftn",
        "api_signature": "torch.fft.irfftn(input, s=None, dim=None, norm=None, *, out=None)",
        "api_description": "Computes the inverse of rfftn().",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor\ns (Tuple[int], optional) – Signal size in the transformed dimensions.\nIf given, each dimension dim[i] will either be zero-padded or\ntrimmed to the length s[i] before computing the real FFT.\nIf a length -1 is specified, no padding is done in that dimension.\nDefaults to even output in the last dimension:\ns[-1] = 2*(input.size(dim[-1]) - 1).\ndim (Tuple[int], optional) – Dimensions to be transformed.\nThe last dimension must be the half-Hermitian compressed dimension.\nDefault: all dimensions, or the last len(s) dimensions if s is given.\nnorm (str, optional) – Normalization mode. For the backward transform\n(irfftn()), these correspond to:\n\"forward\" - no normalization\n\"backward\" - normalize by 1/n\n\"ortho\" - normalize by 1/sqrt(n) (making the real IFFT orthonormal)\nWhere n = prod(s) is the logical IFFT size.\nCalling the forward transform (rfftn()) with the same\nnormalization mode will apply an overall normalization of 1/n between\nthe two transforms. This is required to make irfftn()\nthe exact inverse.\nDefault is \"backward\" (normalize by 1/n).\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.backends.cudnn.is_available",
        "api_url": "https://pytorch.org/docs/stable/backends.html#torch.backends.cudnn.is_available",
        "api_signature": "torch.backends.cudnn.is_available()",
        "api_description": "Return a bool indicating if CUDNN is currently available.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.backends.mkl.is_available",
        "api_url": "https://pytorch.org/docs/stable/backends.html#torch.backends.mkl.is_available",
        "api_signature": "torch.backends.mkl.is_available()",
        "api_description": "Return whether PyTorch is built with MKL support.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.backends.mkldnn.is_available",
        "api_url": "https://pytorch.org/docs/stable/backends.html#torch.backends.mkldnn.is_available",
        "api_signature": "torch.backends.mkldnn.is_available()",
        "api_description": "Return whether PyTorch is built with MKL-DNN support.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.backends.mps.is_available",
        "api_url": "https://pytorch.org/docs/stable/backends.html#torch.backends.mps.is_available",
        "api_signature": "torch.backends.mps.is_available()",
        "api_description": "Return a bool indicating if MPS is currently available.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.backends.nnpack.is_available",
        "api_url": "https://pytorch.org/docs/stable/backends.html#torch.backends.nnpack.is_available",
        "api_signature": "torch.backends.nnpack.is_available()",
        "api_description": "Return whether PyTorch is built with NNPACK support.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.backends.openmp.is_available",
        "api_url": "https://pytorch.org/docs/stable/backends.html#torch.backends.openmp.is_available",
        "api_signature": "torch.backends.openmp.is_available()",
        "api_description": "Return whether PyTorch is built with OpenMP support.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.backends.opt_einsum.is_available",
        "api_url": "https://pytorch.org/docs/stable/backends.html#torch.backends.opt_einsum.is_available",
        "api_signature": "torch.backends.opt_einsum.is_available()",
        "api_description": "Return a bool indicating if opt_einsum is currently available.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cpu.is_available",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.cpu.is_available.html#torch.cpu.is_available",
        "api_signature": "torch.cpu.is_available()",
        "api_description": "Returns a bool indicating if CPU is currently available.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda.is_available",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.cuda.is_available.html#torch.cuda.is_available",
        "api_signature": "torch.cuda.is_available()",
        "api_description": "Return a bool indicating if CUDA is currently available.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.is_available",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#torch.distributed.is_available",
        "api_signature": "torch.distributed.is_available()",
        "api_description": "Return True if the distributed package is available.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.profiler.itt.is_available",
        "api_url": "https://pytorch.org/docs/stable/profiler.html#torch.profiler.itt.is_available",
        "api_signature": "torch.profiler.itt.is_available()",
        "api_description": "Check if ITT feature is available or not",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.xpu.is_available",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.xpu.is_available.html#torch.xpu.is_available",
        "api_signature": "torch.xpu.is_available()",
        "api_description": "Return a bool indicating if XPU is currently available.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.backends.cuda.is_built",
        "api_url": "https://pytorch.org/docs/stable/backends.html#torch.backends.cuda.is_built",
        "api_signature": "torch.backends.cuda.is_built()",
        "api_description": "Return whether PyTorch is built with CUDA support.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.backends.mps.is_built",
        "api_url": "https://pytorch.org/docs/stable/backends.html#torch.backends.mps.is_built",
        "api_signature": "torch.backends.mps.is_built()",
        "api_description": "Return whether PyTorch is built with MPS support.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.rendezvous.RendezvousHandler.is_closed",
        "api_url": "https://pytorch.org/docs/stable/elastic/rendezvous.html#torch.distributed.elastic.rendezvous.RendezvousHandler.is_closed",
        "api_signature": "is_closed()",
        "api_description": "Check whether the rendezvous has been closed.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.is_coalesced",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.is_coalesced.html#torch.Tensor.is_coalesced",
        "api_signature": "Tensor.is_coalesced()",
        "api_description": "Returns True if self is a sparse COO tensor that is coalesced, False otherwise.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.compiler.is_compiling",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.compiler.is_compiling.html#torch.compiler.is_compiling",
        "api_signature": "torch.compiler.is_compiling()",
        "api_description": "Indicates whether a graph is executed/traced as part of torch.compile() or torch.export().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "\ntorch._dynamo.external_utils.is_compiling()\ntorch._utils.is_compiling()\n\n",
        "code_example": ""
    },
    {
        "api_name": "torch.is_complex",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.is_complex.html#torch.is_complex",
        "api_signature": "torch.is_complex(input)",
        "api_description": "Returns True if the data type of input is a complex data type i.e.,\none of torch.complex64, and torch.complex128.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.is_complex",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.is_complex.html#torch.Tensor.is_complex",
        "api_signature": "Tensor.is_complex()",
        "api_description": "Returns True if the data type of self is a complex data type.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.symbolic_shapes.is_concrete_bool",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.fx.experimental.symbolic_shapes.is_concrete_bool.html#torch.fx.experimental.symbolic_shapes.is_concrete_bool",
        "api_signature": "torch.fx.experimental.symbolic_shapes.is_concrete_bool(a)",
        "api_description": "Utility to check if underlying object\nin SymBool is concrete value. Also returns\ntrue if integer is passed in.\n:param a: Object to test if it bool\n:type a: SymBool or bool",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.symbolic_shapes.is_concrete_int",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.fx.experimental.symbolic_shapes.is_concrete_int.html#torch.fx.experimental.symbolic_shapes.is_concrete_int",
        "api_signature": "torch.fx.experimental.symbolic_shapes.is_concrete_int(a)",
        "api_description": "Utility to check if underlying object\nin SymInt is concrete value. Also returns\ntrue if integer is passed in.",
        "return_value": "",
        "parameters": "a (SymInt or int) – Object to test if it int",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.is_conj",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.is_conj.html#torch.is_conj",
        "api_signature": "torch.is_conj(input)",
        "api_description": "Returns True if the input is a conjugated tensor, i.e. its conjugate bit is set to True.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.is_conj",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.is_conj.html#torch.Tensor.is_conj",
        "api_signature": "Tensor.is_conj()",
        "api_description": "Returns True if the conjugate bit of self is set to true.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.is_contiguous",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.is_contiguous.html#torch.Tensor.is_contiguous",
        "api_signature": "Tensor.is_contiguous(memory_format=torch.contiguous_format)",
        "api_description": "Returns True if self tensor is contiguous in memory in the order specified\nby memory format.",
        "return_value": "",
        "parameters": "memory_format (torch.memory_format, optional) – Specifies memory allocation\norder. Default: torch.contiguous_format.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.utils.rnn.PackedSequence.is_cuda",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.PackedSequence.html#torch.nn.utils.rnn.PackedSequence.is_cuda",
        "api_signature": null,
        "api_description": "Return true if self.data stored on a gpu.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.is_cuda",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.is_cuda.html#torch.Tensor.is_cuda",
        "api_signature": null,
        "api_description": "Is True if the Tensor is stored on the GPU, False otherwise.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.TypedStorage.is_cuda",
        "api_url": "https://pytorch.org/docs/stable/storage.html#torch.TypedStorage.is_cuda",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.UntypedStorage.is_cuda",
        "api_url": "https://pytorch.org/docs/stable/storage.html#torch.UntypedStorage.is_cuda",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda.is_current_stream_capturing",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.cuda.is_current_stream_capturing.html#torch.cuda.is_current_stream_capturing",
        "api_signature": "torch.cuda.is_current_stream_capturing()",
        "api_description": "Return True if CUDA graph capture is underway on the current CUDA stream, False otherwise.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.is_deterministic_algorithms_warn_only_enabled",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.is_deterministic_algorithms_warn_only_enabled.html#torch.is_deterministic_algorithms_warn_only_enabled",
        "api_signature": "torch.is_deterministic_algorithms_warn_only_enabled()",
        "api_description": "Returns True if the global deterministic flag is set to warn only.\nRefer to torch.use_deterministic_algorithms() documentation for more\ndetails.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.compiler.is_dynamo_compiling",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.compiler.is_dynamo_compiling.html#torch.compiler.is_dynamo_compiling",
        "api_signature": "torch.compiler.is_dynamo_compiling()",
        "api_description": "Indicates whether a graph is traced via TorchDynamo.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.sparse.check_sparse_tensor_invariants.is_enabled",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.sparse.check_sparse_tensor_invariants.html#torch.sparse.check_sparse_tensor_invariants.is_enabled",
        "api_signature": "is_enabled()",
        "api_description": "Return True if the sparse tensor invariants checking is enabled.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.is_floating_point",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.is_floating_point.html#torch.is_floating_point",
        "api_signature": "torch.is_floating_point(input)",
        "api_description": "Returns True if the data type of input is a floating point data type i.e.,\none of torch.float64, torch.float32, torch.float16, and torch.bfloat16.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.is_floating_point",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.is_floating_point.html#torch.Tensor.is_floating_point",
        "api_signature": "Tensor.is_floating_point()",
        "api_description": "Returns True if the data type of self is a floating point data type.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.is_gloo_available",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#torch.distributed.is_gloo_available",
        "api_signature": "torch.distributed.is_gloo_available()",
        "api_description": "Check if the Gloo backend is available.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.is_grad_enabled",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.is_grad_enabled.html#torch.is_grad_enabled",
        "api_signature": "torch.is_grad_enabled()",
        "api_description": "Returns True if grad mode is currently enabled.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.TypedStorage.is_hpu",
        "api_url": "https://pytorch.org/docs/stable/storage.html#torch.TypedStorage.is_hpu",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.UntypedStorage.is_hpu",
        "api_url": "https://pytorch.org/docs/stable/storage.html#torch.UntypedStorage.is_hpu",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.Node.is_impure",
        "api_url": "https://pytorch.org/docs/stable/fx.html#torch.fx.Node.is_impure",
        "api_signature": "is_impure()",
        "api_description": "Returns whether this op is impure, i.e. if its op is a placeholder or\noutput, or if a call_function or call_module which is impure.",
        "return_value": "If the op is impure or not.\n",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.onnx.is_in_onnx_export",
        "api_url": "https://pytorch.org/docs/stable/onnx_torchscript.html#torch.onnx.is_in_onnx_export",
        "api_signature": "torch.onnx.is_in_onnx_export()",
        "api_description": "Returns whether it is in the middle of ONNX export.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.is_inference",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.is_inference.html#torch.Tensor.is_inference",
        "api_signature": "Tensor.is_inference()",
        "api_description": "See torch.is_inference()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.is_inference_mode_enabled",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.is_inference_mode_enabled.html#torch.is_inference_mode_enabled",
        "api_signature": "torch.is_inference_mode_enabled()",
        "api_description": "Returns True if inference mode is currently enabled.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda.is_initialized",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.cuda.is_initialized.html#torch.cuda.is_initialized",
        "api_signature": "torch.cuda.is_initialized()",
        "api_description": "Return whether PyTorch’s CUDA state has been initialized.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.is_initialized",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#torch.distributed.is_initialized",
        "api_signature": "torch.distributed.is_initialized()",
        "api_description": "Check if the default process group has been initialized.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.xpu.is_initialized",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.xpu.is_initialized.html#torch.xpu.is_initialized",
        "api_signature": "torch.xpu.is_initialized()",
        "api_description": "Return whether PyTorch’s XPU state has been initialized.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.SymFloat.is_integer",
        "api_url": "https://pytorch.org/docs/stable/torch.html#torch.SymFloat.is_integer",
        "api_signature": "is_integer()",
        "api_description": "Return True if the float is an integer.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.GradBucket.is_last",
        "api_url": "https://pytorch.org/docs/stable/ddp_comm_hooks.html#torch.distributed.GradBucket.is_last",
        "api_signature": "torch.distributed.GradBucket.is_last(self: torch._C._distributed_c10d.GradBucket)",
        "api_description": "Whether this bucket is the last bucket to allreduce in an iteration.\nThis also means that this bucket corresponds to the first few layers in the forward pass.",
        "return_value": "Whether this bucket is the last bucket to allreduce in an iteration.\nThis also means that this bucket corresponds to the first few layers in the forward pass.\n",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.is_leaf",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.is_leaf.html#torch.Tensor.is_leaf",
        "api_signature": null,
        "api_description": "All Tensors that have requires_grad which is False will be leaf Tensors by convention.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.ns._numeric_suite_fx.NSTracer.is_leaf_module",
        "api_url": "https://pytorch.org/docs/stable/torch.ao.ns._numeric_suite_fx.html#torch.ao.ns._numeric_suite_fx.NSTracer.is_leaf_module",
        "api_signature": "is_leaf_module(m, module_qualified_name)",
        "api_description": "bool",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.Tracer.is_leaf_module",
        "api_url": "https://pytorch.org/docs/stable/fx.html#torch.fx.Tracer.is_leaf_module",
        "api_signature": "is_leaf_module(m, module_qualified_name)",
        "api_description": "A method to specify whether a given nn.Module is a “leaf” module.",
        "return_value": "",
        "parameters": "m (Module) – The module being queried about\nmodule_qualified_name (str) – The path to root of this module. For example,\nif you have a module hierarchy where submodule foo contains\nsubmodule bar, which contains submodule baz, that module will\nappear with the qualified name foo.bar.baz here.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.is_meta",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.is_meta.html#torch.Tensor.is_meta",
        "api_signature": null,
        "api_description": "Is True if the Tensor is a meta tensor, False otherwise.  Meta tensors\nare like normal tensors, but they carry no data.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.is_mpi_available",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#torch.distributed.is_mpi_available",
        "api_signature": "torch.distributed.is_mpi_available()",
        "api_description": "Check if the MPI backend is available.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.is_nccl_available",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#torch.distributed.is_nccl_available",
        "api_signature": "torch.distributed.is_nccl_available()",
        "api_description": "Check if the NCCL backend is available.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.cpp_extension.is_ninja_available",
        "api_url": "https://pytorch.org/docs/stable/cpp_extension.html#torch.utils.cpp_extension.is_ninja_available",
        "api_signature": "torch.utils.cpp_extension.is_ninja_available()",
        "api_description": "Return True if the ninja build system is available on the system, False otherwise.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.is_nonzero",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.is_nonzero.html#torch.is_nonzero",
        "api_signature": "torch.is_nonzero(input)",
        "api_description": "Returns True if the input is a single element tensor which is not equal to zero\nafter type conversions.\ni.e. not equal to torch.tensor([0.]) or torch.tensor([0]) or\ntorch.tensor([False]).\nThrows a RuntimeError if torch.numel() != 1 (even in case\nof sparse tensors).",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.onnx.is_onnxrt_backend_supported",
        "api_url": "https://pytorch.org/docs/stable/onnx_dynamo_onnxruntime_backend.html#torch.onnx.is_onnxrt_backend_supported",
        "api_signature": "torch.onnx.is_onnxrt_backend_supported()",
        "api_description": "Returns True if ONNX Runtime dependencies are installed and usable\nto support TorchDynamo backend integration; False otherwise.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.rpc.PyRRef.is_owner",
        "api_url": "https://pytorch.org/docs/stable/rpc.html#torch.distributed.rpc.PyRRef.is_owner",
        "api_signature": "is_owner(self: torch._C._distributed_rpc.PyRRef)",
        "api_description": "Returns whether or not the current node is the owner of this\nRRef.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.utils.parametrize.is_parametrized",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.utils.parametrize.is_parametrized.html#torch.nn.utils.parametrize.is_parametrized",
        "api_signature": "torch.nn.utils.parametrize.is_parametrized(module, tensor_name=None)",
        "api_description": "Determine if a module has a parametrization.",
        "return_value": "True if module has a parametrization for the parameter named tensor_name,\nor if it has any parametrization when tensor_name is None;\notherwise False\n",
        "parameters": "module (nn.Module) – module to query\ntensor_name (str, optional) – name of the parameter in the module\nDefault: None",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.utils.rnn.PackedSequence.is_pinned",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.PackedSequence.html#torch.nn.utils.rnn.PackedSequence.is_pinned",
        "api_signature": "is_pinned()",
        "api_description": "Return true if self.data stored on in pinned memory.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.is_pinned",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.is_pinned.html#torch.Tensor.is_pinned",
        "api_signature": "Tensor.is_pinned()",
        "api_description": "Returns true if this tensor resides in pinned memory.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.TypedStorage.is_pinned",
        "api_url": "https://pytorch.org/docs/stable/storage.html#torch.TypedStorage.is_pinned",
        "api_signature": "is_pinned(device='cuda')",
        "api_description": "Determine whether the CPU TypedStorage is already pinned on device.",
        "return_value": "A boolean variable.\n",
        "parameters": "device (str or torch.device) – The device to pin memory on. Default: 'cuda'",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.UntypedStorage.is_pinned",
        "api_url": "https://pytorch.org/docs/stable/storage.html#torch.UntypedStorage.is_pinned",
        "api_signature": "is_pinned(device='cuda')",
        "api_description": "Determine whether the CPU storage is already pinned on device.",
        "return_value": "A boolean variable.\n",
        "parameters": "device (str or torch.device) – The device to pin memory on. Default: 'cuda'.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.utils.prune.is_pruned",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.is_pruned.html#torch.nn.utils.prune.is_pruned",
        "api_signature": "torch.nn.utils.prune.is_pruned(module)",
        "api_description": "Check if a module is pruned by looking for pruning pre-hooks.",
        "return_value": "binary answer to whether module is pruned.\n",
        "parameters": "module (nn.Module) – object that is either pruned or unpruned",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.is_quantized",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.is_quantized.html#torch.Tensor.is_quantized",
        "api_signature": null,
        "api_description": "Is True if the Tensor is quantized, False otherwise.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.onnx.OnnxRegistry.is_registered_op",
        "api_url": "https://pytorch.org/docs/stable/onnx_dynamo.html#torch.onnx.OnnxRegistry.is_registered_op",
        "api_signature": "is_registered_op(namespace, op_name, overload=None)",
        "api_description": "Returns whether the given op is registered: torch.ops.<namespace>.<op_name>.<overload>.",
        "return_value": "True if the given op is registered, otherwise False.\n",
        "parameters": "namespace (str) – The namespace of the operator to check.\nop_name (str) – The name of the operator to check.\noverload (Optional[str]) – The overload of the operator to check. If it’s default overload,\nleave it to None.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.agent.server.WorkerState.is_running",
        "api_url": "https://pytorch.org/docs/stable/elastic/agent.html#torch.distributed.elastic.agent.server.WorkerState.is_running",
        "api_signature": "is_running(state)",
        "api_description": "Return the state of the Worker.",
        "return_value": "True if the worker state represents workers still running\n(e.g. that the process exists but not necessarily healthy).\n",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.jit.is_scripting",
        "api_url": "https://pytorch.org/docs/stable/jit_language_reference.html#torch.jit.is_scripting",
        "api_signature": "torch.jit.is_scripting()",
        "api_description": "Function that returns True when in compilation and False otherwise. This\nis useful especially with the @unused decorator to leave code in your\nmodel that is not yet TorchScript compatible.\n.. testcode:",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.is_set_to",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.is_set_to.html#torch.Tensor.is_set_to",
        "api_signature": "Tensor.is_set_to(tensor)",
        "api_description": "Returns True if both tensors are pointing to the exact same memory (same\nstorage, offset, size and stride).",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.is_shared",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.is_shared.html#torch.Tensor.is_shared",
        "api_signature": "Tensor.is_shared()",
        "api_description": "Checks if tensor is in shared memory.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.TypedStorage.is_shared",
        "api_url": "https://pytorch.org/docs/stable/storage.html#torch.TypedStorage.is_shared",
        "api_signature": "is_shared()",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.UntypedStorage.is_shared",
        "api_url": "https://pytorch.org/docs/stable/storage.html#torch.UntypedStorage.is_shared",
        "api_signature": "is_shared()",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.is_signed",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.is_signed.html#torch.Tensor.is_signed",
        "api_signature": "Tensor.is_signed()",
        "api_description": "Returns True if the data type of self is a signed data type.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.is_sparse",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.is_sparse.html#torch.Tensor.is_sparse",
        "api_signature": null,
        "api_description": "Is True if the Tensor uses sparse COO storage layout, False otherwise.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.TypedStorage.is_sparse",
        "api_url": "https://pytorch.org/docs/stable/storage.html#torch.TypedStorage.is_sparse",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.UntypedStorage.is_sparse",
        "api_url": "https://pytorch.org/docs/stable/storage.html#torch.UntypedStorage.is_sparse",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.is_sparse_csr",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.is_sparse_csr.html#torch.Tensor.is_sparse_csr",
        "api_signature": null,
        "api_description": "Is True if the Tensor uses sparse CSR storage layout, False otherwise.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.UntypedStorage.is_sparse_csr",
        "api_url": "https://pytorch.org/docs/stable/storage.html#torch.UntypedStorage.is_sparse_csr",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.is_storage",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.is_storage.html#torch.is_storage",
        "api_signature": "torch.is_storage(obj)",
        "api_description": "Returns True if obj is a PyTorch storage object.",
        "return_value": "",
        "parameters": "obj (Object) – Object to test",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.is_tensor",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.is_tensor.html#torch.is_tensor",
        "api_signature": "torch.is_tensor(obj)",
        "api_description": "Returns True if obj is a PyTorch tensor.",
        "return_value": "",
        "parameters": "obj (Object) – Object to test",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.overrides.is_tensor_like",
        "api_url": "https://pytorch.org/docs/stable/torch.overrides.html#torch.overrides.is_tensor_like",
        "api_signature": "torch.overrides.is_tensor_like(inp)",
        "api_description": "Returns True if the passed-in input is a Tensor-like.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.overrides.is_tensor_method_or_property",
        "api_url": "https://pytorch.org/docs/stable/torch.overrides.html#torch.overrides.is_tensor_method_or_property",
        "api_signature": "torch.overrides.is_tensor_method_or_property(func)",
        "api_description": "Returns True if the function passed in is a handler for a\nmethod or property belonging to torch.Tensor, as passed\ninto __torch_function__.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.is_torchelastic_launched",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#torch.distributed.is_torchelastic_launched",
        "api_signature": "torch.distributed.is_torchelastic_launched()",
        "api_description": "Check whether this process was launched with torch.distributed.elastic (aka torchelastic).",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.jit.is_tracing",
        "api_url": "https://pytorch.org/docs/stable/jit_language_reference.html#torch.jit.is_tracing",
        "api_signature": "torch.jit.is_tracing()",
        "api_description": "Return a boolean value.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.symbolic_shapes.ShapeEnv.is_unbacked_symint",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.fx.experimental.symbolic_shapes.ShapeEnv.html#torch.fx.experimental.symbolic_shapes.ShapeEnv.is_unbacked_symint",
        "api_signature": "is_unbacked_symint(symbol)",
        "api_description": "Check if a sympy symbol matches the naming convention for unbacked symbols",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.is_warn_always_enabled",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.is_warn_always_enabled.html#torch.is_warn_always_enabled",
        "api_signature": "torch.is_warn_always_enabled()",
        "api_description": "Returns True if the global warn_always flag is turned on. Refer to\ntorch.set_warn_always() documentation for more details.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.isclose",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.isclose.html#torch.isclose",
        "api_signature": "torch.isclose(input, other, rtol=1e-05, atol=1e-08, equal_nan=False)",
        "api_description": "Returns a new tensor with boolean elements representing if each element of\ninput is “close” to the corresponding element of other.\nCloseness is defined as:",
        "return_value": "",
        "parameters": "input (Tensor) – first tensor to compare\nother (Tensor) – second tensor to compare\natol (float, optional) – absolute tolerance. Default: 1e-08\nrtol (float, optional) – relative tolerance. Default: 1e-05\nequal_nan (bool, optional) – if True, then two NaN s will be considered equal. Default: False",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.isclose",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.isclose.html#torch.Tensor.isclose",
        "api_signature": "Tensor.isclose(other, rtol=1e-05, atol=1e-08, equal_nan=False)",
        "api_description": "See torch.isclose()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.isend",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#torch.distributed.isend",
        "api_signature": "torch.distributed.isend(tensor, dst, group=None, tag=0)",
        "api_description": "Send a tensor asynchronously.",
        "return_value": "A distributed request object.\nNone, if not part of the group\n",
        "parameters": "tensor (Tensor) – Tensor to send.\ndst (int) – Destination rank on global process group (regardless of group argument)\ngroup (ProcessGroup, optional) – The process group to work on. If None,\nthe default process group will be used.\ntag (int, optional) – Tag to match send with remote recv",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.isfinite",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.isfinite.html#torch.isfinite",
        "api_signature": "torch.isfinite(input)",
        "api_description": "Returns a new tensor with boolean elements representing if each element is finite or not.",
        "return_value": "A boolean tensor that is True where input is finite and False elsewhere\n",
        "parameters": "input (Tensor) – the input tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.isfinite",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.isfinite.html#torch.Tensor.isfinite",
        "api_signature": "Tensor.isfinite()",
        "api_description": "See torch.isfinite()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.isin",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.isin.html#torch.isin",
        "api_signature": "torch.isin(elements, test_elements, *, assume_unique=False, invert=False)",
        "api_description": "Tests if each element of elements is in test_elements. Returns\na boolean tensor of the same shape as elements that is True for elements\nin test_elements and False otherwise.",
        "return_value": "A boolean tensor of the same shape as elements that is True for elements in\ntest_elements and False otherwise\n",
        "parameters": "elements (Tensor or Scalar) – Input elements\ntest_elements (Tensor or Scalar) – Values against which to test for each input element\nassume_unique (bool, optional) – If True, assumes both elements and\ntest_elements contain unique elements, which can speed up the\ncalculation. Default: False\ninvert (bool, optional) – If True, inverts the boolean return tensor, resulting in True\nvalues for elements not in test_elements. Default: False",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.isinf",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.isinf.html#torch.isinf",
        "api_signature": "torch.isinf(input)",
        "api_description": "Tests if each element of input is infinite\n(positive or negative infinity) or not.",
        "return_value": "A boolean tensor that is True where input is infinite and False elsewhere\n",
        "parameters": "input (Tensor) – the input tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.isinf",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.isinf.html#torch.Tensor.isinf",
        "api_signature": "Tensor.isinf()",
        "api_description": "See torch.isinf()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.jit.isinstance",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.jit.isinstance.html#torch.jit.isinstance",
        "api_signature": "torch.jit.isinstance(obj, target_type)",
        "api_description": "Provide container type refinement in TorchScript.",
        "return_value": "\nTrue if obj was successfully refined to the type of target_type,False otherwise with no new type refinement\n\n\n\n",
        "parameters": "obj – object to refine the type of\ntarget_type – type to try to refine obj to",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.isnan",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.isnan.html#torch.isnan",
        "api_signature": "torch.isnan(input)",
        "api_description": "Returns a new tensor with boolean elements representing if each element of input\nis NaN or not. Complex values are considered NaN when either their real\nand/or imaginary part is NaN.",
        "return_value": "A boolean tensor that is True where input is NaN and False elsewhere\n",
        "parameters": "input (Tensor) – the input tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.isnan",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.isnan.html#torch.Tensor.isnan",
        "api_signature": "Tensor.isnan()",
        "api_description": "See torch.isnan()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.isneginf",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.isneginf.html#torch.isneginf",
        "api_signature": "torch.isneginf(input, *, out=None)",
        "api_description": "Tests if each element of input is negative infinity or not.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.isneginf",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.isneginf.html#torch.Tensor.isneginf",
        "api_signature": "Tensor.isneginf()",
        "api_description": "See torch.isneginf()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.isposinf",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.isposinf.html#torch.isposinf",
        "api_signature": "torch.isposinf(input, *, out=None)",
        "api_description": "Tests if each element of input is positive infinity or not.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.isposinf",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.isposinf.html#torch.Tensor.isposinf",
        "api_signature": "Tensor.isposinf()",
        "api_description": "See torch.isposinf()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.isreal",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.isreal.html#torch.isreal",
        "api_signature": "torch.isreal(input)",
        "api_description": "Returns a new tensor with boolean elements representing if each element of input is real-valued or not.\nAll real-valued types are considered real. Complex values are considered real when their imaginary part is 0.",
        "return_value": "A boolean tensor that is True where input is real and False elsewhere\n",
        "parameters": "input (Tensor) – the input tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.isreal",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.isreal.html#torch.Tensor.isreal",
        "api_signature": "Tensor.isreal()",
        "api_description": "See torch.isreal()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.istft",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.istft.html#torch.istft",
        "api_signature": "torch.istft(input, n_fft, hop_length=None, win_length=None, window=None, center=True, normalized=False, onesided=None, length=None, return_complex=False)",
        "api_description": "Inverse short time Fourier Transform. This is expected to be the inverse of stft().",
        "return_value": "\nLeast squares estimation of the original signal of shape (B?, length) whereB? is an optional batch dimension from the input tensor.\n\n\n\n",
        "parameters": "input (Tensor) – The input tensor. Expected to be in the format of stft(),\noutput. That is a complex tensor of shape (B?, N, T) where\nB? is an optional batch dimension\nN is the number of frequency samples, (n_fft // 2) + 1\nfor onesided input, or otherwise n_fft.\nT is the number of frames, 1 + length // hop_length for centered stft,\nor 1 + (length - n_fft) // hop_length otherwise.\nChanged in version 2.0: Real datatype inputs are no longer supported. Input must now have a\ncomplex datatype, as returned by stft(..., return_complex=True).\nn_fft (int) – Size of Fourier transform\nhop_length (Optional[int]) – The distance between neighboring sliding window frames.\n(Default: n_fft // 4)\nwin_length (Optional[int]) – The size of window frame and STFT filter. (Default: n_fft)\nwindow (Optional[torch.Tensor]) – The optional window function.\nShape must be 1d and <= n_fft\n(Default: torch.ones(win_length))\ncenter (bool) – Whether input was padded on both sides so that the ttt-th frame is\ncentered at time t×hop_lengtht \\times \\text{hop\\_length}t×hop_length.\n(Default: True)\nnormalized (bool) – Whether the STFT was normalized. (Default: False)\nonesided (Optional[bool]) – Whether the STFT was onesided.\n(Default: True if n_fft != fft_size in the input size)\nlength (Optional[int]) – The amount to trim the signal by (i.e. the\noriginal signal length). Defaults to (T - 1) * hop_length for\ncentered stft, or n_fft + (T - 1) * hop_length otherwise, where T\nis the number of input frames.\nreturn_complex (Optional[bool]) – Whether the output should be complex, or if the input should be\nassumed to derive from a real signal and window.\nNote that this is incompatible with onesided=True.\n(Default: False)",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.istft",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.istft.html#torch.Tensor.istft",
        "api_signature": "Tensor.istft(n_fft, hop_length=None, win_length=None, window=None, center=True, normalized=False, onesided=None, length=None, return_complex=False)",
        "api_description": "See torch.istft()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.item",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.item.html#torch.Tensor.item",
        "api_signature": "Tensor.item()",
        "api_description": "Returns the value of this tensor as a standard Python number. This only works\nfor tensors with one element. For other cases, see tolist().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.autograd.profiler_util.StringTable.items",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.autograd.profiler_util.StringTable.html#torch.autograd.profiler_util.StringTable.items",
        "api_signature": "items()",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.ModuleDict.items",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.ModuleDict.html#torch.nn.ModuleDict.items",
        "api_signature": "items()",
        "api_description": "Return an iterable of the ModuleDict key/value pairs.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.ParameterDict.items",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.ParameterDict.html#torch.nn.ParameterDict.items",
        "api_signature": "items()",
        "api_description": "Return an iterable of the ParameterDict key/value pairs.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.itemsize",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.itemsize.html#torch.Tensor.itemsize",
        "api_signature": null,
        "api_description": "Alias for element_size()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.Tracer.iter",
        "api_url": "https://pytorch.org/docs/stable/fx.html#torch.fx.Tracer.iter",
        "api_signature": "iter(obj)",
        "api_description": "when used in control flow.  Normally we don’t know what to do because\nwe don’t know the value of the proxy, but a custom tracer can attach more\ninformation to the graph node using create_node and can choose to return an iterator.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.data.IterableDataset",
        "api_url": "https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset",
        "api_signature": "torch.utils.data.IterableDataset(*args, **kwds)",
        "api_description": "An iterable Dataset.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.func.jacfwd",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.func.jacfwd.html#torch.func.jacfwd",
        "api_signature": "torch.func.jacfwd(func, argnums=0, has_aux=False, *, randomness='error')",
        "api_description": "Computes the Jacobian of func with respect to the arg(s) at index\nargnum using forward-mode autodiff",
        "return_value": "Returns a function that takes in the same inputs as func and\nreturns the Jacobian of func with respect to the arg(s) at\nargnums. If has_aux is True, then the returned function\ninstead returns a (jacobian, aux) tuple where jacobian\nis the Jacobian and aux is auxiliary objects returned by func.\n",
        "parameters": "func (function) – A Python function that takes one or more arguments,\none of which must be a Tensor, and returns one or more Tensors\nargnums (int or Tuple[int]) – Optional, integer or tuple of integers,\nsaying which arguments to get the Jacobian with respect to.\nDefault: 0.\nhas_aux (bool) – Flag indicating that func returns a\n(output, aux) tuple where the first element is the output of\nthe function to be differentiated and the second element is\nauxiliary objects that will not be differentiated.\nDefault: False.\nrandomness (str) – Flag indicating what type of randomness to use.\nSee vmap() for more detail. Allowed: “different”, “same”, “error”.\nDefault: “error”",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.autograd.functional.jacobian",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.autograd.functional.jacobian.html#torch.autograd.functional.jacobian",
        "api_signature": "torch.autograd.functional.jacobian(func, inputs, create_graph=False, strict=False, vectorize=False, strategy='reverse-mode')",
        "api_description": "Compute the Jacobian of a given function.",
        "return_value": "if there is a single\ninput and output, this will be a single Tensor containing the\nJacobian for the linearized inputs and output. If one of the two is\na tuple, then the Jacobian will be a tuple of Tensors. If both of\nthem are tuples, then the Jacobian will be a tuple of tuple of\nTensors where Jacobian[i][j] will contain the Jacobian of the\nith output and jth input and will have as size the\nconcatenation of the sizes of the corresponding output and the\ncorresponding input and will have same dtype and device as the\ncorresponding input. If strategy is forward-mode, the dtype will be\nthat of the output; otherwise, the input.\n",
        "parameters": "func (function) – a Python function that takes Tensor inputs and returns\na tuple of Tensors or a Tensor.\ninputs (tuple of Tensors or Tensor) – inputs to the function func.\ncreate_graph (bool, optional) – If True, the Jacobian will be\ncomputed in a differentiable manner. Note that when strict is\nFalse, the result can not require gradients or be disconnected\nfrom the inputs.  Defaults to False.\nstrict (bool, optional) – If True, an error will be raised when we\ndetect that there exists an input such that all the outputs are\nindependent of it. If False, we return a Tensor of zeros as the\njacobian for said inputs, which is the expected mathematical value.\nDefaults to False.\nvectorize (bool, optional) – This feature is experimental.\nPlease consider using torch.func.jacrev() or\ntorch.func.jacfwd() instead if you are looking for something\nless experimental and more performant.\nWhen computing the jacobian, usually we invoke\nautograd.grad once per row of the jacobian. If this flag is\nTrue, we perform only a single autograd.grad call with\nbatched_grad=True which uses the vmap prototype feature.\nThough this should lead to performance improvements in many cases,\nbecause this feature is still experimental, there may be performance\ncliffs. See torch.autograd.grad()’s batched_grad parameter for\nmore information.\nstrategy (str, optional) – Set to \"forward-mode\" or \"reverse-mode\" to\ndetermine whether the Jacobian will be computed with forward or reverse\nmode AD. Currently, \"forward-mode\" requires vectorized=True.\nDefaults to \"reverse-mode\". If func has more outputs than\ninputs, \"forward-mode\" tends to be more performant. Otherwise,\nprefer to use \"reverse-mode\".",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.func.jacrev",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.func.jacrev.html#torch.func.jacrev",
        "api_signature": "torch.func.jacrev(func, argnums=0, *, has_aux=False, chunk_size=None, _preallocate_and_copy=False)",
        "api_description": "Computes the Jacobian of func with respect to the arg(s) at index\nargnum using reverse mode autodiff",
        "return_value": "Returns a function that takes in the same inputs as func and\nreturns the Jacobian of func with respect to the arg(s) at\nargnums. If has_aux is True, then the returned function\ninstead returns a (jacobian, aux) tuple where jacobian\nis the Jacobian and aux is auxiliary objects returned by func.\n",
        "parameters": "func (function) – A Python function that takes one or more arguments,\none of which must be a Tensor, and returns one or more Tensors\nargnums (int or Tuple[int]) – Optional, integer or tuple of integers,\nsaying which arguments to get the Jacobian with respect to.\nDefault: 0.\nhas_aux (bool) – Flag indicating that func returns a\n(output, aux) tuple where the first element is the output of\nthe function to be differentiated and the second element is\nauxiliary objects that will not be differentiated.\nDefault: False.\nchunk_size (None or int) – If None (default), use the maximum chunk size\n(equivalent to doing a single vmap over vjp to compute the jacobian).\nIf 1, then compute the jacobian row-by-row with a for-loop.\nIf not None, then compute the jacobian chunk_size rows at a time\n(equivalent to doing multiple vmap over vjp). If you run into memory issues computing\nthe jacobian, please try to specify a non-None chunk_size.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.onnx.JitScalarType",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.onnx.JitScalarType.html#torch.onnx.JitScalarType",
        "api_signature": "torch.onnx.JitScalarType(value)",
        "api_description": "Scalar types defined in torch.",
        "return_value": "JitScalarType\nJitScalarType.\n",
        "parameters": "dtype (Optional[dtype]) – A torch.dtype to create a JitScalarType from\nvalue (Union[None, Value, Tensor]) – An object to fetch scalar type from.\ndefault – The JitScalarType to return if a valid scalar cannot be fetched from value",
        "input_shape": "",
        "notes": "A “RuntimeError: INTERNAL ASSERT FAILED at “../aten/src/ATen/core/jit_type_base.h” can\nbe raised in several scenarios where shape info is not present.\nInstead use from_value API which is safer.\n",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.algorithms.Join",
        "api_url": "https://pytorch.org/docs/stable/distributed.algorithms.join.html#torch.distributed.algorithms.Join",
        "api_signature": "torch.distributed.algorithms.Join(joinables, enable=True, throw_on_early_termination=False, **kwargs)",
        "api_description": "This class defines the generic join context manager, which allows custom hooks to be called after a process joins.",
        "return_value": "An async work handle for the all-reduce meant to notify the context\nmanager that the process has not yet joined if joinable is the\nfirst one passed into the context manager; None otherwise.\n",
        "parameters": "joinables (List[Joinable]) – a list of the participating\nJoinable s; their hooks are iterated over in the given\norder.\nenable (bool) – a flag enabling uneven input detection; setting to\nFalse disables the context manager’s functionality and should\nonly be set when the user knows the inputs will not be uneven\n(default: True).\nthrow_on_early_termination (bool) – a flag controlling whether to throw an\nexception upon detecting uneven inputs (default: False).\njoinable (Joinable) – the Joinable object calling this\nmethod.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.rendezvous.dynamic_rendezvous.RendezvousTimeout.join",
        "api_url": "https://pytorch.org/docs/stable/elastic/rendezvous.html#torch.distributed.elastic.rendezvous.dynamic_rendezvous.RendezvousTimeout.join",
        "api_signature": null,
        "api_description": "Get the join timeout.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.multiprocessing.SpawnContext.join",
        "api_url": "https://pytorch.org/docs/stable/multiprocessing.html#torch.multiprocessing.SpawnContext.join",
        "api_signature": "join(timeout=None)",
        "api_description": "Join one or more processes within spawn context.",
        "return_value": "",
        "parameters": "timeout (float) – Wait this long before giving up on waiting.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.parallel.DistributedDataParallel.join",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel.join",
        "api_signature": "join(divide_by_initial_world_size=True, enable=True, throw_on_early_termination=False)",
        "api_description": "Context manager for training with uneven inputs across processes in DDP.",
        "return_value": "",
        "parameters": "divide_by_initial_world_size (bool) – If True, will divide\ngradients by the initial world_size DDP training was launched\nwith. If False, will compute the effective world size\n(number of ranks that have not depleted their inputs yet) and\ndivide gradients by that during allreduce. Set\ndivide_by_initial_world_size=True to ensure every input\nsample including the uneven inputs have equal weight in terms of\nhow much they contribute to the global gradient. This is\nachieved by always dividing the gradient by the initial\nworld_size even when we encounter uneven inputs. If you set\nthis to False, we divide the gradient by the remaining\nnumber of nodes. This ensures parity with training on a smaller\nworld_size although it also means the uneven inputs would\ncontribute more towards the global gradient. Typically, you\nwould want to set this to True for cases where the last few\ninputs of your training job are uneven. In extreme cases, where\nthere is a large discrepancy in the number of inputs, setting\nthis to False might provide better results.\nenable (bool) – Whether to enable uneven input detection or not. Pass\nin enable=False to disable in cases where you know that\ninputs are even across participating processes. Default is\nTrue.\nthrow_on_early_termination (bool) – Whether to throw an error\nor continue training when at least one rank has exhausted\ninputs. If True, will throw upon the first rank reaching end\nof data. If False, will continue training with a smaller\neffective world size until all ranks are joined. Note that if\nthis flag is specified, then the flag\ndivide_by_initial_world_size would be ignored. Default\nis False.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.algorithms.Joinable.join_device",
        "api_url": "https://pytorch.org/docs/stable/distributed.algorithms.join.html#torch.distributed.algorithms.Joinable.join_device",
        "api_signature": null,
        "api_description": "Return the device from which to perform collective communications needed by the join context manager.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.optim.ZeroRedundancyOptimizer.join_device",
        "api_url": "https://pytorch.org/docs/stable/distributed.optim.html#torch.distributed.optim.ZeroRedundancyOptimizer.join_device",
        "api_signature": null,
        "api_description": "Return default device.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.algorithms.Joinable.join_hook",
        "api_url": "https://pytorch.org/docs/stable/distributed.algorithms.join.html#torch.distributed.algorithms.Joinable.join_hook",
        "api_signature": "join_hook(**kwargs)",
        "api_description": "Return a JoinHook instance for the given Joinable.",
        "return_value": "",
        "parameters": "kwargs (dict) – a dict containing any keyword arguments\nto modify the behavior of the join hook at run time; all\nJoinable instances sharing the same join context\nmanager are forwarded the same value for kwargs.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.optim.ZeroRedundancyOptimizer.join_hook",
        "api_url": "https://pytorch.org/docs/stable/distributed.optim.html#torch.distributed.optim.ZeroRedundancyOptimizer.join_hook",
        "api_signature": "join_hook(**kwargs)",
        "api_description": "Return the ZeRO join hook.",
        "return_value": "",
        "parameters": "kwargs (dict) – a dict containing any keyword arguments\nto modify the behavior of the join hook at run time; all\nJoinable instances sharing the same join context\nmanager are forwarded the same value for kwargs.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.parallel.DistributedDataParallel.join_hook",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel.join_hook",
        "api_signature": "join_hook(**kwargs)",
        "api_description": "DDP join hook enables training on uneven inputs by mirroring communications in forward and backward passes.",
        "return_value": "",
        "parameters": "kwargs (dict) – a dict containing any keyword arguments\nto modify the behavior of the join hook at run time; all\nJoinable instances sharing the same join context\nmanager are forwarded the same value for kwargs.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.algorithms.Joinable.join_process_group",
        "api_url": "https://pytorch.org/docs/stable/distributed.algorithms.join.html#torch.distributed.algorithms.Joinable.join_process_group",
        "api_signature": null,
        "api_description": "Returns the process group for the collective communications needed by the join context manager itself.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.optim.ZeroRedundancyOptimizer.join_process_group",
        "api_url": "https://pytorch.org/docs/stable/distributed.optim.html#torch.distributed.optim.ZeroRedundancyOptimizer.join_process_group",
        "api_signature": null,
        "api_description": "Return process group.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.algorithms.Joinable",
        "api_url": "https://pytorch.org/docs/stable/distributed.algorithms.join.html#torch.distributed.algorithms.Joinable",
        "api_signature": null,
        "api_description": "This defines an abstract base class for joinable classes.",
        "return_value": "",
        "parameters": "kwargs (dict) – a dict containing any keyword arguments\nto modify the behavior of the join hook at run time; all\nJoinable instances sharing the same join context\nmanager are forwarded the same value for kwargs.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.algorithms.JoinHook",
        "api_url": "https://pytorch.org/docs/stable/distributed.algorithms.join.html#torch.distributed.algorithms.JoinHook",
        "api_signature": null,
        "api_description": "This defines a join hook, which provides two entry points in the join context manager.",
        "return_value": "",
        "parameters": "is_last_joiner (bool) – True if the rank is one of the last to\njoin; False otherwise.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.autograd.functional.jvp",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.autograd.functional.jvp.html#torch.autograd.functional.jvp",
        "api_signature": "torch.autograd.functional.jvp(func, inputs, v=None, create_graph=False, strict=False)",
        "api_description": "Compute the dot product between the Jacobian of the given function at the point given by the inputs and a vector v.",
        "return_value": "\ntuple with:func_output (tuple of Tensors or Tensor): output of func(inputs)\njvp (tuple of Tensors or Tensor): result of the dot product with\nthe same shape as the output.\n\n\n\n",
        "parameters": "func (function) – a Python function that takes Tensor inputs and returns\na tuple of Tensors or a Tensor.\ninputs (tuple of Tensors or Tensor) – inputs to the function func.\nv (tuple of Tensors or Tensor) – The vector for which the Jacobian\nvector product is computed. Must be the same size as the input of\nfunc. This argument is optional when the input to func\ncontains a single element and (if it is not provided) will be set\nas a Tensor containing a single 1.\ncreate_graph (bool, optional) – If True, both the output and result\nwill be computed in a differentiable way. Note that when strict\nis False, the result can not require gradients or be\ndisconnected from the inputs.  Defaults to False.\nstrict (bool, optional) – If True, an error will be raised when we\ndetect that there exists an input such that all the outputs are\nindependent of it. If False, we return a Tensor of zeros as the\njvp for said inputs, which is the expected mathematical value.\nDefaults to False.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.func.jvp",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.func.jvp.html#torch.func.jvp",
        "api_signature": "torch.func.jvp(func, primals, tangents, *, strict=False, has_aux=False)",
        "api_description": "Standing for the Jacobian-vector product, returns a tuple containing\nthe output of func(*primals) and the “Jacobian of func evaluated at\nprimals” times tangents. This is also known as forward-mode autodiff.",
        "return_value": "Returns a (output, jvp_out) tuple containing the output of func\nevaluated at primals and the Jacobian-vector product.\nIf has_aux is True, then instead returns a (output, jvp_out, aux) tuple.\n",
        "parameters": "func (function) – A Python function that takes one or more arguments,\none of which must be a Tensor, and returns one or more Tensors\nprimals (Tensors) – Positional arguments to func that must all be\nTensors. The returned function will also be computing the\nderivative with respect to these arguments\ntangents (Tensors) – The “vector” for which Jacobian-vector-product is\ncomputed. Must be the same structure and sizes as the inputs to\nfunc.\nhas_aux (bool) – Flag indicating that func returns a\n(output, aux) tuple where the first element is the output of\nthe function to be differentiated and the second element is\nother auxiliary objects that will not be differentiated.\nDefault: False.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.autograd.Function.jvp",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.autograd.Function.jvp.html#torch.autograd.Function.jvp",
        "api_signature": "Function.jvp(ctx, *grad_inputs)",
        "api_description": "Define a formula for differentiating the operation with forward mode automatic differentiation.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.autograd.function.InplaceFunction.jvp",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.autograd.function.InplaceFunction.html#torch.autograd.function.InplaceFunction.jvp",
        "api_signature": "jvp(ctx, *grad_inputs)",
        "api_description": "Define a formula for differentiating the operation with forward mode automatic differentiation.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.autograd.function.NestedIOFunction.jvp",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.autograd.function.NestedIOFunction.html#torch.autograd.function.NestedIOFunction.jvp",
        "api_signature": "jvp(ctx, *grad_inputs)",
        "api_description": "Define a formula for differentiating the operation with forward mode automatic differentiation.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.init.kaiming_normal_",
        "api_url": "https://pytorch.org/docs/stable/nn.init.html#torch.nn.init.kaiming_normal_",
        "api_signature": "torch.nn.init.kaiming_normal_(tensor, a=0, mode='fan_in', nonlinearity='leaky_relu', generator=None)",
        "api_description": "Fill the input Tensor with values using a Kaiming normal distribution.",
        "return_value": "",
        "parameters": "tensor (Tensor) – an n-dimensional torch.Tensor\na (float) – the negative slope of the rectifier used after this layer (only\nused with 'leaky_relu')\nmode (str) – either 'fan_in' (default) or 'fan_out'. Choosing 'fan_in'\npreserves the magnitude of the variance of the weights in the\nforward pass. Choosing 'fan_out' preserves the magnitudes in the\nbackwards pass.\nnonlinearity (str) – the non-linear function (nn.functional name),\nrecommended to use only with 'relu' or 'leaky_relu' (default).\ngenerator (Optional[Generator]) – the torch Generator to sample from (default: None)",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.init.kaiming_uniform_",
        "api_url": "https://pytorch.org/docs/stable/nn.init.html#torch.nn.init.kaiming_uniform_",
        "api_signature": "torch.nn.init.kaiming_uniform_(tensor, a=0, mode='fan_in', nonlinearity='leaky_relu', generator=None)",
        "api_description": "Fill the input Tensor with values using a Kaiming uniform distribution.",
        "return_value": "",
        "parameters": "tensor (Tensor) – an n-dimensional torch.Tensor\na (float) – the negative slope of the rectifier used after this layer (only\nused with 'leaky_relu')\nmode (str) – either 'fan_in' (default) or 'fan_out'. Choosing 'fan_in'\npreserves the magnitude of the variance of the weights in the\nforward pass. Choosing 'fan_out' preserves the magnitudes in the\nbackwards pass.\nnonlinearity (str) – the non-linear function (nn.functional name),\nrecommended to use only with 'relu' or 'leaky_relu' (default).\ngenerator (Optional[Generator]) – the torch Generator to sample from (default: None)",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.signal.windows.kaiser",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.signal.windows.kaiser.html#torch.signal.windows.kaiser",
        "api_signature": "torch.signal.windows.kaiser(M, *, beta=12.0, sym=True, dtype=None, layout=torch.strided, device=None, requires_grad=False)",
        "api_description": "Computes the Kaiser window.",
        "return_value": "",
        "parameters": "M (int) – the length of the window.\nIn other words, the number of points of the returned window.\nbeta (float, optional) – shape parameter for the window. Must be non-negative. Default: 12.0\nsym (bool, optional) – If False, returns a periodic window suitable for use in spectral analysis.\nIf True, returns a symmetric window suitable for use in filter design. Default: True.\ndtype (torch.dtype, optional) – the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_dtype()).\nlayout (torch.layout, optional) – the desired layout of returned Tensor.\nDefault: torch.strided.\ndevice (torch.device, optional) – the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_device()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types.\nrequires_grad (bool, optional) – If autograd should record operations on the\nreturned tensor. Default: False.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.kaiser_window",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window",
        "api_signature": "torch.kaiser_window(window_length, periodic=True, beta=12.0, *, dtype=None, layout=torch.strided, device=None, requires_grad=False)",
        "api_description": "Computes the Kaiser window with window length window_length and shape parameter beta.",
        "return_value": "",
        "parameters": "window_length (int) – length of the window.\nperiodic (bool, optional) – If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design.\nbeta (float, optional) – shape parameter for the window.\ndtype (torch.dtype, optional) – the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_dtype()).\nlayout (torch.layout, optional) – the desired layout of returned window tensor. Only\ntorch.strided (dense layout) is supported.\ndevice (torch.device, optional) – the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_device()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types.\nrequires_grad (bool, optional) – If autograd should record operations on the\nreturned tensor. Default: False.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.autograd.profiler_util.Kernel",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.autograd.profiler_util.Kernel.html#torch.autograd.profiler_util.Kernel",
        "api_signature": "torch.autograd.profiler_util.Kernel(name, device, duration)",
        "api_description": "Return number of occurrences of value.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.autograd.profiler.profile.key_averages",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.autograd.profiler.profile.key_averages.html#torch.autograd.profiler.profile.key_averages",
        "api_signature": "profile.key_averages(group_by_input_shape=False, group_by_stack_n=0)",
        "api_description": "Averages all function events over their keys.",
        "return_value": "An EventList containing FunctionEventAvg objects.\n",
        "parameters": "group_by_input_shapes – group entries by\n(event name, input shapes) rather than just event name.\nThis is useful to see which input shapes contribute to the runtime\nthe most and may help with size-specific optimizations or\nchoosing the best candidates for quantization (aka fitting a roof line)\ngroup_by_stack_n – group by top n stack trace entries",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.profiler._KinetoProfile.key_averages",
        "api_url": "https://pytorch.org/docs/stable/profiler.html#torch.profiler._KinetoProfile.key_averages",
        "api_signature": "key_averages(group_by_input_shape=False, group_by_stack_n=0)",
        "api_description": "Averages events, grouping them by operator name and (optionally) input shapes and\nstack.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.autograd.profiler_util.StringTable.keys",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.autograd.profiler_util.StringTable.html#torch.autograd.profiler_util.StringTable.keys",
        "api_signature": "keys()",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.Tracer.keys",
        "api_url": "https://pytorch.org/docs/stable/fx.html#torch.fx.Tracer.keys",
        "api_signature": "keys(obj)",
        "api_description": "This is what happens when ** is called on a proxy. This should return an\niterator it ** is suppose to work in your custom tracer.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.ModuleDict.keys",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.ModuleDict.html#torch.nn.ModuleDict.keys",
        "api_signature": "keys()",
        "api_description": "Return an iterable of the ModuleDict keys.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.ParameterDict.keys",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.ParameterDict.html#torch.nn.ParameterDict.keys",
        "api_signature": "keys()",
        "api_description": "Return an iterable of the ParameterDict keys.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.autograd.profiler.KinetoStepTracker",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.autograd.profiler.KinetoStepTracker.html#torch.autograd.profiler.KinetoStepTracker",
        "api_signature": null,
        "api_description": "Provides an abstraction for incrementing the step count globally.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.functional.kl_div",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.functional.kl_div.html#torch.nn.functional.kl_div",
        "api_signature": "torch.nn.functional.kl_div(input, target, size_average=None, reduce=None, reduction='mean', log_target=False)",
        "api_description": "Compute the KL Divergence loss.",
        "return_value": "",
        "parameters": "input (Tensor) – Tensor of arbitrary shape in log-probabilities.\ntarget (Tensor) – Tensor of the same shape as input. See log_target for\nthe target’s interpretation.\nsize_average (bool, optional) – Deprecated (see reduction). By default,\nthe losses are averaged over each loss element in the batch. Note that for\nsome losses, there multiple elements per sample. If the field size_average\nis set to False, the losses are instead summed for each minibatch. Ignored\nwhen reduce is False. Default: True\nreduce (bool, optional) – Deprecated (see reduction). By default, the\nlosses are averaged or summed over observations for each minibatch depending\non size_average. When reduce is False, returns a loss per\nbatch element instead and ignores size_average. Default: True\nreduction (str, optional) – Specifies the reduction to apply to the output:\n'none' | 'batchmean' | 'sum' | 'mean'.\n'none': no reduction will be applied\n'batchmean': the sum of the output will be divided by the batchsize\n'sum': the output will be summed\n'mean': the output will be divided by the number of elements in the output\nDefault: 'mean'\nlog_target (bool) – A flag indicating whether target is passed in the log space.\nIt is recommended to pass certain distributions (like softmax)\nin the log space to avoid numerical issues caused by explicit log.\nDefault: False",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.kl.kl_divergence",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.kl.kl_divergence",
        "api_signature": "torch.distributions.kl.kl_divergence(p, q)",
        "api_description": "Compute Kullback-Leibler divergence KL(p∥q)KL(p \\| q)KL(p∥q) between two distributions.",
        "return_value": "A batch of KL divergences of shape batch_shape.\n",
        "parameters": "p (Distribution) – A Distribution object.\nq (Distribution) – A Distribution object.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.KLDivLoss",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.KLDivLoss.html#torch.nn.KLDivLoss",
        "api_signature": "torch.nn.KLDivLoss(size_average=None, reduce=None, reduction='mean', log_target=False)",
        "api_description": "The Kullback-Leibler divergence loss.",
        "return_value": "",
        "parameters": "size_average (bool, optional) – Deprecated (see reduction). By default,\nthe losses are averaged over each loss element in the batch. Note that for\nsome losses, there are multiple elements per sample. If the field size_average\nis set to False, the losses are instead summed for each minibatch. Ignored\nwhen reduce is False. Default: True\nreduce (bool, optional) – Deprecated (see reduction). By default, the\nlosses are averaged or summed over observations for each minibatch depending\non size_average. When reduce is False, returns a loss per\nbatch element instead and ignores size_average. Default: True\nreduction (str, optional) – Specifies the reduction to apply to the output. Default: “mean”\nlog_target (bool, optional) – Specifies whether target is the log space. Default: False",
        "input_shape": "\nInput: (∗)(*)(∗), where ∗*∗ means any number of dimensions.\nTarget: (∗)(*)(∗), same shape as the input.\nOutput: scalar by default. If reduction is ‘none’, then (∗)(*)(∗),\nsame shape as the input.\n\n",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.kron",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.kron.html#torch.kron",
        "api_signature": "torch.kron(input, other, *, out=None)",
        "api_description": "Computes the Kronecker product, denoted by ⊗\\otimes⊗, of input and other.",
        "return_value": "",
        "parameters": "input (Tensor) –\nother (Tensor) –\nout (Tensor, optional) – The output tensor. Ignored if None. Default: None",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.kthvalue",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.kthvalue.html#torch.kthvalue",
        "api_signature": "torch.kthvalue(input, k, dim=None, keepdim=False, *, out=None)",
        "api_description": "Returns a namedtuple (values, indices) where values is the k th\nsmallest element of each row of the input tensor in the given dimension\ndim. And indices is the index location of each element found.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.\nk (int) – k for the k-th smallest element\ndim (int, optional) – the dimension to find the kth value along\nkeepdim (bool) – whether the output tensor has dim retained or not.\nout (tuple, optional) – the output tuple of (Tensor, LongTensor)\ncan be optionally given to be used as output buffers",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.kthvalue",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.kthvalue.html#torch.Tensor.kthvalue",
        "api_signature": "Tensor.kthvalue(k, dim=None, keepdim=False)",
        "api_description": "See torch.kthvalue()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.kumaraswamy.Kumaraswamy",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.kumaraswamy.Kumaraswamy",
        "api_signature": "torch.distributions.kumaraswamy.Kumaraswamy(concentration1, concentration0, validate_args=None)",
        "api_description": "Bases: TransformedDistribution",
        "return_value": "",
        "parameters": "concentration1 (float or Tensor) – 1st concentration parameter of the distribution\n(often referred to as alpha)\nconcentration0 (float or Tensor) – 2nd concentration parameter of the distribution\n(often referred to as beta)",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.Node.kwargs",
        "api_url": "https://pytorch.org/docs/stable/fx.html#torch.fx.Node.kwargs",
        "api_signature": null,
        "api_description": "The dict of keyword arguments to this Node. The interpretation of arguments\ndepends on the node’s opcode. See the Node docstring for more\ninformation.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.functional.l1_loss",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.functional.l1_loss.html#torch.nn.functional.l1_loss",
        "api_signature": "torch.nn.functional.l1_loss(input, target, size_average=None, reduce=None, reduction='mean')",
        "api_description": "Function that takes the mean element-wise absolute value difference.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.utils.prune.l1_unstructured",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.l1_unstructured.html#torch.nn.utils.prune.l1_unstructured",
        "api_signature": "torch.nn.utils.prune.l1_unstructured(module, name, amount, importance_scores=None)",
        "api_description": "Prune tensor by removing units with the lowest L1-norm.",
        "return_value": "modified (i.e. pruned) version of the input module\n",
        "parameters": "module (nn.Module) – module containing the tensor to prune\nname (str) – parameter name within module on which pruning\nwill act.\namount (int or float) – quantity of parameters to prune.\nIf float, should be between 0.0 and 1.0 and represent the\nfraction of parameters to prune. If int, it represents the\nabsolute number of parameters to prune.\nimportance_scores (torch.Tensor) – tensor of importance scores (of same\nshape as module parameter) used to compute mask for pruning.\nThe values in this tensor indicate the importance of the corresponding\nelements in the parameter being pruned.\nIf unspecified or None, the module parameter will be used in its place.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.L1Loss",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.L1Loss.html#torch.nn.L1Loss",
        "api_signature": "torch.nn.L1Loss(size_average=None, reduce=None, reduction='mean')",
        "api_description": "Creates a criterion that measures the mean absolute error (MAE) between each element in\nthe input xxx and target yyy.",
        "return_value": "",
        "parameters": "size_average (bool, optional) – Deprecated (see reduction). By default,\nthe losses are averaged over each loss element in the batch. Note that for\nsome losses, there are multiple elements per sample. If the field size_average\nis set to False, the losses are instead summed for each minibatch. Ignored\nwhen reduce is False. Default: True\nreduce (bool, optional) – Deprecated (see reduction). By default, the\nlosses are averaged or summed over observations for each minibatch depending\non size_average. When reduce is False, returns a loss per\nbatch element instead and ignores size_average. Default: True\nreduction (str, optional) – Specifies the reduction to apply to the output:\n'none' | 'mean' | 'sum'. 'none': no reduction will be applied,\n'mean': the sum of the output will be divided by the number of\nelements in the output, 'sum': the output will be summed. Note: size_average\nand reduce are in the process of being deprecated, and in the meantime,\nspecifying either of those two args will override reduction. Default: 'mean'",
        "input_shape": "\nInput: (∗)(*)(∗), where ∗*∗ means any number of dimensions.\nTarget: (∗)(*)(∗), same shape as the input.\nOutput: scalar. If reduction is 'none', then\n(∗)(*)(∗), same shape as the input.\n\n",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.utils.prune.L1Unstructured",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.L1Unstructured.html#torch.nn.utils.prune.L1Unstructured",
        "api_signature": "torch.nn.utils.prune.L1Unstructured(amount)",
        "api_description": "Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm.",
        "return_value": "pruned version of the input tensor\npruned version of tensor t.\n",
        "parameters": "amount (int or float) – quantity of parameters to prune.\nIf float, should be between 0.0 and 1.0 and represent the\nfraction of parameters to prune. If int, it represents the\nabsolute number of parameters to prune.\nmodule (nn.Module) – module containing the tensor to prune\nname (str) – parameter name within module on which pruning\nwill act.\namount (int or float) – quantity of parameters to prune.\nIf float, should be between 0.0 and 1.0 and represent the\nfraction of parameters to prune. If int, it represents the\nabsolute number of parameters to prune.\nimportance_scores (torch.Tensor) – tensor of importance scores (of same\nshape as module parameter) used to compute mask for pruning.\nThe values in this tensor indicate the importance of the corresponding\nelements in the parameter being pruned.\nIf unspecified or None, the module parameter will be used in its place.\nmodule (nn.Module) – module containing the tensor to prune\nt (torch.Tensor) – tensor to prune (of same dimensions as\ndefault_mask).\nimportance_scores (torch.Tensor) – tensor of importance scores (of\nsame shape as t) used to compute mask for pruning t.\nThe values in this tensor indicate the importance of the\ncorresponding elements in the t that is being pruned.\nIf unspecified or None, the tensor t will be used in its place.\ndefault_mask (torch.Tensor, optional) – mask from previous pruning\niteration, if any. To be considered when determining what\nportion of the tensor that pruning should act on. If None,\ndefault to a mask of ones.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.lr_scheduler.LambdaLR",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.LambdaLR.html#torch.optim.lr_scheduler.LambdaLR",
        "api_signature": "torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda, last_epoch=-1, verbose='deprecated')",
        "api_description": "Sets the learning rate of each parameter group to the initial lr\ntimes a given function. When last_epoch=-1, sets initial lr as lr.",
        "return_value": "",
        "parameters": "optimizer (Optimizer) – Wrapped optimizer.\nlr_lambda (function or list) – A function which computes a multiplicative\nfactor given an integer parameter epoch, or a list of such\nfunctions, one for each group in optimizer.param_groups.\nlast_epoch (int) – The index of last epoch. Default: -1.\nverbose (bool) – If True, prints a message to stdout for\neach update. Default: False.\nDeprecated since version 2.2: verbose is deprecated. Please use get_last_lr() to access the\nlearning rate.\nstate_dict (dict) – scheduler state. Should be an object returned\nfrom a call to state_dict().",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.laplace.Laplace",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.laplace.Laplace",
        "api_signature": "torch.distributions.laplace.Laplace(loc, scale, validate_args=None)",
        "api_description": "Bases: Distribution",
        "return_value": "",
        "parameters": "loc (float or Tensor) – mean of the distribution\nscale (float or Tensor) – scale of the distribution",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.rendezvous.dynamic_rendezvous.RendezvousTimeout.last_call",
        "api_url": "https://pytorch.org/docs/stable/elastic/rendezvous.html#torch.distributed.elastic.rendezvous.dynamic_rendezvous.RendezvousTimeout.last_call",
        "api_signature": null,
        "api_description": "Get the last call timeout.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.functional.layer_norm",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.functional.layer_norm.html#torch.nn.functional.layer_norm",
        "api_signature": "torch.nn.functional.layer_norm(input, normalized_shape, weight=None, bias=None, eps=1e-05)",
        "api_description": "Apply Layer Normalization for last certain number of dimensions.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.quantized.LayerNorm",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.nn.quantized.LayerNorm.html#torch.ao.nn.quantized.LayerNorm",
        "api_signature": "torch.ao.nn.quantized.LayerNorm(normalized_shape, weight, bias, scale, zero_point, eps=1e-05, elementwise_affine=True, device=None, dtype=None)",
        "api_description": "This is the quantized version of LayerNorm.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.LayerNorm",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html#torch.nn.LayerNorm",
        "api_signature": "torch.nn.LayerNorm(normalized_shape, eps=1e-05, elementwise_affine=True, bias=True, device=None, dtype=None)",
        "api_description": "Applies Layer Normalization over a mini-batch of inputs.",
        "return_value": "",
        "parameters": "normalized_shape (int or list or torch.Size) – input shape from an expected input\nof size\n[∗×normalized_shape[0]×normalized_shape[1]×…×normalized_shape[−1]][* \\times \\text{normalized\\_shape}[0] \\times \\text{normalized\\_shape}[1]\n\\times \\ldots \\times \\text{normalized\\_shape}[-1]]\n[∗×normalized_shape[0]×normalized_shape[1]×…×normalized_shape[−1]]If a single integer is used, it is treated as a singleton list, and this module will\nnormalize over the last dimension which is expected to be of that specific size.\neps (float) – a value added to the denominator for numerical stability. Default: 1e-5\nelementwise_affine (bool) – a boolean value that when set to True, this module\nhas learnable per-element affine parameters initialized to ones (for weights)\nand zeros (for biases). Default: True.\nbias (bool) – If set to False, the layer will not learn an additive bias (only relevant if\nelementwise_affine is True). Default: True.",
        "input_shape": "\nInput: (N,∗)(N, *)(N,∗)\nOutput: (N,∗)(N, *)(N,∗) (same shape as input)\n\n",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.layout",
        "api_url": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.layout",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.LazyBatchNorm1d",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.LazyBatchNorm1d.html#torch.nn.LazyBatchNorm1d",
        "api_signature": "torch.nn.LazyBatchNorm1d(eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, device=None, dtype=None)",
        "api_description": "A torch.nn.BatchNorm1d module with lazy initialization.",
        "return_value": "",
        "parameters": "eps (float) – a value added to the denominator for numerical stability.\nDefault: 1e-5\nmomentum (float) – the value used for the running_mean and running_var\ncomputation. Can be set to None for cumulative moving average\n(i.e. simple average). Default: 0.1\naffine (bool) – a boolean value that when set to True, this module has\nlearnable affine parameters. Default: True\ntrack_running_stats (bool) – a boolean value that when set to True, this\nmodule tracks the running mean and variance, and when set to False,\nthis module does not track such statistics, and initializes statistics\nbuffers running_mean and running_var as None.\nWhen these buffers are None, this module always uses batch statistics.\nin both training and eval modes. Default: True",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.LazyBatchNorm2d",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.LazyBatchNorm2d.html#torch.nn.LazyBatchNorm2d",
        "api_signature": "torch.nn.LazyBatchNorm2d(eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, device=None, dtype=None)",
        "api_description": "A torch.nn.BatchNorm2d module with lazy initialization.",
        "return_value": "",
        "parameters": "eps (float) – a value added to the denominator for numerical stability.\nDefault: 1e-5\nmomentum (float) – the value used for the running_mean and running_var\ncomputation. Can be set to None for cumulative moving average\n(i.e. simple average). Default: 0.1\naffine (bool) – a boolean value that when set to True, this module has\nlearnable affine parameters. Default: True\ntrack_running_stats (bool) – a boolean value that when set to True, this\nmodule tracks the running mean and variance, and when set to False,\nthis module does not track such statistics, and initializes statistics\nbuffers running_mean and running_var as None.\nWhen these buffers are None, this module always uses batch statistics.\nin both training and eval modes. Default: True",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.LazyBatchNorm3d",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.LazyBatchNorm3d.html#torch.nn.LazyBatchNorm3d",
        "api_signature": "torch.nn.LazyBatchNorm3d(eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, device=None, dtype=None)",
        "api_description": "A torch.nn.BatchNorm3d module with lazy initialization.",
        "return_value": "",
        "parameters": "eps (float) – a value added to the denominator for numerical stability.\nDefault: 1e-5\nmomentum (float) – the value used for the running_mean and running_var\ncomputation. Can be set to None for cumulative moving average\n(i.e. simple average). Default: 0.1\naffine (bool) – a boolean value that when set to True, this module has\nlearnable affine parameters. Default: True\ntrack_running_stats (bool) – a boolean value that when set to True, this\nmodule tracks the running mean and variance, and when set to False,\nthis module does not track such statistics, and initializes statistics\nbuffers running_mean and running_var as None.\nWhen these buffers are None, this module always uses batch statistics.\nin both training and eval modes. Default: True",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.LazyConv1d",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.LazyConv1d.html#torch.nn.LazyConv1d",
        "api_signature": "torch.nn.LazyConv1d(out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None)",
        "api_description": "A torch.nn.Conv1d module with lazy initialization of the in_channels argument.",
        "return_value": "",
        "parameters": "out_channels (int) – Number of channels produced by the convolution\nkernel_size (int or tuple) – Size of the convolving kernel\nstride (int or tuple, optional) – Stride of the convolution. Default: 1\npadding (int or tuple, optional) – Zero-padding added to both sides of\nthe input. Default: 0\npadding_mode (str, optional) – 'zeros', 'reflect',\n'replicate' or 'circular'. Default: 'zeros'\ndilation (int or tuple, optional) – Spacing between kernel\nelements. Default: 1\ngroups (int, optional) – Number of blocked connections from input\nchannels to output channels. Default: 1\nbias (bool, optional) – If True, adds a learnable bias to the\noutput. Default: True",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.LazyConv2d",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.LazyConv2d.html#torch.nn.LazyConv2d",
        "api_signature": "torch.nn.LazyConv2d(out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None)",
        "api_description": "A torch.nn.Conv2d module with lazy initialization of the in_channels argument.",
        "return_value": "",
        "parameters": "out_channels (int) – Number of channels produced by the convolution\nkernel_size (int or tuple) – Size of the convolving kernel\nstride (int or tuple, optional) – Stride of the convolution. Default: 1\npadding (int or tuple, optional) – Zero-padding added to both sides of\nthe input. Default: 0\npadding_mode (str, optional) – 'zeros', 'reflect',\n'replicate' or 'circular'. Default: 'zeros'\ndilation (int or tuple, optional) – Spacing between kernel\nelements. Default: 1\ngroups (int, optional) – Number of blocked connections from input\nchannels to output channels. Default: 1\nbias (bool, optional) – If True, adds a learnable bias to the\noutput. Default: True",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.LazyConv3d",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.LazyConv3d.html#torch.nn.LazyConv3d",
        "api_signature": "torch.nn.LazyConv3d(out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None)",
        "api_description": "A torch.nn.Conv3d module with lazy initialization of the in_channels argument.",
        "return_value": "",
        "parameters": "out_channels (int) – Number of channels produced by the convolution\nkernel_size (int or tuple) – Size of the convolving kernel\nstride (int or tuple, optional) – Stride of the convolution. Default: 1\npadding (int or tuple, optional) – Zero-padding added to both sides of\nthe input. Default: 0\npadding_mode (str, optional) – 'zeros', 'reflect',\n'replicate' or 'circular'. Default: 'zeros'\ndilation (int or tuple, optional) – Spacing between kernel\nelements. Default: 1\ngroups (int, optional) – Number of blocked connections from input\nchannels to output channels. Default: 1\nbias (bool, optional) – If True, adds a learnable bias to the\noutput. Default: True",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.LazyConvTranspose1d",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.LazyConvTranspose1d.html#torch.nn.LazyConvTranspose1d",
        "api_signature": "torch.nn.LazyConvTranspose1d(out_channels, kernel_size, stride=1, padding=0, output_padding=0, groups=1, bias=True, dilation=1, padding_mode='zeros', device=None, dtype=None)",
        "api_description": "A torch.nn.ConvTranspose1d module with lazy initialization of the in_channels argument.",
        "return_value": "",
        "parameters": "out_channels (int) – Number of channels produced by the convolution\nkernel_size (int or tuple) – Size of the convolving kernel\nstride (int or tuple, optional) – Stride of the convolution. Default: 1\npadding (int or tuple, optional) – dilation * (kernel_size - 1) - padding zero-padding\nwill be added to both sides of the input. Default: 0\noutput_padding (int or tuple, optional) – Additional size added to one side\nof the output shape. Default: 0\ngroups (int, optional) – Number of blocked connections from input channels to output channels. Default: 1\nbias (bool, optional) – If True, adds a learnable bias to the output. Default: True\ndilation (int or tuple, optional) – Spacing between kernel elements. Default: 1",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.LazyConvTranspose2d",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.LazyConvTranspose2d.html#torch.nn.LazyConvTranspose2d",
        "api_signature": "torch.nn.LazyConvTranspose2d(out_channels, kernel_size, stride=1, padding=0, output_padding=0, groups=1, bias=True, dilation=1, padding_mode='zeros', device=None, dtype=None)",
        "api_description": "A torch.nn.ConvTranspose2d module with lazy initialization of the in_channels argument.",
        "return_value": "",
        "parameters": "out_channels (int) – Number of channels produced by the convolution\nkernel_size (int or tuple) – Size of the convolving kernel\nstride (int or tuple, optional) – Stride of the convolution. Default: 1\npadding (int or tuple, optional) – dilation * (kernel_size - 1) - padding zero-padding\nwill be added to both sides of each dimension in the input. Default: 0\noutput_padding (int or tuple, optional) – Additional size added to one side\nof each dimension in the output shape. Default: 0\ngroups (int, optional) – Number of blocked connections from input channels to output channels. Default: 1\nbias (bool, optional) – If True, adds a learnable bias to the output. Default: True\ndilation (int or tuple, optional) – Spacing between kernel elements. Default: 1",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.LazyConvTranspose3d",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.LazyConvTranspose3d.html#torch.nn.LazyConvTranspose3d",
        "api_signature": "torch.nn.LazyConvTranspose3d(out_channels, kernel_size, stride=1, padding=0, output_padding=0, groups=1, bias=True, dilation=1, padding_mode='zeros', device=None, dtype=None)",
        "api_description": "A torch.nn.ConvTranspose3d module with lazy initialization of the in_channels argument.",
        "return_value": "",
        "parameters": "out_channels (int) – Number of channels produced by the convolution\nkernel_size (int or tuple) – Size of the convolving kernel\nstride (int or tuple, optional) – Stride of the convolution. Default: 1\npadding (int or tuple, optional) – dilation * (kernel_size - 1) - padding zero-padding\nwill be added to both sides of each dimension in the input. Default: 0\noutput_padding (int or tuple, optional) – Additional size added to one side\nof each dimension in the output shape. Default: 0\ngroups (int, optional) – Number of blocked connections from input channels to output channels. Default: 1\nbias (bool, optional) – If True, adds a learnable bias to the output. Default: True\ndilation (int or tuple, optional) – Spacing between kernel elements. Default: 1",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.LazyInstanceNorm1d",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.LazyInstanceNorm1d.html#torch.nn.LazyInstanceNorm1d",
        "api_signature": "torch.nn.LazyInstanceNorm1d(eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, device=None, dtype=None)",
        "api_description": "A torch.nn.InstanceNorm1d module with lazy initialization of the num_features argument.",
        "return_value": "",
        "parameters": "num_features – CCC from an expected input of size\n(N,C,L)(N, C, L)(N,C,L) or (C,L)(C, L)(C,L)\neps (float) – a value added to the denominator for numerical stability. Default: 1e-5\nmomentum (float) – the value used for the running_mean and running_var computation. Default: 0.1\naffine (bool) – a boolean value that when set to True, this module has\nlearnable affine parameters, initialized the same way as done for batch normalization.\nDefault: False.\ntrack_running_stats (bool) – a boolean value that when set to True, this\nmodule tracks the running mean and variance, and when set to False,\nthis module does not track such statistics and always uses batch\nstatistics in both training and eval modes. Default: False",
        "input_shape": "\nInput: (N,C,L)(N, C, L)(N,C,L) or (C,L)(C, L)(C,L)\nOutput: (N,C,L)(N, C, L)(N,C,L) or (C,L)(C, L)(C,L) (same shape as input)\n\n",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.LazyInstanceNorm2d",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.LazyInstanceNorm2d.html#torch.nn.LazyInstanceNorm2d",
        "api_signature": "torch.nn.LazyInstanceNorm2d(eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, device=None, dtype=None)",
        "api_description": "A torch.nn.InstanceNorm2d module with lazy initialization of the num_features argument.",
        "return_value": "",
        "parameters": "num_features – CCC from an expected input of size\n(N,C,H,W)(N, C, H, W)(N,C,H,W) or (C,H,W)(C, H, W)(C,H,W)\neps (float) – a value added to the denominator for numerical stability. Default: 1e-5\nmomentum (float) – the value used for the running_mean and running_var computation. Default: 0.1\naffine (bool) – a boolean value that when set to True, this module has\nlearnable affine parameters, initialized the same way as done for batch normalization.\nDefault: False.\ntrack_running_stats (bool) – a boolean value that when set to True, this\nmodule tracks the running mean and variance, and when set to False,\nthis module does not track such statistics and always uses batch\nstatistics in both training and eval modes. Default: False",
        "input_shape": "\nInput: (N,C,H,W)(N, C, H, W)(N,C,H,W) or (C,H,W)(C, H, W)(C,H,W)\nOutput: (N,C,H,W)(N, C, H, W)(N,C,H,W) or (C,H,W)(C, H, W)(C,H,W) (same shape as input)\n\n",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.LazyInstanceNorm3d",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.LazyInstanceNorm3d.html#torch.nn.LazyInstanceNorm3d",
        "api_signature": "torch.nn.LazyInstanceNorm3d(eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, device=None, dtype=None)",
        "api_description": "A torch.nn.InstanceNorm3d module with lazy initialization of the num_features argument.",
        "return_value": "",
        "parameters": "num_features – CCC from an expected input of size\n(N,C,D,H,W)(N, C, D, H, W)(N,C,D,H,W) or (C,D,H,W)(C, D, H, W)(C,D,H,W)\neps (float) – a value added to the denominator for numerical stability. Default: 1e-5\nmomentum (float) – the value used for the running_mean and running_var computation. Default: 0.1\naffine (bool) – a boolean value that when set to True, this module has\nlearnable affine parameters, initialized the same way as done for batch normalization.\nDefault: False.\ntrack_running_stats (bool) – a boolean value that when set to True, this\nmodule tracks the running mean and variance, and when set to False,\nthis module does not track such statistics and always uses batch\nstatistics in both training and eval modes. Default: False",
        "input_shape": "\nInput: (N,C,D,H,W)(N, C, D, H, W)(N,C,D,H,W) or (C,D,H,W)(C, D, H, W)(C,D,H,W)\nOutput: (N,C,D,H,W)(N, C, D, H, W)(N,C,D,H,W) or (C,D,H,W)(C, D, H, W)(C,D,H,W) (same shape as input)\n\n",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.LazyLinear",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.LazyLinear.html#torch.nn.LazyLinear",
        "api_signature": "torch.nn.LazyLinear(out_features, bias=True, device=None, dtype=None)",
        "api_description": "A torch.nn.Linear module where in_features is inferred.",
        "return_value": "",
        "parameters": "out_features (int) – size of each output sample\nbias (UninitializedParameter) – If set to False, the layer will not learn an additive bias.\nDefault: True",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.modules.lazy.LazyModuleMixin",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.modules.lazy.LazyModuleMixin.html#torch.nn.modules.lazy.LazyModuleMixin",
        "api_signature": "torch.nn.modules.lazy.LazyModuleMixin(*args, **kwargs)",
        "api_description": "A mixin for modules that lazily initialize parameters, also known as “lazy modules”.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.LBFGS",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.LBFGS.html#torch.optim.LBFGS",
        "api_signature": "torch.optim.LBFGS(params, lr=1, max_iter=20, max_eval=None, tolerance_grad=1e-07, tolerance_change=1e-09, history_size=100, line_search_fn=None)",
        "api_description": "Implements L-BFGS algorithm.",
        "return_value": "a handle that can be used to remove the added hook by calling\nhandle.remove()\na handle that can be used to remove the added hook by calling\nhandle.remove()\na handle that can be used to remove the added hook by calling\nhandle.remove()\na handle that can be used to remove the added hook by calling\nhandle.remove()\na handle that can be used to remove the added hook by calling\nhandle.remove()\na handle that can be used to remove the added hook by calling\nhandle.remove()\n",
        "parameters": "params (iterable) – iterable of parameters to optimize. Parameters must be real.\nlr (float) – learning rate (default: 1)\nmax_iter (int) – maximal number of iterations per optimization step\n(default: 20)\nmax_eval (int) – maximal number of function evaluations per optimization\nstep (default: max_iter * 1.25).\ntolerance_grad (float) – termination tolerance on first order optimality\n(default: 1e-7).\ntolerance_change (float) – termination tolerance on function\nvalue/parameter changes (default: 1e-9).\nhistory_size (int) – update history size (default: 100).\nline_search_fn (str) – either ‘strong_wolfe’ or None (default: None).\nparam_group (dict) – Specifies what Tensors should be optimized along with group\nspecific optimization options.\nstate_dict (dict) – optimizer state. Should be an object returned\nfrom a call to state_dict().\nhook (Callable) – The user defined hook to be registered.\nprepend (bool) – If True, the provided post hook will be fired before\nall the already registered post-hooks on load_state_dict. Otherwise,\nthe provided hook will be fired after all the already registered\npost-hooks. (default: False)\nhook (Callable) – The user defined hook to be registered.\nprepend (bool) – If True, the provided pre hook will be fired before\nall the already registered pre-hooks on load_state_dict. Otherwise,\nthe provided hook will be fired after all the already registered\npre-hooks. (default: False)\nhook (Callable) – The user defined hook to be registered.\nprepend (bool) – If True, the provided post hook will be fired before\nall the already registered post-hooks on state_dict. Otherwise,\nthe provided hook will be fired after all the already registered\npost-hooks. (default: False)\nhook (Callable) – The user defined hook to be registered.\nprepend (bool) – If True, the provided pre hook will be fired before\nall the already registered pre-hooks on state_dict. Otherwise,\nthe provided hook will be fired after all the already registered\npre-hooks. (default: False)\nhook (Callable) – The user defined hook to be registered.\nhook (Callable) – The user defined hook to be registered.\nclosure (Callable) – A closure that reevaluates the model\nand returns the loss.\nset_to_none (bool) – instead of setting to zero, set the grads to None.\nThis will in general have lower memory footprint, and can modestly improve performance.\nHowever, it changes certain behaviors. For example:\n1. When the user tries to access a gradient and perform manual ops on it,\na None attribute or a Tensor full of 0s will behave differently.\n2. If the user requests zero_grad(set_to_none=True) followed by a backward pass, .grads\nare guaranteed to be None for params that did not receive a gradient.\n3. torch.optim optimizers have a different behavior if the gradient is 0 or None\n(in one case it does the step with a gradient of 0 and in the other it skips\nthe step altogether).",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.lcm",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.lcm.html#torch.lcm",
        "api_signature": "torch.lcm(input, other, *, out=None)",
        "api_description": "Computes the element-wise least common multiple (LCM) of input and other.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.\nother (Tensor) – the second input tensor\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.lcm",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.lcm.html#torch.Tensor.lcm",
        "api_signature": "Tensor.lcm(other)",
        "api_description": "See torch.lcm()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.lcm_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.lcm_.html#torch.Tensor.lcm_",
        "api_signature": "Tensor.lcm_(other)",
        "api_description": "In-place version of lcm()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ldexp",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ldexp.html#torch.ldexp",
        "api_signature": "torch.ldexp(input, other, *, out=None)",
        "api_description": "Multiplies input by 2 ** other.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.\nother (Tensor) – a tensor of exponents, typically integers.\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.ldexp",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.ldexp.html#torch.Tensor.ldexp",
        "api_signature": "Tensor.ldexp(other)",
        "api_description": "See torch.ldexp()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.ldexp_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.ldexp_.html#torch.Tensor.ldexp_",
        "api_signature": "Tensor.ldexp_(other)",
        "api_description": "In-place version of ldexp()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.linalg.ldl_factor",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.linalg.ldl_factor.html#torch.linalg.ldl_factor",
        "api_signature": "torch.linalg.ldl_factor(A, *, hermitian=False, out=None)",
        "api_description": "Computes a compact representation of the LDL factorization of a Hermitian or symmetric (possibly indefinite) matrix.",
        "return_value": "A named tuple (LD, pivots).\n",
        "parameters": "A (Tensor) – tensor of shape (*, n, n) where * is zero or more batch dimensions\nconsisting of symmetric or Hermitian matrices.\nhermitian (bool, optional) – whether to consider the input to be Hermitian or symmetric.\nFor real-valued matrices, this switch has no effect. Default: False.\nout (tuple, optional) – tuple of two tensors to write the output to. Ignored if None. Default: None.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.linalg.ldl_factor_ex",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.linalg.ldl_factor_ex.html#torch.linalg.ldl_factor_ex",
        "api_signature": "torch.linalg.ldl_factor_ex(A, *, hermitian=False, check_errors=False, out=None)",
        "api_description": "This is a version of ldl_factor() that does not perform error checks unless check_errors= True.\nIt also returns the info tensor returned by LAPACK’s sytrf.\ninfo stores integer error codes from the backend library.\nA positive integer indicates the diagonal element of DDD that is zero.\nDivision by 0 will occur if the result is used for solving a system of linear equations.\ninfo filled with zeros indicates that the factorization was successful.\nIf check_errors=True and info contains positive integers, then a RuntimeError is thrown.",
        "return_value": "A named tuple (LD, pivots, info).\n",
        "parameters": "A (Tensor) – tensor of shape (*, n, n) where * is zero or more batch dimensions\nconsisting of symmetric or Hermitian matrices.\nhermitian (bool, optional) – whether to consider the input to be Hermitian or symmetric.\nFor real-valued matrices, this switch has no effect. Default: False.\ncheck_errors (bool, optional) – controls whether to check the content of info and raise\nan error if it is non-zero. Default: False.\nout (tuple, optional) – tuple of three tensors to write the output to. Ignored if None. Default: None.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.linalg.ldl_solve",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.linalg.ldl_solve.html#torch.linalg.ldl_solve",
        "api_signature": "torch.linalg.ldl_solve(LD, pivots, B, *, hermitian=False, out=None)",
        "api_description": "Computes the solution of a system of linear equations using the LDL factorization.",
        "return_value": "",
        "parameters": "LD (Tensor) – the n times n matrix or the batch of such matrices of size\n(*, n, n) where * is one or more batch dimensions.\npivots (Tensor) – the pivots corresponding to the LDL factorization of LD.\nB (Tensor) – right-hand side tensor of shape (*, n, k).\nhermitian (bool, optional) – whether to consider the decomposed matrix to be Hermitian or symmetric.\nFor real-valued matrices, this switch has no effect. Default: False.\nout (tuple, optional) – output tensor. B may be passed as out and the result is computed in-place on B.\nIgnored if None. Default: None.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.le",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.le.html#torch.le",
        "api_signature": "torch.le(input, other, *, out=None)",
        "api_description": "Computes input≤other\\text{input} \\leq \\text{other}input≤other element-wise.",
        "return_value": "A boolean tensor that is True where input is less than or equal to\nother and False elsewhere\n",
        "parameters": "input (Tensor) – the tensor to compare\nother (Tensor or Scalar) – the tensor or value to compare\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.le",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.le.html#torch.Tensor.le",
        "api_signature": "Tensor.le(other)",
        "api_description": "See torch.le().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.le_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.le_.html#torch.Tensor.le_",
        "api_signature": "Tensor.le_(other)",
        "api_description": "In-place version of le().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.quantized.functional.leaky_relu",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.nn.quantized.functional.leaky_relu.html#torch.ao.nn.quantized.functional.leaky_relu",
        "api_signature": "torch.ao.nn.quantized.functional.leaky_relu(input, negative_slope=0.01, inplace=False, scale=None, zero_point=None)",
        "api_description": "Quantized version of the.\nleaky_relu(input, negative_slope=0.01, inplace=False, scale, zero_point) -> Tensor",
        "return_value": "",
        "parameters": "input (Tensor) – Quantized input\nnegative_slope (float) – The slope of the negative input\ninplace (bool) – Inplace modification of the input tensor\nscale (Optional[float]) – Scale and zero point of the output tensor.\nzero_point (Optional[int]) – Scale and zero point of the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.functional.leaky_relu",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.functional.leaky_relu.html#torch.nn.functional.leaky_relu",
        "api_signature": "torch.nn.functional.leaky_relu(input, negative_slope=0.01, inplace=False)",
        "api_description": "Applies element-wise,\nLeakyReLU(x)=max⁡(0,x)+negative_slope∗min⁡(0,x)\\text{LeakyReLU}(x) = \\max(0, x) + \\text{negative\\_slope} * \\min(0, x)LeakyReLU(x)=max(0,x)+negative_slope∗min(0,x)",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.functional.leaky_relu_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.functional.leaky_relu_.html#torch.nn.functional.leaky_relu_",
        "api_signature": "torch.nn.functional.leaky_relu_(input, negative_slope=0.01)",
        "api_description": "In-place version of leaky_relu().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.quantized.LeakyReLU",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.nn.quantized.LeakyReLU.html#torch.ao.nn.quantized.LeakyReLU",
        "api_signature": "torch.ao.nn.quantized.LeakyReLU(scale, zero_point, negative_slope=0.01, inplace=False, device=None, dtype=None)",
        "api_description": "This is the quantized equivalent of LeakyReLU.",
        "return_value": "",
        "parameters": "scale (float) – quantization scale of the output tensor\nzero_point (int) – quantization zero point of the output tensor\nnegative_slope (float) – Controls the angle of the negative slope. Default: 1e-2",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.LeakyReLU",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.LeakyReLU.html#torch.nn.LeakyReLU",
        "api_signature": "torch.nn.LeakyReLU(negative_slope=0.01, inplace=False)",
        "api_description": "Applies the LeakyReLU function element-wise.",
        "return_value": "",
        "parameters": "negative_slope (float) – Controls the angle of the negative slope (which is used for\nnegative input values). Default: 1e-2\ninplace (bool) – can optionally do the operation in-place. Default: False",
        "input_shape": "\nInput: (∗)(*)(∗) where * means, any number of additional\ndimensions\nOutput: (∗)(*)(∗), same shape as the input\n\n",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.lerp",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.lerp.html#torch.lerp",
        "api_signature": "torch.lerp(input, end, weight, *, out=None)",
        "api_description": "Does a linear interpolation of two tensors start (given by input) and end based\non a scalar or tensor weight and returns the resulting out tensor.",
        "return_value": "",
        "parameters": "input (Tensor) – the tensor with the starting points\nend (Tensor) – the tensor with the ending points\nweight (float or tensor) – the weight for the interpolation formula\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.lerp",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.lerp.html#torch.Tensor.lerp",
        "api_signature": "Tensor.lerp(end, weight)",
        "api_description": "See torch.lerp()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.lerp_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.lerp_.html#torch.Tensor.lerp_",
        "api_signature": "Tensor.lerp_(end, weight)",
        "api_description": "In-place version of lerp()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.less",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.less.html#torch.less",
        "api_signature": "torch.less(input, other, *, out=None)",
        "api_description": "Alias for torch.lt().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.less",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.less.html#torch.Tensor.less",
        "api_signature": "Tensor.less()",
        "api_description": "lt(other) -> Tensor",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.less_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.less_.html#torch.Tensor.less_",
        "api_signature": "Tensor.less_(other)",
        "api_description": "In-place version of less().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.less_equal",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.less_equal.html#torch.less_equal",
        "api_signature": "torch.less_equal(input, other, *, out=None)",
        "api_description": "Alias for torch.le().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.less_equal",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.less_equal.html#torch.Tensor.less_equal",
        "api_signature": "Tensor.less_equal(other)",
        "api_description": "See torch.less_equal().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.less_equal_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.less_equal_.html#torch.Tensor.less_equal_",
        "api_signature": "Tensor.less_equal_(other)",
        "api_description": "In-place version of less_equal().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.constraints.less_than",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.constraints.less_than",
        "api_signature": null,
        "api_description": "alias of _LessThan",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.lgamma",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.lgamma.html#torch.lgamma",
        "api_signature": "torch.lgamma(input, *, out=None)",
        "api_description": "Computes the natural logarithm of the absolute value of the gamma function on input.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.lgamma",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.lgamma.html#torch.Tensor.lgamma",
        "api_signature": "Tensor.lgamma()",
        "api_description": "See torch.lgamma()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.lgamma_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.lgamma_.html#torch.Tensor.lgamma_",
        "api_signature": "Tensor.lgamma_()",
        "api_description": "In-place version of lgamma()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.library.Library",
        "api_url": "https://pytorch.org/docs/stable/library.html#torch.library.Library",
        "api_signature": "torch.library.Library(ns, kind, dispatch_key='')",
        "api_description": "A class to create libraries that can be used to register new operators or\noverride operators in existing libraries from Python.\nA user can optionally pass in a dispatch keyname if they only want to register\nkernels corresponding to only one specific dispatch key.",
        "return_value": "name of the operator as inferred from the schema.\n",
        "parameters": "ns – library name\nkind – “DEF”, “IMPL” (default: “IMPL”), “FRAGMENT”\nschema – function schema to define a new operator.\nalias_analysis (optional) – Indicates if the aliasing properties of the operator arguments can be\ninferred from the schema (default behavior) or not (“CONSERVATIVE”).\ntags (Tag | Sequence[Tag]) – one or more torch.Tag to apply to this\noperator. Tagging an operator changes the operator’s behavior\ntorch.Tag carefully before applying it.\nop_name – operator name (along with the overload) or OpOverload object.\nfn – function that’s the operator implementation for the input dispatch key or fallthrough_kernel()\nto register a fallthrough.\ndispatch_key – dispatch key that the input function should be registered for. By default, it uses\nthe dispatch key that the library was created with.",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> my_lib = Library(\"foo\", \"DEF\")\n>>> my_lib.define(\"sum(Tensor self) -> Tensor\")\n\n\n>>> my_lib = Library(\"aten\", \"IMPL\")\n>>> def div_cpu(self, other):\n>>>     return self * (1 / other)\n>>> my_lib.impl(\"div.Tensor\", div_cpu, \"CPU\")\n\n\n"
    },
    {
        "api_name": "torch.ao.nn.qat.Linear",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.nn.qat.Linear.html#torch.ao.nn.qat.Linear",
        "api_signature": "torch.ao.nn.qat.Linear(in_features, out_features, bias=True, qconfig=None, device=None, dtype=None)",
        "api_description": "A linear module attached with FakeQuantize modules for weight,\nused for quantization aware training.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.qat.dynamic.Linear",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.nn.qat.dynamic.Linear.html#torch.ao.nn.qat.dynamic.Linear",
        "api_signature": "torch.ao.nn.qat.dynamic.Linear(in_features, out_features, bias=True, qconfig=None, device=None, dtype=None)",
        "api_description": "A linear module attached with FakeQuantize modules for weight,\nused for dynamic quantization aware training.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.quantized.Linear",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.nn.quantized.Linear.html#torch.ao.nn.quantized.Linear",
        "api_signature": "torch.ao.nn.quantized.Linear(in_features, out_features, bias_=True, dtype=torch.qint8)",
        "api_description": "A quantized linear module with quantized tensor as inputs and outputs.\nWe adopt the same interface as torch.nn.Linear, please see\nhttps://pytorch.org/docs/stable/nn.html#torch.nn.Linear for documentation.",
        "return_value": "",
        "parameters": "mod (Module) – a float module, either produced by torch.ao.quantization\nutilities or provided by the user\nref_qlinear (Module) – a reference quantized linear module, either produced by torch.ao.quantization\nutilities or provided by the user\noutput_scale (float) – scale for output Tensor\noutput_zero_point (int) – zero point for output Tensor",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.quantized.dynamic.Linear",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.nn.quantized.dynamic.Linear.html#torch.ao.nn.quantized.dynamic.Linear",
        "api_signature": "torch.ao.nn.quantized.dynamic.Linear(in_features, out_features, bias_=True, dtype=torch.qint8)",
        "api_description": "A dynamic quantized linear module with floating point tensor as inputs and outputs.\nWe adopt the same interface as torch.nn.Linear, please see\nhttps://pytorch.org/docs/stable/nn.html#torch.nn.Linear for documentation.",
        "return_value": "",
        "parameters": "mod (Module) – a float module, either produced by torch.ao.quantization\nutilities or provided by the user",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.quantized.functional.linear",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.nn.quantized.functional.linear.html#torch.ao.nn.quantized.functional.linear",
        "api_signature": "torch.ao.nn.quantized.functional.linear(input, weight, bias=None, scale=None, zero_point=None)",
        "api_description": "Applies a linear transformation to the incoming quantized data:\ny=xAT+by = xA^T + by=xAT+b.\nSee Linear",
        "return_value": "",
        "parameters": "input (Tensor) – Quantized input of type torch.quint8\nweight (Tensor) – Quantized weight of type torch.qint8\nbias (Tensor) – None or fp32 bias of type torch.float\nscale (double) – output scale. If None, derived from the input scale\nzero_point (python:long) – output zero point. If None, derived from the input zero_point",
        "input_shape": "\nInput: (N,∗,in_features)(N, *, in\\_features)(N,∗,in_features) where * means any number of\nadditional dimensions\nWeight: (out_features,in_features)(out\\_features, in\\_features)(out_features,in_features)\nBias: (out_features)(out\\_features)(out_features)\nOutput: (N,∗,out_features)(N, *, out\\_features)(N,∗,out_features)\n\n",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.Linear",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear",
        "api_signature": "torch.nn.Linear(in_features, out_features, bias=True, device=None, dtype=None)",
        "api_description": "Applies a linear transformation to the incoming data: y=xAT+by = xA^T + by=xAT+b.",
        "return_value": "",
        "parameters": "in_features (int) – size of each input sample\nout_features (int) – size of each output sample\nbias (bool) – If set to False, the layer will not learn an additive bias.\nDefault: True",
        "input_shape": "\nInput: (∗,Hin)(*, H_{in})(∗,Hin​) where ∗*∗ means any number of\ndimensions including none and Hin=in_featuresH_{in} = \\text{in\\_features}Hin​=in_features.\nOutput: (∗,Hout)(*, H_{out})(∗,Hout​) where all but the last dimension\nare the same shape as the input and Hout=out_featuresH_{out} = \\text{out\\_features}Hout​=out_features.\n\n",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.functional.linear",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.functional.linear.html#torch.nn.functional.linear",
        "api_signature": "torch.nn.functional.linear(input, weight, bias=None)",
        "api_description": "Applies a linear transformation to the incoming data: y=xAT+by = xA^T + by=xAT+b.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.func.linearize",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.func.linearize.html#torch.func.linearize",
        "api_signature": "torch.func.linearize(func, *primals)",
        "api_description": "Returns the value of func at primals and linear approximation\nat primals.",
        "return_value": "Returns a (output, jvp_fn) tuple containing the output of func\napplied to primals and a function that computes the jvp of\nfunc evaluated at primals.\n",
        "parameters": "func (Callable) – A Python function that takes one or more arguments.\nprimals (Tensors) – Positional arguments to func that must all be\nTensors. These are the values at which the function is linearly approximated.",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import torch\n>>> from torch.func import linearize\n>>> def fn(x):\n...     return x.sin()\n...\n>>> output, jvp_fn = linearize(fn, torch.zeros(3, 3))\n>>> jvp_fn(torch.ones(3, 3))\ntensor([[1., 1., 1.],\n        [1., 1., 1.],\n        [1., 1., 1.]])\n>>>\n\n\n"
    },
    {
        "api_name": "torch.optim.lr_scheduler.LinearLR",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.LinearLR.html#torch.optim.lr_scheduler.LinearLR",
        "api_signature": "torch.optim.lr_scheduler.LinearLR(optimizer, start_factor=0.3333333333333333, end_factor=1.0, total_iters=5, last_epoch=-1, verbose='deprecated')",
        "api_description": "Decays the learning rate of each parameter group by linearly changing small\nmultiplicative factor until the number of epoch reaches a pre-defined milestone: total_iters.\nNotice that such decay can happen simultaneously with other changes to the learning rate\nfrom outside this scheduler. When last_epoch=-1, sets initial lr as lr.",
        "return_value": "",
        "parameters": "optimizer (Optimizer) – Wrapped optimizer.\nstart_factor (float) – The number we multiply learning rate in the first epoch.\nThe multiplication factor changes towards end_factor in the following epochs.\nDefault: 1./3.\nend_factor (float) – The number we multiply learning rate at the end of linear changing\nprocess. Default: 1.0.\ntotal_iters (int) – The number of iterations that multiplicative factor reaches to 1.\nDefault: 5.\nlast_epoch (int) – The index of the last epoch. Default: -1.\nverbose (bool) – If True, prints a message to stdout for\neach update. Default: False.\nDeprecated since version 2.2: verbose is deprecated. Please use get_last_lr() to access the\nlearning rate.\nstate_dict (dict) – scheduler state. Should be an object returned\nfrom a call to state_dict().",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.intrinsic.LinearReLU",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.nn.intrinsic.LinearReLU.html#torch.ao.nn.intrinsic.LinearReLU",
        "api_signature": "torch.ao.nn.intrinsic.LinearReLU(linear, relu)",
        "api_description": "This is a sequential container which calls the Linear and ReLU modules.\nDuring quantization this will be replaced with the corresponding fused module.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.intrinsic.qat.LinearReLU",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.nn.intrinsic.qat.LinearReLU.html#torch.ao.nn.intrinsic.qat.LinearReLU",
        "api_signature": "torch.ao.nn.intrinsic.qat.LinearReLU(in_features, out_features, bias=True, qconfig=None)",
        "api_description": "A LinearReLU module fused from Linear and ReLU modules, attached with\nFakeQuantize modules for weight, used in\nquantization aware training.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.intrinsic.quantized.LinearReLU",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.nn.intrinsic.quantized.LinearReLU.html#torch.ao.nn.intrinsic.quantized.LinearReLU",
        "api_signature": "torch.ao.nn.intrinsic.quantized.LinearReLU(in_features, out_features, bias=True, dtype=torch.qint8)",
        "api_description": "A LinearReLU module fused from Linear and ReLU modules",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.intrinsic.quantized.dynamic.LinearReLU",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.nn.intrinsic.quantized.dynamic.LinearReLU.html#torch.ao.nn.intrinsic.quantized.dynamic.LinearReLU",
        "api_signature": "torch.ao.nn.intrinsic.quantized.dynamic.LinearReLU(in_features, out_features, bias=True, dtype=torch.qint8)",
        "api_description": "A LinearReLU module fused from Linear and ReLU modules that can be used\nfor dynamic quantization.\nSupports both, FP16 and INT8 quantization.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.linspace",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.linspace.html#torch.linspace",
        "api_signature": "torch.linspace(start, end, steps, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False)",
        "api_description": "Creates a one-dimensional tensor of size steps whose values are evenly\nspaced from start to end, inclusive. That is, the value are:",
        "return_value": "",
        "parameters": "start (float or Tensor) – the starting value for the set of points. If Tensor, it must be 0-dimensional\nend (float or Tensor) – the ending value for the set of points. If Tensor, it must be 0-dimensional\nsteps (int) – size of the constructed tensor\nout (Tensor, optional) – the output tensor.\ndtype (torch.dtype, optional) – the data type to perform the computation in.\nDefault: if None, uses the global default dtype (see torch.get_default_dtype())\nwhen both start and end are real,\nand corresponding complex dtype when either is complex.\nlayout (torch.layout, optional) – the desired layout of returned Tensor.\nDefault: torch.strided.\ndevice (torch.device, optional) – the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_device()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types.\nrequires_grad (bool, optional) – If autograd should record operations on the\nreturned tensor. Default: False.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.Graph.lint",
        "api_url": "https://pytorch.org/docs/stable/fx.html#torch.fx.Graph.lint",
        "api_signature": "lint()",
        "api_description": "Runs various checks on this Graph to make sure it is well-formed. In\nparticular:\n- Checks Nodes have correct ownership (owned by this graph)\n- Checks Nodes appear in topological order\n- If this Graph has an owning GraphModule, checks that targets\nexist in that GraphModule",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.hub.list",
        "api_url": "https://pytorch.org/docs/stable/hub.html#torch.hub.list",
        "api_signature": "torch.hub.list(github, force_reload=False, skip_validation=False, trust_repo=None, verbose=True)",
        "api_description": "List all callable entrypoints available in the repo specified by github.",
        "return_value": "The available callables entrypoint\n",
        "parameters": "github (str) – a string with format “repo_owner/repo_name[:ref]” with an optional\nref (tag or branch). If ref is not specified, the default branch is assumed to be main if\nit exists, and otherwise master.\nforce_reload (bool, optional) – whether to discard the existing cache and force a fresh download.\nDefault is False.\nskip_validation (bool, optional) – if False, torchhub will check that the branch or commit\nspecified by the github argument properly belongs to the repo owner. This will make\nrequests to the GitHub API; you can specify a non-default GitHub token by setting the\nGITHUB_TOKEN environment variable. Default is False.\ntrust_repo (bool, str or None) – \"check\", True, False or None.\nThis parameter was introduced in v1.12 and helps ensuring that users\nonly run code from repos that they trust.\nIf False, a prompt will ask the user whether the repo should\nbe trusted.\nIf True, the repo will be added to the trusted list and loaded\nwithout requiring explicit confirmation.\nIf \"check\", the repo will be checked against the list of\ntrusted repos in the cache. If it is not present in that list, the\nbehaviour will fall back onto the trust_repo=False option.\nIf None: this will raise a warning, inviting the user to set\ntrust_repo to either False, True or \"check\". This\nis only present for backward compatibility and will be removed in\nv2.0.\nDefault is None and will eventually change to \"check\" in v2.0.\nverbose (bool, optional) – If False, mute messages about hitting\nlocal caches. Note that the message about first download cannot be\nmuted. Default is True.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.compiler.list_backends",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.compiler.list_backends.html#torch.compiler.list_backends",
        "api_signature": "torch.compiler.list_backends(exclude_tags=('debug', 'experimental')",
        "api_description": "Return valid strings that can be passed to torch.compile(…, backend=”name”).",
        "return_value": "",
        "parameters": "exclude_tags (optional) – A tuple of strings representing tags to exclude.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda.list_gpu_processes",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.cuda.list_gpu_processes.html#torch.cuda.list_gpu_processes",
        "api_signature": "torch.cuda.list_gpu_processes(device=None)",
        "api_description": "Return a human-readable printout of the running processes and their GPU memory use for a given device.",
        "return_value": "",
        "parameters": "device (torch.device or int, optional) – selected device. Returns\nprintout for the current device, given by current_device(),\nif device is None (default).",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.lkj_cholesky.LKJCholesky",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.lkj_cholesky.LKJCholesky",
        "api_signature": "torch.distributions.lkj_cholesky.LKJCholesky(dim, concentration=1.0, validate_args=None)",
        "api_description": "Bases: Distribution",
        "return_value": "",
        "parameters": "dimension (dim) – dimension of the matrices\nconcentration (float or Tensor) – concentration/shape parameter of the\ndistribution (often referred to as eta)",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.utils.prune.ln_structured",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.ln_structured.html#torch.nn.utils.prune.ln_structured",
        "api_signature": "torch.nn.utils.prune.ln_structured(module, name, amount, n, dim, importance_scores=None)",
        "api_description": "Prune tensor by removing channels with the lowest Ln-norm along the specified dimension.",
        "return_value": "modified (i.e. pruned) version of the input module\n",
        "parameters": "module (nn.Module) – module containing the tensor to prune\nname (str) – parameter name within module on which pruning\nwill act.\namount (int or float) – quantity of parameters to prune.\nIf float, should be between 0.0 and 1.0 and represent the\nfraction of parameters to prune. If int, it represents the\nabsolute number of parameters to prune.\nn (int, float, inf, -inf, 'fro', 'nuc') – See documentation of valid\nentries for argument p in torch.norm().\ndim (int) – index of the dim along which we define channels to prune.\nimportance_scores (torch.Tensor) – tensor of importance scores (of same\nshape as module parameter) used to compute mask for pruning.\nThe values in this tensor indicate the importance of the corresponding\nelements in the parameter being pruned.\nIf unspecified or None, the module parameter will be used in its place.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.utils.prune.LnStructured",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.LnStructured.html#torch.nn.utils.prune.LnStructured",
        "api_signature": "torch.nn.utils.prune.LnStructured(amount, n, dim=-1)",
        "api_description": "Prune entire (currently unpruned) channels in a tensor based on their Ln-norm.",
        "return_value": "pruned version of the input tensor\nmask to apply to t, of same dims as t\npruned version of tensor t.\n",
        "parameters": "amount (int or float) – quantity of channels to prune.\nIf float, should be between 0.0 and 1.0 and represent the\nfraction of parameters to prune. If int, it represents the\nabsolute number of parameters to prune.\nn (int, float, inf, -inf, 'fro', 'nuc') – See documentation of valid\nentries for argument p in torch.norm().\ndim (int, optional) – index of the dim along which we define\nchannels to prune. Default: -1.\nmodule (nn.Module) – module containing the tensor to prune\nname (str) – parameter name within module on which pruning\nwill act.\namount (int or float) – quantity of parameters to prune.\nIf float, should be between 0.0 and 1.0 and represent the\nfraction of parameters to prune. If int, it represents the\nabsolute number of parameters to prune.\nn (int, float, inf, -inf, 'fro', 'nuc') – See documentation of valid\nentries for argument p in torch.norm().\ndim (int) – index of the dim along which we define channels to\nprune.\nimportance_scores (torch.Tensor) – tensor of importance scores (of same\nshape as module parameter) used to compute mask for pruning.\nThe values in this tensor indicate the importance of the corresponding\nelements in the parameter being pruned.\nIf unspecified or None, the module parameter will be used in its place.\nmodule (nn.Module) – module containing the tensor to prune\nt (torch.Tensor) – tensor representing the parameter to prune\ndefault_mask (torch.Tensor) – Base mask from previous pruning\niterations, that need to be respected after the new mask is\napplied.  Same dims as t.\nt (torch.Tensor) – tensor to prune (of same dimensions as\ndefault_mask).\nimportance_scores (torch.Tensor) – tensor of importance scores (of\nsame shape as t) used to compute mask for pruning t.\nThe values in this tensor indicate the importance of the\ncorresponding elements in the t that is being pruned.\nIf unspecified or None, the tensor t will be used in its place.\ndefault_mask (torch.Tensor, optional) – mask from previous pruning\niteration, if any. To be considered when determining what\nportion of the tensor that pruning should act on. If None,\ndefault to a mask of ones.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.load",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.load.html#torch.load",
        "api_signature": "torch.load(f, map_location=None, pickle_module=pickle, *, weights_only=False, mmap=None, **pickle_load_args)",
        "api_description": "Loads an object saved with torch.save() from a file.",
        "return_value": "",
        "parameters": "f (Union[str, PathLike, BinaryIO, IO[bytes]]) – a file-like object (has to implement read(), readline(), tell(), and seek()),\nor a string or os.PathLike object containing a file name\nmap_location (Optional[Union[Callable[[Tensor, str], Tensor], device, str, Dict[str, str]]]) – a function, torch.device, string or a dict specifying how to remap storage\nlocations\npickle_module (Optional[Any]) – module used for unpickling metadata and objects (has to\nmatch the pickle_module used to serialize file)\nweights_only (bool) – Indicates whether unpickler should be restricted to\nloading only tensors, primitive types and dictionaries\nmmap (Optional[bool]) – Indicates whether the file should be mmaped rather than loading all the storages into memory.\nTypically, tensor storages in the file will first be moved from disk to CPU memory, after which they\nare moved to the location that they were tagged with when saving, or specified by map_location. This\nsecond step is a no-op if the final location is CPU. When the mmap flag is set, instead of copying the\ntensor storages from disk to CPU memory in the first step, f is mmaped.\npickle_load_args (Any) – (Python 3 only) optional keyword arguments passed over to\npickle_module.load() and pickle_module.Unpickler(), e.g.,\nerrors=....",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.checkpoint.state_dict_loader.load",
        "api_url": "https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict_loader.load",
        "api_signature": "torch.distributed.checkpoint.state_dict_loader.load(state_dict, *, checkpoint_id=None, storage_reader=None, planner=None, process_group=None)",
        "api_description": "Load a distributed state_dict in SPMD style.",
        "return_value": "None.\n",
        "parameters": "state_dict (Dict[str, Any]) – The state_dict to save.\ncheckpoint_id (Union[str, os.PathLike, None]) – The ID of this checkpoint instance. The meaning of the checkpoint_id\ndepends on the storage. It can be a path to a folder or to a file.\nIt can also be a key if the storage is a key-value store.\n(Default: None)\nstorage_reader (Optional[StorageReader]) – Instance of StorageWriter used to perform reads. If this is not\nspecified, DCP will automatically infer the reader based on the\ncheckpoint_id. If checkpoint_id is also None, an exception will\nbe raised. (Default: None)\nplanner (Optional[LoadPlanner]) – Instance of LoadPlanner. If this is not specificed, the default\nplanner will be used. (Default: None)\nprocess_group (Optional[ProcessGroup]) – ProcessGroup to be used for cross-rank synchronization.\n(Default: None)",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.export.load",
        "api_url": "https://pytorch.org/docs/stable/export.html#torch.export.load",
        "api_signature": "torch.export.load(f, *, extra_files=None, expected_opset_version=None)",
        "api_description": "Warning",
        "return_value": "An ExportedProgram object\n",
        "parameters": "ep (ExportedProgram) – The exported program to save.\nf (Union[str, os.PathLike, io.BytesIO) – A file-like object (has to\nimplement write and flush) or a string containing a file name.\nextra_files (Optional[Dict[str, Any]]) – The extra filenames given in\nthis map would be loaded and their content would be stored in the\nprovided map.\nexpected_opset_version (Optional[Dict[str, int]]) – A map of opset names\nto expected opset versions",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.hub.load",
        "api_url": "https://pytorch.org/docs/stable/hub.html#torch.hub.load",
        "api_signature": "torch.hub.load(repo_or_dir, model, *args, source='github', trust_repo=None, force_reload=False, verbose=True, skip_validation=False, **kwargs)",
        "api_description": "Load a model from a github repo or a local directory.",
        "return_value": "The output of the model callable when called with the given\n*args and **kwargs.\n",
        "parameters": "repo_or_dir (str) – If source is ‘github’,\nthis should correspond to a github repo with format repo_owner/repo_name[:ref] with\nthe default branch is assumed to be main if it exists, and otherwise master.\nIf source is ‘local’  then it should be a path to a local directory.\nmodel (str) – the name of a callable (entrypoint) defined in the\nrepo/dir’s hubconf.py.\n*args (optional) – the corresponding args for callable model.\nsource (str, optional) – ‘github’ or ‘local’. Specifies how\nrepo_or_dir is to be interpreted. Default is ‘github’.\ntrust_repo (bool, str or None) – \"check\", True, False or None.\nThis parameter was introduced in v1.12 and helps ensuring that users\nonly run code from repos that they trust.\nIf False, a prompt will ask the user whether the repo should\nbe trusted.\nIf True, the repo will be added to the trusted list and loaded\nwithout requiring explicit confirmation.\nIf \"check\", the repo will be checked against the list of\ntrusted repos in the cache. If it is not present in that list, the\nbehaviour will fall back onto the trust_repo=False option.\nIf None: this will raise a warning, inviting the user to set\ntrust_repo to either False, True or \"check\". This\nis only present for backward compatibility and will be removed in\nv2.0.\nDefault is None and will eventually change to \"check\" in v2.0.\nforce_reload (bool, optional) – whether to force a fresh download of\nthe github repo unconditionally. Does not have any effect if\nsource = 'local'. Default is False.\nverbose (bool, optional) – If False, mute messages about hitting\nlocal caches. Note that the message about first download cannot be\nmuted. Does not have any effect if source = 'local'.\nDefault is True.\nskip_validation (bool, optional) – if False, torchhub will check that the branch or commit\nspecified by the github argument properly belongs to the repo owner. This will make\nrequests to the GitHub API; you can specify a non-default GitHub token by setting the\nGITHUB_TOKEN environment variable. Default is False.\n**kwargs (optional) – the corresponding kwargs for callable model.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.jit.load",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.jit.load.html#torch.jit.load",
        "api_signature": "torch.jit.load(f, map_location=None, _extra_files=None, _restore_shapes=False)",
        "api_description": "Load a ScriptModule or ScriptFunction previously saved with torch.jit.save.",
        "return_value": "A ScriptModule object.\n",
        "parameters": "f – a file-like object (has to implement read, readline, tell, and seek),\nor a string containing a file name\nmap_location (string or torch.device) – A simplified version of\nmap_location in torch.jit.save used to dynamically remap\nstorages to an alternative set of devices.\n_extra_files (dictionary of filename to content) – The extra\nfilenames given in the map would be loaded and their content\nwould be stored in the provided map.\n_restore_shapes (bool) – Whether or not to retrace the module on load using stored inputs",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.cpp_extension.load",
        "api_url": "https://pytorch.org/docs/stable/cpp_extension.html#torch.utils.cpp_extension.load",
        "api_signature": "torch.utils.cpp_extension.load(name, sources, extra_cflags=None, extra_cuda_cflags=None, extra_ldflags=None, extra_include_paths=None, build_directory=None, verbose=False, with_cuda=None, is_python_module=True, is_standalone=False, keep_intermediates=True)",
        "api_description": "Load a PyTorch C++ extension just-in-time (JIT).",
        "return_value": "Returns the loaded PyTorch extension as a Python module.\n\nIf is_python_module is False and is_standalone is False:Returns nothing. (The shared library is loaded into the process as\na side effect.)\n\nIf is_standalone is True.Return the path to the executable. (On Windows, TORCH_LIB_PATH is\nadded to the PATH environment variable as a side effect.)\n\n\n\n",
        "parameters": "name – The name of the extension to build. This MUST be the same as the\nname of the pybind11 module!\nsources (Union[str, List[str]]) – A list of relative or absolute paths to C++ source files.\nextra_cflags – optional list of compiler flags to forward to the build.\nextra_cuda_cflags – optional list of compiler flags to forward to nvcc\nwhen building CUDA sources.\nextra_ldflags – optional list of linker flags to forward to the build.\nextra_include_paths – optional list of include directories to forward\nto the build.\nbuild_directory – optional path to use as build workspace.\nverbose – If True, turns on verbose logging of load steps.\nwith_cuda (Optional[bool]) – Determines whether CUDA headers and libraries are added to\nthe build. If set to None (default), this value is\nautomatically determined based on the existence of .cu or\n.cuh in sources. Set it to True` to force CUDA headers\nand libraries to be included.\nis_python_module – If True (default), imports the produced shared\nlibrary as a Python module. If False, behavior depends on\nis_standalone.\nis_standalone – If False (default) loads the constructed extension\ninto the process as a plain dynamic library. If True, build a\nstandalone executable.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.package.PackageImporter.load_binary",
        "api_url": "https://pytorch.org/docs/stable/package.html#torch.package.PackageImporter.load_binary",
        "api_signature": "load_binary(package, resource)",
        "api_description": "Load raw bytes.",
        "return_value": "The loaded data.\n",
        "parameters": "package (str) – The name of module package (e.g. \"my_package.my_subpackage\").\nresource (str) – The unique name for the resource.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.checkpoint.LoadPlanner.load_bytes",
        "api_url": "https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.LoadPlanner.load_bytes",
        "api_signature": "load_bytes(read_item, value)",
        "api_description": "Load the item described by read_item``and ``value.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.cpp_extension.load_inline",
        "api_url": "https://pytorch.org/docs/stable/cpp_extension.html#torch.utils.cpp_extension.load_inline",
        "api_signature": "torch.utils.cpp_extension.load_inline(name, cpp_sources, cuda_sources=None, functions=None, extra_cflags=None, extra_cuda_cflags=None, extra_ldflags=None, extra_include_paths=None, build_directory=None, verbose=False, with_cuda=None, is_python_module=True, with_pytorch_error_handling=True, keep_intermediates=True, use_pch=False)",
        "api_description": "Load a PyTorch C++ extension just-in-time (JIT) from string sources.",
        "return_value": "",
        "parameters": "cpp_sources – A string, or list of strings, containing C++ source code.\ncuda_sources – A string, or list of strings, containing CUDA source code.\nfunctions – A list of function names for which to generate function\nbindings. If a dictionary is given, it should map function names to\ndocstrings (which are otherwise just the function names).\nwith_cuda – Determines whether CUDA headers and libraries are added to\nthe build. If set to None (default), this value is\nautomatically determined based on whether cuda_sources is\nprovided. Set it to True to force CUDA headers\nand libraries to be included.\nthis, each function foo is called via an intermediary _safe_foo\nfunction. This redirection might cause issues in obscure cases\nof cpp. This flag should be set to False when this redirect\ncauses issues.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.autograd.profiler.load_nvprof",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.autograd.profiler.load_nvprof.html#torch.autograd.profiler.load_nvprof",
        "api_signature": "torch.autograd.profiler.load_nvprof(path)",
        "api_description": "Open an nvprof trace file and parses autograd annotations.",
        "return_value": "",
        "parameters": "path (str) – path to nvprof trace",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.observer.load_observer_state_dict",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.quantization.observer.load_observer_state_dict.html#torch.ao.quantization.observer.load_observer_state_dict",
        "api_signature": "torch.ao.quantization.observer.load_observer_state_dict(mod, obs_dict)",
        "api_description": "Given input model and a state_dict containing model observer stats,\nload the stats back into the model. The observer state_dict can be saved\nusing torch.ao.quantization.get_observer_state_dict",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.package.PackageImporter.load_pickle",
        "api_url": "https://pytorch.org/docs/stable/package.html#torch.package.PackageImporter.load_pickle",
        "api_signature": "load_pickle(package, resource, map_location=None)",
        "api_description": "Unpickles the resource from the package, loading any modules that are needed to construct the objects\nusing import_module().",
        "return_value": "The unpickled object.\n",
        "parameters": "package (str) – The name of module package (e.g. \"my_package.my_subpackage\").\nresource (str) – The unique name for the resource.\nmap_location – Passed to torch.load to determine how tensors are mapped to devices. Defaults to None.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.checkpoint.state_dict_loader.load_state_dict",
        "api_url": "https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict_loader.load_state_dict",
        "api_signature": "torch.distributed.checkpoint.state_dict_loader.load_state_dict(state_dict, storage_reader, process_group=None, coordinator_rank=0, no_dist=False, planner=None)",
        "api_description": "This method is deprecated. Please switch to ‘load’.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.checkpoint.stateful.Stateful.load_state_dict",
        "api_url": "https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.stateful.Stateful.load_state_dict",
        "api_signature": "load_state_dict(state_dict)",
        "api_description": "Restore the object’s state from the provided state_dict.",
        "return_value": "",
        "parameters": "state_dict (Dict[str, Any]) – The state dict to restore from",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.optim.PostLocalSGDOptimizer.load_state_dict",
        "api_url": "https://pytorch.org/docs/stable/distributed.optim.html#torch.distributed.optim.PostLocalSGDOptimizer.load_state_dict",
        "api_signature": "load_state_dict(state_dict)",
        "api_description": "This is the same as torch.optim.Optimizer load_state_dict(),\nbut also restores model averager’s step value to the one\nsaved in the provided state_dict.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.optim.ZeroRedundancyOptimizer.load_state_dict",
        "api_url": "https://pytorch.org/docs/stable/distributed.optim.html#torch.distributed.optim.ZeroRedundancyOptimizer.load_state_dict",
        "api_signature": "load_state_dict(state_dict)",
        "api_description": "Load the state pertaining to the given rank from the input state_dict, updating the local optimizer as needed.",
        "return_value": "",
        "parameters": "state_dict (dict) – optimizer state; should be an object returned\nfrom a call to state_dict().",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.jit.ScriptModule.load_state_dict",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.jit.ScriptModule.html#torch.jit.ScriptModule.load_state_dict",
        "api_signature": "load_state_dict(state_dict, strict=True, assign=False)",
        "api_description": "Copy parameters and buffers from state_dict into this module and its descendants.",
        "return_value": "\nmissing_keys is a list of str containing the missing keys\nunexpected_keys is a list of str containing the unexpected keys\n\n\n",
        "parameters": "state_dict (dict) – a dict containing parameters and\npersistent buffers.\nstrict (bool, optional) – whether to strictly enforce that the keys\nin state_dict match the keys returned by this module’s\nstate_dict() function. Default: True\nassign (bool, optional) – When False, the properties of the tensors\nin the current module are preserved while when True, the\nproperties of the Tensors in the state dict are preserved. The only\nexception is the requires_grad field of\nDefault: ``False`",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.Module.load_state_dict",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.load_state_dict",
        "api_signature": "load_state_dict(state_dict, strict=True, assign=False)",
        "api_description": "Copy parameters and buffers from state_dict into this module and its descendants.",
        "return_value": "\nmissing_keys is a list of str containing the missing keys\nunexpected_keys is a list of str containing the unexpected keys\n\n\n",
        "parameters": "state_dict (dict) – a dict containing parameters and\npersistent buffers.\nstrict (bool, optional) – whether to strictly enforce that the keys\nin state_dict match the keys returned by this module’s\nstate_dict() function. Default: True\nassign (bool, optional) – When False, the properties of the tensors\nin the current module are preserved while when True, the\nproperties of the Tensors in the state dict are preserved. The only\nexception is the requires_grad field of\nDefault: ``False`",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.Adadelta.load_state_dict",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.Adadelta.html#torch.optim.Adadelta.load_state_dict",
        "api_signature": "load_state_dict(state_dict)",
        "api_description": "Loads the optimizer state.",
        "return_value": "",
        "parameters": "state_dict (dict) – optimizer state. Should be an object returned\nfrom a call to state_dict().",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.Adagrad.load_state_dict",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.Adagrad.html#torch.optim.Adagrad.load_state_dict",
        "api_signature": "load_state_dict(state_dict)",
        "api_description": "Loads the optimizer state.",
        "return_value": "",
        "parameters": "state_dict (dict) – optimizer state. Should be an object returned\nfrom a call to state_dict().",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.Adam.load_state_dict",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam.load_state_dict",
        "api_signature": "load_state_dict(state_dict)",
        "api_description": "Loads the optimizer state.",
        "return_value": "",
        "parameters": "state_dict (dict) – optimizer state. Should be an object returned\nfrom a call to state_dict().",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.Adamax.load_state_dict",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.Adamax.html#torch.optim.Adamax.load_state_dict",
        "api_signature": "load_state_dict(state_dict)",
        "api_description": "Loads the optimizer state.",
        "return_value": "",
        "parameters": "state_dict (dict) – optimizer state. Should be an object returned\nfrom a call to state_dict().",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.AdamW.load_state_dict",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html#torch.optim.AdamW.load_state_dict",
        "api_signature": "load_state_dict(state_dict)",
        "api_description": "Loads the optimizer state.",
        "return_value": "",
        "parameters": "state_dict (dict) – optimizer state. Should be an object returned\nfrom a call to state_dict().",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.ASGD.load_state_dict",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.ASGD.html#torch.optim.ASGD.load_state_dict",
        "api_signature": "load_state_dict(state_dict)",
        "api_description": "Loads the optimizer state.",
        "return_value": "",
        "parameters": "state_dict (dict) – optimizer state. Should be an object returned\nfrom a call to state_dict().",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.LBFGS.load_state_dict",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.LBFGS.html#torch.optim.LBFGS.load_state_dict",
        "api_signature": "load_state_dict(state_dict)",
        "api_description": "Loads the optimizer state.",
        "return_value": "",
        "parameters": "state_dict (dict) – optimizer state. Should be an object returned\nfrom a call to state_dict().",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.lr_scheduler.ChainedScheduler.load_state_dict",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.ChainedScheduler.html#torch.optim.lr_scheduler.ChainedScheduler.load_state_dict",
        "api_signature": "load_state_dict(state_dict)",
        "api_description": "Loads the schedulers state.",
        "return_value": "",
        "parameters": "state_dict (dict) – scheduler state. Should be an object returned\nfrom a call to state_dict().",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.lr_scheduler.ConstantLR.load_state_dict",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.ConstantLR.html#torch.optim.lr_scheduler.ConstantLR.load_state_dict",
        "api_signature": "load_state_dict(state_dict)",
        "api_description": "Loads the schedulers state.",
        "return_value": "",
        "parameters": "state_dict (dict) – scheduler state. Should be an object returned\nfrom a call to state_dict().",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.lr_scheduler.CosineAnnealingLR.load_state_dict",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.CosineAnnealingLR.html#torch.optim.lr_scheduler.CosineAnnealingLR.load_state_dict",
        "api_signature": "load_state_dict(state_dict)",
        "api_description": "Loads the schedulers state.",
        "return_value": "",
        "parameters": "state_dict (dict) – scheduler state. Should be an object returned\nfrom a call to state_dict().",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.lr_scheduler.CosineAnnealingWarmRestarts.load_state_dict",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.CosineAnnealingWarmRestarts.html#torch.optim.lr_scheduler.CosineAnnealingWarmRestarts.load_state_dict",
        "api_signature": "load_state_dict(state_dict)",
        "api_description": "Loads the schedulers state.",
        "return_value": "",
        "parameters": "state_dict (dict) – scheduler state. Should be an object returned\nfrom a call to state_dict().",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.lr_scheduler.ExponentialLR.load_state_dict",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.ExponentialLR.html#torch.optim.lr_scheduler.ExponentialLR.load_state_dict",
        "api_signature": "load_state_dict(state_dict)",
        "api_description": "Loads the schedulers state.",
        "return_value": "",
        "parameters": "state_dict (dict) – scheduler state. Should be an object returned\nfrom a call to state_dict().",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.lr_scheduler.LambdaLR.load_state_dict",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.LambdaLR.html#torch.optim.lr_scheduler.LambdaLR.load_state_dict",
        "api_signature": "load_state_dict(state_dict)",
        "api_description": "Loads the schedulers state.",
        "return_value": "",
        "parameters": "state_dict (dict) – scheduler state. Should be an object returned\nfrom a call to state_dict().",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.lr_scheduler.LinearLR.load_state_dict",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.LinearLR.html#torch.optim.lr_scheduler.LinearLR.load_state_dict",
        "api_signature": "load_state_dict(state_dict)",
        "api_description": "Loads the schedulers state.",
        "return_value": "",
        "parameters": "state_dict (dict) – scheduler state. Should be an object returned\nfrom a call to state_dict().",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.lr_scheduler.MultiplicativeLR.load_state_dict",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.MultiplicativeLR.html#torch.optim.lr_scheduler.MultiplicativeLR.load_state_dict",
        "api_signature": "load_state_dict(state_dict)",
        "api_description": "Loads the schedulers state.",
        "return_value": "",
        "parameters": "state_dict (dict) – scheduler state. Should be an object returned\nfrom a call to state_dict().",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.lr_scheduler.MultiStepLR.load_state_dict",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.MultiStepLR.html#torch.optim.lr_scheduler.MultiStepLR.load_state_dict",
        "api_signature": "load_state_dict(state_dict)",
        "api_description": "Loads the schedulers state.",
        "return_value": "",
        "parameters": "state_dict (dict) – scheduler state. Should be an object returned\nfrom a call to state_dict().",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.lr_scheduler.OneCycleLR.load_state_dict",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.OneCycleLR.html#torch.optim.lr_scheduler.OneCycleLR.load_state_dict",
        "api_signature": "load_state_dict(state_dict)",
        "api_description": "Loads the schedulers state.",
        "return_value": "",
        "parameters": "state_dict (dict) – scheduler state. Should be an object returned\nfrom a call to state_dict().",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.lr_scheduler.PolynomialLR.load_state_dict",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.PolynomialLR.html#torch.optim.lr_scheduler.PolynomialLR.load_state_dict",
        "api_signature": "load_state_dict(state_dict)",
        "api_description": "Loads the schedulers state.",
        "return_value": "",
        "parameters": "state_dict (dict) – scheduler state. Should be an object returned\nfrom a call to state_dict().",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.lr_scheduler.SequentialLR.load_state_dict",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.SequentialLR.html#torch.optim.lr_scheduler.SequentialLR.load_state_dict",
        "api_signature": "load_state_dict(state_dict)",
        "api_description": "Loads the schedulers state.",
        "return_value": "",
        "parameters": "state_dict (dict) – scheduler state. Should be an object returned\nfrom a call to state_dict().",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.lr_scheduler.StepLR.load_state_dict",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.StepLR.html#torch.optim.lr_scheduler.StepLR.load_state_dict",
        "api_signature": "load_state_dict(state_dict)",
        "api_description": "Loads the schedulers state.",
        "return_value": "",
        "parameters": "state_dict (dict) – scheduler state. Should be an object returned\nfrom a call to state_dict().",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.NAdam.load_state_dict",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.NAdam.html#torch.optim.NAdam.load_state_dict",
        "api_signature": "load_state_dict(state_dict)",
        "api_description": "Loads the optimizer state.",
        "return_value": "",
        "parameters": "state_dict (dict) – optimizer state. Should be an object returned\nfrom a call to state_dict().",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.Optimizer.load_state_dict",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.Optimizer.load_state_dict.html#torch.optim.Optimizer.load_state_dict",
        "api_signature": "Optimizer.load_state_dict(state_dict)",
        "api_description": "Loads the optimizer state.",
        "return_value": "",
        "parameters": "state_dict (dict) – optimizer state. Should be an object returned\nfrom a call to state_dict().",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.RAdam.load_state_dict",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.RAdam.html#torch.optim.RAdam.load_state_dict",
        "api_signature": "load_state_dict(state_dict)",
        "api_description": "Loads the optimizer state.",
        "return_value": "",
        "parameters": "state_dict (dict) – optimizer state. Should be an object returned\nfrom a call to state_dict().",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.RMSprop.load_state_dict",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.RMSprop.html#torch.optim.RMSprop.load_state_dict",
        "api_signature": "load_state_dict(state_dict)",
        "api_description": "Loads the optimizer state.",
        "return_value": "",
        "parameters": "state_dict (dict) – optimizer state. Should be an object returned\nfrom a call to state_dict().",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.Rprop.load_state_dict",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.Rprop.html#torch.optim.Rprop.load_state_dict",
        "api_signature": "load_state_dict(state_dict)",
        "api_description": "Loads the optimizer state.",
        "return_value": "",
        "parameters": "state_dict (dict) – optimizer state. Should be an object returned\nfrom a call to state_dict().",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.SGD.load_state_dict",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.SGD.html#torch.optim.SGD.load_state_dict",
        "api_signature": "load_state_dict(state_dict)",
        "api_description": "Loads the optimizer state.",
        "return_value": "",
        "parameters": "state_dict (dict) – optimizer state. Should be an object returned\nfrom a call to state_dict().",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.SparseAdam.load_state_dict",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.SparseAdam.html#torch.optim.SparseAdam.load_state_dict",
        "api_signature": "load_state_dict(state_dict)",
        "api_description": "Loads the optimizer state.",
        "return_value": "",
        "parameters": "state_dict (dict) – optimizer state. Should be an object returned\nfrom a call to state_dict().",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.hub.load_state_dict_from_url",
        "api_url": "https://pytorch.org/docs/stable/hub.html#torch.hub.load_state_dict_from_url",
        "api_signature": "torch.hub.load_state_dict_from_url(url, model_dir=None, map_location=None, progress=True, check_hash=False, file_name=None, weights_only=False)",
        "api_description": "Loads the Torch serialized object at the given URL.",
        "return_value": "",
        "parameters": "url (str) – URL of the object to download\nmodel_dir (str, optional) – directory in which to save the object\nmap_location (optional) – a function or a dict specifying how to remap storage locations (see torch.load)\nprogress (bool, optional) – whether or not to display a progress bar to stderr.\nDefault: True\ncheck_hash (bool, optional) – If True, the filename part of the URL should follow the naming convention\nfilename-<sha256>.ext where <sha256> is the first eight or more\ndigits of the SHA256 hash of the contents of the file. The hash is used to\nensure unique names and to verify the contents of the file.\nDefault: False\nfile_name (str, optional) – name for the downloaded file. Filename from url will be used if not set.\nweights_only (bool, optional) – If True, only weights will be loaded and no complex pickled objects.\nRecommended for untrusted sources. See load() for more details.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.package.PackageImporter.load_text",
        "api_url": "https://pytorch.org/docs/stable/package.html#torch.package.PackageImporter.load_text",
        "api_signature": "load_text(package, resource, encoding='utf-8', errors='strict')",
        "api_description": "Load a string.",
        "return_value": "The loaded text.\n",
        "parameters": "package (str) – The name of module package (e.g. \"my_package.my_subpackage\").\nresource (str) – The unique name for the resource.\nencoding (str, optional) – Passed to decode. Defaults to 'utf-8'.\nerrors (str, optional) – Passed to decode. Defaults to 'strict'.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.model_zoo.load_url",
        "api_url": "https://pytorch.org/docs/stable/model_zoo.html#torch.utils.model_zoo.load_url",
        "api_signature": "torch.utils.model_zoo.load_url(url, model_dir=None, map_location=None, progress=True, check_hash=False, file_name=None, weights_only=False)",
        "api_description": "Loads the Torch serialized object at the given URL.",
        "return_value": "",
        "parameters": "url (str) – URL of the object to download\nmodel_dir (str, optional) – directory in which to save the object\nmap_location (optional) – a function or a dict specifying how to remap storage locations (see torch.load)\nprogress (bool, optional) – whether or not to display a progress bar to stderr.\nDefault: True\ncheck_hash (bool, optional) – If True, the filename part of the URL should follow the naming convention\nfilename-<sha256>.ext where <sha256> is the first eight or more\ndigits of the SHA256 hash of the contents of the file. The hash is used to\nensure unique names and to verify the contents of the file.\nDefault: False\nfile_name (str, optional) – name for the downloaded file. Filename from url will be used if not set.\nweights_only (bool, optional) – If True, only weights will be loaded and no complex pickled objects.\nRecommended for untrusted sources. See load() for more details.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.checkpoint.LoadPlan",
        "api_url": "https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.LoadPlan",
        "api_signature": "torch.distributed.checkpoint.LoadPlan(items: List[torch.distributed.checkpoint.planner.ReadItem], storage_data: Any = None, planner_data: Any = None)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.checkpoint.LoadPlanner",
        "api_url": "https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.LoadPlanner",
        "api_signature": null,
        "api_description": "Abstract class defining the protocol used by load_state_dict to plan the load process.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.lobpcg",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg",
        "api_signature": "torch.lobpcg(A, k=None, B=None, X=None, n=None, iK=None, niter=None, tol=None, largest=None, method=None, tracker=None, ortho_iparams=None, ortho_fparams=None, ortho_bparams=None)",
        "api_description": "Find the k largest (or smallest) eigenvalues and the corresponding\neigenvectors of a symmetric positive definite generalized\neigenvalue problem using matrix-free LOBPCG methods.",
        "return_value": "tensor of eigenvalues of size (∗,k)(*, k)(∗,k)\nX (Tensor): tensor of eigenvectors of size (∗,m,k)(*, m, k)(∗,m,k)\n\n",
        "parameters": "A (Tensor) – the input tensor of size (∗,m,m)(*, m, m)(∗,m,m)\nB (Tensor, optional) – the input tensor of size (∗,m,m)(*, m,\nm)(∗,m,m). When not specified, B is interpreted as\nidentity matrix.\nX (tensor, optional) – the input tensor of size (∗,m,n)(*, m, n)(∗,m,n)\nwhere k <= n <= m. When specified, it is used as\ninitial approximation of eigenvectors. X must be a\ndense tensor.\niK (tensor, optional) – the input tensor of size (∗,m,m)(*, m,\nm)(∗,m,m). When specified, it will be used as preconditioner.\nk (integer, optional) – the number of requested\neigenpairs. Default is the number of XXX\ncolumns (when specified) or 1.\nn (integer, optional) – if XXX is not specified then n\nspecifies the size of the generated random\napproximation of eigenvectors. Default value for n\nis k. If XXX is specified, the value of n\n(when specified) must be the number of XXX\ncolumns.\ntol (float, optional) – residual tolerance for stopping\ncriterion. Default is feps ** 0.5 where feps is\nsmallest non-zero floating-point number of the given\ninput tensor A data type.\nlargest (bool, optional) – when True, solve the eigenproblem for\nthe largest eigenvalues. Otherwise, solve the\neigenproblem for smallest eigenvalues. Default is\nTrue.\nmethod (str, optional) – select LOBPCG method. See the\ndescription of the function above. Default is\n“ortho”.\nniter (int, optional) – maximum number of iterations. When\nreached, the iteration process is hard-stopped and\nthe current approximation of eigenpairs is returned.\nFor infinite iteration but until convergence criteria\nis met, use -1.\ntracker (callable, optional) – a function for tracing the\niteration process. When specified, it is called at\neach iteration step with LOBPCG instance as an\nargument. The LOBPCG instance holds the full state of\nthe iteration process in the following attributes:\niparams, fparams, bparams - dictionaries of\ninteger, float, and boolean valued input\nparameters, respectively\nivars, fvars, bvars, tvars - dictionaries\nof integer, float, boolean, and Tensor valued\niteration variables, respectively.\nA, B, iK - input Tensor arguments.\nE, X, S, R - iteration Tensor variables.\nFor instance:\nivars[“istep”] - the current iteration step\nX - the current approximation of eigenvectors\nE - the current approximation of eigenvalues\nR - the current residual\nivars[“converged_count”] - the current number of converged eigenpairs\ntvars[“rerr”] - the current state of convergence criteria\nNote that when tracker stores Tensor objects from\nthe LOBPCG instance, it must make copies of these.\nIf tracker sets bvars[“force_stop”] = True, the\niteration process will be hard-stopped.\northo_iparams (dict, optional) – various parameters to LOBPCG algorithm when using\nmethod=”ortho”.\northo_fparams (dict, optional) – various parameters to LOBPCG algorithm when using\nmethod=”ortho”.\northo_bparams (dict, optional) – various parameters to LOBPCG algorithm when using\nmethod=”ortho”.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.log_normal.LogNormal.loc",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.log_normal.LogNormal.loc",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.functional.local_response_norm",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.functional.local_response_norm.html#torch.nn.functional.local_response_norm",
        "api_signature": "torch.nn.functional.local_response_norm(input, size, alpha=0.0001, beta=0.75, k=1.0)",
        "api_description": "Apply local response normalization over an input signal.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.rpc.PyRRef.local_value",
        "api_url": "https://pytorch.org/docs/stable/rpc.html#torch.distributed.rpc.PyRRef.local_value",
        "api_signature": "local_value(self: torch._C._distributed_rpc.PyRRef)",
        "api_description": "If the current node is the owner, returns a reference to the\nlocal value. Otherwise, throws an exception.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.agent.server.local_elastic_agent.LocalElasticAgent",
        "api_url": "https://pytorch.org/docs/stable/elastic/agent.html#torch.distributed.elastic.agent.server.local_elastic_agent.LocalElasticAgent",
        "api_signature": "torch.distributed.elastic.agent.server.local_elastic_agent.LocalElasticAgent(spec, logs_specs, start_method='spawn', exit_barrier_timeout=300, log_line_prefix_template=None)",
        "api_description": "An implementation of torchelastic.agent.server.ElasticAgent that handles host-local workers.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.fsdp.LocalOptimStateDictConfig",
        "api_url": "https://pytorch.org/docs/stable/fsdp.html#torch.distributed.fsdp.LocalOptimStateDictConfig",
        "api_signature": "torch.distributed.fsdp.LocalOptimStateDictConfig(offload_to_cpu: bool = False)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.LocalResponseNorm",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.LocalResponseNorm.html#torch.nn.LocalResponseNorm",
        "api_signature": "torch.nn.LocalResponseNorm(size, alpha=0.0001, beta=0.75, k=1.0)",
        "api_description": "Applies local response normalization over an input signal.",
        "return_value": "",
        "parameters": "size (int) – amount of neighbouring channels used for normalization\nalpha (float) – multiplicative factor. Default: 0.0001\nbeta (float) – exponent. Default: 0.75\nk (float) – additive factor. Default: 1",
        "input_shape": "\nInput: (N,C,∗)(N, C, *)(N,C,∗)\nOutput: (N,C,∗)(N, C, *)(N,C,∗) (same shape as input)\n\n",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.fsdp.LocalStateDictConfig",
        "api_url": "https://pytorch.org/docs/stable/fsdp.html#torch.distributed.fsdp.LocalStateDictConfig",
        "api_signature": "torch.distributed.fsdp.LocalStateDictConfig(offload_to_cpu: bool = False)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.timer.LocalTimerClient",
        "api_url": "https://pytorch.org/docs/stable/elastic/timer.html#torch.distributed.elastic.timer.LocalTimerClient",
        "api_signature": "torch.distributed.elastic.timer.LocalTimerClient(mp_queue)",
        "api_description": "Client side of LocalTimerServer. This client is meant to be used\non the same host that the LocalTimerServer is running on and uses\npid to uniquely identify a worker. This is particularly useful in situations\nwhere one spawns a subprocess (trainer) per GPU on a host with multiple\nGPU devices.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.timer.LocalTimerServer",
        "api_url": "https://pytorch.org/docs/stable/elastic/timer.html#torch.distributed.elastic.timer.LocalTimerServer",
        "api_signature": "torch.distributed.elastic.timer.LocalTimerServer(mp_queue, max_interval=60, daemon=True)",
        "api_description": "Server that works with LocalTimerClient. Clients are expected to be\nsubprocesses to the parent process that is running this server. Each host\nin the job is expected to start its own timer server locally and each\nserver instance manages timers for local workers (running on processes\non the same host).",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.log",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.log.html#torch.log",
        "api_signature": "torch.log(input, *, out=None)",
        "api_description": "Returns a new tensor with the natural logarithm of the elements\nof input.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.log",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.log.html#torch.Tensor.log",
        "api_signature": "Tensor.log()",
        "api_description": "See torch.log()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.log10",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.log10.html#torch.log10",
        "api_signature": "torch.log10(input, *, out=None)",
        "api_description": "Returns a new tensor with the logarithm to the base 10 of the elements\nof input.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.log10",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.log10.html#torch.Tensor.log10",
        "api_signature": "Tensor.log10()",
        "api_description": "See torch.log10()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.log10_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.log10_.html#torch.Tensor.log10_",
        "api_signature": "Tensor.log10_()",
        "api_description": "In-place version of log10()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.log1p",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.log1p.html#torch.log1p",
        "api_signature": "torch.log1p(input, *, out=None)",
        "api_description": "Returns a new tensor with the natural logarithm of (1 + input).",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.special.log1p",
        "api_url": "https://pytorch.org/docs/stable/special.html#torch.special.log1p",
        "api_signature": "torch.special.log1p(input, *, out=None)",
        "api_description": "Alias for torch.log1p().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.log1p",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.log1p.html#torch.Tensor.log1p",
        "api_signature": "Tensor.log1p()",
        "api_description": "See torch.log1p()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.log1p_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.log1p_.html#torch.Tensor.log1p_",
        "api_signature": "Tensor.log1p_()",
        "api_description": "In-place version of log1p()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.log2",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.log2.html#torch.log2",
        "api_signature": "torch.log2(input, *, out=None)",
        "api_description": "Returns a new tensor with the logarithm to the base 2 of the elements\nof input.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.log2",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.log2.html#torch.Tensor.log2",
        "api_signature": "Tensor.log2()",
        "api_description": "See torch.log2()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.log2_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.log2_.html#torch.Tensor.log2_",
        "api_signature": "Tensor.log2_()",
        "api_description": "In-place version of log2()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.log_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.log_.html#torch.Tensor.log_",
        "api_signature": "Tensor.log_()",
        "api_description": "In-place version of log()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.transforms.Transform.log_abs_det_jacobian",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.transforms.Transform.log_abs_det_jacobian",
        "api_signature": "log_abs_det_jacobian(x, y)",
        "api_description": "Computes the log det jacobian log |dy/dx| given input and output.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.monitor.log_event",
        "api_url": "https://pytorch.org/docs/stable/monitor.html#torch.monitor.log_event",
        "api_signature": "torch.monitor.log_event(event: torch._C._monitor.Event)",
        "api_description": "log_event logs the specified event to all of the registered event\nhandlers. It’s up to the event handlers to log the event out to the\ncorresponding event sink.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.special.log_ndtr",
        "api_url": "https://pytorch.org/docs/stable/special.html#torch.special.log_ndtr",
        "api_signature": "torch.special.log_ndtr(input, *, out=None)",
        "api_description": "Computes the log of the area under the standard Gaussian probability density function,\nintegrated from minus infinity to input, elementwise.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> torch.special.log_ndtr(torch.tensor([-3., -2, -1, 0, 1, 2, 3]))\ntensor([-6.6077 -3.7832 -1.841  -0.6931 -0.1728 -0.023  -0.0014])\n\n\n"
    },
    {
        "api_name": "torch.Tensor.log_normal_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.log_normal_.html#torch.Tensor.log_normal_",
        "api_signature": "Tensor.log_normal_(mean=1, std=2, *, generator=None)",
        "api_description": "Fills self tensor with numbers samples from the log-normal distribution\nparameterized by the given mean μ\\muμ and standard deviation\nσ\\sigmaσ. Note that mean and std are the mean and\nstandard deviation of the underlying normal distribution, and not of the\nreturned distribution:",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.bernoulli.Bernoulli.log_prob",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.bernoulli.Bernoulli.log_prob",
        "api_signature": "log_prob(value)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.beta.Beta.log_prob",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.beta.Beta.log_prob",
        "api_signature": "log_prob(value)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.binomial.Binomial.log_prob",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.binomial.Binomial.log_prob",
        "api_signature": "log_prob(value)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.categorical.Categorical.log_prob",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.categorical.Categorical.log_prob",
        "api_signature": "log_prob(value)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.cauchy.Cauchy.log_prob",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.cauchy.Cauchy.log_prob",
        "api_signature": "log_prob(value)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.continuous_bernoulli.ContinuousBernoulli.log_prob",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.continuous_bernoulli.ContinuousBernoulli.log_prob",
        "api_signature": "log_prob(value)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.dirichlet.Dirichlet.log_prob",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.dirichlet.Dirichlet.log_prob",
        "api_signature": "log_prob(value)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.distribution.Distribution.log_prob",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.distribution.Distribution.log_prob",
        "api_signature": "log_prob(value)",
        "api_description": "Returns the log of the probability density/mass function evaluated at\nvalue.",
        "return_value": "",
        "parameters": "value (Tensor) –",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.exponential.Exponential.log_prob",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.exponential.Exponential.log_prob",
        "api_signature": "log_prob(value)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.fishersnedecor.FisherSnedecor.log_prob",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.fishersnedecor.FisherSnedecor.log_prob",
        "api_signature": "log_prob(value)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.gamma.Gamma.log_prob",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.gamma.Gamma.log_prob",
        "api_signature": "log_prob(value)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.geometric.Geometric.log_prob",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.geometric.Geometric.log_prob",
        "api_signature": "log_prob(value)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.gumbel.Gumbel.log_prob",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.gumbel.Gumbel.log_prob",
        "api_signature": "log_prob(value)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.half_cauchy.HalfCauchy.log_prob",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.half_cauchy.HalfCauchy.log_prob",
        "api_signature": "log_prob(value)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.half_normal.HalfNormal.log_prob",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.half_normal.HalfNormal.log_prob",
        "api_signature": "log_prob(value)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.independent.Independent.log_prob",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.independent.Independent.log_prob",
        "api_signature": "log_prob(value)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.laplace.Laplace.log_prob",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.laplace.Laplace.log_prob",
        "api_signature": "log_prob(value)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.lkj_cholesky.LKJCholesky.log_prob",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.lkj_cholesky.LKJCholesky.log_prob",
        "api_signature": "log_prob(value)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.log_prob",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.log_prob",
        "api_signature": "log_prob(value)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.mixture_same_family.MixtureSameFamily.log_prob",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.mixture_same_family.MixtureSameFamily.log_prob",
        "api_signature": "log_prob(x)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.multinomial.Multinomial.log_prob",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.multinomial.Multinomial.log_prob",
        "api_signature": "log_prob(value)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.multivariate_normal.MultivariateNormal.log_prob",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.multivariate_normal.MultivariateNormal.log_prob",
        "api_signature": "log_prob(value)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.negative_binomial.NegativeBinomial.log_prob",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.negative_binomial.NegativeBinomial.log_prob",
        "api_signature": "log_prob(value)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.normal.Normal.log_prob",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.normal.Normal.log_prob",
        "api_signature": "log_prob(value)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.one_hot_categorical.OneHotCategorical.log_prob",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.one_hot_categorical.OneHotCategorical.log_prob",
        "api_signature": "log_prob(value)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.poisson.Poisson.log_prob",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.poisson.Poisson.log_prob",
        "api_signature": "log_prob(value)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.log_prob",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.log_prob",
        "api_signature": "log_prob(value)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.studentT.StudentT.log_prob",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.studentT.StudentT.log_prob",
        "api_signature": "log_prob(value)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.transformed_distribution.TransformedDistribution.log_prob",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.transformed_distribution.TransformedDistribution.log_prob",
        "api_signature": "log_prob(value)",
        "api_description": "Scores the sample by inverting the transform(s) and computing the score\nusing the score of the base distribution and the log abs det jacobian.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.uniform.Uniform.log_prob",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.uniform.Uniform.log_prob",
        "api_signature": "log_prob(value)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.von_mises.VonMises.log_prob",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.von_mises.VonMises.log_prob",
        "api_signature": "log_prob(value)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.wishart.Wishart.log_prob",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.wishart.Wishart.log_prob",
        "api_signature": "log_prob(value)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.AdaptiveLogSoftmaxWithLoss.log_prob",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.AdaptiveLogSoftmaxWithLoss.html#torch.nn.AdaptiveLogSoftmaxWithLoss.log_prob",
        "api_signature": "log_prob(input)",
        "api_description": "Compute log probabilities for all n_classes\\texttt{n\\_classes}n_classes.",
        "return_value": "log-probabilities of for each class ccc\nin range 0<=c<=n_classes0 <= c <= \\texttt{n\\_classes}0<=c<=n_classes, where n_classes\\texttt{n\\_classes}n_classes is a\nparameter passed to AdaptiveLogSoftmaxWithLoss constructor.\n",
        "parameters": "input (Tensor) – a minibatch of examples",
        "input_shape": "\nInput: (N,in_features)(N, \\texttt{in\\_features})(N,in_features)\nOutput: (N,n_classes)(N, \\texttt{n\\_classes})(N,n_classes)\n\n",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.functional.log_softmax",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.functional.log_softmax.html#torch.nn.functional.log_softmax",
        "api_signature": "torch.nn.functional.log_softmax(input, dim=None, _stacklevel=3, dtype=None)",
        "api_description": "Apply a softmax followed by a logarithm.",
        "return_value": "",
        "parameters": "input (Tensor) – input\ndim (int) – A dimension along which log_softmax will be computed.\ndtype (torch.dtype, optional) – the desired data type of returned tensor.\nIf specified, the input tensor is cast to dtype before the operation\nis performed. This is useful for preventing data type overflows. Default: None.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.sparse.log_softmax",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.sparse.log_softmax.html#torch.sparse.log_softmax",
        "api_signature": "torch.sparse.log_softmax(input, dim, *, dtype=None)",
        "api_description": "Applies a softmax function followed by logarithm.",
        "return_value": "",
        "parameters": "input (Tensor) – input\ndim (int) – A dimension along which softmax will be computed.\ndtype (torch.dtype, optional) – the desired data type\nof returned tensor.  If specified, the input tensor is\ncasted to dtype before the operation is\nperformed. This is useful for preventing data type\noverflows. Default: None",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.special.log_softmax",
        "api_url": "https://pytorch.org/docs/stable/special.html#torch.special.log_softmax",
        "api_signature": "torch.special.log_softmax(input, dim, *, dtype=None)",
        "api_description": "Computes softmax followed by a logarithm.",
        "return_value": "",
        "parameters": "input (Tensor) – input\ndim (int) – A dimension along which log_softmax will be computed.\ndtype (torch.dtype, optional) – the desired data type of returned tensor.\nIf specified, the input tensor is cast to dtype before the operation\nis performed. This is useful for preventing data type overflows. Default: None.",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> t = torch.ones(2, 2)\n>>> torch.special.log_softmax(t, 0)\ntensor([[-0.6931, -0.6931],\n        [-0.6931, -0.6931]])\n\n\n"
    },
    {
        "api_name": "torch.logaddexp",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.logaddexp.html#torch.logaddexp",
        "api_signature": "torch.logaddexp(input, other, *, out=None)",
        "api_description": "Logarithm of the sum of exponentiations of the inputs.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.\nother (Tensor) – the second input tensor\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.logaddexp",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.logaddexp.html#torch.Tensor.logaddexp",
        "api_signature": "Tensor.logaddexp(other)",
        "api_description": "See torch.logaddexp()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.logaddexp2",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.logaddexp2.html#torch.logaddexp2",
        "api_signature": "torch.logaddexp2(input, other, *, out=None)",
        "api_description": "Logarithm of the sum of exponentiations of the inputs in base-2.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.\nother (Tensor) – the second input tensor\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.logaddexp2",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.logaddexp2.html#torch.Tensor.logaddexp2",
        "api_signature": "Tensor.logaddexp2(other)",
        "api_description": "See torch.logaddexp2()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.logcumsumexp",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.logcumsumexp.html#torch.logcumsumexp",
        "api_signature": "torch.logcumsumexp(input, dim, *, out=None)",
        "api_description": "Returns the logarithm of the cumulative summation of the exponentiation of\nelements of input in the dimension dim.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.\ndim (int) – the dimension to do the operation over\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.logcumsumexp",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.logcumsumexp.html#torch.Tensor.logcumsumexp",
        "api_signature": "Tensor.logcumsumexp(dim)",
        "api_description": "See torch.logcumsumexp()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.logdet",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.logdet.html#torch.logdet",
        "api_signature": "torch.logdet(input)",
        "api_description": "Calculates log determinant of a square matrix or batches of square matrices.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor of size (*, n, n) where * is zero or more\nbatch dimensions.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.logdet",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.logdet.html#torch.Tensor.logdet",
        "api_signature": "Tensor.logdet()",
        "api_description": "See torch.logdet()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.ns._numeric_suite.Logger",
        "api_url": "https://pytorch.org/docs/stable/torch.ao.ns._numeric_suite.html#torch.ao.ns._numeric_suite.Logger",
        "api_signature": null,
        "api_description": "Base class for stats logging",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.ns._numeric_suite_fx.loggers_set_enabled",
        "api_url": "https://pytorch.org/docs/stable/torch.ao.ns._numeric_suite_fx.html#torch.ao.ns._numeric_suite_fx.loggers_set_enabled",
        "api_signature": "torch.ao.ns._numeric_suite_fx.loggers_set_enabled(model, enabled)",
        "api_description": "Sets the enabled setting on a model’s loggers",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.ns._numeric_suite_fx.loggers_set_save_activations",
        "api_url": "https://pytorch.org/docs/stable/torch.ao.ns._numeric_suite_fx.html#torch.ao.ns._numeric_suite_fx.loggers_set_save_activations",
        "api_signature": "torch.ao.ns._numeric_suite_fx.loggers_set_save_activations(model, save_activations)",
        "api_description": "Sets the save_activations setting on a model’s loggers",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.logical_and",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.logical_and.html#torch.logical_and",
        "api_signature": "torch.logical_and(input, other, *, out=None)",
        "api_description": "Computes the element-wise logical AND of the given input tensors. Zeros are treated as False and nonzeros are\ntreated as True.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.\nother (Tensor) – the tensor to compute AND with\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.logical_and",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.logical_and.html#torch.Tensor.logical_and",
        "api_signature": "Tensor.logical_and()",
        "api_description": "See torch.logical_and()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.logical_and_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.logical_and_.html#torch.Tensor.logical_and_",
        "api_signature": "Tensor.logical_and_()",
        "api_description": "In-place version of logical_and()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.logical_not",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.logical_not.html#torch.logical_not",
        "api_signature": "torch.logical_not(input, *, out=None)",
        "api_description": "Computes the element-wise logical NOT of the given input tensor. If not specified, the output tensor will have the bool\ndtype. If the input tensor is not a bool tensor, zeros are treated as False and non-zeros are treated as True.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.logical_not",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.logical_not.html#torch.Tensor.logical_not",
        "api_signature": "Tensor.logical_not()",
        "api_description": "See torch.logical_not()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.logical_not_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.logical_not_.html#torch.Tensor.logical_not_",
        "api_signature": "Tensor.logical_not_()",
        "api_description": "In-place version of logical_not()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.logical_or",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.logical_or.html#torch.logical_or",
        "api_signature": "torch.logical_or(input, other, *, out=None)",
        "api_description": "Computes the element-wise logical OR of the given input tensors. Zeros are treated as False and nonzeros are\ntreated as True.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.\nother (Tensor) – the tensor to compute OR with\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.logical_or",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.logical_or.html#torch.Tensor.logical_or",
        "api_signature": "Tensor.logical_or()",
        "api_description": "See torch.logical_or()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.logical_or_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.logical_or_.html#torch.Tensor.logical_or_",
        "api_signature": "Tensor.logical_or_()",
        "api_description": "In-place version of logical_or()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.logical_xor",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.logical_xor.html#torch.logical_xor",
        "api_signature": "torch.logical_xor(input, other, *, out=None)",
        "api_description": "Computes the element-wise logical XOR of the given input tensors. Zeros are treated as False and nonzeros are\ntreated as True.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.\nother (Tensor) – the tensor to compute XOR with\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.logical_xor",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.logical_xor.html#torch.Tensor.logical_xor",
        "api_signature": "Tensor.logical_xor()",
        "api_description": "See torch.logical_xor()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.logical_xor_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.logical_xor_.html#torch.Tensor.logical_xor_",
        "api_signature": "Tensor.logical_xor_()",
        "api_description": "In-place version of logical_xor()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.logit",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.logit.html#torch.logit",
        "api_signature": "torch.logit(input, eps=None, *, out=None)",
        "api_description": "Alias for torch.special.logit().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.special.logit",
        "api_url": "https://pytorch.org/docs/stable/special.html#torch.special.logit",
        "api_signature": "torch.special.logit(input, eps=None, *, out=None)",
        "api_description": "Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.\neps (float, optional) – the epsilon for input clamp bound. Default: None\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.logit",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.logit.html#torch.Tensor.logit",
        "api_signature": "Tensor.logit()",
        "api_description": "See torch.logit()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.logit_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.logit_.html#torch.Tensor.logit_",
        "api_signature": "Tensor.logit_()",
        "api_description": "In-place version of logit()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli",
        "api_signature": "torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli(temperature, probs=None, logits=None, validate_args=None)",
        "api_description": "Bases: Distribution",
        "return_value": "",
        "parameters": "temperature (Tensor) – relaxation temperature\nprobs (Number, Tensor) – the probability of sampling 1\nlogits (Number, Tensor) – the log-odds of sampling 1",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.bernoulli.Bernoulli.logits",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.bernoulli.Bernoulli.logits",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.binomial.Binomial.logits",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.binomial.Binomial.logits",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.categorical.Categorical.logits",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.categorical.Categorical.logits",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.continuous_bernoulli.ContinuousBernoulli.logits",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.continuous_bernoulli.ContinuousBernoulli.logits",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.geometric.Geometric.logits",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.geometric.Geometric.logits",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.multinomial.Multinomial.logits",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.multinomial.Multinomial.logits",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.negative_binomial.NegativeBinomial.logits",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.negative_binomial.NegativeBinomial.logits",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.one_hot_categorical.OneHotCategorical.logits",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.one_hot_categorical.OneHotCategorical.logits",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.logits",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.logits",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.relaxed_bernoulli.RelaxedBernoulli.logits",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.relaxed_bernoulli.RelaxedBernoulli.logits",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.logits",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.logits",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.log_normal.LogNormal",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.log_normal.LogNormal",
        "api_signature": "torch.distributions.log_normal.LogNormal(loc, scale, validate_args=None)",
        "api_description": "Bases: TransformedDistribution",
        "return_value": "",
        "parameters": "loc (float or Tensor) – mean of log of distribution\nscale (float or Tensor) – standard deviation of log of the distribution",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.multiprocessing.api.LogsDest",
        "api_url": "https://pytorch.org/docs/stable/elastic/multiprocessing.html#torch.distributed.elastic.multiprocessing.api.LogsDest",
        "api_signature": "torch.distributed.elastic.multiprocessing.api.LogsDest(stdouts=<factory>, stderrs=<factory>, tee_stdouts=<factory>, tee_stderrs=<factory>, error_files=<factory>)",
        "api_description": "For each log type, holds mapping of local rank ids to file paths.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.LogSigmoid",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.LogSigmoid.html#torch.nn.LogSigmoid",
        "api_signature": "torch.nn.LogSigmoid(*args, **kwargs)",
        "api_description": "Applies the Logsigmoid function element-wise.",
        "return_value": "",
        "parameters": "",
        "input_shape": "\nInput: (∗)(*)(∗), where ∗*∗ means any number of dimensions.\nOutput: (∗)(*)(∗), same shape as the input.\n\n",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.functional.logsigmoid",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.functional.logsigmoid.html#torch.nn.functional.logsigmoid",
        "api_signature": "torch.nn.functional.logsigmoid(input)",
        "api_description": "Applies element-wise LogSigmoid(xi)=log⁡(11+exp⁡(−xi))\\text{LogSigmoid}(x_i) = \\log \\left(\\frac{1}{1 + \\exp(-x_i)}\\right)LogSigmoid(xi​)=log(1+exp(−xi​)1​)",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.LogSoftmax",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.LogSoftmax.html#torch.nn.LogSoftmax",
        "api_signature": "torch.nn.LogSoftmax(dim=None)",
        "api_description": "Applies the log⁡(Softmax(x))\\log(\\text{Softmax}(x))log(Softmax(x)) function to an n-dimensional input Tensor.",
        "return_value": "a Tensor of the same dimension and shape as the input with\nvalues in the range [-inf, 0)\n",
        "parameters": "dim (int) – A dimension along which LogSoftmax will be computed.",
        "input_shape": "\nInput: (∗)(*)(∗) where * means, any number of additional\ndimensions\nOutput: (∗)(*)(∗), same shape as the input\n\n",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.logspace",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.logspace.html#torch.logspace",
        "api_signature": "torch.logspace(start, end, steps, base=10.0, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False)",
        "api_description": "Creates a one-dimensional tensor of size steps whose values are evenly\nspaced from basestart{{\\text{{base}}}}^{{\\text{{start}}}}basestart to\nbaseend{{\\text{{base}}}}^{{\\text{{end}}}}baseend, inclusive, on a logarithmic scale\nwith base base. That is, the values are:",
        "return_value": "",
        "parameters": "start (float or Tensor) – the starting value for the set of points. If Tensor, it must be 0-dimensional\nend (float or Tensor) – the ending value for the set of points. If Tensor, it must be 0-dimensional\nsteps (int) – size of the constructed tensor\nbase (float, optional) – base of the logarithm function. Default: 10.0.\nout (Tensor, optional) – the output tensor.\ndtype (torch.dtype, optional) – the data type to perform the computation in.\nDefault: if None, uses the global default dtype (see torch.get_default_dtype())\nwhen both start and end are real,\nand corresponding complex dtype when either is complex.\nlayout (torch.layout, optional) – the desired layout of returned Tensor.\nDefault: torch.strided.\ndevice (torch.device, optional) – the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_device()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types.\nrequires_grad (bool, optional) – If autograd should record operations on the\nreturned tensor. Default: False.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.multiprocessing.api.LogsSpecs",
        "api_url": "https://pytorch.org/docs/stable/elastic/multiprocessing.html#torch.distributed.elastic.multiprocessing.api.LogsSpecs",
        "api_signature": "torch.distributed.elastic.multiprocessing.api.LogsSpecs(log_dir=None, redirects=Std.NONE, tee=Std.NONE, local_ranks_filter=None)",
        "api_description": "Defines logs processing and redirection for each worker process.",
        "return_value": "",
        "parameters": "log_dir (Optional[str]) – Base directory where logs will be written.\nredirects (Union[Std, Dict[int, Std]]) – Streams to redirect to files. Pass a single Std\nenum to redirect for all workers, or a mapping keyed\nby local_rank to selectively redirect.\ntee (Union[Std, Dict[int, Std]]) – Streams to duplicate to stdout/stderr.\nPass a single Std enum to duplicate streams for all workers,\nor a mapping keyed by local_rank to selectively duplicate.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.logsumexp",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.logsumexp.html#torch.logsumexp",
        "api_signature": "torch.logsumexp(input, dim, keepdim=False, *, out=None)",
        "api_description": "Returns the log of summed exponentials of each row of the input\ntensor in the given dimension dim. The computation is numerically\nstabilized.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.\ndim (int or tuple of ints, optional) – the dimension or dimensions to reduce.\nIf None, all dimensions are reduced.\nkeepdim (bool) – whether the output tensor has dim retained or not.\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.special.logsumexp",
        "api_url": "https://pytorch.org/docs/stable/special.html#torch.special.logsumexp",
        "api_signature": "torch.special.logsumexp(input, dim, keepdim=False, *, out=None)",
        "api_description": "Alias for torch.logsumexp().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.logsumexp",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.logsumexp.html#torch.Tensor.logsumexp",
        "api_signature": "Tensor.logsumexp(dim, keepdim=False)",
        "api_description": "See torch.logsumexp()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.long",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.long.html#torch.Tensor.long",
        "api_signature": "Tensor.long(memory_format=torch.preserve_format)",
        "api_description": "self.long() is equivalent to self.to(torch.int64). See to().",
        "return_value": "",
        "parameters": "memory_format (torch.memory_format, optional) – the desired memory format of\nreturned Tensor. Default: torch.preserve_format.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.TypedStorage.long",
        "api_url": "https://pytorch.org/docs/stable/storage.html#torch.TypedStorage.long",
        "api_signature": "long()",
        "api_description": "Casts this storage to long type.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.UntypedStorage.long",
        "api_url": "https://pytorch.org/docs/stable/storage.html#torch.UntypedStorage.long",
        "api_signature": "long()",
        "api_description": "Casts this storage to long type.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.LongStorage",
        "api_url": "https://pytorch.org/docs/stable/storage.html#torch.LongStorage",
        "api_signature": "torch.LongStorage(*args, wrap_storage=None, dtype=None, device=None, _internal=False)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.checkpoint.DefaultSavePlanner.lookup_object",
        "api_url": "https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.DefaultSavePlanner.lookup_object",
        "api_signature": "lookup_object(index)",
        "api_description": "Extension from the planner interface to make it easy to extend the default planner.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.checkpoint.DefaultLoadPlanner.lookup_tensor",
        "api_url": "https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.DefaultLoadPlanner.lookup_tensor",
        "api_signature": "lookup_tensor(index)",
        "api_description": "Extension from the planner interface to make it easy to extend the default planner.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.tensor.parallel.loss_parallel",
        "api_url": "https://pytorch.org/docs/stable/distributed.tensor.parallel.html#torch.distributed.tensor.parallel.loss_parallel",
        "api_signature": "torch.distributed.tensor.parallel.loss_parallel()",
        "api_description": "A context manager that enables loss parallelism, where efficient parallelized loss computation\ncan be performed when the input is sharded on the class dimension. Currently only the cross-entropy\nloss is supported.",
        "return_value": "A replicated DTensor.\n",
        "parameters": "input (DTensor) – Input logits. Assumed to be sharded on the class dimension.\ntarget (Union[torch.Tensor, DTensor]) – Must be ground truth class indices (class probabilities currently not supported).\nAssumed to be replicated across the DeviceMesh.\nweight (Union[torch.Tensor, DTensor], optional) – If given, assumed to be replicated across the DeviceMesh.\nlabel_smoothing – Currently not supported.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.transforms.LowerCholeskyTransform",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.transforms.LowerCholeskyTransform",
        "api_signature": "torch.distributions.transforms.LowerCholeskyTransform(cache_size=0)",
        "api_description": "Transform from unconstrained matrices to lower-triangular matrices with\nnonnegative diagonal entries.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal",
        "api_signature": "torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal(loc, cov_factor, cov_diag, validate_args=None)",
        "api_description": "Bases: Distribution",
        "return_value": "",
        "parameters": "loc (Tensor) – mean of the distribution with shape batch_shape + event_shape\ncov_factor (Tensor) – factor part of low-rank form of covariance matrix with shape\nbatch_shape + event_shape + (rank,)\ncov_diag (Tensor) – diagonal part of low-rank form of covariance matrix with shape\nbatch_shape + event_shape",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.functional.lp_pool1d",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.functional.lp_pool1d.html#torch.nn.functional.lp_pool1d",
        "api_signature": "torch.nn.functional.lp_pool1d(input, norm_type, kernel_size, stride=None, ceil_mode=False)",
        "api_description": "Apply a 1D power-average pooling over an input signal composed of several input planes.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.functional.lp_pool2d",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.functional.lp_pool2d.html#torch.nn.functional.lp_pool2d",
        "api_signature": "torch.nn.functional.lp_pool2d(input, norm_type, kernel_size, stride=None, ceil_mode=False)",
        "api_description": "Apply a 2D power-average pooling over an input signal composed of several input planes.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.functional.lp_pool3d",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.functional.lp_pool3d.html#torch.nn.functional.lp_pool3d",
        "api_signature": "torch.nn.functional.lp_pool3d(input, norm_type, kernel_size, stride=None, ceil_mode=False)",
        "api_description": "Apply a 3D power-average pooling over an input signal composed of several input planes.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.LPPool1d",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.LPPool1d.html#torch.nn.LPPool1d",
        "api_signature": "torch.nn.LPPool1d(norm_type, kernel_size, stride=None, ceil_mode=False)",
        "api_description": "Applies a 1D power-average pooling over an input signal composed of several input planes.",
        "return_value": "",
        "parameters": "kernel_size (Union[int, Tuple[int]]) – a single int, the size of the window\nstride (Union[int, Tuple[int]]) – a single int, the stride of the window. Default value is kernel_size\nceil_mode (bool) – when True, will use ceil instead of floor to compute the output shape",
        "input_shape": "\nInput: (N,C,Lin)(N, C, L_{in})(N,C,Lin​) or (C,Lin)(C, L_{in})(C,Lin​).\nOutput: (N,C,Lout)(N, C, L_{out})(N,C,Lout​) or (C,Lout)(C, L_{out})(C,Lout​), where\n\nLout=⌊Lin−kernel_sizestride+1⌋L_{out} = \\left\\lfloor\\frac{L_{in} - \\text{kernel\\_size}}{\\text{stride}} + 1\\right\\rfloor\n\nLout​=⌊strideLin​−kernel_size​+1⌋\n\n",
        "notes": "",
        "code_example": ">>> # power-2 pool of window of length 3, with stride 2.\n>>> m = nn.LPPool1d(2, 3, stride=2)\n>>> input = torch.randn(20, 16, 50)\n>>> output = m(input)\n\n\n"
    },
    {
        "api_name": "torch.nn.LPPool2d",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.LPPool2d.html#torch.nn.LPPool2d",
        "api_signature": "torch.nn.LPPool2d(norm_type, kernel_size, stride=None, ceil_mode=False)",
        "api_description": "Applies a 2D power-average pooling over an input signal composed of several input planes.",
        "return_value": "",
        "parameters": "kernel_size (Union[int, Tuple[int, int]]) – the size of the window\nstride (Union[int, Tuple[int, int]]) – the stride of the window. Default value is kernel_size\nceil_mode (bool) – when True, will use ceil instead of floor to compute the output shape",
        "input_shape": "\nInput: (N,C,Hin,Win)(N, C, H_{in}, W_{in})(N,C,Hin​,Win​) or (C,Hin,Win)(C, H_{in}, W_{in})(C,Hin​,Win​).\nOutput: (N,C,Hout,Wout)(N, C, H_{out}, W_{out})(N,C,Hout​,Wout​) or (C,Hout,Wout)(C, H_{out}, W_{out})(C,Hout​,Wout​), where\n\nHout=⌊Hin−kernel_size[0]stride[0]+1⌋H_{out} = \\left\\lfloor\\frac{H_{in} - \\text{kernel\\_size}[0]}{\\text{stride}[0]} + 1\\right\\rfloor\n\nHout​=⌊stride[0]Hin​−kernel_size[0]​+1⌋\nWout=⌊Win−kernel_size[1]stride[1]+1⌋W_{out} = \\left\\lfloor\\frac{W_{in} - \\text{kernel\\_size}[1]}{\\text{stride}[1]} + 1\\right\\rfloor\n\nWout​=⌊stride[1]Win​−kernel_size[1]​+1⌋\n\n",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.LPPool3d",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.LPPool3d.html#torch.nn.LPPool3d",
        "api_signature": "torch.nn.LPPool3d(norm_type, kernel_size, stride=None, ceil_mode=False)",
        "api_description": "Applies a 3D power-average pooling over an input signal composed of several input planes.",
        "return_value": "",
        "parameters": "kernel_size (Union[int, Tuple[int, int, int]]) – the size of the window\nstride (Union[int, Tuple[int, int, int]]) – the stride of the window. Default value is kernel_size\nceil_mode (bool) – when True, will use ceil instead of floor to compute the output shape",
        "input_shape": "\nInput: (N,C,Din,Hin,Win)(N, C, D_{in}, H_{in}, W_{in})(N,C,Din​,Hin​,Win​) or (C,Din,Hin,Win)(C, D_{in}, H_{in}, W_{in})(C,Din​,Hin​,Win​).\nOutput: (N,C,Dout,Hout,Wout)(N, C, D_{out}, H_{out}, W_{out})(N,C,Dout​,Hout​,Wout​) or\n(C,Dout,Hout,Wout)(C, D_{out}, H_{out}, W_{out})(C,Dout​,Hout​,Wout​), where\n\nDout=⌊Din−kernel_size[0]stride[0]+1⌋D_{out} = \\left\\lfloor\\frac{D_{in} - \\text{kernel\\_size}[0]}{\\text{stride}[0]} + 1\\right\\rfloor\n\nDout​=⌊stride[0]Din​−kernel_size[0]​+1⌋\nHout=⌊Hin−kernel_size[1]stride[1]+1⌋H_{out} = \\left\\lfloor\\frac{H_{in} - \\text{kernel\\_size}[1]}{\\text{stride}[1]} + 1\\right\\rfloor\n\nHout​=⌊stride[1]Hin​−kernel_size[1]​+1⌋\nWout=⌊Win−kernel_size[2]stride[2]+1⌋W_{out} = \\left\\lfloor\\frac{W_{in} - \\text{kernel\\_size}[2]}{\\text{stride}[2]} + 1\\right\\rfloor\n\nWout​=⌊stride[2]Win​−kernel_size[2]​+1⌋\n\n",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.quantizable.LSTM",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.nn.quantizable.LSTM.html#torch.ao.nn.quantizable.LSTM",
        "api_signature": "torch.ao.nn.quantizable.LSTM(input_size, hidden_size, num_layers=1, bias=True, batch_first=False, dropout=0.0, bidirectional=False, device=None, dtype=None)",
        "api_description": "A quantizable long short-term memory (LSTM).",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.quantized.dynamic.LSTM",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.nn.quantized.dynamic.LSTM.html#torch.ao.nn.quantized.dynamic.LSTM",
        "api_signature": "torch.ao.nn.quantized.dynamic.LSTM(*args, **kwargs)",
        "api_description": "A dynamic quantized LSTM module with floating point tensor as inputs and outputs.\nWe adopt the same interface as torch.nn.LSTM, please see\nhttps://pytorch.org/docs/stable/nn.html#torch.nn.LSTM for documentation.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.LSTM",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html#torch.nn.LSTM",
        "api_signature": "torch.nn.LSTM(input_size, hidden_size, num_layers=1, bias=True, batch_first=False, dropout=0.0, bidirectional=False, proj_size=0, device=None, dtype=None)",
        "api_description": "Apply a multi-layer long short-term memory (LSTM) RNN to an input sequence.\nFor each element in the input sequence, each layer computes the following\nfunction:",
        "return_value": "",
        "parameters": "input_size – The number of expected features in the input x\nhidden_size – The number of features in the hidden state h\nnum_layers – Number of recurrent layers. E.g., setting num_layers=2\nwould mean stacking two LSTMs together to form a stacked LSTM,\nwith the second LSTM taking in outputs of the first LSTM and\ncomputing the final results. Default: 1\nbias – If False, then the layer does not use bias weights b_ih and b_hh.\nDefault: True\nbatch_first – If True, then the input and output tensors are provided\nas (batch, seq, feature) instead of (seq, batch, feature).\nNote that this does not apply to hidden or cell states. See the\nInputs/Outputs sections below for details.  Default: False\ndropout – If non-zero, introduces a Dropout layer on the outputs of each\nLSTM layer except the last layer, with dropout probability equal to\ndropout. Default: 0\nbidirectional – If True, becomes a bidirectional LSTM. Default: False\nproj_size – If > 0, will use LSTM with projections of corresponding size. Default: 0",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.quantized.dynamic.LSTMCell",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.nn.quantized.dynamic.LSTMCell.html#torch.ao.nn.quantized.dynamic.LSTMCell",
        "api_signature": "torch.ao.nn.quantized.dynamic.LSTMCell(*args, **kwargs)",
        "api_description": "A long short-term memory (LSTM) cell.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.LSTMCell",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.LSTMCell.html#torch.nn.LSTMCell",
        "api_signature": "torch.nn.LSTMCell(input_size, hidden_size, bias=True, device=None, dtype=None)",
        "api_description": "A long short-term memory (LSTM) cell.",
        "return_value": "",
        "parameters": "input_size (int) – The number of expected features in the input x\nhidden_size (int) – The number of features in the hidden state h\nbias (bool) – If False, then the layer does not use bias weights b_ih and\nb_hh. Default: True",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.linalg.lstsq",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.linalg.lstsq.html#torch.linalg.lstsq",
        "api_signature": "torch.linalg.lstsq(A, B, rcond=None, *, driver=None)",
        "api_description": "Computes a solution to the least squares problem of a system of linear equations.",
        "return_value": "A named tuple (solution, residuals, rank, singular_values).\n",
        "parameters": "A (Tensor) – lhs tensor of shape (*, m, n) where * is zero or more batch dimensions.\nB (Tensor) – rhs tensor of shape (*, m, k) where * is zero or more batch dimensions.\nrcond (float, optional) – used to determine the effective rank of A.\nIf rcond= None, rcond is set to the machine\nprecision of the dtype of A times max(m, n). Default: None.\ndriver (str, optional) – name of the LAPACK/MAGMA method to be used.\nIf None, ‘gelsy’ is used for CPU inputs and ‘gels’ for CUDA inputs.\nDefault: None.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.lt",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.lt.html#torch.lt",
        "api_signature": "torch.lt(input, other, *, out=None)",
        "api_description": "Computes input<other\\text{input} < \\text{other}input<other element-wise.",
        "return_value": "A boolean tensor that is True where input is less than other and False elsewhere\n",
        "parameters": "input (Tensor) – the tensor to compare\nother (Tensor or float) – the tensor or value to compare\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.lt",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.lt.html#torch.Tensor.lt",
        "api_signature": "Tensor.lt(other)",
        "api_description": "See torch.lt().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.lt_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.lt_.html#torch.Tensor.lt_",
        "api_signature": "Tensor.lt_(other)",
        "api_description": "In-place version of lt().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.lu",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.lu.html#torch.lu",
        "api_signature": "torch.lu(*args, **kwargs)",
        "api_description": "Computes the LU factorization of a matrix or batches of matrices\nA. Returns a tuple containing the LU factorization and\npivots of A.  Pivoting is done if pivot is set to\nTrue.",
        "return_value": "A tuple of tensors containing\n\n\nfactorization (Tensor): the factorization of size (∗,m,n)(*, m, n)(∗,m,n)\npivots (IntTensor): the pivots of size (∗,min(m,n))(*, \\text{min}(m, n))(∗,min(m,n)).\npivots stores all the intermediate transpositions of rows.\nThe final permutation perm could be reconstructed by\napplying swap(perm[i], perm[pivots[i] - 1]) for i = 0, ..., pivots.size(-1) - 1,\nwhere perm is initially the identity permutation of mmm elements\n(essentially this is what torch.lu_unpack() is doing).\ninfos (IntTensor, optional): if get_infos is True, this is a tensor of\nsize (∗)(*)(∗) where non-zero values indicate whether factorization for the matrix or\neach minibatch has succeeded or failed\n\n\n\n",
        "parameters": "A (Tensor) – the tensor to factor of size (∗,m,n)(*, m, n)(∗,m,n)\npivot (bool, optional) – controls whether pivoting is done. Default: True\nget_infos (bool, optional) – if set to True, returns an info IntTensor.\nDefault: False\nout (tuple, optional) – optional output tuple. If get_infos is True,\nthen the elements in the tuple are Tensor, IntTensor,\nand IntTensor. If get_infos is False, then the\nelements in the tuple are Tensor, IntTensor. Default: None",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.linalg.lu",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.linalg.lu.html#torch.linalg.lu",
        "api_signature": "torch.linalg.lu(A, *, pivot=True, out=None)",
        "api_description": "Computes the LU decomposition with partial pivoting of a matrix.",
        "return_value": "A named tuple (P, L, U).\n",
        "parameters": "A (Tensor) – tensor of shape (*, m, n) where * is zero or more batch dimensions.\npivot (bool, optional) – Controls whether to compute the LU decomposition with partial pivoting or\nno pivoting. Default: True.\nout (tuple, optional) – output tuple of three tensors. Ignored if None. Default: None.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.lu",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.lu.html#torch.Tensor.lu",
        "api_signature": "Tensor.lu(pivot=True, get_infos=False)",
        "api_description": "See torch.lu()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.linalg.lu_factor",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.linalg.lu_factor.html#torch.linalg.lu_factor",
        "api_signature": "torch.linalg.lu_factor(A, *, bool pivot=True, out=None)",
        "api_description": "Computes a compact representation of the LU factorization with partial pivoting of a matrix.",
        "return_value": "A named tuple (LU, pivots).\n",
        "parameters": "A (Tensor) – tensor of shape (*, m, n) where * is zero or more batch dimensions.\npivot (bool, optional) – Whether to compute the LU decomposition with partial pivoting, or the regular LU\ndecomposition. pivot= False not supported on CPU. Default: True.\nout (tuple, optional) – tuple of two tensors to write the output to. Ignored if None. Default: None.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.linalg.lu_factor_ex",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.linalg.lu_factor_ex.html#torch.linalg.lu_factor_ex",
        "api_signature": "torch.linalg.lu_factor_ex(A, *, pivot=True, check_errors=False, out=None)",
        "api_description": "This is a version of lu_factor() that does not perform error checks unless check_errors= True.\nIt also returns the info tensor returned by LAPACK’s getrf.",
        "return_value": "A named tuple (LU, pivots, info).\n",
        "parameters": "A (Tensor) – tensor of shape (*, m, n) where * is zero or more batch dimensions.\npivot (bool, optional) – Whether to compute the LU decomposition with partial pivoting, or the regular LU\ndecomposition. pivot= False not supported on CPU. Default: True.\ncheck_errors (bool, optional) – controls whether to check the content of infos and raise\nan error if it is non-zero. Default: False.\nout (tuple, optional) – tuple of three tensors to write the output to. Ignored if None. Default: None.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.lu_solve",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.lu_solve.html#torch.lu_solve",
        "api_signature": "torch.lu_solve(b, LU_data, LU_pivots, *, out=None)",
        "api_description": "Returns the LU solve of the linear system Ax=bAx = bAx=b using the partially pivoted\nLU factorization of A from lu_factor().",
        "return_value": "",
        "parameters": "b (Tensor) – the RHS tensor of size (∗,m,k)(*, m, k)(∗,m,k), where ∗*∗\nis zero or more batch dimensions.\nLU_data (Tensor) – the pivoted LU factorization of A from lu_factor() of size (∗,m,m)(*, m, m)(∗,m,m),\nwhere ∗*∗ is zero or more batch dimensions.\nLU_pivots (IntTensor) – the pivots of the LU factorization from lu_factor() of size (∗,m)(*, m)(∗,m),\nwhere ∗*∗ is zero or more batch dimensions.\nThe batch dimensions of LU_pivots must be equal to the batch dimensions of\nLU_data.\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.linalg.lu_solve",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.linalg.lu_solve.html#torch.linalg.lu_solve",
        "api_signature": "torch.linalg.lu_solve(LU, pivots, B, *, left=True, adjoint=False, out=None)",
        "api_description": "Computes the solution of a square system of linear equations with a unique solution given an LU decomposition.",
        "return_value": "",
        "parameters": "LU (Tensor) – tensor of shape (*, n, n) (or (*, k, k) if left= True)\nwhere * is zero or more batch dimensions as returned by lu_factor().\npivots (Tensor) – tensor of shape (*, n) (or (*, k) if left= True)\nwhere * is zero or more batch dimensions as returned by lu_factor().\nB (Tensor) – right-hand side tensor of shape (*, n, k).\nleft (bool, optional) – whether to solve the system AX=BAX=BAX=B or XA=BXA = BXA=B. Default: True.\nadjoint (bool, optional) – whether to solve the system AX=BAX=BAX=B or AHX=BA^{\\text{H}}X = BAHX=B. Default: False.\nout (Tensor, optional) – output tensor. Ignored if None. Default: None.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.lu_solve",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.lu_solve.html#torch.Tensor.lu_solve",
        "api_signature": "Tensor.lu_solve(LU_data, LU_pivots)",
        "api_description": "See torch.lu_solve()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.lu_unpack",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.lu_unpack.html#torch.lu_unpack",
        "api_signature": "torch.lu_unpack(LU_data, LU_pivots, unpack_data=True, unpack_pivots=True, *, out=None)",
        "api_description": "Unpacks the LU decomposition returned by lu_factor() into the P, L, U matrices.",
        "return_value": "A namedtuple (P, L, U)\n",
        "parameters": "LU_data (Tensor) – the packed LU factorization data\nLU_pivots (Tensor) – the packed LU factorization pivots\nunpack_data (bool) – flag indicating if the data should be unpacked.\nIf False, then the returned L and U are empty tensors.\nDefault: True\nunpack_pivots (bool) – flag indicating if the pivots should be unpacked into a permutation matrix P.\nIf False, then the returned P is  an empty tensor.\nDefault: True\nout (tuple, optional) – output tuple of three tensors. Ignored if None.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.algorithms.JoinHook.main_hook",
        "api_url": "https://pytorch.org/docs/stable/distributed.algorithms.join.html#torch.distributed.algorithms.JoinHook.main_hook",
        "api_signature": "main_hook()",
        "api_description": "Call this hook while there exists a non-joined process to shadow collective communications in a training iteration.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.autograd.forward_ad.make_dual",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.autograd.forward_ad.make_dual.html#torch.autograd.forward_ad.make_dual",
        "api_signature": "torch.autograd.forward_ad.make_dual(tensor, tangent, *, level=None)",
        "api_description": "Associate a tensor value with its tangent to create a “dual tensor” for forward AD gradient computation.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda.make_graphed_callables",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.cuda.make_graphed_callables.html#torch.cuda.make_graphed_callables",
        "api_signature": "torch.cuda.make_graphed_callables(callables, sample_args, num_warmup_iters=3, allow_unused_input=False, pool=None)",
        "api_description": "Accept callables (functions or nn.Modules) and returns graphed versions.",
        "return_value": "",
        "parameters": "callables (torch.nn.Module or Python function, or tuple of these) – Callable or callables to graph.\nSee Graph memory management for when passing a tuple of callables\nis appropriate.  If you pass a tuple of callables, their order in the tuple must be the same order\nthey’ll run in the live workload.\nsample_args (tuple of Tensors, or tuple of tuples of Tensors) – Samples args for each callable.\nIf a single callable was passed, sample_args must be a single tuple of argument Tensors.\nIf a tuple of callables was passed, sample_args must be tuple of tuples of argument Tensors.\nnum_warmup_iters (int) – The number of warmup iterations. Currently, DataDistributedParallel needs\n11 iterations for warm up. Default: 3.\nallow_unused_input (bool) – If False, specifying inputs that were not used when computing outputs\n(and therefore their grad is always zero) is an error. Defaults to False.\npool (optional) – Token (returned by graph_pool_handle() or\nother_Graph_instance.pool()) that hints this graph may share memory\nwith the indicated pool.  See Graph memory management.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.testing.make_tensor",
        "api_url": "https://pytorch.org/docs/stable/testing.html#torch.testing.make_tensor",
        "api_signature": "torch.testing.make_tensor(*shape, dtype, device, low=None, high=None, requires_grad=False, noncontiguous=False, exclude_zero=False, memory_format=None)",
        "api_description": "Creates a tensor with the given shape, device, and dtype, and filled with\nvalues uniformly drawn from [low, high).",
        "return_value": "",
        "parameters": "shape (Tuple[int, ...]) – Single integer or a sequence of integers defining the shape of the output tensor.\ndtype (torch.dtype) – The data type of the returned tensor.\ndevice (Union[str, torch.device]) – The device of the returned tensor.\nlow (Optional[Number]) – Sets the lower limit (inclusive) of the given range. If a number is provided it is\nclamped to the least representable finite value of the given dtype. When None (default),\nthis value is determined based on the dtype (see the table above). Default: None.\nhigh (Optional[Number]) – Sets the upper limit (exclusive) of the given range. If a number is provided it is\nclamped to the greatest representable finite value of the given dtype. When None (default) this value\nis determined based on the dtype (see the table above). Default: None.\nDeprecated since version 2.1: Passing low==high to make_tensor() for floating or complex types is deprecated\nsince 2.1 and will be removed in 2.3. Use torch.full() instead.\nrequires_grad (Optional[bool]) – If autograd should record operations on the returned tensor. Default: False.\nnoncontiguous (Optional[bool]) – If True, the returned tensor will be noncontiguous. This argument is\nignored if the constructed tensor has fewer than two elements. Mutually exclusive with memory_format.\nexclude_zero (Optional[bool]) – If True then zeros are replaced with the dtype’s small positive value\ndepending on the dtype. For bool and integer types zero is replaced with one. For floating\npoint types it is replaced with the dtype’s smallest positive normal number (the “tiny” value of the\ndtype’s finfo() object), and for complex types it is replaced with a complex number\nwhose real and imaginary parts are both the smallest positive normal number representable by the complex\ntype. Default False.\nmemory_format (Optional[torch.memory_format]) – The memory format of the returned tensor. Mutually exclusive\nwith noncontiguous.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.manual_seed",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.manual_seed.html#torch.manual_seed",
        "api_signature": "torch.manual_seed(seed)",
        "api_description": "Sets the seed for generating random numbers on all devices. Returns a\ntorch.Generator object.",
        "return_value": "",
        "parameters": "seed (int) – The desired seed. Value must be within the inclusive range\n[-0x8000_0000_0000_0000, 0xffff_ffff_ffff_ffff]. Otherwise, a RuntimeError\nis raised. Negative inputs are remapped to positive values with the formula\n0xffff_ffff_ffff_ffff + seed.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda.manual_seed",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.cuda.manual_seed.html#torch.cuda.manual_seed",
        "api_signature": "torch.cuda.manual_seed(seed)",
        "api_description": "Set the seed for generating random numbers for the current GPU.",
        "return_value": "",
        "parameters": "seed (int) – The desired seed.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.mps.manual_seed",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.mps.manual_seed.html#torch.mps.manual_seed",
        "api_signature": "torch.mps.manual_seed(seed)",
        "api_description": "Sets the seed for generating random numbers.",
        "return_value": "",
        "parameters": "seed (int) – The desired seed.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.random.manual_seed",
        "api_url": "https://pytorch.org/docs/stable/random.html#torch.random.manual_seed",
        "api_signature": "torch.random.manual_seed(seed)",
        "api_description": "Sets the seed for generating random numbers on all devices. Returns a\ntorch.Generator object.",
        "return_value": "",
        "parameters": "seed (int) – The desired seed. Value must be within the inclusive range\n[-0x8000_0000_0000_0000, 0xffff_ffff_ffff_ffff]. Otherwise, a RuntimeError\nis raised. Negative inputs are remapped to positive values with the formula\n0xffff_ffff_ffff_ffff + seed.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.xpu.manual_seed",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.xpu.manual_seed.html#torch.xpu.manual_seed",
        "api_signature": "torch.xpu.manual_seed(seed)",
        "api_description": "Set the seed for generating random numbers for the current GPU.",
        "return_value": "",
        "parameters": "seed (int) – The desired seed.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Generator.manual_seed",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator.manual_seed",
        "api_signature": "manual_seed(seed)",
        "api_description": "Sets the seed for generating random numbers. Returns a torch.Generator object. Any 32-bit integer is a valid seed.",
        "return_value": "An torch.Generator object.\n",
        "parameters": "seed (int) – The desired seed. Value must be within the inclusive range\n[-0x8000_0000_0000_0000, 0xffff_ffff_ffff_ffff]. Otherwise, a RuntimeError\nis raised. Negative inputs are remapped to positive values with the formula\n0xffff_ffff_ffff_ffff + seed.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda.manual_seed_all",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.cuda.manual_seed_all.html#torch.cuda.manual_seed_all",
        "api_signature": "torch.cuda.manual_seed_all(seed)",
        "api_description": "Set the seed for generating random numbers on all GPUs.",
        "return_value": "",
        "parameters": "seed (int) – The desired seed.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.xpu.manual_seed_all",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.xpu.manual_seed_all.html#torch.xpu.manual_seed_all",
        "api_signature": "torch.xpu.manual_seed_all(seed)",
        "api_description": "Set the seed for generating random numbers on all GPUs.",
        "return_value": "",
        "parameters": "seed (int) – The desired seed.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.map_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.map_.html#torch.Tensor.map_",
        "api_signature": "Tensor.map_(tensor, callable)",
        "api_description": "Applies callable for each element in self tensor and the given\ntensor and stores the results in self tensor. self tensor and\nthe given tensor must be broadcastable.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.Interpreter.map_nodes_to_values",
        "api_url": "https://pytorch.org/docs/stable/fx.html#torch.fx.Interpreter.map_nodes_to_values",
        "api_signature": "map_nodes_to_values(args, n)",
        "api_description": "Recursively descend through args and look up the concrete value\nfor each Node in the current execution environment.",
        "return_value": "",
        "parameters": "args (Argument) – Data structure within which to look up concrete values\nn (Node) – Node to which args belongs. This is only used for error reporting.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.functional.margin_ranking_loss",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.functional.margin_ranking_loss.html#torch.nn.functional.margin_ranking_loss",
        "api_signature": "torch.nn.functional.margin_ranking_loss(input1, input2, target, margin=0, size_average=None, reduce=None, reduction='mean')",
        "api_description": "See MarginRankingLoss for details.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.MarginRankingLoss",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.MarginRankingLoss.html#torch.nn.MarginRankingLoss",
        "api_signature": "torch.nn.MarginRankingLoss(margin=0.0, size_average=None, reduce=None, reduction='mean')",
        "api_description": "Creates a criterion that measures the loss given\ninputs x1x1x1, x2x2x2, two 1D mini-batch or 0D Tensors,\nand a label 1D mini-batch or 0D Tensor yyy (containing 1 or -1).",
        "return_value": "",
        "parameters": "margin (float, optional) – Has a default value of 000.\nsize_average (bool, optional) – Deprecated (see reduction). By default,\nthe losses are averaged over each loss element in the batch. Note that for\nsome losses, there are multiple elements per sample. If the field size_average\nis set to False, the losses are instead summed for each minibatch. Ignored\nwhen reduce is False. Default: True\nreduce (bool, optional) – Deprecated (see reduction). By default, the\nlosses are averaged or summed over observations for each minibatch depending\non size_average. When reduce is False, returns a loss per\nbatch element instead and ignores size_average. Default: True\nreduction (str, optional) – Specifies the reduction to apply to the output:\n'none' | 'mean' | 'sum'. 'none': no reduction will be applied,\n'mean': the sum of the output will be divided by the number of\nelements in the output, 'sum': the output will be summed. Note: size_average\nand reduce are in the process of being deprecated, and in the meantime,\nspecifying either of those two args will override reduction. Default: 'mean'",
        "input_shape": "\nInput1: (N)(N)(N) or ()()() where N is the batch size.\nInput2: (N)(N)(N) or ()()(), same shape as the Input1.\nTarget: (N)(N)(N) or ()()(), same shape as the inputs.\nOutput: scalar. If reduction is 'none' and Input size is not ()()(), then (N)(N)(N).\n\n",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda.nvtx.mark",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.cuda.nvtx.mark.html#torch.cuda.nvtx.mark",
        "api_signature": "torch.cuda.nvtx.mark(msg)",
        "api_description": "Describe an instantaneous event that occurred at some point.",
        "return_value": "",
        "parameters": "msg (str) – ASCII message to associate with the event.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.profiler.itt.mark",
        "api_url": "https://pytorch.org/docs/stable/profiler.html#torch.profiler.itt.mark",
        "api_signature": "torch.profiler.itt.mark(msg)",
        "api_description": "Describe an instantaneous event that occurred at some point.",
        "return_value": "",
        "parameters": "msg (str) – ASCII message to associate with the event.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.autograd.function.BackwardCFunction.mark_dirty",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.autograd.function.BackwardCFunction.html#torch.autograd.function.BackwardCFunction.mark_dirty",
        "api_signature": "mark_dirty(*args)",
        "api_description": "Mark given tensors as modified in an in-place operation.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> class Inplace(Function):\n>>>     @staticmethod\n>>>     def forward(ctx, x):\n>>>         x_npy = x.numpy() # x_npy shares storage with x\n>>>         x_npy += 1\n>>>         ctx.mark_dirty(x)\n>>>         return x\n>>>\n>>>     @staticmethod\n>>>     @once_differentiable\n>>>     def backward(ctx, grad_output):\n>>>         return grad_output\n>>>\n>>> a = torch.tensor(1., requires_grad=True, dtype=torch.double).clone()\n>>> b = a * a\n>>> Inplace.apply(a)  # This would lead to wrong gradients!\n>>>                   # but the engine would not know unless we mark_dirty\n>>> b.backward() # RuntimeError: one of the variables needed for gradient\n>>>              # computation has been modified by an inplace operation\n\n\n"
    },
    {
        "api_name": "torch.autograd.function.FunctionCtx.mark_dirty",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.autograd.function.FunctionCtx.mark_dirty.html#torch.autograd.function.FunctionCtx.mark_dirty",
        "api_signature": "FunctionCtx.mark_dirty(*args)",
        "api_description": "Mark given tensors as modified in an in-place operation.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> class Inplace(Function):\n>>>     @staticmethod\n>>>     def forward(ctx, x):\n>>>         x_npy = x.numpy() # x_npy shares storage with x\n>>>         x_npy += 1\n>>>         ctx.mark_dirty(x)\n>>>         return x\n>>>\n>>>     @staticmethod\n>>>     @once_differentiable\n>>>     def backward(ctx, grad_output):\n>>>         return grad_output\n>>>\n>>> a = torch.tensor(1., requires_grad=True, dtype=torch.double).clone()\n>>> b = a * a\n>>> Inplace.apply(a)  # This would lead to wrong gradients!\n>>>                   # but the engine would not know unless we mark_dirty\n>>> b.backward() # RuntimeError: one of the variables needed for gradient\n>>>              # computation has been modified by an inplace operation\n\n\n"
    },
    {
        "api_name": "torch.autograd.function.InplaceFunction.mark_dirty",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.autograd.function.InplaceFunction.html#torch.autograd.function.InplaceFunction.mark_dirty",
        "api_signature": "mark_dirty(*args)",
        "api_description": "Mark given tensors as modified in an in-place operation.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> class Inplace(Function):\n>>>     @staticmethod\n>>>     def forward(ctx, x):\n>>>         x_npy = x.numpy() # x_npy shares storage with x\n>>>         x_npy += 1\n>>>         ctx.mark_dirty(x)\n>>>         return x\n>>>\n>>>     @staticmethod\n>>>     @once_differentiable\n>>>     def backward(ctx, grad_output):\n>>>         return grad_output\n>>>\n>>> a = torch.tensor(1., requires_grad=True, dtype=torch.double).clone()\n>>> b = a * a\n>>> Inplace.apply(a)  # This would lead to wrong gradients!\n>>>                   # but the engine would not know unless we mark_dirty\n>>> b.backward() # RuntimeError: one of the variables needed for gradient\n>>>              # computation has been modified by an inplace operation\n\n\n"
    },
    {
        "api_name": "torch.autograd.function.NestedIOFunction.mark_dirty",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.autograd.function.NestedIOFunction.html#torch.autograd.function.NestedIOFunction.mark_dirty",
        "api_signature": "mark_dirty(*args, **kwargs)",
        "api_description": "See Function.mark_dirty().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.autograd.function.BackwardCFunction.mark_non_differentiable",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.autograd.function.BackwardCFunction.html#torch.autograd.function.BackwardCFunction.mark_non_differentiable",
        "api_signature": "mark_non_differentiable(*args)",
        "api_description": "Mark outputs as non-differentiable.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.autograd.function.FunctionCtx.mark_non_differentiable",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.autograd.function.FunctionCtx.mark_non_differentiable.html#torch.autograd.function.FunctionCtx.mark_non_differentiable",
        "api_signature": "FunctionCtx.mark_non_differentiable(*args)",
        "api_description": "Mark outputs as non-differentiable.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.autograd.function.InplaceFunction.mark_non_differentiable",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.autograd.function.InplaceFunction.html#torch.autograd.function.InplaceFunction.mark_non_differentiable",
        "api_signature": "mark_non_differentiable(*args)",
        "api_description": "Mark outputs as non-differentiable.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.autograd.function.NestedIOFunction.mark_non_differentiable",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.autograd.function.NestedIOFunction.html#torch.autograd.function.NestedIOFunction.mark_non_differentiable",
        "api_signature": "mark_non_differentiable(*args, **kwargs)",
        "api_description": "See Function.mark_non_differentiable().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.masked_fill",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.masked_fill.html#torch.Tensor.masked_fill",
        "api_signature": "Tensor.masked_fill(mask, value)",
        "api_description": "Out-of-place version of torch.Tensor.masked_fill_()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.masked_fill_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.masked_fill_.html#torch.Tensor.masked_fill_",
        "api_signature": "Tensor.masked_fill_(mask, value)",
        "api_description": "Fills elements of self tensor with value where mask is\nTrue. The shape of mask must be\nbroadcastable with the shape of the underlying\ntensor.",
        "return_value": "",
        "parameters": "mask (BoolTensor) – the boolean mask\nvalue (float) – the value to fill in with",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.masked_scatter",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.masked_scatter.html#torch.Tensor.masked_scatter",
        "api_signature": "Tensor.masked_scatter(mask, tensor)",
        "api_description": "Out-of-place version of torch.Tensor.masked_scatter_()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.masked_scatter_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.masked_scatter_.html#torch.Tensor.masked_scatter_",
        "api_signature": "Tensor.masked_scatter_(mask, source)",
        "api_description": "Copies elements from source into self tensor at positions where\nthe mask is True. Elements from source are copied into self\nstarting at position 0 of source and continuing in order one-by-one for each\noccurrence of mask being True.\nThe shape of mask must be broadcastable\nwith the shape of the underlying tensor. The source should have at least\nas many elements as the number of ones in mask.",
        "return_value": "",
        "parameters": "mask (BoolTensor) – the boolean mask\nsource (Tensor) – the tensor to copy from",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.masked_select",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.masked_select.html#torch.masked_select",
        "api_signature": "torch.masked_select(input, mask, *, out=None)",
        "api_description": "Returns a new 1-D tensor which indexes the input tensor according to\nthe boolean mask mask which is a BoolTensor.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.\nmask (BoolTensor) – the tensor containing the binary mask to index with\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.masked_select",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.masked_select.html#torch.Tensor.masked_select",
        "api_signature": "Tensor.masked_select(mask)",
        "api_description": "See torch.masked_select()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.backends.cuda.math_sdp_enabled",
        "api_url": "https://pytorch.org/docs/stable/backends.html#torch.backends.cuda.math_sdp_enabled",
        "api_signature": "torch.backends.cuda.math_sdp_enabled()",
        "api_description": "Warning",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.matmul",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul",
        "api_signature": "torch.matmul(input, other, *, out=None)",
        "api_description": "Matrix product of two tensors.",
        "return_value": "",
        "parameters": "input (Tensor) – the first tensor to be multiplied\nother (Tensor) – the second tensor to be multiplied\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.linalg.matmul",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.linalg.matmul.html#torch.linalg.matmul",
        "api_signature": "torch.linalg.matmul(input, other, *, out=None)",
        "api_description": "Alias for torch.matmul()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.matmul",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.matmul.html#torch.Tensor.matmul",
        "api_signature": "Tensor.matmul(tensor2)",
        "api_description": "See torch.matmul()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.matrix_exp",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.matrix_exp.html#torch.matrix_exp",
        "api_signature": "torch.matrix_exp(A)",
        "api_description": "Alias for torch.linalg.matrix_exp().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.linalg.matrix_exp",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.linalg.matrix_exp.html#torch.linalg.matrix_exp",
        "api_signature": "torch.linalg.matrix_exp(A)",
        "api_description": "Computes the matrix exponential of a square matrix.",
        "return_value": "",
        "parameters": "A (Tensor) – tensor of shape (*, n, n) where * is zero or more batch dimensions.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.matrix_exp",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.matrix_exp.html#torch.Tensor.matrix_exp",
        "api_signature": "Tensor.matrix_exp()",
        "api_description": "See torch.matrix_exp()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.linalg.matrix_norm",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.linalg.matrix_norm.html#torch.linalg.matrix_norm",
        "api_signature": "torch.linalg.matrix_norm(A, ord='fro', dim=(-2, -1)",
        "api_description": "Computes a matrix norm.",
        "return_value": "A real-valued tensor, even when A is complex.\n",
        "parameters": "A (Tensor) – tensor with two or more dimensions. By default its\nshape is interpreted as (*, m, n) where * is zero or more\nbatch dimensions, but this behavior can be controlled using dim.\nord (int, inf, -inf, 'fro', 'nuc', optional) – order of norm. Default: ‘fro’\ndim (Tuple[int, int], optional) – dimensions over which to compute the norm. Default: (-2, -1)\nkeepdim (bool, optional) – If set to True, the reduced dimensions are retained\nin the result as dimensions with size one. Default: False\nout (Tensor, optional) – output tensor. Ignored if None. Default: None.\ndtype (torch.dtype, optional) – If specified, the input tensor is cast to\ndtype before performing the operation, and the returned tensor’s type\nwill be dtype. Default: None",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.matrix_power",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.matrix_power.html#torch.matrix_power",
        "api_signature": "torch.matrix_power(input, n, *, out=None)",
        "api_description": "Alias for torch.linalg.matrix_power()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.linalg.matrix_power",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.linalg.matrix_power.html#torch.linalg.matrix_power",
        "api_signature": "torch.linalg.matrix_power(A, n, *, out=None)",
        "api_description": "Computes the n-th power of a square matrix for an integer n.",
        "return_value": "",
        "parameters": "A (Tensor) – tensor of shape (*, m, m) where * is zero or more batch dimensions.\nn (int) – the exponent.\nout (Tensor, optional) – output tensor. Ignored if None. Default: None.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.matrix_power",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.matrix_power.html#torch.Tensor.matrix_power",
        "api_signature": "Tensor.matrix_power(n)",
        "api_description": "Note",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.linalg.matrix_rank",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.linalg.matrix_rank.html#torch.linalg.matrix_rank",
        "api_signature": "torch.linalg.matrix_rank(A, *, atol=None, rtol=None, hermitian=False, out=None)",
        "api_description": "Computes the numerical rank of a matrix.",
        "return_value": "",
        "parameters": "A (Tensor) – tensor of shape (*, m, n) where * is zero or more batch dimensions.\ntol (float, Tensor, optional) – [NumPy Compat] Alias for atol. Default: None.\natol (float, Tensor, optional) – the absolute tolerance value. When None it’s considered to be zero.\nDefault: None.\nrtol (float, Tensor, optional) – the relative tolerance value. See above for the value it takes when None.\nDefault: None.\nhermitian (bool) – indicates whether A is Hermitian if complex\nor symmetric if real. Default: False.\nout (Tensor, optional) – output tensor. Ignored if None. Default: None.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.max",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.max.html#torch.max",
        "api_signature": "torch.max(input)",
        "api_description": "Returns the maximum value of all elements in the input tensor.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.\ninput (Tensor) – the input tensor.\ndim (int) – the dimension to reduce.\nkeepdim (bool) – whether the output tensor has dim retained or not. Default: False.\nout (tuple, optional) – the result tuple of two output tensors (max, max_indices)",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.max",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.max.html#torch.Tensor.max",
        "api_signature": "Tensor.max(dim=None, keepdim=False)",
        "api_description": "See torch.max()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda.max_memory_allocated",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.cuda.max_memory_allocated.html#torch.cuda.max_memory_allocated",
        "api_signature": "torch.cuda.max_memory_allocated(device=None)",
        "api_description": "Return the maximum GPU memory occupied by tensors in bytes for a given device.",
        "return_value": "",
        "parameters": "device (torch.device or int, optional) – selected device. Returns\nstatistic for the current device, given by current_device(),\nif device is None (default).",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda.max_memory_cached",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.cuda.max_memory_cached.html#torch.cuda.max_memory_cached",
        "api_signature": "torch.cuda.max_memory_cached(device=None)",
        "api_description": "Deprecated; see max_memory_reserved().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda.max_memory_reserved",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.cuda.max_memory_reserved.html#torch.cuda.max_memory_reserved",
        "api_signature": "torch.cuda.max_memory_reserved(device=None)",
        "api_description": "Return the maximum GPU memory managed by the caching allocator in bytes for a given device.",
        "return_value": "",
        "parameters": "device (torch.device or int, optional) – selected device. Returns\nstatistic for the current device, given by current_device(),\nif device is None (default).",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.quantized.functional.max_pool1d",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.nn.quantized.functional.max_pool1d.html#torch.ao.nn.quantized.functional.max_pool1d",
        "api_signature": "torch.ao.nn.quantized.functional.max_pool1d(input, kernel_size, stride=None, padding=0, dilation=1, ceil_mode=False, return_indices=False)",
        "api_description": "Applies a 1D max pooling over a quantized input signal composed of\nseveral quantized input planes.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.functional.max_pool1d",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.functional.max_pool1d.html#torch.nn.functional.max_pool1d",
        "api_signature": "torch.nn.functional.max_pool1d(input, kernel_size, stride=None, padding=0, dilation=1, ceil_mode=False, return_indices=False)",
        "api_description": "Applies a 1D max pooling over an input signal composed of several input\nplanes.",
        "return_value": "",
        "parameters": "input – input tensor of shape (minibatch,in_channels,iW)(\\text{minibatch} , \\text{in\\_channels} , iW)(minibatch,in_channels,iW), minibatch dim optional.\nkernel_size – the size of the window. Can be a single number or a\ntuple (kW,)\nstride – the stride of the window. Can be a single number or a tuple\n(sW,). Default: kernel_size\npadding – Implicit negative infinity padding to be added on both sides, must be >= 0 and <= kernel_size / 2.\ndilation – The stride between elements within a sliding window, must be > 0.\nceil_mode – If True, will use ceil instead of floor to compute the output shape. This\nensures that every element in the input tensor is covered by a sliding window.\nreturn_indices – If True, will return the argmax along with the max values.\nUseful for torch.nn.functional.max_unpool1d later",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.quantized.functional.max_pool2d",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.nn.quantized.functional.max_pool2d.html#torch.ao.nn.quantized.functional.max_pool2d",
        "api_signature": "torch.ao.nn.quantized.functional.max_pool2d(input, kernel_size, stride=None, padding=0, dilation=1, ceil_mode=False, return_indices=False)",
        "api_description": "Applies a 2D max pooling over a quantized input signal composed of\nseveral quantized input planes.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.functional.max_pool2d",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.functional.max_pool2d.html#torch.nn.functional.max_pool2d",
        "api_signature": "torch.nn.functional.max_pool2d(input, kernel_size, stride=None, padding=0, dilation=1, ceil_mode=False, return_indices=False)",
        "api_description": "Applies a 2D max pooling over an input signal composed of several input\nplanes.",
        "return_value": "",
        "parameters": "input – input tensor (minibatch,in_channels,iH,iW)(\\text{minibatch} , \\text{in\\_channels} , iH , iW)(minibatch,in_channels,iH,iW), minibatch dim optional.\nkernel_size – size of the pooling region. Can be a single number or a\ntuple (kH, kW)\nstride – stride of the pooling operation. Can be a single number or a\ntuple (sH, sW). Default: kernel_size\npadding – Implicit negative infinity padding to be added on both sides, must be >= 0 and <= kernel_size / 2.\ndilation – The stride between elements within a sliding window, must be > 0.\nceil_mode – If True, will use ceil instead of floor to compute the output shape. This\nensures that every element in the input tensor is covered by a sliding window.\nreturn_indices – If True, will return the argmax along with the max values.\nUseful for torch.nn.functional.max_unpool2d later",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.functional.max_pool3d",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.functional.max_pool3d.html#torch.nn.functional.max_pool3d",
        "api_signature": "torch.nn.functional.max_pool3d(input, kernel_size, stride=None, padding=0, dilation=1, ceil_mode=False, return_indices=False)",
        "api_description": "Applies a 3D max pooling over an input signal composed of several input\nplanes.",
        "return_value": "",
        "parameters": "input – input tensor (minibatch,in_channels,iD,iH,iW)(\\text{minibatch} , \\text{in\\_channels} , iD, iH , iW)(minibatch,in_channels,iD,iH,iW), minibatch dim optional.\nkernel_size – size of the pooling region. Can be a single number or a\ntuple (kT, kH, kW)\nstride – stride of the pooling operation. Can be a single number or a\ntuple (sT, sH, sW). Default: kernel_size\npadding – Implicit negative infinity padding to be added on both sides, must be >= 0 and <= kernel_size / 2.\ndilation – The stride between elements within a sliding window, must be > 0.\nceil_mode – If True, will use ceil instead of floor to compute the output shape. This\nensures that every element in the input tensor is covered by a sliding window.\nreturn_indices – If True, will return the argmax along with the max values.\nUseful for torch.nn.functional.max_unpool3d later",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.backends.cuda.cufft_plan_cache.max_size",
        "api_url": "https://pytorch.org/docs/stable/backends.html#torch.backends.cuda.cufft_plan_cache.max_size",
        "api_signature": null,
        "api_description": "A int that controls the capacity of a cuFFT plan cache.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.functional.max_unpool1d",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.functional.max_unpool1d.html#torch.nn.functional.max_unpool1d",
        "api_signature": "torch.nn.functional.max_unpool1d(input, indices, kernel_size, stride=None, padding=0, output_size=None)",
        "api_description": "Compute a partial inverse of MaxPool1d.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.functional.max_unpool2d",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.functional.max_unpool2d.html#torch.nn.functional.max_unpool2d",
        "api_signature": "torch.nn.functional.max_unpool2d(input, indices, kernel_size, stride=None, padding=0, output_size=None)",
        "api_description": "Compute a partial inverse of MaxPool2d.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.functional.max_unpool3d",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.functional.max_unpool3d.html#torch.nn.functional.max_unpool3d",
        "api_signature": "torch.nn.functional.max_unpool3d(input, indices, kernel_size, stride=None, padding=0, output_size=None)",
        "api_description": "Compute a partial inverse of MaxPool3d.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.maximum",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.maximum.html#torch.maximum",
        "api_signature": "torch.maximum(input, other, *, out=None)",
        "api_description": "Computes the element-wise maximum of input and other.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.\nother (Tensor) – the second input tensor\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.maximum",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.maximum.html#torch.Tensor.maximum",
        "api_signature": "Tensor.maximum(other)",
        "api_description": "See torch.maximum()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.MaxPool1d",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.MaxPool1d.html#torch.nn.MaxPool1d",
        "api_signature": "torch.nn.MaxPool1d(kernel_size, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False)",
        "api_description": "Applies a 1D max pooling over an input signal composed of several input planes.",
        "return_value": "",
        "parameters": "kernel_size (Union[int, Tuple[int]]) – The size of the sliding window, must be > 0.\nstride (Union[int, Tuple[int]]) – The stride of the sliding window, must be > 0. Default value is kernel_size.\npadding (Union[int, Tuple[int]]) – Implicit negative infinity padding to be added on both sides, must be >= 0 and <= kernel_size / 2.\ndilation (Union[int, Tuple[int]]) – The stride between elements within a sliding window, must be > 0.\nreturn_indices (bool) – If True, will return the argmax along with the max values.\nUseful for torch.nn.MaxUnpool1d later\nceil_mode (bool) – If True, will use ceil instead of floor to compute the output shape. This\nensures that every element in the input tensor is covered by a sliding window.",
        "input_shape": "\nInput: (N,C,Lin)(N, C, L_{in})(N,C,Lin​) or (C,Lin)(C, L_{in})(C,Lin​).\nOutput: (N,C,Lout)(N, C, L_{out})(N,C,Lout​) or (C,Lout)(C, L_{out})(C,Lout​), where\n\nLout=⌊Lin+2×padding−dilation×(kernel_size−1)−1stride+1⌋L_{out} = \\left\\lfloor \\frac{L_{in} + 2 \\times \\text{padding} - \\text{dilation}\n      \\times (\\text{kernel\\_size} - 1) - 1}{\\text{stride}} + 1\\right\\rfloor\n\nLout​=⌊strideLin​+2×padding−dilation×(kernel_size−1)−1​+1⌋\n\n",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.MaxPool2d",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html#torch.nn.MaxPool2d",
        "api_signature": "torch.nn.MaxPool2d(kernel_size, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False)",
        "api_description": "Applies a 2D max pooling over an input signal composed of several input planes.",
        "return_value": "",
        "parameters": "kernel_size (Union[int, Tuple[int, int]]) – the size of the window to take a max over\nstride (Union[int, Tuple[int, int]]) – the stride of the window. Default value is kernel_size\npadding (Union[int, Tuple[int, int]]) – Implicit negative infinity padding to be added on both sides\ndilation (Union[int, Tuple[int, int]]) – a parameter that controls the stride of elements in the window\nreturn_indices (bool) – if True, will return the max indices along with the outputs.\nUseful for torch.nn.MaxUnpool2d later\nceil_mode (bool) – when True, will use ceil instead of floor to compute the output shape",
        "input_shape": "\nInput: (N,C,Hin,Win)(N, C, H_{in}, W_{in})(N,C,Hin​,Win​) or (C,Hin,Win)(C, H_{in}, W_{in})(C,Hin​,Win​)\nOutput: (N,C,Hout,Wout)(N, C, H_{out}, W_{out})(N,C,Hout​,Wout​) or (C,Hout,Wout)(C, H_{out}, W_{out})(C,Hout​,Wout​), where\n\nHout=⌊Hin+2∗padding[0]−dilation[0]×(kernel_size[0]−1)−1stride[0]+1⌋H_{out} = \\left\\lfloor\\frac{H_{in} + 2 * \\text{padding[0]} - \\text{dilation[0]}\n      \\times (\\text{kernel\\_size[0]} - 1) - 1}{\\text{stride[0]}} + 1\\right\\rfloor\n\nHout​=⌊stride[0]Hin​+2∗padding[0]−dilation[0]×(kernel_size[0]−1)−1​+1⌋\nWout=⌊Win+2∗padding[1]−dilation[1]×(kernel_size[1]−1)−1stride[1]+1⌋W_{out} = \\left\\lfloor\\frac{W_{in} + 2 * \\text{padding[1]} - \\text{dilation[1]}\n      \\times (\\text{kernel\\_size[1]} - 1) - 1}{\\text{stride[1]}} + 1\\right\\rfloor\n\nWout​=⌊stride[1]Win​+2∗padding[1]−dilation[1]×(kernel_size[1]−1)−1​+1⌋\n\n",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.MaxPool3d",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.MaxPool3d.html#torch.nn.MaxPool3d",
        "api_signature": "torch.nn.MaxPool3d(kernel_size, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False)",
        "api_description": "Applies a 3D max pooling over an input signal composed of several input planes.",
        "return_value": "",
        "parameters": "kernel_size (Union[int, Tuple[int, int, int]]) – the size of the window to take a max over\nstride (Union[int, Tuple[int, int, int]]) – the stride of the window. Default value is kernel_size\npadding (Union[int, Tuple[int, int, int]]) – Implicit negative infinity padding to be added on all three sides\ndilation (Union[int, Tuple[int, int, int]]) – a parameter that controls the stride of elements in the window\nreturn_indices (bool) – if True, will return the max indices along with the outputs.\nUseful for torch.nn.MaxUnpool3d later\nceil_mode (bool) – when True, will use ceil instead of floor to compute the output shape",
        "input_shape": "\nInput: (N,C,Din,Hin,Win)(N, C, D_{in}, H_{in}, W_{in})(N,C,Din​,Hin​,Win​) or (C,Din,Hin,Win)(C, D_{in}, H_{in}, W_{in})(C,Din​,Hin​,Win​).\nOutput: (N,C,Dout,Hout,Wout)(N, C, D_{out}, H_{out}, W_{out})(N,C,Dout​,Hout​,Wout​) or (C,Dout,Hout,Wout)(C, D_{out}, H_{out}, W_{out})(C,Dout​,Hout​,Wout​), where\n\nDout=⌊Din+2×padding[0]−dilation[0]×(kernel_size[0]−1)−1stride[0]+1⌋D_{out} = \\left\\lfloor\\frac{D_{in} + 2 \\times \\text{padding}[0] - \\text{dilation}[0] \\times\n  (\\text{kernel\\_size}[0] - 1) - 1}{\\text{stride}[0]} + 1\\right\\rfloor\n\nDout​=⌊stride[0]Din​+2×padding[0]−dilation[0]×(kernel_size[0]−1)−1​+1⌋\nHout=⌊Hin+2×padding[1]−dilation[1]×(kernel_size[1]−1)−1stride[1]+1⌋H_{out} = \\left\\lfloor\\frac{H_{in} + 2 \\times \\text{padding}[1] - \\text{dilation}[1] \\times\n  (\\text{kernel\\_size}[1] - 1) - 1}{\\text{stride}[1]} + 1\\right\\rfloor\n\nHout​=⌊stride[1]Hin​+2×padding[1]−dilation[1]×(kernel_size[1]−1)−1​+1⌋\nWout=⌊Win+2×padding[2]−dilation[2]×(kernel_size[2]−1)−1stride[2]+1⌋W_{out} = \\left\\lfloor\\frac{W_{in} + 2 \\times \\text{padding}[2] - \\text{dilation}[2] \\times\n  (\\text{kernel\\_size}[2] - 1) - 1}{\\text{stride}[2]} + 1\\right\\rfloor\n\nWout​=⌊stride[2]Win​+2×padding[2]−dilation[2]×(kernel_size[2]−1)−1​+1⌋\n\n",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.MaxUnpool1d",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.MaxUnpool1d.html#torch.nn.MaxUnpool1d",
        "api_signature": "torch.nn.MaxUnpool1d(kernel_size, stride=None, padding=0)",
        "api_description": "Computes a partial inverse of MaxPool1d.",
        "return_value": "",
        "parameters": "kernel_size (int or tuple) – Size of the max pooling window.\nstride (int or tuple) – Stride of the max pooling window.\nIt is set to kernel_size by default.\npadding (int or tuple) – Padding that was added to the input",
        "input_shape": "\nInput: (N,C,Hin)(N, C, H_{in})(N,C,Hin​) or (C,Hin)(C, H_{in})(C,Hin​).\nOutput: (N,C,Hout)(N, C, H_{out})(N,C,Hout​) or (C,Hout)(C, H_{out})(C,Hout​), where\n\nHout=(Hin−1)×stride[0]−2×padding[0]+kernel_size[0]H_{out} = (H_{in} - 1) \\times \\text{stride}[0] - 2 \\times \\text{padding}[0] + \\text{kernel\\_size}[0]\n\nHout​=(Hin​−1)×stride[0]−2×padding[0]+kernel_size[0]or as given by output_size in the call operator\n\n\n",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.MaxUnpool2d",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.MaxUnpool2d.html#torch.nn.MaxUnpool2d",
        "api_signature": "torch.nn.MaxUnpool2d(kernel_size, stride=None, padding=0)",
        "api_description": "Computes a partial inverse of MaxPool2d.",
        "return_value": "",
        "parameters": "kernel_size (int or tuple) – Size of the max pooling window.\nstride (int or tuple) – Stride of the max pooling window.\nIt is set to kernel_size by default.\npadding (int or tuple) – Padding that was added to the input",
        "input_shape": "\nInput: (N,C,Hin,Win)(N, C, H_{in}, W_{in})(N,C,Hin​,Win​) or (C,Hin,Win)(C, H_{in}, W_{in})(C,Hin​,Win​).\nOutput: (N,C,Hout,Wout)(N, C, H_{out}, W_{out})(N,C,Hout​,Wout​) or (C,Hout,Wout)(C, H_{out}, W_{out})(C,Hout​,Wout​), where\n\nHout=(Hin−1)×stride[0]−2×padding[0]+kernel_size[0]H_{out} = (H_{in} - 1) \\times \\text{stride[0]} - 2 \\times \\text{padding[0]} + \\text{kernel\\_size[0]}\n\nHout​=(Hin​−1)×stride[0]−2×padding[0]+kernel_size[0]\nWout=(Win−1)×stride[1]−2×padding[1]+kernel_size[1]W_{out} = (W_{in} - 1) \\times \\text{stride[1]} - 2 \\times \\text{padding[1]} + \\text{kernel\\_size[1]}\n\nWout​=(Win​−1)×stride[1]−2×padding[1]+kernel_size[1]or as given by output_size in the call operator\n\n\n",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.MaxUnpool3d",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.MaxUnpool3d.html#torch.nn.MaxUnpool3d",
        "api_signature": "torch.nn.MaxUnpool3d(kernel_size, stride=None, padding=0)",
        "api_description": "Computes a partial inverse of MaxPool3d.",
        "return_value": "",
        "parameters": "kernel_size (int or tuple) – Size of the max pooling window.\nstride (int or tuple) – Stride of the max pooling window.\nIt is set to kernel_size by default.\npadding (int or tuple) – Padding that was added to the input",
        "input_shape": "\nInput: (N,C,Din,Hin,Win)(N, C, D_{in}, H_{in}, W_{in})(N,C,Din​,Hin​,Win​) or (C,Din,Hin,Win)(C, D_{in}, H_{in}, W_{in})(C,Din​,Hin​,Win​).\nOutput: (N,C,Dout,Hout,Wout)(N, C, D_{out}, H_{out}, W_{out})(N,C,Dout​,Hout​,Wout​) or (C,Dout,Hout,Wout)(C, D_{out}, H_{out}, W_{out})(C,Dout​,Hout​,Wout​), where\n\nDout=(Din−1)×stride[0]−2×padding[0]+kernel_size[0]D_{out} = (D_{in} - 1) \\times \\text{stride[0]} - 2 \\times \\text{padding[0]} + \\text{kernel\\_size[0]}\n\nDout​=(Din​−1)×stride[0]−2×padding[0]+kernel_size[0]\nHout=(Hin−1)×stride[1]−2×padding[1]+kernel_size[1]H_{out} = (H_{in} - 1) \\times \\text{stride[1]} - 2 \\times \\text{padding[1]} + \\text{kernel\\_size[1]}\n\nHout​=(Hin​−1)×stride[1]−2×padding[1]+kernel_size[1]\nWout=(Win−1)×stride[2]−2×padding[2]+kernel_size[2]W_{out} = (W_{in} - 1) \\times \\text{stride[2]} - 2 \\times \\text{padding[2]} + \\text{kernel\\_size[2]}\n\nWout​=(Win​−1)×stride[2]−2×padding[2]+kernel_size[2]or as given by output_size in the call operator\n\n\n",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.bernoulli.Bernoulli.mean",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.bernoulli.Bernoulli.mean",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.beta.Beta.mean",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.beta.Beta.mean",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.binomial.Binomial.mean",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.binomial.Binomial.mean",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.categorical.Categorical.mean",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.categorical.Categorical.mean",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.cauchy.Cauchy.mean",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.cauchy.Cauchy.mean",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.continuous_bernoulli.ContinuousBernoulli.mean",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.continuous_bernoulli.ContinuousBernoulli.mean",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.dirichlet.Dirichlet.mean",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.dirichlet.Dirichlet.mean",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.distribution.Distribution.mean",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.distribution.Distribution.mean",
        "api_signature": null,
        "api_description": "Returns the mean of the distribution.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.exponential.Exponential.mean",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.exponential.Exponential.mean",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.fishersnedecor.FisherSnedecor.mean",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.fishersnedecor.FisherSnedecor.mean",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.gamma.Gamma.mean",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.gamma.Gamma.mean",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.geometric.Geometric.mean",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.geometric.Geometric.mean",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.gumbel.Gumbel.mean",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.gumbel.Gumbel.mean",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.half_cauchy.HalfCauchy.mean",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.half_cauchy.HalfCauchy.mean",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.half_normal.HalfNormal.mean",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.half_normal.HalfNormal.mean",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.independent.Independent.mean",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.independent.Independent.mean",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.inverse_gamma.InverseGamma.mean",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.inverse_gamma.InverseGamma.mean",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.kumaraswamy.Kumaraswamy.mean",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.kumaraswamy.Kumaraswamy.mean",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.laplace.Laplace.mean",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.laplace.Laplace.mean",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.log_normal.LogNormal.mean",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.log_normal.LogNormal.mean",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.mean",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.mean",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.mixture_same_family.MixtureSameFamily.mean",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.mixture_same_family.MixtureSameFamily.mean",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.multinomial.Multinomial.mean",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.multinomial.Multinomial.mean",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.multivariate_normal.MultivariateNormal.mean",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.multivariate_normal.MultivariateNormal.mean",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.negative_binomial.NegativeBinomial.mean",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.negative_binomial.NegativeBinomial.mean",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.normal.Normal.mean",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.normal.Normal.mean",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.one_hot_categorical.OneHotCategorical.mean",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.one_hot_categorical.OneHotCategorical.mean",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.pareto.Pareto.mean",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.pareto.Pareto.mean",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.poisson.Poisson.mean",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.poisson.Poisson.mean",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.studentT.StudentT.mean",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.studentT.StudentT.mean",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.uniform.Uniform.mean",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.uniform.Uniform.mean",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.von_mises.VonMises.mean",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.von_mises.VonMises.mean",
        "api_signature": null,
        "api_description": "The provided mean is the circular one.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.weibull.Weibull.mean",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.weibull.Weibull.mean",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.wishart.Wishart.mean",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.wishart.Wishart.mean",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.mean",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.mean.html#torch.mean",
        "api_signature": "torch.mean(input, *, dtype=None)",
        "api_description": "Returns the mean value of all elements in the input tensor. Input must be floating point or complex.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor, either of floating point or complex dtype\ndtype (torch.dtype, optional) – the desired data type of returned tensor.\nIf specified, the input tensor is casted to dtype before the operation\nis performed. This is useful for preventing data type overflows. Default: None.\ninput (Tensor) – the input tensor.\ndim (int or tuple of ints) – the dimension or dimensions to reduce.\nkeepdim (bool) – whether the output tensor has dim retained or not.\ndtype (torch.dtype, optional) – the desired data type of returned tensor.\nIf specified, the input tensor is casted to dtype before the operation\nis performed. This is useful for preventing data type overflows. Default: None.\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.mean",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.mean.html#torch.Tensor.mean",
        "api_signature": "Tensor.mean(dim=None, keepdim=False, *, dtype=None)",
        "api_description": "See torch.mean()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.benchmark.Measurement",
        "api_url": "https://pytorch.org/docs/stable/benchmark_utils.html#torch.utils.benchmark.Measurement",
        "api_signature": "torch.utils.benchmark.Measurement(number_per_run, raw_times, task_spec, metadata=None)",
        "api_description": "The result of a Timer measurement.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.median",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.median.html#torch.median",
        "api_signature": "torch.median(input)",
        "api_description": "Returns the median of the values in input.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.\ninput (Tensor) – the input tensor.\ndim (int) – the dimension to reduce.\nkeepdim (bool) – whether the output tensor has dim retained or not.\nout ((Tensor, Tensor), optional) – The first tensor will be populated with the median values and the second\ntensor, which must have dtype long, with their indices in the dimension\ndim of input.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.median",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.median.html#torch.Tensor.median",
        "api_signature": "Tensor.median(dim=None, keepdim=False)",
        "api_description": "See torch.median()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.backends.cuda.mem_efficient_sdp_enabled",
        "api_url": "https://pytorch.org/docs/stable/backends.html#torch.backends.cuda.mem_efficient_sdp_enabled",
        "api_signature": "torch.backends.cuda.mem_efficient_sdp_enabled()",
        "api_description": "Warning",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda.mem_get_info",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.cuda.mem_get_info.html#torch.cuda.mem_get_info",
        "api_signature": "torch.cuda.mem_get_info(device=None)",
        "api_description": "Return the global free and total GPU memory for a given device using cudaMemGetInfo.",
        "return_value": "",
        "parameters": "device (torch.device or int, optional) – selected device. Returns\nstatistic for the current device, given by current_device(),\nif device is None (default).",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda.memory_allocated",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.cuda.memory_allocated.html#torch.cuda.memory_allocated",
        "api_signature": "torch.cuda.memory_allocated(device=None)",
        "api_description": "Return the current GPU memory occupied by tensors in bytes for a given device.",
        "return_value": "",
        "parameters": "device (torch.device or int, optional) – selected device. Returns\nstatistic for the current device, given by current_device(),\nif device is None (default).",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda.memory_cached",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.cuda.memory_cached.html#torch.cuda.memory_cached",
        "api_signature": "torch.cuda.memory_cached(device=None)",
        "api_description": "Deprecated; see memory_reserved().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.memory_format",
        "api_url": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.memory_format",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda.memory_reserved",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.cuda.memory_reserved.html#torch.cuda.memory_reserved",
        "api_signature": "torch.cuda.memory_reserved(device=None)",
        "api_description": "Return the current GPU memory managed by the caching allocator in bytes for a given device.",
        "return_value": "",
        "parameters": "device (torch.device or int, optional) – selected device. Returns\nstatistic for the current device, given by current_device(),\nif device is None (default).",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda.memory_snapshot",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.cuda.memory_snapshot.html#torch.cuda.memory_snapshot",
        "api_signature": "torch.cuda.memory_snapshot()",
        "api_description": "Return a snapshot of the CUDA memory allocator state across all devices.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda.memory_stats",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.cuda.memory_stats.html#torch.cuda.memory_stats",
        "api_signature": "torch.cuda.memory_stats(device=None)",
        "api_description": "Return a dictionary of CUDA memory allocator statistics for a given device.",
        "return_value": "",
        "parameters": "device (torch.device or int, optional) – selected device. Returns\nstatistics for the current device, given by current_device(),\nif device is None (default).",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda.memory_summary",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.cuda.memory_summary.html#torch.cuda.memory_summary",
        "api_signature": "torch.cuda.memory_summary(device=None, abbreviated=False)",
        "api_description": "Return a human-readable printout of the current memory allocator statistics for a given device.",
        "return_value": "",
        "parameters": "device (torch.device or int, optional) – selected device. Returns\nprintout for the current device, given by current_device(),\nif device is None (default).\nabbreviated (bool, optional) – whether to return an abbreviated summary\n(default: False).",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda.memory_usage",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.cuda.memory_usage.html#torch.cuda.memory_usage",
        "api_signature": "torch.cuda.memory_usage(device=None)",
        "api_description": "Return the percent of time over the past sample period during which global (device)\nmemory was being read or written as given by nvidia-smi.",
        "return_value": "",
        "parameters": "device (torch.device or int, optional) – selected device. Returns\nstatistic for the current device, given by current_device(),\nif device is None (default).",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.autograd.profiler_util.MemRecordsAcc",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.autograd.profiler_util.MemRecordsAcc.html#torch.autograd.profiler_util.MemRecordsAcc",
        "api_signature": "torch.autograd.profiler_util.MemRecordsAcc(mem_records)",
        "api_description": "Acceleration structure for accessing mem_records in interval.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.benchmark.Measurement.merge",
        "api_url": "https://pytorch.org/docs/stable/benchmark_utils.html#torch.utils.benchmark.Measurement.merge",
        "api_signature": "merge(measurements)",
        "api_description": "Convenience method for merging replicates.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.MultiheadAttention.merge_masks",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html#torch.nn.MultiheadAttention.merge_masks",
        "api_signature": "merge_masks(attn_mask, key_padding_mask, query)",
        "api_description": "Determine mask type and combine masks if necessary.",
        "return_value": "merged mask\nmask_type: merged mask type (0, 1, or 2)\n",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.meshgrid",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.meshgrid.html#torch.meshgrid",
        "api_signature": "torch.meshgrid(*tensors, indexing=None)",
        "api_description": "Creates grids of coordinates specified by the 1D inputs in attr:tensors.",
        "return_value": "If the input has NNN\ntensors of size S0…SN−1S_0 \\ldots S_{N-1}S0​…SN−1​, then the\noutput will also have NNN tensors, where each tensor\nis of shape (S0,...,SN−1)(S_0, ..., S_{N-1})(S0​,...,SN−1​).\n",
        "parameters": "tensors (list of Tensor) – list of scalars or 1 dimensional tensors. Scalars will be\ntreated as tensors of size (1,)(1,)(1,) automatically\nindexing (Optional[str]) – (str, optional): the indexing mode, either “xy”\nor “ij”, defaults to “ij”. See warning for future changes.\nIf “xy” is selected, the first dimension corresponds\nto the cardinality of the second input and the second\ndimension corresponds to the cardinality of the first\ninput.\nIf “ij” is selected, the dimensions are in the same\norder as the cardinality of the inputs.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.autograd.graph.Node.metadata",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.autograd.graph.Node.metadata.html#torch.autograd.graph.Node.metadata",
        "api_signature": "Node.metadata()",
        "api_description": "Return the metadata.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.metrics.api.MetricHandler",
        "api_url": "https://pytorch.org/docs/stable/elastic/metrics.html#torch.distributed.elastic.metrics.api.MetricHandler",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.mH",
        "api_url": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor.mH",
        "api_signature": null,
        "api_description": "Accessing this property is equivalent to calling adjoint().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.min",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.min.html#torch.min",
        "api_signature": "torch.min(input)",
        "api_description": "Returns the minimum value of all elements in the input tensor.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.\ninput (Tensor) – the input tensor.\ndim (int) – the dimension to reduce.\nkeepdim (bool) – whether the output tensor has dim retained or not.\nout (tuple, optional) – the tuple of two output tensors (min, min_indices)",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.min",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.min.html#torch.Tensor.min",
        "api_signature": "Tensor.min(dim=None, keepdim=False)",
        "api_description": "See torch.min()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.minimum",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.minimum.html#torch.minimum",
        "api_signature": "torch.minimum(input, other, *, out=None)",
        "api_description": "Computes the element-wise minimum of input and other.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.\nother (Tensor) – the second input tensor\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.minimum",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.minimum.html#torch.Tensor.minimum",
        "api_signature": "Tensor.minimum(other)",
        "api_description": "See torch.minimum()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.observer.MinMaxObserver",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.quantization.observer.MinMaxObserver.html#torch.ao.quantization.observer.MinMaxObserver",
        "api_signature": "torch.ao.quantization.observer.MinMaxObserver(dtype=torch.quint8, qscheme=torch.per_tensor_affine, reduce_range=False, quant_min=None, quant_max=None, factory_kwargs=None, eps=1.1920928955078125e-07, is_dynamic=False, **kwargs)",
        "api_description": "Observer module for computing the quantization parameters based on the\nrunning min and max values.",
        "return_value": "",
        "parameters": "dtype – dtype argument to the quantize node needed to implement the\nreference model spec.\nqscheme – Quantization scheme to be used\nreduce_range – Reduces the range of the quantized data type by 1 bit\nquant_min – Minimum quantization value. If unspecified, it will follow the 8-bit setup.\nquant_max – Maximum quantization value. If unspecified, it will follow the 8-bit setup.\neps (Tensor) – Epsilon value for float32, Defaults to torch.finfo(torch.float32).eps.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.Mish",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.Mish.html#torch.nn.Mish",
        "api_signature": "torch.nn.Mish(inplace=False)",
        "api_description": "Applies the Mish function, element-wise.",
        "return_value": "",
        "parameters": "",
        "input_shape": "\nInput: (∗)(*)(∗), where ∗*∗ means any number of dimensions.\nOutput: (∗)(*)(∗), same shape as the input.\n\n",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.functional.mish",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.functional.mish.html#torch.nn.functional.mish",
        "api_signature": "torch.nn.functional.mish(input, inplace=False)",
        "api_description": "Apply the Mish function, element-wise.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.fsdp.MixedPrecision",
        "api_url": "https://pytorch.org/docs/stable/fsdp.html#torch.distributed.fsdp.MixedPrecision",
        "api_signature": "torch.distributed.fsdp.MixedPrecision(param_dtype=None, reduce_dtype=None, buffer_dtype=None, keep_low_precision_grads=False, cast_forward_inputs=False, cast_root_forward_inputs=True, _module_classes_to_ignore=(<class 'torch.nn.modules.batchnorm._BatchNorm'>, )",
        "api_description": "This configures FSDP-native mixed precision training.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.mixture_same_family.MixtureSameFamily.mixture_distribution",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.mixture_same_family.MixtureSameFamily.mixture_distribution",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.mixture_same_family.MixtureSameFamily",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.mixture_same_family.MixtureSameFamily",
        "api_signature": "torch.distributions.mixture_same_family.MixtureSameFamily(mixture_distribution, component_distribution, validate_args=None)",
        "api_description": "Bases: Distribution",
        "return_value": "",
        "parameters": "mixture_distribution – torch.distributions.Categorical-like\ninstance. Manages the probability of selecting component.\nThe number of categories must match the rightmost batch\ndimension of the component_distribution. Must have either\nscalar batch_shape or batch_shape matching\ncomponent_distribution.batch_shape[:-1]\ncomponent_distribution – torch.distributions.Distribution-like\ninstance. Right-most batch dimension indexes component.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.mm",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.mm.html#torch.mm",
        "api_signature": "torch.mm(input, mat2, *, out=None)",
        "api_description": "Performs a matrix multiplication of the matrices input and mat2.",
        "return_value": "",
        "parameters": "input (Tensor) – the first matrix to be matrix multiplied\nmat2 (Tensor) – the second matrix to be matrix multiplied\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.sparse.mm",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.sparse.mm.html#torch.sparse.mm",
        "api_signature": "torch.sparse.mm()",
        "api_description": "Performs a matrix multiplication of the sparse matrix mat1\nand the (sparse or strided) matrix mat2. Similar to torch.mm(), if mat1 is a\n(n×m)(n \\times m)(n×m) tensor, mat2 is a (m×p)(m \\times p)(m×p) tensor, out will be a\n(n×p)(n \\times p)(n×p) tensor.\nWhen mat1 is a COO tensor it must have sparse_dim = 2.\nWhen inputs are COO tensors, this function also supports backward for both inputs.",
        "return_value": "",
        "parameters": "mat1 (Tensor) – the first sparse matrix to be multiplied\nmat2 (Tensor) – the second matrix to be multiplied, which could be sparse or dense\nreduce (str, optional) – the reduction operation to apply for non-unique indices\n(\"sum\", \"mean\", \"amax\", \"amin\"). Default \"sum\".",
        "input_shape": "The format of the output tensor of this function follows:\n- sparse x sparse -> sparse\n- sparse x dense -> dense\n",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.mm",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.mm.html#torch.Tensor.mm",
        "api_signature": "Tensor.mm(mat2)",
        "api_description": "See torch.mm()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.package.PackageExporter.mock",
        "api_url": "https://pytorch.org/docs/stable/package.html#torch.package.PackageExporter.mock",
        "api_signature": "mock(include, *, exclude=()",
        "api_description": "Replace some required modules with a mock implementation.  Mocked modules will return a fake\nobject for any attribute accessed from it. Because we copy file-by-file, the dependency resolution will sometimes\nfind files that are imported by model files but whose functionality is never used\n(e.g. custom serialization code or training helpers).\nUse this function to mock this functionality out without having to modify the original code.",
        "return_value": "",
        "parameters": "include (Union[List[str], str]) – A string e.g. \"my_package.my_subpackage\", or list of strings\nfor the names of the modules to be mocked out. Strings can also be a glob-style pattern\nstring that may match multiple modules. Any required dependencies that match this pattern\nstring will be mocked out automatically.\nExamples :'torch.**' – matches torch and all submodules of torch, e.g. 'torch.nn'\nand 'torch.nn.functional'\n'torch.*' – matches 'torch.nn' or 'torch.functional', but not\n'torch.nn.functional'\nexclude (Union[List[str], str]) – An optional pattern that excludes some patterns that match the include string.\ne.g. include='torch.**', exclude='torch.foo' will mock all torch packages except 'torch.foo',\nDefault: is [].\nallow_empty (bool) – An optional flag that specifies whether the mock implementation(s) specified by this call\nto the mock() method must be matched to some module during packaging. If a mock is added with\nallow_empty=False, and close() is called (either explicitly or via __exit__) and the mock has\nnot been matched to a module used by the package being exported, an exception is thrown.\nIf allow_empty=True, no such exception is thrown.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.package.PackageExporter.mocked_modules",
        "api_url": "https://pytorch.org/docs/stable/package.html#torch.package.PackageExporter.mocked_modules",
        "api_signature": "mocked_modules()",
        "api_description": "Return all modules that are currently mocked.",
        "return_value": "A list containing the names of modules which will be\nmocked in this package.\n",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.bernoulli.Bernoulli.mode",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.bernoulli.Bernoulli.mode",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.beta.Beta.mode",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.beta.Beta.mode",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.binomial.Binomial.mode",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.binomial.Binomial.mode",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.categorical.Categorical.mode",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.categorical.Categorical.mode",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.cauchy.Cauchy.mode",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.cauchy.Cauchy.mode",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.dirichlet.Dirichlet.mode",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.dirichlet.Dirichlet.mode",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.distribution.Distribution.mode",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.distribution.Distribution.mode",
        "api_signature": null,
        "api_description": "Returns the mode of the distribution.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.exponential.Exponential.mode",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.exponential.Exponential.mode",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.fishersnedecor.FisherSnedecor.mode",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.fishersnedecor.FisherSnedecor.mode",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.gamma.Gamma.mode",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.gamma.Gamma.mode",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.geometric.Geometric.mode",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.geometric.Geometric.mode",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.gumbel.Gumbel.mode",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.gumbel.Gumbel.mode",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.half_cauchy.HalfCauchy.mode",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.half_cauchy.HalfCauchy.mode",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.half_normal.HalfNormal.mode",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.half_normal.HalfNormal.mode",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.independent.Independent.mode",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.independent.Independent.mode",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.inverse_gamma.InverseGamma.mode",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.inverse_gamma.InverseGamma.mode",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.kumaraswamy.Kumaraswamy.mode",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.kumaraswamy.Kumaraswamy.mode",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.laplace.Laplace.mode",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.laplace.Laplace.mode",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.log_normal.LogNormal.mode",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.log_normal.LogNormal.mode",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.mode",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.mode",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.multivariate_normal.MultivariateNormal.mode",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.multivariate_normal.MultivariateNormal.mode",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.negative_binomial.NegativeBinomial.mode",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.negative_binomial.NegativeBinomial.mode",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.normal.Normal.mode",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.normal.Normal.mode",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.one_hot_categorical.OneHotCategorical.mode",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.one_hot_categorical.OneHotCategorical.mode",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.pareto.Pareto.mode",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.pareto.Pareto.mode",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.poisson.Poisson.mode",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.poisson.Poisson.mode",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.studentT.StudentT.mode",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.studentT.StudentT.mode",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.uniform.Uniform.mode",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.uniform.Uniform.mode",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.von_mises.VonMises.mode",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.von_mises.VonMises.mode",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.weibull.Weibull.mode",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.weibull.Weibull.mode",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.wishart.Wishart.mode",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.wishart.Wishart.mode",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.mode",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.mode.html#torch.mode",
        "api_signature": "torch.mode(input, dim=-1, keepdim=False, *, out=None)",
        "api_description": "Returns a namedtuple (values, indices) where values is the mode\nvalue of each row of the input tensor in the given dimension\ndim, i.e. a value which appears most often\nin that row, and indices is the index location of each mode value found.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.\ndim (int) – the dimension to reduce.\nkeepdim (bool) – whether the output tensor has dim retained or not.\nout (tuple, optional) – the result tuple of two output tensors (values, indices)",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.mode",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.mode.html#torch.Tensor.mode",
        "api_signature": "Tensor.mode(dim=None, keepdim=False)",
        "api_description": "See torch.mode()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.pt2e.export_utils.model_is_exported",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.quantization.pt2e.export_utils.model_is_exported.html#torch.ao.quantization.pt2e.export_utils.model_is_exported",
        "api_signature": "torch.ao.quantization.pt2e.export_utils.model_is_exported(m)",
        "api_description": "Return True if the torch.nn.Module was exported, False otherwise\n(e.g. if the model was FX symbolically traced or not traced at all).",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.onnx.ONNXProgram.model_proto",
        "api_url": "https://pytorch.org/docs/stable/onnx_dynamo.html#torch.onnx.ONNXProgram.model_proto",
        "api_signature": null,
        "api_description": "The exported ONNX model as an onnx.ModelProto.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.onnx.ONNXProgram.model_signature",
        "api_url": "https://pytorch.org/docs/stable/onnx_dynamo.html#torch.onnx.ONNXProgram.model_signature",
        "api_signature": null,
        "api_description": "The model signature for the exported ONNX graph.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch",
        "api_url": "https://pytorch.org/docs/stable/torch.html#module-torch",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch",
        "api_url": "https://pytorch.org/docs/stable/torch.html#module-torch",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.__config__",
        "api_url": "https://pytorch.org/docs/stable/config_mod.html#module-torch.__config__",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.__future__",
        "api_url": "https://pytorch.org/docs/stable/future_mod.html#module-torch.__future__",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch._logging",
        "api_url": "https://pytorch.org/docs/stable/logging.html#module-torch._logging",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.amp",
        "api_url": "https://pytorch.org/docs/stable/amp.html#module-torch.amp",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.amp.autocast_mode",
        "api_url": "https://pytorch.org/docs/stable/amp.html#module-torch.amp.autocast_mode",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.amp.grad_scaler",
        "api_url": "https://pytorch.org/docs/stable/amp.html#module-torch.amp.grad_scaler",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.nn",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.intrinsic",
        "api_url": "https://pytorch.org/docs/stable/quantization-support.html#module-torch.ao.nn.intrinsic",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.intrinsic.modules",
        "api_url": "https://pytorch.org/docs/stable/quantization-support.html#module-torch.ao.nn.intrinsic.modules",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.intrinsic.modules.fused",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.nn.intrinsic.modules.fused",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.intrinsic.qat",
        "api_url": "https://pytorch.org/docs/stable/quantization-support.html#module-torch.ao.nn.intrinsic.qat",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.intrinsic.qat.modules",
        "api_url": "https://pytorch.org/docs/stable/quantization-support.html#module-torch.ao.nn.intrinsic.qat.modules",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.intrinsic.qat.modules.conv_fused",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.nn.intrinsic.qat.modules.conv_fused",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.intrinsic.qat.modules.linear_fused",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.nn.intrinsic.qat.modules.linear_fused",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.intrinsic.qat.modules.linear_relu",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.nn.intrinsic.qat.modules.linear_relu",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.intrinsic.quantized",
        "api_url": "https://pytorch.org/docs/stable/quantization-support.html#module-torch.ao.nn.intrinsic.quantized",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.intrinsic.quantized.dynamic",
        "api_url": "https://pytorch.org/docs/stable/quantization-support.html#module-torch.ao.nn.intrinsic.quantized.dynamic",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.intrinsic.quantized.dynamic.modules",
        "api_url": "https://pytorch.org/docs/stable/quantization-support.html#module-torch.ao.nn.intrinsic.quantized.dynamic.modules",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.intrinsic.quantized.dynamic.modules.linear_relu",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.nn.intrinsic.quantized.dynamic.modules.linear_relu",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.intrinsic.quantized.modules",
        "api_url": "https://pytorch.org/docs/stable/quantization-support.html#module-torch.ao.nn.intrinsic.quantized.modules",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.intrinsic.quantized.modules.bn_relu",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.nn.intrinsic.quantized.modules.bn_relu",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.intrinsic.quantized.modules.conv_add",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.nn.intrinsic.quantized.modules.conv_add",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.intrinsic.quantized.modules.conv_relu",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.nn.intrinsic.quantized.modules.conv_relu",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.intrinsic.quantized.modules.linear_relu",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.nn.intrinsic.quantized.modules.linear_relu",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.qat",
        "api_url": "https://pytorch.org/docs/stable/quantization-support.html#module-torch.ao.nn.qat",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.qat.dynamic",
        "api_url": "https://pytorch.org/docs/stable/quantization-support.html#module-torch.ao.nn.qat.dynamic",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.qat.dynamic.modules",
        "api_url": "https://pytorch.org/docs/stable/quantization-support.html#module-torch.ao.nn.qat.dynamic.modules",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.qat.dynamic.modules.linear",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.nn.qat.dynamic.modules.linear",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.qat.modules",
        "api_url": "https://pytorch.org/docs/stable/quantization-support.html#module-torch.ao.nn.qat.modules",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.qat.modules.conv",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.nn.qat.modules.conv",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.qat.modules.embedding_ops",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.nn.qat.modules.embedding_ops",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.qat.modules.linear",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.nn.qat.modules.linear",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.quantizable",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.nn.quantizable",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.quantizable.modules",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.nn.quantizable.modules",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.quantizable.modules.activation",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.nn.quantizable.modules.activation",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.quantizable.modules.rnn",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.nn.quantizable.modules.rnn",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.quantized",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.nn.quantized",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.quantized.dynamic",
        "api_url": "https://pytorch.org/docs/stable/quantization-support.html#module-torch.ao.nn.quantized.dynamic",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.quantized.dynamic.modules",
        "api_url": "https://pytorch.org/docs/stable/quantization-support.html#module-torch.ao.nn.quantized.dynamic.modules",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.quantized.dynamic.modules.conv",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.nn.quantized.dynamic.modules.conv",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.quantized.dynamic.modules.linear",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.nn.quantized.dynamic.modules.linear",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.quantized.dynamic.modules.rnn",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.nn.quantized.dynamic.modules.rnn",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.quantized.functional",
        "api_url": "https://pytorch.org/docs/stable/quantization-support.html#module-torch.ao.nn.quantized.functional",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.quantized.modules",
        "api_url": "https://pytorch.org/docs/stable/quantization-support.html#module-torch.ao.nn.quantized.modules",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.quantized.modules.activation",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.nn.quantized.modules.activation",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.quantized.modules.batchnorm",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.nn.quantized.modules.batchnorm",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.quantized.modules.conv",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.nn.quantized.modules.conv",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.quantized.modules.dropout",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.nn.quantized.modules.dropout",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.quantized.modules.embedding_ops",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.nn.quantized.modules.embedding_ops",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.quantized.modules.functional_modules",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.nn.quantized.modules.functional_modules",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.quantized.modules.linear",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.nn.quantized.modules.linear",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.quantized.modules.normalization",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.nn.quantized.modules.normalization",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.quantized.modules.rnn",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.nn.quantized.modules.rnn",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.quantized.modules.utils",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.nn.quantized.modules.utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.quantized.reference",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.nn.quantized.reference",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.quantized.reference.modules",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.nn.quantized.reference.modules",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.quantized.reference.modules.conv",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.nn.quantized.reference.modules.conv",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.quantized.reference.modules.linear",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.nn.quantized.reference.modules.linear",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.quantized.reference.modules.rnn",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.nn.quantized.reference.modules.rnn",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.quantized.reference.modules.sparse",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.nn.quantized.reference.modules.sparse",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.quantized.reference.modules.utils",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.nn.quantized.reference.modules.utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.sparse",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.nn.sparse",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.sparse.quantized",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.nn.sparse.quantized",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.sparse.quantized.dynamic",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.nn.sparse.quantized.dynamic",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.sparse.quantized.dynamic.linear",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.nn.sparse.quantized.dynamic.linear",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.sparse.quantized.linear",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.nn.sparse.quantized.linear",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.sparse.quantized.utils",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.nn.sparse.quantized.utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.ns",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.ns",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.ns._numeric_suite",
        "api_url": "https://pytorch.org/docs/stable/torch.ao.ns._numeric_suite.html#module-torch.ao.ns._numeric_suite",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.ns._numeric_suite_fx",
        "api_url": "https://pytorch.org/docs/stable/torch.ao.ns._numeric_suite_fx.html#module-torch.ao.ns._numeric_suite_fx",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.ns.fx",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.ns.fx",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.ns.fx.graph_matcher",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.ns.fx.graph_matcher",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.ns.fx.graph_passes",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.ns.fx.graph_passes",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.ns.fx.mappings",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.ns.fx.mappings",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.ns.fx.n_shadows_utils",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.ns.fx.n_shadows_utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.ns.fx.ns_types",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.ns.fx.ns_types",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.ns.fx.pattern_utils",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.ns.fx.pattern_utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.ns.fx.qconfig_multi_mapping",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.ns.fx.qconfig_multi_mapping",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.ns.fx.utils",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.ns.fx.utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.ns.fx.weight_utils",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.ns.fx.weight_utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.pruning",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.pruning",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.pruning.scheduler",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.pruning.scheduler",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.pruning.scheduler.base_scheduler",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.pruning.scheduler.base_scheduler",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.pruning.scheduler.cubic_scheduler",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.pruning.scheduler.cubic_scheduler",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.pruning.scheduler.lambda_scheduler",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.pruning.scheduler.lambda_scheduler",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.pruning.sparsifier",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.pruning.sparsifier",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.pruning.sparsifier.base_sparsifier",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.pruning.sparsifier.base_sparsifier",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.pruning.sparsifier.nearly_diagonal_sparsifier",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.pruning.sparsifier.nearly_diagonal_sparsifier",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.pruning.sparsifier.utils",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.pruning.sparsifier.utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.pruning.sparsifier.weight_norm_sparsifier",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.pruning.sparsifier.weight_norm_sparsifier",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.backend_config",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.backend_config",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.backend_config.backend_config",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.backend_config.backend_config",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.backend_config.executorch",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.backend_config.executorch",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.backend_config.fbgemm",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.backend_config.fbgemm",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.backend_config.native",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.backend_config.native",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.backend_config.observation_type",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.backend_config.observation_type",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.backend_config.onednn",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.backend_config.onednn",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.backend_config.qnnpack",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.backend_config.qnnpack",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.backend_config.tensorrt",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.backend_config.tensorrt",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.backend_config.utils",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.backend_config.utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.backend_config.x86",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.backend_config.x86",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.fake_quantize",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.fake_quantize",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.fuse_modules",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.fuse_modules",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.fuser_method_mappings",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.fuser_method_mappings",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.fx",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.fx",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.fx.convert",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.fx.convert",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.fx.custom_config",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.fx.custom_config",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.fx.fuse",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.fx.fuse",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.fx.fuse_handler",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.fx.fuse_handler",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.fx.graph_module",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.fx.graph_module",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.fx.lower_to_fbgemm",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.fx.lower_to_fbgemm",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.fx.lower_to_qnnpack",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.fx.lower_to_qnnpack",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.fx.lstm_utils",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.fx.lstm_utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.fx.match_utils",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.fx.match_utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.fx.pattern_utils",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.fx.pattern_utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.fx.prepare",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.fx.prepare",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.fx.qconfig_mapping_utils",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.fx.qconfig_mapping_utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.fx.quantize_handler",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.fx.quantize_handler",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.fx.tracer",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.fx.tracer",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.fx.utils",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.fx.utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.observer",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.observer",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.pt2e",
        "api_url": "https://pytorch.org/docs/stable/quantization-support.html#module-torch.ao.quantization.pt2e",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.pt2e.duplicate_dq_pass",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.pt2e.duplicate_dq_pass",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.pt2e.export_utils",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.pt2e.export_utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.pt2e.generate_numeric_debug_handle",
        "api_url": "https://pytorch.org/docs/stable/quantization-support.html#module-torch.ao.quantization.pt2e.generate_numeric_debug_handle",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.pt2e.graph_utils",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.pt2e.graph_utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.pt2e.port_metadata_pass",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.pt2e.port_metadata_pass",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.pt2e.prepare",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.pt2e.prepare",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.pt2e.qat_utils",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.pt2e.qat_utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.pt2e.representation",
        "api_url": "https://pytorch.org/docs/stable/quantization-support.html#module-torch.ao.quantization.pt2e.representation",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.pt2e.representation.rewrite",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.pt2e.representation.rewrite",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.pt2e.utils",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.pt2e.utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.qconfig",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.qconfig",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.qconfig_mapping",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.qconfig_mapping",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.quant_type",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.quant_type",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.quantization_mappings",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.quantization_mappings",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.quantize_fx",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.quantize_fx",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.quantize_jit",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.quantize_jit",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.quantize_pt2e",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.quantize_pt2e",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.quantizer",
        "api_url": "https://pytorch.org/docs/stable/quantization-support.html#module-torch.ao.quantization.quantizer",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.quantizer.composable_quantizer",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.quantizer.composable_quantizer",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.quantizer.embedding_quantizer",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.quantizer.embedding_quantizer",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.quantizer.quantizer",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.quantizer.quantizer",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.quantizer.utils",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.quantizer.utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.quantizer.x86_inductor_quantizer",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.quantizer.x86_inductor_quantizer",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.quantizer.xnnpack_quantizer",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.quantizer.xnnpack_quantizer",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.quantizer.xnnpack_quantizer_utils",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.quantizer.xnnpack_quantizer_utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.stubs",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.stubs",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.utils",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.autograd",
        "api_url": "https://pytorch.org/docs/stable/autograd.html#module-torch.autograd",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.autograd.anomaly_mode",
        "api_url": "https://pytorch.org/docs/stable/autograd.html#module-torch.autograd.anomaly_mode",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.autograd.forward_ad",
        "api_url": "https://pytorch.org/docs/stable/autograd.html#module-torch.autograd.forward_ad",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.autograd.function",
        "api_url": "https://pytorch.org/docs/stable/autograd.html#module-torch.autograd.function",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.autograd.functional",
        "api_url": "https://pytorch.org/docs/stable/autograd.html#module-torch.autograd.functional",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.autograd.grad_mode",
        "api_url": "https://pytorch.org/docs/stable/autograd.html#module-torch.autograd.grad_mode",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.autograd.gradcheck",
        "api_url": "https://pytorch.org/docs/stable/autograd.html#module-torch.autograd.gradcheck",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.autograd.graph",
        "api_url": "https://pytorch.org/docs/stable/autograd.html#module-torch.autograd.graph",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.autograd.profiler",
        "api_url": "https://pytorch.org/docs/stable/autograd.html#module-torch.autograd.profiler",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.autograd.profiler_legacy",
        "api_url": "https://pytorch.org/docs/stable/autograd.html#module-torch.autograd.profiler_legacy",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.autograd.profiler_util",
        "api_url": "https://pytorch.org/docs/stable/autograd.html#module-torch.autograd.profiler_util",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.autograd.variable",
        "api_url": "https://pytorch.org/docs/stable/autograd.html#module-torch.autograd.variable",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.backends",
        "api_url": "https://pytorch.org/docs/stable/backends.html#module-torch.backends",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.backends.cpu",
        "api_url": "https://pytorch.org/docs/stable/backends.html#module-torch.backends.cpu",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.backends.cuda",
        "api_url": "https://pytorch.org/docs/stable/backends.html#module-torch.backends.cuda",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.backends.cudnn",
        "api_url": "https://pytorch.org/docs/stable/backends.html#module-torch.backends.cudnn",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.backends.cudnn.rnn",
        "api_url": "https://pytorch.org/docs/stable/backends.html#module-torch.backends.cudnn.rnn",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.backends.mha",
        "api_url": "https://pytorch.org/docs/stable/backends.html#module-torch.backends.mha",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.backends.mkl",
        "api_url": "https://pytorch.org/docs/stable/backends.html#module-torch.backends.mkl",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.backends.mkldnn",
        "api_url": "https://pytorch.org/docs/stable/backends.html#module-torch.backends.mkldnn",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.backends.mps",
        "api_url": "https://pytorch.org/docs/stable/backends.html#module-torch.backends.mps",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.backends.nnpack",
        "api_url": "https://pytorch.org/docs/stable/backends.html#module-torch.backends.nnpack",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.backends.openmp",
        "api_url": "https://pytorch.org/docs/stable/backends.html#module-torch.backends.openmp",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.backends.opt_einsum",
        "api_url": "https://pytorch.org/docs/stable/backends.html#module-torch.backends.opt_einsum",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.backends.quantized",
        "api_url": "https://pytorch.org/docs/stable/backends.html#module-torch.backends.quantized",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.backends.xeon",
        "api_url": "https://pytorch.org/docs/stable/backends.html#module-torch.backends.xeon",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.backends.xeon.run_cpu",
        "api_url": "https://pytorch.org/docs/stable/backends.html#module-torch.backends.xeon.run_cpu",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.backends.xnnpack",
        "api_url": "https://pytorch.org/docs/stable/backends.html#module-torch.backends.xnnpack",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.compiler",
        "api_url": "https://pytorch.org/docs/stable/torch.compiler_api.html#module-torch.compiler",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.contrib",
        "api_url": "https://pytorch.org/docs/stable/torch.html#module-torch.contrib",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cpu",
        "api_url": "https://pytorch.org/docs/stable/cpu.html#module-torch.cpu",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cpu.amp",
        "api_url": "https://pytorch.org/docs/stable/amp.html#module-torch.cpu.amp",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cpu.amp.autocast_mode",
        "api_url": "https://pytorch.org/docs/stable/amp.html#module-torch.cpu.amp.autocast_mode",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cpu.amp.grad_scaler",
        "api_url": "https://pytorch.org/docs/stable/amp.html#module-torch.cpu.amp.grad_scaler",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda",
        "api_url": "https://pytorch.org/docs/stable/cuda.html#module-torch.cuda",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda._sanitizer",
        "api_url": "https://pytorch.org/docs/stable/cuda._sanitizer.html#module-torch.cuda._sanitizer",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda.amp",
        "api_url": "https://pytorch.org/docs/stable/amp.html#module-torch.cuda.amp",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda.amp.autocast_mode",
        "api_url": "https://pytorch.org/docs/stable/amp.html#module-torch.cuda.amp.autocast_mode",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda.amp.common",
        "api_url": "https://pytorch.org/docs/stable/amp.html#module-torch.cuda.amp.common",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda.amp.grad_scaler",
        "api_url": "https://pytorch.org/docs/stable/amp.html#module-torch.cuda.amp.grad_scaler",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda.comm",
        "api_url": "https://pytorch.org/docs/stable/cuda.html#module-torch.cuda.comm",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda.error",
        "api_url": "https://pytorch.org/docs/stable/cuda.html#module-torch.cuda.error",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda.graphs",
        "api_url": "https://pytorch.org/docs/stable/cuda.html#module-torch.cuda.graphs",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda.jiterator",
        "api_url": "https://pytorch.org/docs/stable/cuda.html#module-torch.cuda.jiterator",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda.memory",
        "api_url": "https://pytorch.org/docs/stable/cuda.html#module-torch.cuda.memory",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda.nccl",
        "api_url": "https://pytorch.org/docs/stable/cuda.html#module-torch.cuda.nccl",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda.nvtx",
        "api_url": "https://pytorch.org/docs/stable/cuda.html#module-torch.cuda.nvtx",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda.profiler",
        "api_url": "https://pytorch.org/docs/stable/cuda.html#module-torch.cuda.profiler",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda.random",
        "api_url": "https://pytorch.org/docs/stable/cuda.html#module-torch.cuda.random",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda.sparse",
        "api_url": "https://pytorch.org/docs/stable/cuda.html#module-torch.cuda.sparse",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda.streams",
        "api_url": "https://pytorch.org/docs/stable/cuda.html#module-torch.cuda.streams",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.algorithms",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.algorithms",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.algorithms.ddp_comm_hooks",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.algorithms.ddp_comm_hooks",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.algorithms.ddp_comm_hooks.ddp_zero_hook",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.algorithms.ddp_comm_hooks.ddp_zero_hook",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.algorithms.ddp_comm_hooks.debugging_hooks",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.algorithms.ddp_comm_hooks.debugging_hooks",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.algorithms.ddp_comm_hooks.default_hooks",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.algorithms.ddp_comm_hooks.default_hooks",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.algorithms.ddp_comm_hooks.mixed_precision_hooks",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.algorithms.ddp_comm_hooks.mixed_precision_hooks",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.algorithms.ddp_comm_hooks.optimizer_overlap_hooks",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.algorithms.ddp_comm_hooks.optimizer_overlap_hooks",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.algorithms.ddp_comm_hooks.post_localSGD_hook",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.algorithms.ddp_comm_hooks.post_localSGD_hook",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.algorithms.ddp_comm_hooks.quantization_hooks",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.algorithms.ddp_comm_hooks.quantization_hooks",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.algorithms.join",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.algorithms.join",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.algorithms.model_averaging",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.algorithms.model_averaging",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.algorithms.model_averaging.averagers",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.algorithms.model_averaging.averagers",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.algorithms.model_averaging.hierarchical_model_averager",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.algorithms.model_averaging.hierarchical_model_averager",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.algorithms.model_averaging.utils",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.algorithms.model_averaging.utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.argparse_util",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.argparse_util",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.autograd",
        "api_url": "https://pytorch.org/docs/stable/rpc.html#module-torch.distributed.autograd",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.c10d_logger",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.c10d_logger",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.checkpoint",
        "api_url": "https://pytorch.org/docs/stable/distributed.checkpoint.html#module-torch.distributed.checkpoint",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.checkpoint.api",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.checkpoint.api",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.checkpoint.default_planner",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.checkpoint.default_planner",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.checkpoint.filesystem",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.checkpoint.filesystem",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.checkpoint.format_utils",
        "api_url": "https://pytorch.org/docs/stable/distributed.checkpoint.html#module-torch.distributed.checkpoint.format_utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.checkpoint.fsspec",
        "api_url": "https://pytorch.org/docs/stable/distributed.checkpoint.html#module-torch.distributed.checkpoint.fsspec",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.checkpoint.metadata",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.checkpoint.metadata",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.checkpoint.optimizer",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.checkpoint.optimizer",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.checkpoint.planner",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.checkpoint.planner",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.checkpoint.planner_helpers",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.checkpoint.planner_helpers",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.checkpoint.resharding",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.checkpoint.resharding",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.checkpoint.state_dict",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.checkpoint.state_dict",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.checkpoint.state_dict_loader",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.checkpoint.state_dict_loader",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.checkpoint.state_dict_saver",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.checkpoint.state_dict_saver",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.checkpoint.stateful",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.checkpoint.stateful",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.checkpoint.storage",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.checkpoint.storage",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.checkpoint.utils",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.checkpoint.utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.collective_utils",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.collective_utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.constants",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.constants",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.device_mesh",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.device_mesh",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.distributed_c10d",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.distributed_c10d",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.elastic",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.agent",
        "api_url": "https://pytorch.org/docs/stable/elastic/agent.html#module-torch.distributed.elastic.agent",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.agent.server",
        "api_url": "https://pytorch.org/docs/stable/elastic/agent.html#module-torch.distributed.elastic.agent.server",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.agent.server.api",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.elastic.agent.server.api",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.agent.server.local_elastic_agent",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.elastic.agent.server.local_elastic_agent",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.events",
        "api_url": "https://pytorch.org/docs/stable/elastic/events.html#module-torch.distributed.elastic.events",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.events.api",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.elastic.events.api",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.events.handlers",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.elastic.events.handlers",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.metrics",
        "api_url": "https://pytorch.org/docs/stable/elastic/metrics.html#module-torch.distributed.elastic.metrics",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.metrics.api",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.elastic.metrics.api",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.multiprocessing",
        "api_url": "https://pytorch.org/docs/stable/elastic/multiprocessing.html#module-torch.distributed.elastic.multiprocessing",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.multiprocessing.api",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.elastic.multiprocessing.api",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.multiprocessing.errors",
        "api_url": "https://pytorch.org/docs/stable/elastic/errors.html#module-torch.distributed.elastic.multiprocessing.errors",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.multiprocessing.errors.error_handler",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.elastic.multiprocessing.errors.error_handler",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.multiprocessing.errors.handlers",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.elastic.multiprocessing.errors.handlers",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.multiprocessing.redirects",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.elastic.multiprocessing.redirects",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.multiprocessing.subprocess_handler",
        "api_url": "https://pytorch.org/docs/stable/elastic/subprocess_handler.html#module-torch.distributed.elastic.multiprocessing.subprocess_handler",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.multiprocessing.subprocess_handler.handlers",
        "api_url": "https://pytorch.org/docs/stable/elastic/subprocess_handler.html#module-torch.distributed.elastic.multiprocessing.subprocess_handler.handlers",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.multiprocessing.subprocess_handler.subprocess_handler",
        "api_url": "https://pytorch.org/docs/stable/elastic/subprocess_handler.html#module-torch.distributed.elastic.multiprocessing.subprocess_handler.subprocess_handler",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.multiprocessing.tail_log",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.elastic.multiprocessing.tail_log",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.rendezvous",
        "api_url": "https://pytorch.org/docs/stable/elastic/rendezvous.html#module-torch.distributed.elastic.rendezvous",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.rendezvous.api",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.elastic.rendezvous.api",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.rendezvous.c10d_rendezvous_backend",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.elastic.rendezvous.c10d_rendezvous_backend",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.rendezvous.dynamic_rendezvous",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.elastic.rendezvous.dynamic_rendezvous",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.rendezvous.etcd_rendezvous",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.elastic.rendezvous.etcd_rendezvous",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.rendezvous.etcd_rendezvous_backend",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.elastic.rendezvous.etcd_rendezvous_backend",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.rendezvous.etcd_server",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.elastic.rendezvous.etcd_server",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.rendezvous.etcd_store",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.elastic.rendezvous.etcd_store",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.rendezvous.registry",
        "api_url": "https://pytorch.org/docs/stable/elastic/rendezvous.html#module-torch.distributed.elastic.rendezvous.registry",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.rendezvous.static_tcp_rendezvous",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.elastic.rendezvous.static_tcp_rendezvous",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.rendezvous.utils",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.elastic.rendezvous.utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.timer",
        "api_url": "https://pytorch.org/docs/stable/elastic/timer.html#module-torch.distributed.elastic.timer",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.timer.api",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.elastic.timer.api",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.timer.file_based_local_timer",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.elastic.timer.file_based_local_timer",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.timer.local_timer",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.elastic.timer.local_timer",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.utils",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.elastic.utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.utils.api",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.elastic.utils.api",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.utils.data",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.elastic.utils.data",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.utils.data.cycling_iterator",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.elastic.utils.data.cycling_iterator",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.utils.data.elastic_distributed_sampler",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.elastic.utils.data.elastic_distributed_sampler",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.utils.distributed",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.elastic.utils.distributed",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.utils.log_level",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.elastic.utils.log_level",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.utils.logging",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.elastic.utils.logging",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.utils.store",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.elastic.utils.store",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.fsdp",
        "api_url": "https://pytorch.org/docs/stable/fsdp.html#module-torch.distributed.fsdp",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.fsdp.api",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.fsdp.api",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.fsdp.fully_sharded_data_parallel",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.fsdp.fully_sharded_data_parallel",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.fsdp.sharded_grad_scaler",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.fsdp.sharded_grad_scaler",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.fsdp.wrap",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.fsdp.wrap",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.launch",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.launch",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.launcher",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.launcher",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.launcher.api",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.launcher.api",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.logging_handlers",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.logging_handlers",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.nn",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.nn",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.nn.api",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.nn.api",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.nn.api.remote_module",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.nn.api.remote_module",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.nn.functional",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.nn.functional",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.nn.jit",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.nn.jit",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.nn.jit.instantiator",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.nn.jit.instantiator",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.nn.jit.templates",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.nn.jit.templates",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.nn.jit.templates.remote_module_template",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.nn.jit.templates.remote_module_template",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.optim",
        "api_url": "https://pytorch.org/docs/stable/distributed.optim.html#module-torch.distributed.optim",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.optim.apply_optimizer_in_backward",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.optim.apply_optimizer_in_backward",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.optim.functional_adadelta",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.optim.functional_adadelta",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.optim.functional_adagrad",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.optim.functional_adagrad",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.optim.functional_adam",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.optim.functional_adam",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.optim.functional_adamax",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.optim.functional_adamax",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.optim.functional_adamw",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.optim.functional_adamw",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.optim.functional_rmsprop",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.optim.functional_rmsprop",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.optim.functional_rprop",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.optim.functional_rprop",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.optim.functional_sgd",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.optim.functional_sgd",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.optim.named_optimizer",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.optim.named_optimizer",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.optim.optimizer",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.optim.optimizer",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.optim.post_localSGD_optimizer",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.optim.post_localSGD_optimizer",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.optim.utils",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.optim.utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.optim.zero_redundancy_optimizer",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.optim.zero_redundancy_optimizer",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.pipeline",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.pipeline",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.pipeline.sync",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.pipeline.sync",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.pipeline.sync.batchnorm",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.pipeline.sync.batchnorm",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.pipeline.sync.checkpoint",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.pipeline.sync.checkpoint",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.pipeline.sync.copy",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.pipeline.sync.copy",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.pipeline.sync.dependency",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.pipeline.sync.dependency",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.pipeline.sync.microbatch",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.pipeline.sync.microbatch",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.pipeline.sync.phony",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.pipeline.sync.phony",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.pipeline.sync.pipe",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.pipeline.sync.pipe",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.pipeline.sync.pipeline",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.pipeline.sync.pipeline",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.pipeline.sync.skip",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.pipeline.sync.skip",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.pipeline.sync.skip.layout",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.pipeline.sync.skip.layout",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.pipeline.sync.skip.namespace",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.pipeline.sync.skip.namespace",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.pipeline.sync.skip.portal",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.pipeline.sync.skip.portal",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.pipeline.sync.skip.skippable",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.pipeline.sync.skip.skippable",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.pipeline.sync.skip.tracker",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.pipeline.sync.skip.tracker",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.pipeline.sync.stream",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.pipeline.sync.stream",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.pipeline.sync.utils",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.pipeline.sync.utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.pipeline.sync.worker",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.pipeline.sync.worker",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.remote_device",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.remote_device",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.rendezvous",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.rendezvous",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.rpc",
        "api_url": "https://pytorch.org/docs/stable/rpc.html#module-torch.distributed.rpc",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.rpc.api",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.rpc.api",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.rpc.backend_registry",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.rpc.backend_registry",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.rpc.constants",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.rpc.constants",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.rpc.functions",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.rpc.functions",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.rpc.internal",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.rpc.internal",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.rpc.options",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.rpc.options",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.rpc.rref_proxy",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.rpc.rref_proxy",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.rpc.server_process_global_profiler",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.rpc.server_process_global_profiler",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.run",
        "api_url": "https://pytorch.org/docs/stable/elastic/run.html#module-torch.distributed.run",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.tensor",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.tensor",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.tensor.parallel",
        "api_url": "https://pytorch.org/docs/stable/distributed.tensor.parallel.html#module-torch.distributed.tensor.parallel",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.tensor.parallel.api",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.tensor.parallel.api",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.tensor.parallel.ddp",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.tensor.parallel.ddp",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.tensor.parallel.fsdp",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.tensor.parallel.fsdp",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.tensor.parallel.input_reshard",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.tensor.parallel.input_reshard",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.tensor.parallel.loss",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.tensor.parallel.loss",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.tensor.parallel.style",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.tensor.parallel.style",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.utils",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#module-torch.distributions",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.bernoulli",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#module-torch.distributions.bernoulli",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.beta",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#module-torch.distributions.beta",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.binomial",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#module-torch.distributions.binomial",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.categorical",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#module-torch.distributions.categorical",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.cauchy",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#module-torch.distributions.cauchy",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.chi2",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#module-torch.distributions.chi2",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.constraint_registry",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#module-torch.distributions.constraint_registry",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.constraints",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#module-torch.distributions.constraints",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.continuous_bernoulli",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#module-torch.distributions.continuous_bernoulli",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.dirichlet",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#module-torch.distributions.dirichlet",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.distribution",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#module-torch.distributions.distribution",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.exp_family",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#module-torch.distributions.exp_family",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.exponential",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#module-torch.distributions.exponential",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.fishersnedecor",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#module-torch.distributions.fishersnedecor",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.gamma",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#module-torch.distributions.gamma",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.geometric",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#module-torch.distributions.geometric",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.gumbel",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#module-torch.distributions.gumbel",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.half_cauchy",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#module-torch.distributions.half_cauchy",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.half_normal",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#module-torch.distributions.half_normal",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.independent",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#module-torch.distributions.independent",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.inverse_gamma",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#module-torch.distributions.inverse_gamma",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.kl",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#module-torch.distributions.kl",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.kumaraswamy",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#module-torch.distributions.kumaraswamy",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.laplace",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#module-torch.distributions.laplace",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.lkj_cholesky",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#module-torch.distributions.lkj_cholesky",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.log_normal",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#module-torch.distributions.log_normal",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.logistic_normal",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#module-torch.distributions.logistic_normal",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.lowrank_multivariate_normal",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#module-torch.distributions.lowrank_multivariate_normal",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.mixture_same_family",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#module-torch.distributions.mixture_same_family",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.multinomial",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#module-torch.distributions.multinomial",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.multivariate_normal",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#module-torch.distributions.multivariate_normal",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.negative_binomial",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#module-torch.distributions.negative_binomial",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.normal",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#module-torch.distributions.normal",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.one_hot_categorical",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#module-torch.distributions.one_hot_categorical",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.pareto",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#module-torch.distributions.pareto",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.poisson",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#module-torch.distributions.poisson",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.relaxed_bernoulli",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#module-torch.distributions.relaxed_bernoulli",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.relaxed_categorical",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#module-torch.distributions.relaxed_categorical",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.studentT",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#module-torch.distributions.studentT",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.transformed_distribution",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#module-torch.distributions.transformed_distribution",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.transforms",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#module-torch.distributions.transforms",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.uniform",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#module-torch.distributions.uniform",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.utils",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#module-torch.distributions.utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.von_mises",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#module-torch.distributions.von_mises",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.weibull",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#module-torch.distributions.weibull",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.wishart",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#module-torch.distributions.wishart",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.export",
        "api_url": "https://pytorch.org/docs/stable/export.html#module-torch.export",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.export.custom_obj",
        "api_url": "https://pytorch.org/docs/stable/export.html#module-torch.export.custom_obj",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.export.dynamic_shapes",
        "api_url": "https://pytorch.org/docs/stable/export.html#module-torch.export.dynamic_shapes",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.export.exported_program",
        "api_url": "https://pytorch.org/docs/stable/export.html#module-torch.export.exported_program",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.export.graph_signature",
        "api_url": "https://pytorch.org/docs/stable/export.html#module-torch.export.graph_signature",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.export.unflatten",
        "api_url": "https://pytorch.org/docs/stable/export.html#module-torch.export.unflatten",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fft",
        "api_url": "https://pytorch.org/docs/stable/fft.html#module-torch.fft",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.func",
        "api_url": "https://pytorch.org/docs/stable/func.api.html#module-torch.func",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.functional",
        "api_url": "https://pytorch.org/docs/stable/torch.html#module-torch.functional",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.futures",
        "api_url": "https://pytorch.org/docs/stable/futures.html#module-torch.futures",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.annotate",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.annotate",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.config",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.config",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.experimental",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.accelerator_partitioner",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.experimental.accelerator_partitioner",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.const_fold",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.experimental.const_fold",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.debug",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.experimental.debug",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.graph_gradual_typechecker",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.experimental.graph_gradual_typechecker",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.merge_matmul",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.experimental.merge_matmul",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.meta_tracer",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.experimental.meta_tracer",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.migrate_gradual_types",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.experimental.migrate_gradual_types",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.migrate_gradual_types.constraint",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.experimental.migrate_gradual_types.constraint",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.migrate_gradual_types.constraint_generator",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.experimental.migrate_gradual_types.constraint_generator",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.migrate_gradual_types.constraint_transformation",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.experimental.migrate_gradual_types.constraint_transformation",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.migrate_gradual_types.operation",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.experimental.migrate_gradual_types.operation",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.migrate_gradual_types.transform_to_z3",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.experimental.migrate_gradual_types.transform_to_z3",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.migrate_gradual_types.util",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.experimental.migrate_gradual_types.util",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.migrate_gradual_types.z3_types",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.experimental.migrate_gradual_types.z3_types",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.normalize",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.experimental.normalize",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.optimization",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.experimental.optimization",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.partitioner_utils",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.experimental.partitioner_utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.proxy_tensor",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.experimental.proxy_tensor",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.recording",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.experimental.recording",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.refinement_types",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.experimental.refinement_types",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.rewriter",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.experimental.rewriter",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.schema_type_annotation",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.experimental.schema_type_annotation",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.sym_node",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.experimental.sym_node",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.symbolic_shapes",
        "api_url": "https://pytorch.org/docs/stable/fx.experimental.html#module-torch.fx.experimental.symbolic_shapes",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.unification",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.experimental.unification",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.unification.core",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.experimental.unification.core",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.unification.dispatch",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.experimental.unification.dispatch",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.unification.match",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.experimental.unification.match",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.unification.more",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.experimental.unification.more",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.unification.multipledispatch",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.experimental.unification.multipledispatch",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.unification.multipledispatch.conflict",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.experimental.unification.multipledispatch.conflict",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.unification.multipledispatch.core",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.experimental.unification.multipledispatch.core",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.unification.multipledispatch.dispatcher",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.experimental.unification.multipledispatch.dispatcher",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.unification.multipledispatch.utils",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.experimental.unification.multipledispatch.utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.unification.multipledispatch.variadic",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.experimental.unification.multipledispatch.variadic",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.unification.unification_tools",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.experimental.unification.unification_tools",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.unification.utils",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.experimental.unification.utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.unification.variable",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.experimental.unification.variable",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.unify_refinements",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.experimental.unify_refinements",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.validator",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.experimental.validator",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.graph",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.graph",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.graph_module",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.graph_module",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.immutable_collections",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.immutable_collections",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.interpreter",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.interpreter",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.node",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.node",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.operator_schemas",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.operator_schemas",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.passes",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.passes",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.passes.annotate_getitem_nodes",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.passes.annotate_getitem_nodes",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.passes.backends",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.passes.backends",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.passes.backends.cudagraphs",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.passes.backends.cudagraphs",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.passes.dialect",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.passes.dialect",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.passes.dialect.common",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.passes.dialect.common",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.passes.dialect.common.cse_pass",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.passes.dialect.common.cse_pass",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.passes.fake_tensor_prop",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.passes.fake_tensor_prop",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.passes.graph_drawer",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.passes.graph_drawer",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.passes.graph_manipulation",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.passes.graph_manipulation",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.passes.infra",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.passes.infra",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.passes.infra.partitioner",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.passes.infra.partitioner",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.passes.infra.pass_base",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.passes.infra.pass_base",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.passes.infra.pass_manager",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.passes.infra.pass_manager",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.passes.net_min_base",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.passes.net_min_base",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.passes.operator_support",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.passes.operator_support",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.passes.param_fetch",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.passes.param_fetch",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.passes.pass_manager",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.passes.pass_manager",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.passes.reinplace",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.passes.reinplace",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.passes.shape_prop",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.passes.shape_prop",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.passes.split_module",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.passes.split_module",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.passes.split_utils",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.passes.split_utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.passes.splitter_base",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.passes.splitter_base",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.passes.tests",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.passes.tests",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.passes.tests.test_pass_manager",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.passes.tests.test_pass_manager",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.passes.tools_common",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.passes.tools_common",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.passes.utils",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.passes.utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.passes.utils.common",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.passes.utils.common",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.passes.utils.fuser_utils",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.passes.utils.fuser_utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.passes.utils.matcher_utils",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.passes.utils.matcher_utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.passes.utils.matcher_with_name_node_map_utils",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.passes.utils.matcher_with_name_node_map_utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.passes.utils.source_matcher_utils",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.passes.utils.source_matcher_utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.proxy",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.proxy",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.subgraph_rewriter",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.subgraph_rewriter",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.tensor_type",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.tensor_type",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.traceback",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.traceback",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.hub",
        "api_url": "https://pytorch.org/docs/stable/hub.html#module-torch.hub",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.jit",
        "api_url": "https://pytorch.org/docs/stable/jit.html#module-torch.jit",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.jit.annotations",
        "api_url": "https://pytorch.org/docs/stable/jit.html#module-torch.jit.annotations",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.jit.frontend",
        "api_url": "https://pytorch.org/docs/stable/jit.html#module-torch.jit.frontend",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.jit.generate_bytecode",
        "api_url": "https://pytorch.org/docs/stable/jit.html#module-torch.jit.generate_bytecode",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.jit.mobile",
        "api_url": "https://pytorch.org/docs/stable/jit.html#module-torch.jit.mobile",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.jit.quantized",
        "api_url": "https://pytorch.org/docs/stable/jit.html#module-torch.jit.quantized",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.jit.supported_ops",
        "api_url": "https://pytorch.org/docs/stable/jit_builtin_functions.html#module-torch.jit.supported_ops",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.jit.unsupported_tensor_ops",
        "api_url": "https://pytorch.org/docs/stable/jit_unsupported.html#module-torch.jit.unsupported_tensor_ops",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.library",
        "api_url": "https://pytorch.org/docs/stable/library.html#module-torch.library",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.linalg",
        "api_url": "https://pytorch.org/docs/stable/linalg.html#module-torch.linalg",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.masked",
        "api_url": "https://pytorch.org/docs/stable/masked.html#module-torch.masked",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.masked.maskedtensor",
        "api_url": "https://pytorch.org/docs/stable/masked.html#module-torch.masked.maskedtensor",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.masked.maskedtensor.binary",
        "api_url": "https://pytorch.org/docs/stable/masked.html#module-torch.masked.maskedtensor.binary",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.masked.maskedtensor.core",
        "api_url": "https://pytorch.org/docs/stable/masked.html#module-torch.masked.maskedtensor.core",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.masked.maskedtensor.creation",
        "api_url": "https://pytorch.org/docs/stable/masked.html#module-torch.masked.maskedtensor.creation",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.masked.maskedtensor.passthrough",
        "api_url": "https://pytorch.org/docs/stable/masked.html#module-torch.masked.maskedtensor.passthrough",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.masked.maskedtensor.reductions",
        "api_url": "https://pytorch.org/docs/stable/masked.html#module-torch.masked.maskedtensor.reductions",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.masked.maskedtensor.unary",
        "api_url": "https://pytorch.org/docs/stable/masked.html#module-torch.masked.maskedtensor.unary",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.monitor",
        "api_url": "https://pytorch.org/docs/stable/monitor.html#module-torch.monitor",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.mps",
        "api_url": "https://pytorch.org/docs/stable/mps.html#module-torch.mps",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.mps.event",
        "api_url": "https://pytorch.org/docs/stable/mps.html#module-torch.mps.event",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.mps.profiler",
        "api_url": "https://pytorch.org/docs/stable/mps.html#module-torch.mps.profiler",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.multiprocessing",
        "api_url": "https://pytorch.org/docs/stable/multiprocessing.html#module-torch.multiprocessing",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.multiprocessing.pool",
        "api_url": "https://pytorch.org/docs/stable/multiprocessing.html#module-torch.multiprocessing.pool",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.multiprocessing.queue",
        "api_url": "https://pytorch.org/docs/stable/multiprocessing.html#module-torch.multiprocessing.queue",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.multiprocessing.reductions",
        "api_url": "https://pytorch.org/docs/stable/multiprocessing.html#module-torch.multiprocessing.reductions",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.multiprocessing.spawn",
        "api_url": "https://pytorch.org/docs/stable/multiprocessing.html#module-torch.multiprocessing.spawn",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nested",
        "api_url": "https://pytorch.org/docs/stable/nested.html#module-torch.nested",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.attention",
        "api_url": "https://pytorch.org/docs/stable/nn.attention.html#module-torch.nn.attention",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.attention.bias",
        "api_url": "https://pytorch.org/docs/stable/nn.attention.bias.html#module-torch.nn.attention.bias",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.backends",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn.backends",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.backends.thnn",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn.backends.thnn",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.common_types",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn.common_types",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.cpp",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn.cpp",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.functional",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn.functional",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.grad",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn.grad",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.init",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn.init",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.intrinsic",
        "api_url": "https://pytorch.org/docs/stable/quantization-support.html#module-torch.nn.intrinsic",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.intrinsic.modules",
        "api_url": "https://pytorch.org/docs/stable/quantization-support.html#module-torch.nn.intrinsic.modules",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.intrinsic.modules.fused",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.nn.intrinsic.modules.fused",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.intrinsic.qat",
        "api_url": "https://pytorch.org/docs/stable/quantization-support.html#module-torch.nn.intrinsic.qat",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.intrinsic.qat.modules",
        "api_url": "https://pytorch.org/docs/stable/quantization-support.html#module-torch.nn.intrinsic.qat.modules",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.intrinsic.qat.modules.conv_fused",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.nn.intrinsic.qat.modules.conv_fused",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.intrinsic.qat.modules.linear_fused",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.nn.intrinsic.qat.modules.linear_fused",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.intrinsic.qat.modules.linear_relu",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.nn.intrinsic.qat.modules.linear_relu",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.intrinsic.quantized",
        "api_url": "https://pytorch.org/docs/stable/quantization-support.html#module-torch.nn.intrinsic.quantized",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.intrinsic.quantized.dynamic",
        "api_url": "https://pytorch.org/docs/stable/quantization-support.html#module-torch.nn.intrinsic.quantized.dynamic",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.intrinsic.quantized.dynamic.modules",
        "api_url": "https://pytorch.org/docs/stable/quantization-support.html#module-torch.nn.intrinsic.quantized.dynamic.modules",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.intrinsic.quantized.dynamic.modules.linear_relu",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.nn.intrinsic.quantized.dynamic.modules.linear_relu",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.intrinsic.quantized.modules",
        "api_url": "https://pytorch.org/docs/stable/quantization-support.html#module-torch.nn.intrinsic.quantized.modules",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.intrinsic.quantized.modules.bn_relu",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.nn.intrinsic.quantized.modules.bn_relu",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.intrinsic.quantized.modules.conv_relu",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.nn.intrinsic.quantized.modules.conv_relu",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.intrinsic.quantized.modules.linear_relu",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.nn.intrinsic.quantized.modules.linear_relu",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.modules",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn.modules",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.modules.activation",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn.modules.activation",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.modules.adaptive",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn.modules.adaptive",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.modules.batchnorm",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn.modules.batchnorm",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.modules.channelshuffle",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn.modules.channelshuffle",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.modules.container",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn.modules.container",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.modules.conv",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn.modules.conv",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.modules.distance",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn.modules.distance",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.modules.dropout",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn.modules.dropout",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.modules.flatten",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn.modules.flatten",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.modules.fold",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn.modules.fold",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.modules.instancenorm",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn.modules.instancenorm",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.modules.lazy",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn.modules.lazy",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.modules.linear",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn.modules.linear",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.modules.loss",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn.modules.loss",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.modules.module",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn.modules.module",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.modules.normalization",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn.modules.normalization",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.modules.padding",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn.modules.padding",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.modules.pixelshuffle",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn.modules.pixelshuffle",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.modules.pooling",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn.modules.pooling",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.modules.rnn",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn.modules.rnn",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.modules.sparse",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn.modules.sparse",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.modules.transformer",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn.modules.transformer",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.modules.upsampling",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn.modules.upsampling",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.modules.utils",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn.modules.utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.parallel",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn.parallel",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.parallel.comm",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn.parallel.comm",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.parallel.distributed",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn.parallel.distributed",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.parallel.parallel_apply",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn.parallel.parallel_apply",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.parallel.replicate",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn.parallel.replicate",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.parallel.scatter_gather",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn.parallel.scatter_gather",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.parameter",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn.parameter",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.qat",
        "api_url": "https://pytorch.org/docs/stable/quantization-support.html#module-torch.nn.qat",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.qat.dynamic",
        "api_url": "https://pytorch.org/docs/stable/quantization-support.html#module-torch.nn.qat.dynamic",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.qat.dynamic.modules",
        "api_url": "https://pytorch.org/docs/stable/quantization-support.html#module-torch.nn.qat.dynamic.modules",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.qat.dynamic.modules.linear",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.nn.qat.dynamic.modules.linear",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.qat.modules",
        "api_url": "https://pytorch.org/docs/stable/quantization-support.html#module-torch.nn.qat.modules",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.qat.modules.conv",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.nn.qat.modules.conv",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.qat.modules.embedding_ops",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.nn.qat.modules.embedding_ops",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.qat.modules.linear",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.nn.qat.modules.linear",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.quantizable",
        "api_url": "https://pytorch.org/docs/stable/quantization-support.html#module-torch.nn.quantizable",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.quantizable.modules",
        "api_url": "https://pytorch.org/docs/stable/quantization-support.html#module-torch.nn.quantizable.modules",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.quantizable.modules.activation",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.nn.quantizable.modules.activation",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.quantizable.modules.rnn",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.nn.quantizable.modules.rnn",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.quantized",
        "api_url": "https://pytorch.org/docs/stable/quantization-support.html#module-torch.nn.quantized",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.quantized.dynamic",
        "api_url": "https://pytorch.org/docs/stable/quantization-support.html#module-torch.nn.quantized.dynamic",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.quantized.dynamic.modules",
        "api_url": "https://pytorch.org/docs/stable/quantization-support.html#module-torch.nn.quantized.dynamic.modules",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.quantized.dynamic.modules.conv",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.nn.quantized.dynamic.modules.conv",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.quantized.dynamic.modules.linear",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.nn.quantized.dynamic.modules.linear",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.quantized.dynamic.modules.rnn",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.nn.quantized.dynamic.modules.rnn",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.quantized.functional",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.nn.quantized.functional",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.quantized.modules",
        "api_url": "https://pytorch.org/docs/stable/quantization-support.html#module-torch.nn.quantized.modules",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.quantized.modules.activation",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.nn.quantized.modules.activation",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.quantized.modules.batchnorm",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.nn.quantized.modules.batchnorm",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.quantized.modules.conv",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.nn.quantized.modules.conv",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.quantized.modules.dropout",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.nn.quantized.modules.dropout",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.quantized.modules.embedding_ops",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.nn.quantized.modules.embedding_ops",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.quantized.modules.functional_modules",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.nn.quantized.modules.functional_modules",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.quantized.modules.linear",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.nn.quantized.modules.linear",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.quantized.modules.normalization",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.nn.quantized.modules.normalization",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.quantized.modules.rnn",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.nn.quantized.modules.rnn",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.quantized.modules.utils",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.nn.quantized.modules.utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.utils",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn.utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.utils.clip_grad",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn.utils.clip_grad",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.utils.convert_parameters",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn.utils.convert_parameters",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.utils.fusion",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn.utils.fusion",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.utils.init",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn.utils.init",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.utils.memory_format",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn.utils.memory_format",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.utils.parametrizations",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn.utils.parametrizations",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.utils.parametrize",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn.utils.parametrize",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.utils.prune",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn.utils.prune",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.utils.rnn",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn.utils.rnn",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.utils.stateless",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn.utils.stateless",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.onnx",
        "api_url": "https://pytorch.org/docs/stable/onnx_torchscript.html#module-torch.onnx",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.onnx.errors",
        "api_url": "https://pytorch.org/docs/stable/onnx.html#module-torch.onnx.errors",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.onnx.operators",
        "api_url": "https://pytorch.org/docs/stable/onnx.html#module-torch.onnx.operators",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.onnx.symbolic_caffe2",
        "api_url": "https://pytorch.org/docs/stable/onnx.html#module-torch.onnx.symbolic_caffe2",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.onnx.symbolic_helper",
        "api_url": "https://pytorch.org/docs/stable/onnx.html#module-torch.onnx.symbolic_helper",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.onnx.symbolic_opset10",
        "api_url": "https://pytorch.org/docs/stable/onnx.html#module-torch.onnx.symbolic_opset10",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.onnx.symbolic_opset11",
        "api_url": "https://pytorch.org/docs/stable/onnx.html#module-torch.onnx.symbolic_opset11",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.onnx.symbolic_opset12",
        "api_url": "https://pytorch.org/docs/stable/onnx.html#module-torch.onnx.symbolic_opset12",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.onnx.symbolic_opset13",
        "api_url": "https://pytorch.org/docs/stable/onnx.html#module-torch.onnx.symbolic_opset13",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.onnx.symbolic_opset14",
        "api_url": "https://pytorch.org/docs/stable/onnx.html#module-torch.onnx.symbolic_opset14",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.onnx.symbolic_opset15",
        "api_url": "https://pytorch.org/docs/stable/onnx.html#module-torch.onnx.symbolic_opset15",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.onnx.symbolic_opset16",
        "api_url": "https://pytorch.org/docs/stable/onnx.html#module-torch.onnx.symbolic_opset16",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.onnx.symbolic_opset17",
        "api_url": "https://pytorch.org/docs/stable/onnx.html#module-torch.onnx.symbolic_opset17",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.onnx.symbolic_opset18",
        "api_url": "https://pytorch.org/docs/stable/onnx.html#module-torch.onnx.symbolic_opset18",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.onnx.symbolic_opset7",
        "api_url": "https://pytorch.org/docs/stable/onnx.html#module-torch.onnx.symbolic_opset7",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.onnx.symbolic_opset8",
        "api_url": "https://pytorch.org/docs/stable/onnx.html#module-torch.onnx.symbolic_opset8",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.onnx.symbolic_opset9",
        "api_url": "https://pytorch.org/docs/stable/onnx.html#module-torch.onnx.symbolic_opset9",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.onnx.utils",
        "api_url": "https://pytorch.org/docs/stable/onnx.html#module-torch.onnx.utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.onnx.verification",
        "api_url": "https://pytorch.org/docs/stable/onnx.html#module-torch.onnx.verification",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim",
        "api_url": "https://pytorch.org/docs/stable/optim.html#module-torch.optim",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.adadelta",
        "api_url": "https://pytorch.org/docs/stable/optim.html#module-torch.optim.adadelta",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.adagrad",
        "api_url": "https://pytorch.org/docs/stable/optim.html#module-torch.optim.adagrad",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.adam",
        "api_url": "https://pytorch.org/docs/stable/optim.html#module-torch.optim.adam",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.adamax",
        "api_url": "https://pytorch.org/docs/stable/optim.html#module-torch.optim.adamax",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.adamw",
        "api_url": "https://pytorch.org/docs/stable/optim.html#module-torch.optim.adamw",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.asgd",
        "api_url": "https://pytorch.org/docs/stable/optim.html#module-torch.optim.asgd",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.lbfgs",
        "api_url": "https://pytorch.org/docs/stable/optim.html#module-torch.optim.lbfgs",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.lr_scheduler",
        "api_url": "https://pytorch.org/docs/stable/optim.html#module-torch.optim.lr_scheduler",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.nadam",
        "api_url": "https://pytorch.org/docs/stable/optim.html#module-torch.optim.nadam",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.optimizer",
        "api_url": "https://pytorch.org/docs/stable/optim.html#module-torch.optim.optimizer",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.radam",
        "api_url": "https://pytorch.org/docs/stable/optim.html#module-torch.optim.radam",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.rmsprop",
        "api_url": "https://pytorch.org/docs/stable/optim.html#module-torch.optim.rmsprop",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.rprop",
        "api_url": "https://pytorch.org/docs/stable/optim.html#module-torch.optim.rprop",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.sgd",
        "api_url": "https://pytorch.org/docs/stable/optim.html#module-torch.optim.sgd",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.sparse_adam",
        "api_url": "https://pytorch.org/docs/stable/optim.html#module-torch.optim.sparse_adam",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.swa_utils",
        "api_url": "https://pytorch.org/docs/stable/optim.html#module-torch.optim.swa_utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.overrides",
        "api_url": "https://pytorch.org/docs/stable/torch.overrides.html#module-torch.overrides",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.package",
        "api_url": "https://pytorch.org/docs/stable/package.html#module-torch.package",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.package.analyze",
        "api_url": "https://pytorch.org/docs/stable/package.html#module-torch.package.analyze",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.package.analyze.find_first_use_of_broken_modules",
        "api_url": "https://pytorch.org/docs/stable/package.html#module-torch.package.analyze.find_first_use_of_broken_modules",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.package.analyze.is_from_package",
        "api_url": "https://pytorch.org/docs/stable/package.html#module-torch.package.analyze.is_from_package",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.package.analyze.trace_dependencies",
        "api_url": "https://pytorch.org/docs/stable/package.html#module-torch.package.analyze.trace_dependencies",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.package.file_structure_representation",
        "api_url": "https://pytorch.org/docs/stable/package.html#module-torch.package.file_structure_representation",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.package.find_file_dependencies",
        "api_url": "https://pytorch.org/docs/stable/package.html#module-torch.package.find_file_dependencies",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.package.glob_group",
        "api_url": "https://pytorch.org/docs/stable/package.html#module-torch.package.glob_group",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.package.importer",
        "api_url": "https://pytorch.org/docs/stable/package.html#module-torch.package.importer",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.package.package_exporter",
        "api_url": "https://pytorch.org/docs/stable/package.html#module-torch.package.package_exporter",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.package.package_importer",
        "api_url": "https://pytorch.org/docs/stable/package.html#module-torch.package.package_importer",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.profiler",
        "api_url": "https://pytorch.org/docs/stable/profiler.html#module-torch.profiler",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.profiler.itt",
        "api_url": "https://pytorch.org/docs/stable/profiler.html#module-torch.profiler.itt",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.profiler.profiler",
        "api_url": "https://pytorch.org/docs/stable/profiler.html#module-torch.profiler.profiler",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.profiler.python_tracer",
        "api_url": "https://pytorch.org/docs/stable/profiler.html#module-torch.profiler.python_tracer",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.quantization",
        "api_url": "https://pytorch.org/docs/stable/quantization-support.html#module-torch.quantization",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.quantization.fake_quantize",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.quantization.fake_quantize",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.quantization.fuse_modules",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.quantization.fuse_modules",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.quantization.fuser_method_mappings",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.quantization.fuser_method_mappings",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.quantization.fx",
        "api_url": "https://pytorch.org/docs/stable/quantization-support.html#module-torch.quantization.fx",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.quantization.fx.convert",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.quantization.fx.convert",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.quantization.fx.fuse",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.quantization.fx.fuse",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.quantization.fx.fusion_patterns",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.quantization.fx.fusion_patterns",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.quantization.fx.graph_module",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.quantization.fx.graph_module",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.quantization.fx.match_utils",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.quantization.fx.match_utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.quantization.fx.pattern_utils",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.quantization.fx.pattern_utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.quantization.fx.prepare",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.quantization.fx.prepare",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.quantization.fx.quantization_patterns",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.quantization.fx.quantization_patterns",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.quantization.fx.quantization_types",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.quantization.fx.quantization_types",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.quantization.fx.utils",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.quantization.fx.utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.quantization.observer",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.quantization.observer",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.quantization.qconfig",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.quantization.qconfig",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.quantization.quant_type",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.quantization.quant_type",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.quantization.quantization_mappings",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.quantization.quantization_mappings",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.quantization.quantize",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.quantization.quantize",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.quantization.quantize_fx",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.quantization.quantize_fx",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.quantization.quantize_jit",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.quantization.quantize_jit",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.quantization.stubs",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.quantization.stubs",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.quantization.utils",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.quantization.utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.quasirandom",
        "api_url": "https://pytorch.org/docs/stable/torch.html#module-torch.quasirandom",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.random",
        "api_url": "https://pytorch.org/docs/stable/random.html#module-torch.random",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.return_types",
        "api_url": "https://pytorch.org/docs/stable/torch.html#module-torch.return_types",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.serialization",
        "api_url": "https://pytorch.org/docs/stable/torch.html#module-torch.serialization",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.signal",
        "api_url": "https://pytorch.org/docs/stable/signal.html#module-torch.signal",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.signal.windows",
        "api_url": "https://pytorch.org/docs/stable/signal.html#module-torch.signal.windows",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.signal.windows.windows",
        "api_url": "https://pytorch.org/docs/stable/torch.html#module-torch.signal.windows.windows",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.sparse",
        "api_url": "https://pytorch.org/docs/stable/sparse.html#module-torch.sparse",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.sparse.semi_structured",
        "api_url": "https://pytorch.org/docs/stable/torch.html#module-torch.sparse.semi_structured",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.special",
        "api_url": "https://pytorch.org/docs/stable/special.html#module-torch.special",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.storage",
        "api_url": "https://pytorch.org/docs/stable/torch.html#module-torch.storage",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.testing",
        "api_url": "https://pytorch.org/docs/stable/testing.html#module-torch.testing",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.torch_version",
        "api_url": "https://pytorch.org/docs/stable/torch.html#module-torch.torch_version",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.types",
        "api_url": "https://pytorch.org/docs/stable/torch.html#module-torch.types",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.backcompat",
        "api_url": "https://pytorch.org/docs/stable/torch.html#module-torch.utils.backcompat",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.backend_registration",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.backend_registration",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.benchmark",
        "api_url": "https://pytorch.org/docs/stable/benchmark_utils.html#module-torch.utils.benchmark",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.benchmark.examples",
        "api_url": "https://pytorch.org/docs/stable/benchmark_utils.html#module-torch.utils.benchmark.examples",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.benchmark.examples.blas_compare_setup",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.benchmark.examples.blas_compare_setup",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.benchmark.examples.compare",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.benchmark.examples.compare",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.benchmark.examples.fuzzer",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.benchmark.examples.fuzzer",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.benchmark.examples.op_benchmark",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.benchmark.examples.op_benchmark",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.benchmark.examples.simple_timeit",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.benchmark.examples.simple_timeit",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.benchmark.examples.spectral_ops_fuzz_test",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.benchmark.examples.spectral_ops_fuzz_test",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.benchmark.op_fuzzers",
        "api_url": "https://pytorch.org/docs/stable/benchmark_utils.html#module-torch.utils.benchmark.op_fuzzers",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.benchmark.op_fuzzers.binary",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.benchmark.op_fuzzers.binary",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.benchmark.op_fuzzers.sparse_binary",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.benchmark.op_fuzzers.sparse_binary",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.benchmark.op_fuzzers.sparse_unary",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.benchmark.op_fuzzers.sparse_unary",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.benchmark.op_fuzzers.spectral",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.benchmark.op_fuzzers.spectral",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.benchmark.op_fuzzers.unary",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.benchmark.op_fuzzers.unary",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.benchmark.utils",
        "api_url": "https://pytorch.org/docs/stable/benchmark_utils.html#module-torch.utils.benchmark.utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.benchmark.utils.common",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.benchmark.utils.common",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.benchmark.utils.compare",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.benchmark.utils.compare",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.benchmark.utils.compile",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.benchmark.utils.compile",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.benchmark.utils.cpp_jit",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.benchmark.utils.cpp_jit",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.benchmark.utils.fuzzer",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.benchmark.utils.fuzzer",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.benchmark.utils.sparse_fuzzer",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.benchmark.utils.sparse_fuzzer",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.benchmark.utils.timer",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.benchmark.utils.timer",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.benchmark.utils.valgrind_wrapper",
        "api_url": "https://pytorch.org/docs/stable/benchmark_utils.html#module-torch.utils.benchmark.utils.valgrind_wrapper",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.benchmark.utils.valgrind_wrapper.timer_interface",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.benchmark.utils.valgrind_wrapper.timer_interface",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.bottleneck",
        "api_url": "https://pytorch.org/docs/stable/bottleneck.html#module-torch.utils.bottleneck",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.bundled_inputs",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.bundled_inputs",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.checkpoint",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.checkpoint",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.collect_env",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.collect_env",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.cpp_backtrace",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.cpp_backtrace",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.cpp_extension",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.cpp_extension",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.data",
        "api_url": "https://pytorch.org/docs/stable/data.html#module-torch.utils.data",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.data.backward_compatibility",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.data.backward_compatibility",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.data.dataloader",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.data.dataloader",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.data.datapipes",
        "api_url": "https://pytorch.org/docs/stable/data.html#module-torch.utils.data.datapipes",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.data.datapipes.dataframe",
        "api_url": "https://pytorch.org/docs/stable/data.html#module-torch.utils.data.datapipes.dataframe",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.data.datapipes.dataframe.dataframe_wrapper",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.data.datapipes.dataframe.dataframe_wrapper",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.data.datapipes.dataframe.dataframes",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.data.datapipes.dataframe.dataframes",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.data.datapipes.dataframe.datapipes",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.data.datapipes.dataframe.datapipes",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.data.datapipes.dataframe.structures",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.data.datapipes.dataframe.structures",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.data.datapipes.datapipe",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.data.datapipes.datapipe",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.data.datapipes.gen_pyi",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.data.datapipes.gen_pyi",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.data.datapipes.iter",
        "api_url": "https://pytorch.org/docs/stable/data.html#module-torch.utils.data.datapipes.iter",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.data.datapipes.iter.callable",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.data.datapipes.iter.callable",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.data.datapipes.iter.combinatorics",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.data.datapipes.iter.combinatorics",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.data.datapipes.iter.combining",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.data.datapipes.iter.combining",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.data.datapipes.iter.filelister",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.data.datapipes.iter.filelister",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.data.datapipes.iter.fileopener",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.data.datapipes.iter.fileopener",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.data.datapipes.iter.grouping",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.data.datapipes.iter.grouping",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.data.datapipes.iter.routeddecoder",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.data.datapipes.iter.routeddecoder",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.data.datapipes.iter.selecting",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.data.datapipes.iter.selecting",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.data.datapipes.iter.sharding",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.data.datapipes.iter.sharding",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.data.datapipes.iter.streamreader",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.data.datapipes.iter.streamreader",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.data.datapipes.iter.utils",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.data.datapipes.iter.utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.data.datapipes.map",
        "api_url": "https://pytorch.org/docs/stable/data.html#module-torch.utils.data.datapipes.map",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.data.datapipes.map.callable",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.data.datapipes.map.callable",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.data.datapipes.map.combinatorics",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.data.datapipes.map.combinatorics",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.data.datapipes.map.combining",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.data.datapipes.map.combining",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.data.datapipes.map.grouping",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.data.datapipes.map.grouping",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.data.datapipes.map.utils",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.data.datapipes.map.utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.data.datapipes.utils",
        "api_url": "https://pytorch.org/docs/stable/data.html#module-torch.utils.data.datapipes.utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.data.datapipes.utils.common",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.data.datapipes.utils.common",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.data.datapipes.utils.decoder",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.data.datapipes.utils.decoder",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.data.datapipes.utils.snapshot",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.data.datapipes.utils.snapshot",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.data.dataset",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.data.dataset",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.data.distributed",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.data.distributed",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.data.graph",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.data.graph",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.data.graph_settings",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.data.graph_settings",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.data.sampler",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.data.sampler",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.deterministic",
        "api_url": "https://pytorch.org/docs/stable/deterministic.html#module-torch.utils.deterministic",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.dlpack",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.dlpack",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.file_baton",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.file_baton",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.flop_counter",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.flop_counter",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.hipify",
        "api_url": "https://pytorch.org/docs/stable/torch.html#module-torch.utils.hipify",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.hipify.constants",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.hipify.constants",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.hipify.cuda_to_hip_mappings",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.hipify.cuda_to_hip_mappings",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.hipify.hipify_python",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.hipify.hipify_python",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.hipify.version",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.hipify.version",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.hooks",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.hooks",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.jit",
        "api_url": "https://pytorch.org/docs/stable/jit_utils.html#module-torch.utils.jit",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.jit.log_extract",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.jit.log_extract",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.mkldnn",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.mkldnn",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.mobile_optimizer",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.mobile_optimizer",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.model_dump",
        "api_url": "https://pytorch.org/docs/stable/torch.html#module-torch.utils.model_dump",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.model_zoo",
        "api_url": "https://pytorch.org/docs/stable/model_zoo.html#module-torch.utils.model_zoo",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.show_pickle",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.show_pickle",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.tensorboard",
        "api_url": "https://pytorch.org/docs/stable/tensorboard.html#module-torch.utils.tensorboard",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.tensorboard.summary",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.tensorboard.summary",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.tensorboard.writer",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.tensorboard.writer",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.throughput_benchmark",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.throughput_benchmark",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.viz",
        "api_url": "https://pytorch.org/docs/stable/torch.html#module-torch.utils.viz",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.weak",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.weak",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.version",
        "api_url": "https://pytorch.org/docs/stable/torch.html#module-torch.version",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.xpu",
        "api_url": "https://pytorch.org/docs/stable/xpu.html#module-torch.xpu",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.xpu.random",
        "api_url": "https://pytorch.org/docs/stable/xpu.html#module-torch.xpu.random",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.xpu.streams",
        "api_url": "https://pytorch.org/docs/stable/xpu.html#module-torch.xpu.streams",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.Module",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module",
        "api_signature": "torch.nn.Module(*args, **kwargs)",
        "api_description": "Base class for all neural network modules.",
        "return_value": "self\nself\nself\nself\nself\nself\nself\nThe buffer referenced by target\nAny extra state to store in the module’s state_dict\nThe Parameter referenced by target\nThe submodule referenced by target\nself\nself\n\nmissing_keys is a list of str containing the missing keys\nunexpected_keys is a list of str containing the unexpected keys\n\n\na handle that can be used to remove the added hook by calling\nhandle.remove()\na handle that can be used to remove the added hook by calling\nhandle.remove()\na handle that can be used to remove the added hook by calling\nhandle.remove()\na handle that can be used to remove the added hook by calling\nhandle.remove()\na handle that can be used to remove the added hook by calling\nhandle.remove()\na handle that can be used to remove the added hook by calling\nhandle.remove()\nself\na dictionary containing a whole state of the module\nself\nself\nself\nself\nself\n",
        "parameters": "name (str) – name of the child module. The child module can be\naccessed from this module using the given name\nmodule (Module) – child module to be added to the module.\nfn (Module -> None) – function to be applied to each submodule\nrecurse (bool) – if True, then yields buffers of this module\nand all submodules. Otherwise, yields only buffers that\nare direct members of this module.\ndevice (int, optional) – if specified, all parameters will be\ncopied to that device\ntarget (str) – The fully-qualified string name of the buffer\nto look for. (See get_submodule for how to specify a\nfully-qualified string.)\ntarget (str) – The fully-qualified string name of the Parameter\nto look for. (See get_submodule for how to specify a\nfully-qualified string.)\ntarget (str) – The fully-qualified string name of the submodule\nto look for. (See above example for how to specify a\nfully-qualified string.)\ndevice (int, optional) – if specified, all parameters will be\ncopied to that device\nstate_dict (dict) – a dict containing parameters and\npersistent buffers.\nstrict (bool, optional) – whether to strictly enforce that the keys\nin state_dict match the keys returned by this module’s\nstate_dict() function. Default: True\nassign (bool, optional) – When False, the properties of the tensors\nin the current module are preserved while when True, the\nproperties of the Tensors in the state dict are preserved. The only\nexception is the requires_grad field of\nDefault: ``False`\nprefix (str) – prefix to prepend to all buffer names.\nrecurse (bool, optional) – if True, then yields buffers of this module\nand all submodules. Otherwise, yields only buffers that\nare direct members of this module. Defaults to True.\nremove_duplicate (bool, optional) – whether to remove the duplicated buffers in the result. Defaults to True.\nmemo (Optional[Set[Module]]) – a memo to store the set of modules already added to the result\nprefix (str) – a prefix that will be added to the name of the module\nremove_duplicate (bool) – whether to remove the duplicated module instances in the result\nor not\nprefix (str) – prefix to prepend to all parameter names.\nrecurse (bool) – if True, then yields parameters of this module\nand all submodules. Otherwise, yields only parameters that\nare direct members of this module.\nremove_duplicate (bool, optional) – whether to remove the duplicated\nparameters in the result. Defaults to True.\nrecurse (bool) – if True, then yields parameters of this module\nand all submodules. Otherwise, yields only parameters that\nare direct members of this module.\nname (str) – name of the buffer. The buffer can be accessed\nfrom this module using the given name\ntensor (Tensor or None) – buffer to be registered. If None, then operations\nthat run on buffers, such as cuda, are ignored. If None,\nthe buffer is not included in the module’s state_dict.\npersistent (bool) – whether the buffer is part of this module’s\nstate_dict.\nhook (Callable) – The user defined hook to be registered.\nprepend (bool) – If True, the provided hook will be fired\nbefore all existing forward hooks on this\ntorch.nn.modules.Module. Otherwise, the provided\nhook will be fired after all existing forward hooks on\nthis torch.nn.modules.Module. Note that global\nforward hooks registered with\nregister_module_forward_hook() will fire before all hooks\nregistered by this method.\nDefault: False\nwith_kwargs (bool) – If True, the hook will be passed the\nkwargs given to the forward function.\nDefault: False\nalways_call (bool) – If True the hook will be run regardless of\nwhether an exception is raised while calling the Module.\nDefault: False\nhook (Callable) – The user defined hook to be registered.\nprepend (bool) – If true, the provided hook will be fired before\nall existing forward_pre hooks on this\ntorch.nn.modules.Module. Otherwise, the provided\nhook will be fired after all existing forward_pre hooks\non this torch.nn.modules.Module. Note that global\nforward_pre hooks registered with\nregister_module_forward_pre_hook() will fire before all\nhooks registered by this method.\nDefault: False\nwith_kwargs (bool) – If true, the hook will be passed the kwargs\ngiven to the forward function.\nDefault: False\nhook (Callable) – The user-defined hook to be registered.\nprepend (bool) – If true, the provided hook will be fired before\nall existing backward hooks on this\ntorch.nn.modules.Module. Otherwise, the provided\nhook will be fired after all existing backward hooks on\nthis torch.nn.modules.Module. Note that global\nbackward hooks registered with\nregister_module_full_backward_hook() will fire before\nall hooks registered by this method.\nhook (Callable) – The user-defined hook to be registered.\nprepend (bool) – If true, the provided hook will be fired before\nall existing backward_pre hooks on this\ntorch.nn.modules.Module. Otherwise, the provided\nhook will be fired after all existing backward_pre hooks\non this torch.nn.modules.Module. Note that global\nbackward_pre hooks registered with\nregister_module_full_backward_pre_hook() will fire before\nall hooks registered by this method.\nname (str) – name of the parameter. The parameter can be accessed\nfrom this module using the given name\nparam (Parameter or None) – parameter to be added to the module. If\nNone, then operations that run on parameters, such as cuda,\nare ignored. If None, the parameter is not included in the\nmodule’s state_dict.\nrequires_grad (bool) – whether autograd should record operations on\nparameters in this module. Default: True.\nstate (dict) – Extra state from the state_dict\ndestination (dict, optional) – If provided, the state of module will\nbe updated into the dict and the same object is returned.\nOtherwise, an OrderedDict will be created and returned.\nDefault: None.\nprefix (str, optional) – a prefix added to parameter and buffer\nnames to compose the keys in state_dict. Default: ''.\nkeep_vars (bool, optional) – by default the Tensor s\nreturned in the state dict are detached from autograd. If it’s\nset to True, detaching will not be performed.\nDefault: False.\ndevice (torch.device) – the desired device of the parameters\nand buffers in this module\ndtype (torch.dtype) – the desired floating point or complex dtype of\nthe parameters and buffers in this module\ntensor (torch.Tensor) – Tensor whose dtype and device are the desired\ndtype and device for all parameters and buffers in this module\nmemory_format (torch.memory_format) – the desired memory\nformat for 4D parameters and buffers in this module (keyword\nonly argument)\ndevice (torch.device) – The desired device of the parameters\nand buffers in this module.\nrecurse (bool) – Whether parameters and buffers of submodules should\nbe recursively moved to the specified device.\nmode (bool) – whether to set training mode (True) or evaluation\nmode (False). Default: True.\ndst_type (type or string) – the desired type\ndevice (int, optional) – if specified, all parameters will be\ncopied to that device\nset_to_none (bool) – instead of setting to zero, set the grads to None.\nSee torch.optim.Optimizer.zero_grad() for details.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.fsdp.FullyShardedDataParallel.module",
        "api_url": "https://pytorch.org/docs/stable/fsdp.html#torch.distributed.fsdp.FullyShardedDataParallel.module",
        "api_signature": null,
        "api_description": "Return the wrapped module.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.export.ExportedProgram.module",
        "api_url": "https://pytorch.org/docs/stable/export.html#torch.export.ExportedProgram.module",
        "api_signature": "module()",
        "api_description": "Returns a self contained GraphModule with all the parameters/buffers inlined.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.module_load",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.module_load.html#torch.Tensor.module_load",
        "api_signature": "Tensor.module_load(other, assign=False)",
        "api_description": "Defines how to transform other when loading it into self in load_state_dict().",
        "return_value": "",
        "parameters": "other (Tensor) – value in state dict with key corresponding to self\nassign (bool) – the assign argument passed to nn.Module.load_state_dict()",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.export.ModuleCallEntry",
        "api_url": "https://pytorch.org/docs/stable/export.html#torch.export.ModuleCallEntry",
        "api_signature": "torch.export.ModuleCallEntry(fqn: str, signature: Union[torch.export.exported_program.ModuleCallSignature, NoneType] = None)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.export.ModuleCallSignature",
        "api_url": "https://pytorch.org/docs/stable/export.html#torch.export.ModuleCallSignature",
        "api_signature": "torch.export.ModuleCallSignature(inputs: List[Union[torch.export.graph_signature.TensorArgument, torch.export.graph_signature.SymIntArgument, torch.export.graph_signature.ConstantArgument, torch.export.graph_signature.CustomObjArgument]], outputs: List[Union[torch.export.graph_signature.TensorArgument, torch.export.graph_signature.SymIntArgument, torch.export.graph_signature.ConstantArgument, torch.export.graph_signature.CustomObjArgument]], in_spec: torch.utils._pytree.TreeSpec, out_spec: torch.utils._pytree.TreeSpec)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.ModuleDict",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.ModuleDict.html#torch.nn.ModuleDict",
        "api_signature": "torch.nn.ModuleDict(modules=None)",
        "api_description": "Holds submodules in a dictionary.",
        "return_value": "",
        "parameters": "modules (iterable, optional) – a mapping (dictionary) of (string: module)\nor an iterable of key-value pairs of type (string, module)\nkey (str) – key to pop from the ModuleDict\nmodules (iterable) – a mapping (dictionary) from string to Module,\nor an iterable of key-value pairs of type (string, Module)",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.ModuleList",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.ModuleList.html#torch.nn.ModuleList",
        "api_signature": "torch.nn.ModuleList(modules=None)",
        "api_description": "Holds submodules in a list.",
        "return_value": "",
        "parameters": "modules (iterable, optional) – an iterable of modules to add\nmodule (nn.Module) – module to append\nmodules (iterable) – iterable of modules to append\nindex (int) – index to insert.\nmodule (nn.Module) – module to insert",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.jit.ScriptModule.modules",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.jit.ScriptModule.html#torch.jit.ScriptModule.modules",
        "api_signature": "modules()",
        "api_description": "Return an iterator over all modules in the network.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.Module.modules",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.modules",
        "api_signature": "modules()",
        "api_description": "Return an iterator over all modules in the network.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.monitored_barrier",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#torch.distributed.monitored_barrier",
        "api_signature": "torch.distributed.monitored_barrier(group=None, timeout=None, wait_all_ranks=False)",
        "api_description": "Synchronize processes similar to torch.distributed.barrier, but consider a configurable timeout.",
        "return_value": "None.\n",
        "parameters": "group (ProcessGroup, optional) – The process group to work on. If\nNone, the default process group will be used.\ntimeout (datetime.timedelta, optional) – Timeout for monitored_barrier.\nIf None, the default process group timeout will be used.\nwait_all_ranks (bool, optional) – Whether to collect all failed ranks or\nnot. By default, this is False and monitored_barrier on rank 0\nwill throw on the first failed rank it encounters in order to fail\nfast. By setting wait_all_ranks=True monitored_barrier will\ncollect all failed ranks and throw an error containing information\nabout all failed ranks.",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> # Note: Process group initialization omitted on each rank.\n>>> import torch.distributed as dist\n>>> if dist.get_rank() != 1:\n>>>     dist.monitored_barrier() # Raises exception indicating that\n>>> # rank 1 did not call into monitored_barrier.\n>>> # Example with wait_all_ranks=True\n>>> if dist.get_rank() == 0:\n>>>     dist.monitored_barrier(wait_all_ranks=True) # Raises exception\n>>> # indicating that ranks 1, 2, ... world_size - 1 did not call into\n>>> # monitored_barrier.\n\n\n"
    },
    {
        "api_name": "torch.moveaxis",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.moveaxis.html#torch.moveaxis",
        "api_signature": "torch.moveaxis(input, source, destination)",
        "api_description": "Alias for torch.movedim().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.moveaxis",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.moveaxis.html#torch.Tensor.moveaxis",
        "api_signature": "Tensor.moveaxis(source, destination)",
        "api_description": "See torch.moveaxis()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.movedim",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.movedim.html#torch.movedim",
        "api_signature": "torch.movedim(input, source, destination)",
        "api_description": "Moves the dimension(s) of input at the position(s) in source\nto the position(s) in destination.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.\nsource (int or tuple of ints) – Original positions of the dims to move. These must be unique.\ndestination (int or tuple of ints) – Destination positions for each of the original dims. These must also be unique.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.movedim",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.movedim.html#torch.Tensor.movedim",
        "api_signature": "Tensor.movedim(source, destination)",
        "api_description": "See torch.movedim()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.observer.MovingAverageMinMaxObserver",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.quantization.observer.MovingAverageMinMaxObserver.html#torch.ao.quantization.observer.MovingAverageMinMaxObserver",
        "api_signature": "torch.ao.quantization.observer.MovingAverageMinMaxObserver(averaging_constant=0.01, dtype=torch.quint8, qscheme=torch.per_tensor_affine, reduce_range=False, quant_min=None, quant_max=None, eps=1.1920928955078125e-07, is_dynamic=False, **kwargs)",
        "api_description": "Observer module for computing the quantization parameters based on the\nmoving average of the min and max values.",
        "return_value": "",
        "parameters": "averaging_constant – Averaging constant for min/max.\ndtype – dtype argument to the quantize node needed to implement the\nreference model spec.\nqscheme – Quantization scheme to be used\nreduce_range – Reduces the range of the quantized data type by 1 bit\nquant_min – Minimum quantization value. If unspecified, it will follow the 8-bit setup.\nquant_max – Maximum quantization value. If unspecified, it will follow the 8-bit setup.\neps (Tensor) – Epsilon value for float32, Defaults to torch.finfo(torch.float32).eps.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver.html#torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver",
        "api_signature": "torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver(averaging_constant=0.01, ch_axis=0, dtype=torch.quint8, qscheme=torch.per_channel_affine, reduce_range=False, quant_min=None, quant_max=None, eps=1.1920928955078125e-07, is_dynamic=False, **kwargs)",
        "api_description": "Observer module for computing the quantization parameters based on the\nrunning per channel min and max values.",
        "return_value": "",
        "parameters": "averaging_constant – Averaging constant for min/max.\nch_axis – Channel axis\ndtype – Quantized data type\nqscheme – Quantization scheme to be used\nreduce_range – Reduces the range of the quantized data type by 1 bit\nquant_min – Minimum quantization value. If unspecified, it will follow the 8-bit setup.\nquant_max – Maximum quantization value. If unspecified, it will follow the 8-bit setup.\neps (Tensor) – Epsilon value for float32, Defaults to torch.finfo(torch.float32).eps.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.UntypedStorage.mps",
        "api_url": "https://pytorch.org/docs/stable/storage.html#torch.UntypedStorage.mps",
        "api_signature": "mps()",
        "api_description": "Return a MPS copy of this storage if it’s not already on the MPS.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.functional.mse_loss",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.functional.mse_loss.html#torch.nn.functional.mse_loss",
        "api_signature": "torch.nn.functional.mse_loss(input, target, size_average=None, reduce=None, reduction='mean')",
        "api_description": "Measures the element-wise mean squared error.\nSee MSELoss for details.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.MSELoss",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html#torch.nn.MSELoss",
        "api_signature": "torch.nn.MSELoss(size_average=None, reduce=None, reduction='mean')",
        "api_description": "Creates a criterion that measures the mean squared error (squared L2 norm) between\neach element in the input xxx and target yyy.",
        "return_value": "",
        "parameters": "size_average (bool, optional) – Deprecated (see reduction). By default,\nthe losses are averaged over each loss element in the batch. Note that for\nsome losses, there are multiple elements per sample. If the field size_average\nis set to False, the losses are instead summed for each minibatch. Ignored\nwhen reduce is False. Default: True\nreduce (bool, optional) – Deprecated (see reduction). By default, the\nlosses are averaged or summed over observations for each minibatch depending\non size_average. When reduce is False, returns a loss per\nbatch element instead and ignores size_average. Default: True\nreduction (str, optional) – Specifies the reduction to apply to the output:\n'none' | 'mean' | 'sum'. 'none': no reduction will be applied,\n'mean': the sum of the output will be divided by the number of\nelements in the output, 'sum': the output will be summed. Note: size_average\nand reduce are in the process of being deprecated, and in the meantime,\nspecifying either of those two args will override reduction. Default: 'mean'",
        "input_shape": "\nInput: (∗)(*)(∗), where ∗*∗ means any number of dimensions.\nTarget: (∗)(*)(∗), same shape as the input.\n\n",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.msort",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.msort.html#torch.msort",
        "api_signature": "torch.msort(input, *, out=None)",
        "api_description": "Sorts the elements of the input tensor along its first dimension\nin ascending order by value.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.msort",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.msort.html#torch.Tensor.msort",
        "api_signature": "Tensor.msort()",
        "api_description": "See torch.msort()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.mT",
        "api_url": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor.mT",
        "api_signature": null,
        "api_description": "Returns a view of this tensor with the last two dimensions transposed.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.mul",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.mul.html#torch.mul",
        "api_signature": "torch.mul(input, other, *, out=None)",
        "api_description": "Multiplies input by other.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.\nother (Tensor or Number) –\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.ns._numeric_suite.Shadow.mul",
        "api_url": "https://pytorch.org/docs/stable/torch.ao.ns._numeric_suite.html#torch.ao.ns._numeric_suite.Shadow.mul",
        "api_signature": "mul(x, y)",
        "api_description": "Tensor",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.mul",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.mul.html#torch.Tensor.mul",
        "api_signature": "Tensor.mul(value)",
        "api_description": "See torch.mul().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.mul_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.mul_.html#torch.Tensor.mul_",
        "api_signature": "Tensor.mul_(value)",
        "api_description": "In-place version of mul().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.ns._numeric_suite.Shadow.mul_scalar",
        "api_url": "https://pytorch.org/docs/stable/torch.ao.ns._numeric_suite.html#torch.ao.ns._numeric_suite.Shadow.mul_scalar",
        "api_signature": "mul_scalar(x, y)",
        "api_description": "Tensor",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.linalg.multi_dot",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.linalg.multi_dot.html#torch.linalg.multi_dot",
        "api_signature": "torch.linalg.multi_dot(tensors, *, out=None)",
        "api_description": "Efficiently multiplies two or more matrices by reordering the multiplications so that\nthe fewest arithmetic operations are performed.",
        "return_value": "",
        "parameters": "tensors (Sequence[Tensor]) – two or more tensors to multiply. The first and last\ntensors may be 1D or 2D. Every other tensor must be 2D.\nout (Tensor, optional) – output tensor. Ignored if None. Default: None.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.functional.multi_margin_loss",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.functional.multi_margin_loss.html#torch.nn.functional.multi_margin_loss",
        "api_signature": "torch.nn.functional.multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None, reduce=None, reduction='mean')",
        "api_description": "See MultiMarginLoss for details.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.special.multigammaln",
        "api_url": "https://pytorch.org/docs/stable/special.html#torch.special.multigammaln",
        "api_signature": "torch.special.multigammaln(input, p, *, out=None)",
        "api_description": "Computes the multivariate log-gamma function with dimension\nppp element-wise, given by",
        "return_value": "",
        "parameters": "input (Tensor) – the tensor to compute the multivariate log-gamma function\np (int) – the number of dimensions\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.quantizable.MultiheadAttention",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.nn.quantizable.MultiheadAttention.html#torch.ao.nn.quantizable.MultiheadAttention",
        "api_signature": "torch.ao.nn.quantizable.MultiheadAttention(embed_dim, num_heads, dropout=0.0, bias=True, add_bias_kv=False, add_zero_attn=False, kdim=None, vdim=None, batch_first=False, device=None, dtype=None)",
        "api_description": "Utility to convert the quantized MHA back to float.",
        "return_value": "",
        "parameters": "query (Tensor) – map a query and a set of key-value pairs to an output.\nSee “Attention Is All You Need” for more details.\nkey (Tensor) – map a query and a set of key-value pairs to an output.\nSee “Attention Is All You Need” for more details.\nvalue (Tensor) – map a query and a set of key-value pairs to an output.\nSee “Attention Is All You Need” for more details.\nkey_padding_mask (Optional[Tensor]) – if provided, specified padding elements in the key will\nbe ignored by the attention. When given a binary mask and a value is True,\nthe corresponding value on the attention layer will be ignored.\nneed_weights (bool) – output attn_output_weights.\nattn_mask (Optional[Tensor]) – 2D or 3D mask that prevents attention to certain positions. A 2D mask will be broadcasted for all\nthe batches while a 3D mask allows to specify a different mask for the entries of each batch.",
        "input_shape": "\nInputs:\nquery: (L,N,E)(L, N, E)(L,N,E) where L is the target sequence length, N is the batch size, E is\nthe embedding dimension. (N,L,E)(N, L, E)(N,L,E) if batch_first is True.\nkey: (S,N,E)(S, N, E)(S,N,E), where S is the source sequence length, N is the batch size, E is\nthe embedding dimension. (N,S,E)(N, S, E)(N,S,E) if batch_first is True.\nvalue: (S,N,E)(S, N, E)(S,N,E) where S is the source sequence length, N is the batch size, E is\nthe embedding dimension. (N,S,E)(N, S, E)(N,S,E) if batch_first is True.\nkey_padding_mask: (N,S)(N, S)(N,S) where N is the batch size, S is the source sequence length.\nIf a BoolTensor is provided, the positions with the\nvalue of True will be ignored while the position with the value of False will be unchanged.\nattn_mask: 2D mask (L,S)(L, S)(L,S) where L is the target sequence length, S is the source sequence length.\n3D mask (N∗numheads,L,S)(N*num_heads, L, S)(N∗numh​eads,L,S) where N is the batch size, L is the target sequence length,\nS is the source sequence length. attn_mask ensure that position i is allowed to attend the unmasked\npositions. If a BoolTensor is provided, positions with True\nis not allowed to attend while False values will be unchanged. If a FloatTensor\nis provided, it will be added to the attention weight.\nis_causal: If specified, applies a causal mask as attention mask. Mutually exclusive with providing attn_mask.\nDefault: False.\naverage_attn_weights: If true, indicates that the returned attn_weights should be averaged across\nheads. Otherwise, attn_weights are provided separately per head. Note that this flag only has an\neffect when need_weights=True.. Default: True (i.e. average weights across heads)\nOutputs:\nattn_output: (L,N,E)(L, N, E)(L,N,E) where L is the target sequence length, N is the batch size,\nE is the embedding dimension. (N,L,E)(N, L, E)(N,L,E) if batch_first is True.\nattn_output_weights: If average_attn_weights=True, returns attention weights averaged\nacross heads of shape (N,L,S)(N, L, S)(N,L,S), where N is the batch size, L is the target sequence length,\nS is the source sequence length. If average_attn_weights=False, returns attention weights per\nhead of shape (N,numheads,L,S)(N, num_heads, L, S)(N,numh​eads,L,S).\n\n",
        "notes": "Please, refer to forward() for more\ninformation\n",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.MultiheadAttention",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html#torch.nn.MultiheadAttention",
        "api_signature": "torch.nn.MultiheadAttention(embed_dim, num_heads, dropout=0.0, bias=True, add_bias_kv=False, add_zero_attn=False, kdim=None, vdim=None, batch_first=False, device=None, dtype=None)",
        "api_description": "Allows the model to jointly attend to information from different representation subspaces.",
        "return_value": "merged mask\nmask_type: merged mask type (0, 1, or 2)\n",
        "parameters": "embed_dim – Total dimension of the model.\nnum_heads – Number of parallel attention heads. Note that embed_dim will be split\nacross num_heads (i.e. each head will have dimension embed_dim // num_heads).\ndropout – Dropout probability on attn_output_weights. Default: 0.0 (no dropout).\nbias – If specified, adds bias to input / output projection layers. Default: True.\nadd_bias_kv – If specified, adds bias to the key and value sequences at dim=0. Default: False.\nadd_zero_attn – If specified, adds a new batch of zeros to the key and value sequences at dim=1.\nDefault: False.\nkdim – Total number of features for keys. Default: None (uses kdim=embed_dim).\nvdim – Total number of features for values. Default: None (uses vdim=embed_dim).\nbatch_first – If True, then the input and output tensors are provided\nas (batch, seq, feature). Default: False (seq, batch, feature).\nquery (Tensor) – Query embeddings of shape (L,Eq)(L, E_q)(L,Eq​) for unbatched input, (L,N,Eq)(L, N, E_q)(L,N,Eq​) when batch_first=False\nor (N,L,Eq)(N, L, E_q)(N,L,Eq​) when batch_first=True, where LLL is the target sequence length,\nNNN is the batch size, and EqE_qEq​ is the query embedding dimension embed_dim.\nQueries are compared against key-value pairs to produce the output.\nSee “Attention Is All You Need” for more details.\nkey (Tensor) – Key embeddings of shape (S,Ek)(S, E_k)(S,Ek​) for unbatched input, (S,N,Ek)(S, N, E_k)(S,N,Ek​) when batch_first=False\nor (N,S,Ek)(N, S, E_k)(N,S,Ek​) when batch_first=True, where SSS is the source sequence length,\nNNN is the batch size, and EkE_kEk​ is the key embedding dimension kdim.\nSee “Attention Is All You Need” for more details.\nvalue (Tensor) – Value embeddings of shape (S,Ev)(S, E_v)(S,Ev​) for unbatched input, (S,N,Ev)(S, N, E_v)(S,N,Ev​) when\nbatch_first=False or (N,S,Ev)(N, S, E_v)(N,S,Ev​) when batch_first=True, where SSS is the source\nsequence length, NNN is the batch size, and EvE_vEv​ is the value embedding dimension vdim.\nSee “Attention Is All You Need” for more details.\nkey_padding_mask (Optional[Tensor]) – If specified, a mask of shape (N,S)(N, S)(N,S) indicating which elements within key\nto ignore for the purpose of attention (i.e. treat as “padding”). For unbatched query, shape should be (S)(S)(S).\nBinary and float masks are supported.\nFor a binary mask, a True value indicates that the corresponding key value will be ignored for\nthe purpose of attention. For a float mask, it will be directly added to the corresponding key value.\nneed_weights (bool) – If specified, returns attn_output_weights in addition to attn_outputs.\nSet need_weights=False to use the optimized scaled_dot_product_attention\nand achieve the best performance for MHA.\nDefault: True.\nattn_mask (Optional[Tensor]) – If specified, a 2D or 3D mask preventing attention to certain positions. Must be of shape\n(L,S)(L, S)(L,S) or (N⋅num_heads,L,S)(N\\cdot\\text{num\\_heads}, L, S)(N⋅num_heads,L,S), where NNN is the batch size,\nLLL is the target sequence length, and SSS is the source sequence length. A 2D mask will be\nbroadcasted across the batch while a 3D mask allows for a different mask for each entry in the batch.\nBinary and float masks are supported. For a binary mask, a True value indicates that the\ncorresponding position is not allowed to attend. For a float mask, the mask values will be added to\nthe attention weight.\nIf both attn_mask and key_padding_mask are supplied, their types should match.\naverage_attn_weights (bool) – If true, indicates that the returned attn_weights should be averaged across\nheads. Otherwise, attn_weights are provided separately per head. Note that this flag only has an\neffect when need_weights=True. Default: True (i.e. average weights across heads)\nis_causal (bool) – If specified, applies a causal mask as attention mask.\nDefault: False.\nWarning:\nis_causal provides a hint that attn_mask is the\ncausal mask. Providing incorrect hints can result in\nincorrect execution, including forward and backward\ncompatibility.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.functional.multilabel_margin_loss",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.functional.multilabel_margin_loss.html#torch.nn.functional.multilabel_margin_loss",
        "api_signature": "torch.nn.functional.multilabel_margin_loss(input, target, size_average=None, reduce=None, reduction='mean')",
        "api_description": "See MultiLabelMarginLoss for details.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.functional.multilabel_soft_margin_loss",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.functional.multilabel_soft_margin_loss.html#torch.nn.functional.multilabel_soft_margin_loss",
        "api_signature": "torch.nn.functional.multilabel_soft_margin_loss(input, target, weight=None, size_average=None, reduce=None, reduction='mean')",
        "api_description": "See MultiLabelSoftMarginLoss for details.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.MultiLabelMarginLoss",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.MultiLabelMarginLoss.html#torch.nn.MultiLabelMarginLoss",
        "api_signature": "torch.nn.MultiLabelMarginLoss(size_average=None, reduce=None, reduction='mean')",
        "api_description": "Creates a criterion that optimizes a multi-class multi-classification\nhinge loss (margin-based loss) between input xxx (a 2D mini-batch Tensor)\nand output yyy (which is a 2D Tensor of target class indices).\nFor each sample in the mini-batch:",
        "return_value": "",
        "parameters": "size_average (bool, optional) – Deprecated (see reduction). By default,\nthe losses are averaged over each loss element in the batch. Note that for\nsome losses, there are multiple elements per sample. If the field size_average\nis set to False, the losses are instead summed for each minibatch. Ignored\nwhen reduce is False. Default: True\nreduce (bool, optional) – Deprecated (see reduction). By default, the\nlosses are averaged or summed over observations for each minibatch depending\non size_average. When reduce is False, returns a loss per\nbatch element instead and ignores size_average. Default: True\nreduction (str, optional) – Specifies the reduction to apply to the output:\n'none' | 'mean' | 'sum'. 'none': no reduction will be applied,\n'mean': the sum of the output will be divided by the number of\nelements in the output, 'sum': the output will be summed. Note: size_average\nand reduce are in the process of being deprecated, and in the meantime,\nspecifying either of those two args will override reduction. Default: 'mean'",
        "input_shape": "\nInput: (C)(C)(C) or (N,C)(N, C)(N,C) where N is the batch size and C\nis the number of classes.\nTarget: (C)(C)(C) or (N,C)(N, C)(N,C), label targets padded by -1 ensuring same shape as the input.\nOutput: scalar. If reduction is 'none', then (N)(N)(N).\n\n",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.MultiLabelSoftMarginLoss",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.MultiLabelSoftMarginLoss.html#torch.nn.MultiLabelSoftMarginLoss",
        "api_signature": "torch.nn.MultiLabelSoftMarginLoss(weight=None, size_average=None, reduce=None, reduction='mean')",
        "api_description": "Creates a criterion that optimizes a multi-label one-versus-all\nloss based on max-entropy, between input xxx and target yyy of size\n(N,C)(N, C)(N,C).\nFor each sample in the minibatch:",
        "return_value": "",
        "parameters": "weight (Tensor, optional) – a manual rescaling weight given to each\nclass. If given, it has to be a Tensor of size C. Otherwise, it is\ntreated as if having all ones.\nsize_average (bool, optional) – Deprecated (see reduction). By default,\nthe losses are averaged over each loss element in the batch. Note that for\nsome losses, there are multiple elements per sample. If the field size_average\nis set to False, the losses are instead summed for each minibatch. Ignored\nwhen reduce is False. Default: True\nreduce (bool, optional) – Deprecated (see reduction). By default, the\nlosses are averaged or summed over observations for each minibatch depending\non size_average. When reduce is False, returns a loss per\nbatch element instead and ignores size_average. Default: True\nreduction (str, optional) – Specifies the reduction to apply to the output:\n'none' | 'mean' | 'sum'. 'none': no reduction will be applied,\n'mean': the sum of the output will be divided by the number of\nelements in the output, 'sum': the output will be summed. Note: size_average\nand reduce are in the process of being deprecated, and in the meantime,\nspecifying either of those two args will override reduction. Default: 'mean'",
        "input_shape": "\nInput: (N,C)(N, C)(N,C) where N is the batch size and C is the number of classes.\nTarget: (N,C)(N, C)(N,C), label targets must have the same shape as the input.\nOutput: scalar. If reduction is 'none', then (N)(N)(N).\n\n",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.MultiMarginLoss",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.MultiMarginLoss.html#torch.nn.MultiMarginLoss",
        "api_signature": "torch.nn.MultiMarginLoss(p=1, margin=1.0, weight=None, size_average=None, reduce=None, reduction='mean')",
        "api_description": "Creates a criterion that optimizes a multi-class classification hinge\nloss (margin-based loss) between input xxx (a 2D mini-batch Tensor) and\noutput yyy (which is a 1D tensor of target class indices,\n0≤y≤x.size(1)−10 \\leq y \\leq \\text{x.size}(1)-10≤y≤x.size(1)−1):",
        "return_value": "",
        "parameters": "p (int, optional) – Has a default value of 111. 111 and 222\nare the only supported values.\nmargin (float, optional) – Has a default value of 111.\nweight (Tensor, optional) – a manual rescaling weight given to each\nclass. If given, it has to be a Tensor of size C. Otherwise, it is\ntreated as if having all ones.\nsize_average (bool, optional) – Deprecated (see reduction). By default,\nthe losses are averaged over each loss element in the batch. Note that for\nsome losses, there are multiple elements per sample. If the field size_average\nis set to False, the losses are instead summed for each minibatch. Ignored\nwhen reduce is False. Default: True\nreduce (bool, optional) – Deprecated (see reduction). By default, the\nlosses are averaged or summed over observations for each minibatch depending\non size_average. When reduce is False, returns a loss per\nbatch element instead and ignores size_average. Default: True\nreduction (str, optional) – Specifies the reduction to apply to the output:\n'none' | 'mean' | 'sum'. 'none': no reduction will be applied,\n'mean': the sum of the output will be divided by the number of\nelements in the output, 'sum': the output will be summed. Note: size_average\nand reduce are in the process of being deprecated, and in the meantime,\nspecifying either of those two args will override reduction. Default: 'mean'",
        "input_shape": "\nInput: (N,C)(N, C)(N,C) or (C)(C)(C), where NNN is the batch size and CCC is the number of classes.\nTarget: (N)(N)(N) or ()()(), where each value is 0≤targets[i]≤C−10 \\leq \\text{targets}[i] \\leq C-10≤targets[i]≤C−1.\nOutput: scalar. If reduction is 'none', then same shape as the target.\n\n",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.multinomial.Multinomial",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.multinomial.Multinomial",
        "api_signature": "torch.distributions.multinomial.Multinomial(total_count=1, probs=None, logits=None, validate_args=None)",
        "api_description": "Bases: Distribution",
        "return_value": "",
        "parameters": "total_count (int) – number of trials\nprobs (Tensor) – event probabilities\nlogits (Tensor) – event log probabilities (unnormalized)",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.constraints.multinomial",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.constraints.multinomial",
        "api_signature": null,
        "api_description": "alias of _Multinomial",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.multinomial",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.multinomial.html#torch.multinomial",
        "api_signature": "torch.multinomial(input, num_samples, replacement=False, *, generator=None, out=None)",
        "api_description": "Returns a tensor where each row contains num_samples indices sampled\nfrom the multinomial (a stricter definition would be multivariate,\nrefer to torch.distributions.multinomial.Multinomial for more details)\nprobability distribution located in the corresponding row\nof tensor input.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor containing probabilities\nnum_samples (int) – number of samples to draw\nreplacement (bool, optional) – whether to draw with replacement or not\ngenerator (torch.Generator, optional) – a pseudorandom number generator for sampling\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.multinomial",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.multinomial.html#torch.Tensor.multinomial",
        "api_signature": "Tensor.multinomial(num_samples, replacement=False, *, generator=None)",
        "api_description": "See torch.multinomial()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.lr_scheduler.MultiplicativeLR",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.MultiplicativeLR.html#torch.optim.lr_scheduler.MultiplicativeLR",
        "api_signature": "torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda, last_epoch=-1, verbose='deprecated')",
        "api_description": "Multiply the learning rate of each parameter group by the factor given\nin the specified function. When last_epoch=-1, sets initial lr as lr.",
        "return_value": "",
        "parameters": "optimizer (Optimizer) – Wrapped optimizer.\nlr_lambda (function or list) – A function which computes a multiplicative\nfactor given an integer parameter epoch, or a list of such\nfunctions, one for each group in optimizer.param_groups.\nlast_epoch (int) – The index of last epoch. Default: -1.\nverbose (bool) – If True, prints a message to stdout for\neach update. Default: False.\nDeprecated since version 2.2: verbose is deprecated. Please use get_last_lr() to access the\nlearning rate.\nstate_dict (dict) – scheduler state. Should be an object returned\nfrom a call to state_dict().",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.multiply",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.multiply.html#torch.multiply",
        "api_signature": "torch.multiply(input, other, *, out=None)",
        "api_description": "Alias for torch.mul().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.multiply",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.multiply.html#torch.Tensor.multiply",
        "api_signature": "Tensor.multiply(value)",
        "api_description": "See torch.multiply().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.multiply_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.multiply_.html#torch.Tensor.multiply_",
        "api_signature": "Tensor.multiply_(value)",
        "api_description": "In-place version of multiply().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.multiprocessing.api.MultiprocessContext",
        "api_url": "https://pytorch.org/docs/stable/elastic/multiprocessing.html#torch.distributed.elastic.multiprocessing.api.MultiprocessContext",
        "api_signature": "torch.distributed.elastic.multiprocessing.api.MultiprocessContext(name, entrypoint, args, envs, start_method, logs_specs, log_line_prefixes=None)",
        "api_description": "PContext holding worker processes invoked as a function.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.lr_scheduler.MultiStepLR",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.MultiStepLR.html#torch.optim.lr_scheduler.MultiStepLR",
        "api_signature": "torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones, gamma=0.1, last_epoch=-1, verbose='deprecated')",
        "api_description": "Decays the learning rate of each parameter group by gamma once the\nnumber of epoch reaches one of the milestones. Notice that such decay can\nhappen simultaneously with other changes to the learning rate from outside\nthis scheduler. When last_epoch=-1, sets initial lr as lr.",
        "return_value": "",
        "parameters": "optimizer (Optimizer) – Wrapped optimizer.\nmilestones (list) – List of epoch indices. Must be increasing.\ngamma (float) – Multiplicative factor of learning rate decay.\nDefault: 0.1.\nlast_epoch (int) – The index of last epoch. Default: -1.\nverbose (bool) – If True, prints a message to stdout for\neach update. Default: False.\nDeprecated since version 2.2: verbose is deprecated. Please use get_last_lr() to access the\nlearning rate.\nstate_dict (dict) – scheduler state. Should be an object returned\nfrom a call to state_dict().",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.multivariate_normal.MultivariateNormal",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.multivariate_normal.MultivariateNormal",
        "api_signature": "torch.distributions.multivariate_normal.MultivariateNormal(loc, covariance_matrix=None, precision_matrix=None, scale_tril=None, validate_args=None)",
        "api_description": "Bases: Distribution",
        "return_value": "",
        "parameters": "loc (Tensor) – mean of the distribution\ncovariance_matrix (Tensor) – positive-definite covariance matrix\nprecision_matrix (Tensor) – positive-definite precision matrix\nscale_tril (Tensor) – lower-triangular factor of covariance, with positive-valued diagonal",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.mv",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.mv.html#torch.mv",
        "api_signature": "torch.mv(input, vec, *, out=None)",
        "api_description": "Performs a matrix-vector product of the matrix input and the vector\nvec.",
        "return_value": "",
        "parameters": "input (Tensor) – matrix to be multiplied\nvec (Tensor) – vector to be multiplied\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.mv",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.mv.html#torch.Tensor.mv",
        "api_signature": "Tensor.mv(vec)",
        "api_description": "See torch.mv()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.mvlgamma",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.mvlgamma.html#torch.mvlgamma",
        "api_signature": "torch.mvlgamma(input, p, *, out=None)",
        "api_description": "Alias for torch.special.multigammaln().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.mvlgamma",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.mvlgamma.html#torch.Tensor.mvlgamma",
        "api_signature": "Tensor.mvlgamma(p)",
        "api_description": "See torch.mvlgamma()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.mvlgamma_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.mvlgamma_.html#torch.Tensor.mvlgamma_",
        "api_signature": "Tensor.mvlgamma_(p)",
        "api_description": "In-place version of mvlgamma()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.NAdam",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.NAdam.html#torch.optim.NAdam",
        "api_signature": "torch.optim.NAdam(params, lr=0.002, betas=(0.9, 0.999)",
        "api_description": "Implements NAdam algorithm.",
        "return_value": "a handle that can be used to remove the added hook by calling\nhandle.remove()\na handle that can be used to remove the added hook by calling\nhandle.remove()\na handle that can be used to remove the added hook by calling\nhandle.remove()\na handle that can be used to remove the added hook by calling\nhandle.remove()\na handle that can be used to remove the added hook by calling\nhandle.remove()\na handle that can be used to remove the added hook by calling\nhandle.remove()\n",
        "parameters": "params (iterable) – iterable of parameters to optimize or dicts defining\nparameter groups\nlr (float, optional) – learning rate (default: 2e-3)\nbetas (Tuple[float, float], optional) – coefficients used for computing\nrunning averages of gradient and its square (default: (0.9, 0.999))\neps (float, optional) – term added to the denominator to improve\nnumerical stability (default: 1e-8)\nweight_decay (float, optional) – weight decay (L2 penalty) (default: 0)\nmomentum_decay (float, optional) – momentum momentum_decay (default: 4e-3)\ndecoupled_weight_decay (bool, optional) – whether to use decoupled weight\ndecay as in AdamW to obtain NAdamW (default: False)\nforeach (bool, optional) – whether foreach implementation of optimizer\nis used. If unspecified by the user (so foreach is None), we will try to use\nforeach over the for-loop implementation on CUDA, since it is usually\nsignificantly more performant. Note that the foreach implementation uses\n~ sizeof(params) more peak memory than the for-loop version due to the intermediates\nbeing a tensorlist vs just one tensor. If memory is prohibitive, batch fewer\nparameters through the optimizer at a time or switch this flag to False (default: None)\ncapturable (bool, optional) – whether this instance is safe to\ncapture in a CUDA graph. Passing True can impair ungraphed performance,\nso if you don’t intend to graph capture this instance, leave it False\n(default: False)\ndifferentiable (bool, optional) – whether autograd should\noccur through the optimizer step in training. Otherwise, the step()\nfunction runs in a torch.no_grad() context. Setting to True can impair\nperformance, so leave it False if you don’t intend to run autograd\nthrough this instance (default: False)\nparam_group (dict) – Specifies what Tensors should be optimized along with group\nspecific optimization options.\nstate_dict (dict) – optimizer state. Should be an object returned\nfrom a call to state_dict().\nhook (Callable) – The user defined hook to be registered.\nprepend (bool) – If True, the provided post hook will be fired before\nall the already registered post-hooks on load_state_dict. Otherwise,\nthe provided hook will be fired after all the already registered\npost-hooks. (default: False)\nhook (Callable) – The user defined hook to be registered.\nprepend (bool) – If True, the provided pre hook will be fired before\nall the already registered pre-hooks on load_state_dict. Otherwise,\nthe provided hook will be fired after all the already registered\npre-hooks. (default: False)\nhook (Callable) – The user defined hook to be registered.\nprepend (bool) – If True, the provided post hook will be fired before\nall the already registered post-hooks on state_dict. Otherwise,\nthe provided hook will be fired after all the already registered\npost-hooks. (default: False)\nhook (Callable) – The user defined hook to be registered.\nprepend (bool) – If True, the provided pre hook will be fired before\nall the already registered pre-hooks on state_dict. Otherwise,\nthe provided hook will be fired after all the already registered\npre-hooks. (default: False)\nhook (Callable) – The user defined hook to be registered.\nhook (Callable) – The user defined hook to be registered.\nclosure (Callable, optional) – A closure that reevaluates the model\nand returns the loss.\nset_to_none (bool) – instead of setting to zero, set the grads to None.\nThis will in general have lower memory footprint, and can modestly improve performance.\nHowever, it changes certain behaviors. For example:\n1. When the user tries to access a gradient and perform manual ops on it,\na None attribute or a Tensor full of 0s will behave differently.\n2. If the user requests zero_grad(set_to_none=True) followed by a backward pass, .grads\nare guaranteed to be None for params that did not receive a gradient.\n3. torch.optim optimizers have a different behavior if the gradient is 0 or None\n(in one case it does the step with a gradient of 0 and in the other it skips\nthe step altogether).",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.autograd.profiler_util.Kernel.name",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.autograd.profiler_util.Kernel.html#torch.autograd.profiler_util.Kernel.name",
        "api_signature": null,
        "api_description": "Alias for field number 0",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.rendezvous.c10d_rendezvous_backend.C10dRendezvousBackend.name",
        "api_url": "https://pytorch.org/docs/stable/elastic/rendezvous.html#torch.distributed.elastic.rendezvous.c10d_rendezvous_backend.C10dRendezvousBackend.name",
        "api_signature": null,
        "api_description": "See base class.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.rendezvous.dynamic_rendezvous.RendezvousBackend.name",
        "api_url": "https://pytorch.org/docs/stable/elastic/rendezvous.html#torch.distributed.elastic.rendezvous.dynamic_rendezvous.RendezvousBackend.name",
        "api_signature": null,
        "api_description": "Get the name of the backend.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.rendezvous.etcd_rendezvous_backend.EtcdRendezvousBackend.name",
        "api_url": "https://pytorch.org/docs/stable/elastic/rendezvous.html#torch.distributed.elastic.rendezvous.etcd_rendezvous_backend.EtcdRendezvousBackend.name",
        "api_signature": null,
        "api_description": "See base class.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.rpc.WorkerInfo.name",
        "api_url": "https://pytorch.org/docs/stable/rpc.html#torch.distributed.rpc.WorkerInfo.name",
        "api_signature": null,
        "api_description": "The name of the worker.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.monitor.Aggregation.name",
        "api_url": "https://pytorch.org/docs/stable/monitor.html#torch.monitor.Aggregation.name",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.monitor.Event.name",
        "api_url": "https://pytorch.org/docs/stable/monitor.html#torch.monitor.Event.name",
        "api_signature": null,
        "api_description": "The name of the Event.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.monitor.Stat.name",
        "api_url": "https://pytorch.org/docs/stable/monitor.html#torch.monitor.Stat.name",
        "api_signature": null,
        "api_description": "The name of the stat that was set during creation.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.attention.SDPBackend.name",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.attention.SDPBackend.html#torch.nn.attention.SDPBackend.name",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.profiler.ProfilerActivity.name",
        "api_url": "https://pytorch.org/docs/stable/profiler.html#torch.profiler.ProfilerActivity.name",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tag.name",
        "api_url": "https://pytorch.org/docs/stable/torch.html#torch.Tag.name",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.autograd.graph.Node.name",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.autograd.graph.Node.name.html#torch.autograd.graph.Node.name",
        "api_signature": "Node.name()",
        "api_description": "Return the name.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.fsdp.FullyShardedDataParallel.named_buffers",
        "api_url": "https://pytorch.org/docs/stable/fsdp.html#torch.distributed.fsdp.FullyShardedDataParallel.named_buffers",
        "api_signature": "named_buffers(*args, **kwargs)",
        "api_description": "Return an iterator over module buffers, yielding both the name of the buffer and the buffer itself.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.export.ExportedProgram.named_buffers",
        "api_url": "https://pytorch.org/docs/stable/export.html#torch.export.ExportedProgram.named_buffers",
        "api_signature": "named_buffers()",
        "api_description": "Returns an iterator over original module buffers, yielding\nboth the name of the buffer as well as the buffer itself.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.jit.ScriptModule.named_buffers",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.jit.ScriptModule.html#torch.jit.ScriptModule.named_buffers",
        "api_signature": "named_buffers(prefix='', recurse=True, remove_duplicate=True)",
        "api_description": "Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself.",
        "return_value": "",
        "parameters": "prefix (str) – prefix to prepend to all buffer names.\nrecurse (bool, optional) – if True, then yields buffers of this module\nand all submodules. Otherwise, yields only buffers that\nare direct members of this module. Defaults to True.\nremove_duplicate (bool, optional) – whether to remove the duplicated buffers in the result. Defaults to True.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.Module.named_buffers",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.named_buffers",
        "api_signature": "named_buffers(prefix='', recurse=True, remove_duplicate=True)",
        "api_description": "Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself.",
        "return_value": "",
        "parameters": "prefix (str) – prefix to prepend to all buffer names.\nrecurse (bool, optional) – if True, then yields buffers of this module\nand all submodules. Otherwise, yields only buffers that\nare direct members of this module. Defaults to True.\nremove_duplicate (bool, optional) – whether to remove the duplicated buffers in the result. Defaults to True.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.jit.ScriptModule.named_children",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.jit.ScriptModule.html#torch.jit.ScriptModule.named_children",
        "api_signature": "named_children()",
        "api_description": "Return an iterator over immediate children modules, yielding both the name of the module as well as the module itself.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.Module.named_children",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.named_children",
        "api_signature": "named_children()",
        "api_description": "Return an iterator over immediate children modules, yielding both the name of the module as well as the module itself.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.jit.ScriptModule.named_modules",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.jit.ScriptModule.html#torch.jit.ScriptModule.named_modules",
        "api_signature": "named_modules(memo=None, prefix='', remove_duplicate=True)",
        "api_description": "Return an iterator over all modules in the network, yielding both the name of the module as well as the module itself.",
        "return_value": "",
        "parameters": "memo (Optional[Set[Module]]) – a memo to store the set of modules already added to the result\nprefix (str) – a prefix that will be added to the name of the module\nremove_duplicate (bool) – whether to remove the duplicated module instances in the result\nor not",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.Module.named_modules",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.named_modules",
        "api_signature": "named_modules(memo=None, prefix='', remove_duplicate=True)",
        "api_description": "Return an iterator over all modules in the network, yielding both the name of the module as well as the module itself.",
        "return_value": "",
        "parameters": "memo (Optional[Set[Module]]) – a memo to store the set of modules already added to the result\nprefix (str) – a prefix that will be added to the name of the module\nremove_duplicate (bool) – whether to remove the duplicated module instances in the result\nor not",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.fsdp.FullyShardedDataParallel.named_parameters",
        "api_url": "https://pytorch.org/docs/stable/fsdp.html#torch.distributed.fsdp.FullyShardedDataParallel.named_parameters",
        "api_signature": "named_parameters(*args, **kwargs)",
        "api_description": "Return an iterator over module parameters, yielding both the name of the parameter and the parameter itself.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.export.ExportedProgram.named_parameters",
        "api_url": "https://pytorch.org/docs/stable/export.html#torch.export.ExportedProgram.named_parameters",
        "api_signature": "named_parameters()",
        "api_description": "Returns an iterator over original module parameters, yielding\nboth the name of the parameter as well as the parameter itself.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.jit.ScriptModule.named_parameters",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.jit.ScriptModule.html#torch.jit.ScriptModule.named_parameters",
        "api_signature": "named_parameters(prefix='', recurse=True, remove_duplicate=True)",
        "api_description": "Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.",
        "return_value": "",
        "parameters": "prefix (str) – prefix to prepend to all parameter names.\nrecurse (bool) – if True, then yields parameters of this module\nand all submodules. Otherwise, yields only parameters that\nare direct members of this module.\nremove_duplicate (bool, optional) – whether to remove the duplicated\nparameters in the result. Defaults to True.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.Module.named_parameters",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.named_parameters",
        "api_signature": "named_parameters(prefix='', recurse=True, remove_duplicate=True)",
        "api_description": "Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.",
        "return_value": "",
        "parameters": "prefix (str) – prefix to prepend to all parameter names.\nrecurse (bool) – if True, then yields parameters of this module\nand all submodules. Otherwise, yields only parameters that\nare direct members of this module.\nremove_duplicate (bool, optional) – whether to remove the duplicated\nparameters in the result. Defaults to True.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.names",
        "api_url": "https://pytorch.org/docs/stable/named_tensor.html#torch.Tensor.names",
        "api_signature": null,
        "api_description": "Stores names for each of this tensor’s dimensions.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nan_to_num",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nan_to_num.html#torch.nan_to_num",
        "api_signature": "torch.nan_to_num(input, nan=0.0, posinf=None, neginf=None, *, out=None)",
        "api_description": "Replaces NaN, positive infinity, and negative infinity values in input\nwith the values specified by nan, posinf, and neginf, respectively.\nBy default, NaNs are replaced with zero, positive infinity is replaced with the\ngreatest finite value representable by input’s dtype, and negative infinity\nis replaced with the least finite value representable by input’s dtype.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.\nnan (Number, optional) – the value to replace NaNs with. Default is zero.\nposinf (Number, optional) – if a Number, the value to replace positive infinity values with.\nIf None, positive infinity values are replaced with the greatest finite value representable by input’s dtype.\nDefault is None.\nneginf (Number, optional) – if a Number, the value to replace negative infinity values with.\nIf None, negative infinity values are replaced with the lowest finite value representable by input’s dtype.\nDefault is None.\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.nan_to_num",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.nan_to_num.html#torch.Tensor.nan_to_num",
        "api_signature": "Tensor.nan_to_num(nan=0.0, posinf=None, neginf=None)",
        "api_description": "See torch.nan_to_num().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.nan_to_num_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.nan_to_num_.html#torch.Tensor.nan_to_num_",
        "api_signature": "Tensor.nan_to_num_(nan=0.0, posinf=None, neginf=None)",
        "api_description": "In-place version of nan_to_num().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nanmean",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nanmean.html#torch.nanmean",
        "api_signature": "torch.nanmean(input, dim=None, keepdim=False, *, dtype=None, out=None)",
        "api_description": "Computes the mean of all non-NaN elements along the specified dimensions.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.\ndim (int or tuple of ints, optional) – the dimension or dimensions to reduce.\nIf None, all dimensions are reduced.\nkeepdim (bool) – whether the output tensor has dim retained or not.\ndtype (torch.dtype, optional) – the desired data type of returned tensor.\nIf specified, the input tensor is casted to dtype before the operation\nis performed. This is useful for preventing data type overflows. Default: None.\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.nanmean",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.nanmean.html#torch.Tensor.nanmean",
        "api_signature": "Tensor.nanmean(dim=None, keepdim=False, *, dtype=None)",
        "api_description": "See torch.nanmean()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nanmedian",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nanmedian.html#torch.nanmedian",
        "api_signature": "torch.nanmedian(input)",
        "api_description": "Returns the median of the values in input, ignoring NaN values.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.\ninput (Tensor) – the input tensor.\ndim (int) – the dimension to reduce.\nkeepdim (bool) – whether the output tensor has dim retained or not.\nout ((Tensor, Tensor), optional) – The first tensor will be populated with the median values and the second\ntensor, which must have dtype long, with their indices in the dimension\ndim of input.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.nanmedian",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.nanmedian.html#torch.Tensor.nanmedian",
        "api_signature": "Tensor.nanmedian(dim=None, keepdim=False)",
        "api_description": "See torch.nanmedian()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nanquantile",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nanquantile.html#torch.nanquantile",
        "api_signature": "torch.nanquantile(input, q, dim=None, keepdim=False, *, interpolation='linear', out=None)",
        "api_description": "This is a variant of torch.quantile() that “ignores” NaN values,\ncomputing the quantiles q as if NaN values in input did\nnot exist. If all values in a reduced row are NaN then the quantiles for\nthat reduction will be NaN. See the documentation for torch.quantile().",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.\nq (float or Tensor) – a scalar or 1D tensor of quantile values in the range [0, 1]\ndim (int) – the dimension to reduce.\nkeepdim (bool) – whether the output tensor has dim retained or not.\ninterpolation (str) – interpolation method to use when the desired quantile lies between two data points.\nCan be linear, lower, higher, midpoint and nearest.\nDefault is linear.\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.nanquantile",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.nanquantile.html#torch.Tensor.nanquantile",
        "api_signature": "Tensor.nanquantile(q, dim=None, keepdim=False, *, interpolation='linear')",
        "api_description": "See torch.nanquantile()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nansum",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nansum.html#torch.nansum",
        "api_signature": "torch.nansum(input, *, dtype=None)",
        "api_description": "Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.\ndtype (torch.dtype, optional) – the desired data type of returned tensor.\nIf specified, the input tensor is casted to dtype before the operation\nis performed. This is useful for preventing data type overflows. Default: None.\ninput (Tensor) – the input tensor.\ndim (int or tuple of ints, optional) – the dimension or dimensions to reduce.\nIf None, all dimensions are reduced.\nkeepdim (bool) – whether the output tensor has dim retained or not.\ndtype (torch.dtype, optional) – the desired data type of returned tensor.\nIf specified, the input tensor is casted to dtype before the operation\nis performed. This is useful for preventing data type overflows. Default: None.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.nansum",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.nansum.html#torch.Tensor.nansum",
        "api_signature": "Tensor.nansum(dim=None, keepdim=False, dtype=None)",
        "api_description": "See torch.nansum()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.narrow",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.narrow.html#torch.narrow",
        "api_signature": "torch.narrow(input, dim, start, length)",
        "api_description": "Returns a new tensor that is a narrowed version of input tensor. The\ndimension dim is input from start to start + length. The\nreturned tensor and input tensor share the same underlying storage.",
        "return_value": "",
        "parameters": "input (Tensor) – the tensor to narrow\ndim (int) – the dimension along which to narrow\nstart (int or Tensor) – index of the element to start the narrowed dimension\nfrom. Can be negative, which means indexing from the end of dim. If\nTensor, it must be an 0-dim integral Tensor (bools not allowed)\nlength (int) – length of the narrowed dimension, must be weakly positive",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.narrow",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.narrow.html#torch.Tensor.narrow",
        "api_signature": "Tensor.narrow(dimension, start, length)",
        "api_description": "See torch.narrow().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.narrow_copy",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.narrow_copy.html#torch.narrow_copy",
        "api_signature": "torch.narrow_copy(input, dim, start, length, *, out=None)",
        "api_description": "Same as Tensor.narrow() except this returns a copy rather\nthan shared storage. This is primarily for sparse tensors, which\ndo not have a shared-storage narrow method.",
        "return_value": "",
        "parameters": "input (Tensor) – the tensor to narrow\ndim (int) – the dimension along which to narrow\nstart (int) – index of the element to start the narrowed dimension from. Can\nbe negative, which means indexing from the end of dim\nlength (int) – length of the narrowed dimension, must be weakly positive\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.narrow_copy",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.narrow_copy.html#torch.Tensor.narrow_copy",
        "api_signature": "Tensor.narrow_copy(dimension, start, length)",
        "api_description": "See torch.narrow_copy().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.nbytes",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.nbytes.html#torch.Tensor.nbytes",
        "api_signature": null,
        "api_description": "Returns the number of bytes consumed by the “view” of elements of the Tensor\nif the Tensor does not use sparse storage layout.\nDefined to be numel() * element_size()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.TypedStorage.nbytes",
        "api_url": "https://pytorch.org/docs/stable/storage.html#torch.TypedStorage.nbytes",
        "api_signature": "nbytes()",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.UntypedStorage.nbytes",
        "api_url": "https://pytorch.org/docs/stable/storage.html#torch.UntypedStorage.nbytes",
        "api_signature": "nbytes()",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.ndim",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.ndim.html#torch.Tensor.ndim",
        "api_signature": null,
        "api_description": "Alias for dim()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.ndimension",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.ndimension.html#torch.Tensor.ndimension",
        "api_signature": "Tensor.ndimension()",
        "api_description": "Alias for dim()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.special.ndtr",
        "api_url": "https://pytorch.org/docs/stable/special.html#torch.special.ndtr",
        "api_signature": "torch.special.ndtr(input, *, out=None)",
        "api_description": "Computes the area under the standard Gaussian probability density function,\nintegrated from minus infinity to input, elementwise.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> torch.special.ndtr(torch.tensor([-3., -2, -1, 0, 1, 2, 3]))\ntensor([0.0013, 0.0228, 0.1587, 0.5000, 0.8413, 0.9772, 0.9987])\n\n\n"
    },
    {
        "api_name": "torch.special.ndtri",
        "api_url": "https://pytorch.org/docs/stable/special.html#torch.special.ndtri",
        "api_signature": "torch.special.ndtri(input, *, out=None)",
        "api_description": "Computes the argument, x, for which the area under the Gaussian probability density function\n(integrated from minus infinity to x) is equal to input, elementwise.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> torch.special.ndtri(torch.tensor([0, 0.25, 0.5, 0.75, 1]))\ntensor([   -inf, -0.6745,  0.0000,  0.6745,     inf])\n\n\n"
    },
    {
        "api_name": "torch.ne",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ne.html#torch.ne",
        "api_signature": "torch.ne(input, other, *, out=None)",
        "api_description": "Computes input≠other\\text{input} \\neq \\text{other}input=other element-wise.",
        "return_value": "A boolean tensor that is True where input is not equal to other and False elsewhere\n",
        "parameters": "input (Tensor) – the tensor to compare\nother (Tensor or float) – the tensor or value to compare\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.ne",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.ne.html#torch.Tensor.ne",
        "api_signature": "Tensor.ne(other)",
        "api_description": "See torch.ne().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.ne_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.ne_.html#torch.Tensor.ne_",
        "api_signature": "Tensor.ne_(other)",
        "api_description": "In-place version of ne().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.neg",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.neg.html#torch.neg",
        "api_signature": "torch.neg(input, *, out=None)",
        "api_description": "Returns a new tensor with the negative of the elements of input.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.neg",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.neg.html#torch.Tensor.neg",
        "api_signature": "Tensor.neg()",
        "api_description": "See torch.neg()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.neg_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.neg_.html#torch.Tensor.neg_",
        "api_signature": "Tensor.neg_()",
        "api_description": "In-place version of neg()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.negative",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.negative.html#torch.negative",
        "api_signature": "torch.negative(input, *, out=None)",
        "api_description": "Alias for torch.neg()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.negative",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.negative.html#torch.Tensor.negative",
        "api_signature": "Tensor.negative()",
        "api_description": "See torch.negative()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.negative_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.negative_.html#torch.Tensor.negative_",
        "api_signature": "Tensor.negative_()",
        "api_description": "In-place version of negative()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.negative_binomial.NegativeBinomial",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.negative_binomial.NegativeBinomial",
        "api_signature": "torch.distributions.negative_binomial.NegativeBinomial(total_count, probs=None, logits=None, validate_args=None)",
        "api_description": "Bases: Distribution",
        "return_value": "",
        "parameters": "total_count (float or Tensor) – non-negative number of negative Bernoulli\ntrials to stop, although the distribution is still valid for real\nvalued count\nprobs (Tensor) – Event probabilities of success in the half open interval [0, 1)\nlogits (Tensor) – Event log-odds for probabilities of success",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.nelement",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.nelement.html#torch.Tensor.nelement",
        "api_signature": "Tensor.nelement()",
        "api_description": "Alias for numel()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nested.nested_tensor",
        "api_url": "https://pytorch.org/docs/stable/nested.html#torch.nested.nested_tensor",
        "api_signature": "torch.nested.nested_tensor(tensor_list, *, dtype=None, layout=None, device=None, requires_grad=False, pin_memory=False)",
        "api_description": "Constructs a nested tensor with no autograd history (also known as a “leaf tensor”, see\nAutograd mechanics) from tensor_list a list of tensors.",
        "return_value": "",
        "parameters": "tensor_list (List[array_like]) – a list of tensors, or anything that can be passed to torch.tensor,\ndimensionality. (where each element of the list has the same) –\ndtype (torch.dtype, optional) – the desired type of returned nested tensor.\nDefault: if None, same torch.dtype as leftmost tensor in the list.\nlayout (torch.layout, optional) – the desired layout of returned nested tensor.\nOnly strided and jagged layouts are supported. Default: if None, the strided layout.\ndevice (torch.device, optional) – the desired device of returned nested tensor.\nDefault: if None, same torch.device as leftmost tensor in the list\nrequires_grad (bool, optional) – If autograd should record operations on the\nreturned nested tensor. Default: False.\npin_memory (bool, optional) – If set, returned nested tensor would be allocated in\nthe pinned memory. Works only for CPU tensors. Default: False.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.autograd.function.NestedIOFunction",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.autograd.function.NestedIOFunction.html#torch.autograd.function.NestedIOFunction",
        "api_signature": "torch.autograd.function.NestedIOFunction(*args, **kwargs)",
        "api_description": "This class is here only for backward compatibility reasons.\nUse Function instead of this for any new use case.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> class Func(torch.autograd.Function):\n>>>     @staticmethod\n>>>     def forward(ctx, x: torch.Tensor, y: torch.Tensor, z: int):\n>>>         ctx.save_for_backward(x, y)\n>>>         ctx.save_for_forward(x, y)\n>>>         ctx.z = z\n>>>         return x * y * z\n>>>\n>>>     @staticmethod\n>>>     def jvp(ctx, x_t, y_t, _):\n>>>         x, y = ctx.saved_tensors\n>>>         z = ctx.z\n>>>         return z * (y * x_t + x * y_t)\n>>>\n>>>     @staticmethod\n>>>     def vjp(ctx, grad_out):\n>>>         x, y = ctx.saved_tensors\n>>>         z = ctx.z\n>>>         return z * grad_out * y, z * grad_out * x, None\n>>>\n>>>     a = torch.tensor(1., requires_grad=True, dtype=torch.double)\n>>>     t = torch.tensor(1., dtype=torch.double)\n>>>     b = torch.tensor(2., requires_grad=True, dtype=torch.double)\n>>>     c = 4\n>>>\n>>>     with fwAD.dual_level():\n>>>         a_dual = fwAD.make_dual(a, t)\n>>>         d = Func.apply(a_dual, b, c)\n\n\n>>> class SimpleFunc(Function):\n>>>     @staticmethod\n>>>     def forward(ctx, x):\n>>>         return x.clone(), x.clone()\n>>>\n>>>     @staticmethod\n>>>     @once_differentiable\n>>>     def backward(ctx, g1, g2):\n>>>         return g1 + g2  # No check for None necessary\n>>>\n>>> # We modify SimpleFunc to handle non-materialized grad outputs\n>>> class Func(Function):\n>>>     @staticmethod\n>>>     def forward(ctx, x):\n>>>         ctx.set_materialize_grads(False)\n>>>         ctx.save_for_backward(x)\n>>>         return x.clone(), x.clone()\n>>>\n>>>     @staticmethod\n>>>     @once_differentiable\n>>>     def backward(ctx, g1, g2):\n>>>         x, = ctx.saved_tensors\n>>>         grad_input = torch.zeros_like(x)\n>>>         if g1 is not None:  # We must check for None now\n>>>             grad_input += g1\n>>>         if g2 is not None:\n>>>             grad_input += g2\n>>>         return grad_input\n>>>\n>>> a = torch.tensor(1., requires_grad=True)\n>>> b, _ = Func.apply(a)  # induces g2 to be undefined\n\n\n"
    },
    {
        "api_name": "torch.UntypedStorage.new",
        "api_url": "https://pytorch.org/docs/stable/storage.html#torch.UntypedStorage.new",
        "api_signature": "new()",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.new_empty",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.new_empty.html#torch.Tensor.new_empty",
        "api_signature": "Tensor.new_empty(size, *, dtype=None, device=None, requires_grad=False, layout=torch.strided, pin_memory=False)",
        "api_description": "Returns a Tensor of size size filled with uninitialized data.\nBy default, the returned Tensor has the same torch.dtype and\ntorch.device as this tensor.",
        "return_value": "",
        "parameters": "size (int...) – a list, tuple, or torch.Size of integers defining the\nshape of the output tensor.\ndtype (torch.dtype, optional) – the desired type of returned tensor.\nDefault: if None, same torch.dtype as this tensor.\ndevice (torch.device, optional) – the desired device of returned tensor.\nDefault: if None, same torch.device as this tensor.\nrequires_grad (bool, optional) – If autograd should record operations on the\nreturned tensor. Default: False.\nlayout (torch.layout, optional) – the desired layout of returned Tensor.\nDefault: torch.strided.\npin_memory (bool, optional) – If set, returned tensor would be allocated in\nthe pinned memory. Works only for CPU tensors. Default: False.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.new_full",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.new_full.html#torch.Tensor.new_full",
        "api_signature": "Tensor.new_full(size, fill_value, *, dtype=None, device=None, requires_grad=False, layout=torch.strided, pin_memory=False)",
        "api_description": "Returns a Tensor of size size filled with fill_value.\nBy default, the returned Tensor has the same torch.dtype and\ntorch.device as this tensor.",
        "return_value": "",
        "parameters": "fill_value (scalar) – the number to fill the output tensor with.\ndtype (torch.dtype, optional) – the desired type of returned tensor.\nDefault: if None, same torch.dtype as this tensor.\ndevice (torch.device, optional) – the desired device of returned tensor.\nDefault: if None, same torch.device as this tensor.\nrequires_grad (bool, optional) – If autograd should record operations on the\nreturned tensor. Default: False.\nlayout (torch.layout, optional) – the desired layout of returned Tensor.\nDefault: torch.strided.\npin_memory (bool, optional) – If set, returned tensor would be allocated in\nthe pinned memory. Works only for CPU tensors. Default: False.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.new_group",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#torch.distributed.new_group",
        "api_signature": "torch.distributed.new_group(ranks=None, timeout=None, backend=None, pg_options=None, use_local_synchronization=False)",
        "api_description": "Create a new distributed group.",
        "return_value": "A handle of distributed group that can be given to collective calls or None if the rank is not part of ranks.\n",
        "parameters": "ranks (list[int]) – List of ranks of group members. If None, will be\nset to all ranks. Default is None.\ntimeout (timedelta, optional) – see init_process_group for details and default value.\nbackend (str or Backend, optional) – The backend to use. Depending on\nbuild-time configurations, valid values are gloo and nccl.\nBy default uses the same backend as the global group. This field\nshould be given as a lowercase string (e.g., \"gloo\"), which can\nalso be accessed via Backend attributes (e.g.,\nBackend.GLOO). If None is passed in, the backend\ncorresponding to the default process group will be used. Default is\nNone.\npg_options (ProcessGroupOptions, optional) – process group options\nspecifying what additional options need to be passed in during\nthe construction of specific process groups. i.e. for the nccl\nbackend, is_high_priority_stream can be specified so that\nprocess group can pick up high priority cuda streams.\nuse_local_synchronization (bool, optional) – perform a group-local\nbarrier at the end of the process group creation. This is different\nin that non-member ranks don’t need to call into API and don’t\njoin the barrier.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.new_ones",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.new_ones.html#torch.Tensor.new_ones",
        "api_signature": "Tensor.new_ones(size, *, dtype=None, device=None, requires_grad=False, layout=torch.strided, pin_memory=False)",
        "api_description": "Returns a Tensor of size size filled with 1.\nBy default, the returned Tensor has the same torch.dtype and\ntorch.device as this tensor.",
        "return_value": "",
        "parameters": "size (int...) – a list, tuple, or torch.Size of integers defining the\nshape of the output tensor.\ndtype (torch.dtype, optional) – the desired type of returned tensor.\nDefault: if None, same torch.dtype as this tensor.\ndevice (torch.device, optional) – the desired device of returned tensor.\nDefault: if None, same torch.device as this tensor.\nrequires_grad (bool, optional) – If autograd should record operations on the\nreturned tensor. Default: False.\nlayout (torch.layout, optional) – the desired layout of returned Tensor.\nDefault: torch.strided.\npin_memory (bool, optional) – If set, returned tensor would be allocated in\nthe pinned memory. Works only for CPU tensors. Default: False.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.new_tensor",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.new_tensor.html#torch.Tensor.new_tensor",
        "api_signature": "Tensor.new_tensor(data, *, dtype=None, device=None, requires_grad=False, layout=torch.strided, pin_memory=False)",
        "api_description": "Returns a new Tensor with data as the tensor data.\nBy default, the returned Tensor has the same torch.dtype and\ntorch.device as this tensor.",
        "return_value": "",
        "parameters": "data (array_like) – The returned Tensor copies data.\ndtype (torch.dtype, optional) – the desired type of returned tensor.\nDefault: if None, same torch.dtype as this tensor.\ndevice (torch.device, optional) – the desired device of returned tensor.\nDefault: if None, same torch.device as this tensor.\nrequires_grad (bool, optional) – If autograd should record operations on the\nreturned tensor. Default: False.\nlayout (torch.layout, optional) – the desired layout of returned Tensor.\nDefault: torch.strided.\npin_memory (bool, optional) – If set, returned tensor would be allocated in\nthe pinned memory. Works only for CPU tensors. Default: False.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.new_zeros",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.new_zeros.html#torch.Tensor.new_zeros",
        "api_signature": "Tensor.new_zeros(size, *, dtype=None, device=None, requires_grad=False, layout=torch.strided, pin_memory=False)",
        "api_description": "Returns a Tensor of size size filled with 0.\nBy default, the returned Tensor has the same torch.dtype and\ntorch.device as this tensor.",
        "return_value": "",
        "parameters": "size (int...) – a list, tuple, or torch.Size of integers defining the\nshape of the output tensor.\ndtype (torch.dtype, optional) – the desired type of returned tensor.\nDefault: if None, same torch.dtype as this tensor.\ndevice (torch.device, optional) – the desired device of returned tensor.\nDefault: if None, same torch.device as this tensor.\nrequires_grad (bool, optional) – If autograd should record operations on the\nreturned tensor. Default: False.\nlayout (torch.layout, optional) – the desired layout of returned Tensor.\nDefault: torch.strided.\npin_memory (bool, optional) – If set, returned tensor would be allocated in\nthe pinned memory. Works only for CPU tensors. Default: False.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.Node.next",
        "api_url": "https://pytorch.org/docs/stable/fx.html#torch.fx.Node.next",
        "api_signature": null,
        "api_description": "Returns the next Node in the linked list of Nodes.",
        "return_value": "The next Node in the linked list of Nodes.\n",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.autograd.graph.Node.next_functions",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.autograd.graph.Node.next_functions.html#torch.autograd.graph.Node.next_functions",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.rendezvous.RendezvousHandler.next_rendezvous",
        "api_url": "https://pytorch.org/docs/stable/elastic/rendezvous.html#torch.distributed.elastic.rendezvous.RendezvousHandler.next_rendezvous",
        "api_signature": "next_rendezvous()",
        "api_description": "Main entry-point into the rendezvous barrier.",
        "return_value": "A tuple of torch.distributed.Store, rank, and\nworld size.\n",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nextafter",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nextafter.html#torch.nextafter",
        "api_signature": "torch.nextafter(input, other, *, out=None)",
        "api_description": "Return the next floating-point value after input towards other, elementwise.",
        "return_value": "",
        "parameters": "input (Tensor) – the first input tensor\nother (Tensor) – the second input tensor\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.nextafter",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.nextafter.html#torch.Tensor.nextafter",
        "api_signature": "Tensor.nextafter(other)",
        "api_description": "See torch.nextafter()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.nextafter_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.nextafter_.html#torch.Tensor.nextafter_",
        "api_signature": "Tensor.nextafter_(other)",
        "api_description": "In-place version of nextafter()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.functional.nll_loss",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.functional.nll_loss.html#torch.nn.functional.nll_loss",
        "api_signature": "torch.nn.functional.nll_loss(input, target, weight=None, size_average=None, ignore_index=-100, reduce=None, reduction='mean')",
        "api_description": "Compute the negative log likelihood loss.",
        "return_value": "",
        "parameters": "input (Tensor) – (N,C)(N, C)(N,C) where C = number of classes or (N,C,H,W)(N, C, H, W)(N,C,H,W)\nin case of 2D Loss, or (N,C,d1,d2,...,dK)(N, C, d_1, d_2, ..., d_K)(N,C,d1​,d2​,...,dK​) where K≥1K \\geq 1K≥1\nin the case of K-dimensional loss. input is expected to be log-probabilities.\ntarget (Tensor) – (N)(N)(N) where each value is 0≤targets[i]≤C−10 \\leq \\text{targets}[i] \\leq C-10≤targets[i]≤C−1,\nor (N,d1,d2,...,dK)(N, d_1, d_2, ..., d_K)(N,d1​,d2​,...,dK​) where K≥1K \\geq 1K≥1 for\nK-dimensional loss.\nweight (Tensor, optional) – a manual rescaling weight given to each\nclass. If given, has to be a Tensor of size C\nsize_average (bool, optional) – Deprecated (see reduction). By default,\nthe losses are averaged over each loss element in the batch. Note that for\nsome losses, there multiple elements per sample. If the field size_average\nis set to False, the losses are instead summed for each minibatch. Ignored\nwhen reduce is False. Default: True\nignore_index (int, optional) – Specifies a target value that is ignored\nand does not contribute to the input gradient. When size_average is\nTrue, the loss is averaged over non-ignored targets. Default: -100\nreduce (bool, optional) – Deprecated (see reduction). By default, the\nlosses are averaged or summed over observations for each minibatch depending\non size_average. When reduce is False, returns a loss per\nbatch element instead and ignores size_average. Default: True\nreduction (str, optional) – Specifies the reduction to apply to the output:\n'none' | 'mean' | 'sum'. 'none': no reduction will be applied,\n'mean': the sum of the output will be divided by the number of\nelements in the output, 'sum': the output will be summed. Note: size_average\nand reduce are in the process of being deprecated, and in the meantime,\nspecifying either of those two args will override reduction. Default: 'mean'",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.NLLLoss",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html#torch.nn.NLLLoss",
        "api_signature": "torch.nn.NLLLoss(weight=None, size_average=None, ignore_index=-100, reduce=None, reduction='mean')",
        "api_description": "The negative log likelihood loss. It is useful to train a classification\nproblem with C classes.",
        "return_value": "",
        "parameters": "weight (Tensor, optional) – a manual rescaling weight given to each\nclass. If given, it has to be a Tensor of size C. Otherwise, it is\ntreated as if having all ones.\nsize_average (bool, optional) – Deprecated (see reduction). By default,\nthe losses are averaged over each loss element in the batch. Note that for\nsome losses, there are multiple elements per sample. If the field size_average\nis set to False, the losses are instead summed for each minibatch. Ignored\nwhen reduce is False. Default: None\nignore_index (int, optional) – Specifies a target value that is ignored\nand does not contribute to the input gradient. When\nsize_average is True, the loss is averaged over\nnon-ignored targets.\nreduce (bool, optional) – Deprecated (see reduction). By default, the\nlosses are averaged or summed over observations for each minibatch depending\non size_average. When reduce is False, returns a loss per\nbatch element instead and ignores size_average. Default: None\nreduction (str, optional) – Specifies the reduction to apply to the output:\n'none' | 'mean' | 'sum'. 'none': no reduction will\nbe applied, 'mean': the weighted mean of the output is taken,\n'sum': the output will be summed. Note: size_average\nand reduce are in the process of being deprecated, and in\nthe meantime, specifying either of those two args will override\nreduction. Default: 'mean'",
        "input_shape": "\nInput: (N,C)(N, C)(N,C) or (C)(C)(C), where C = number of classes, or\n(N,C,d1,d2,...,dK)(N, C, d_1, d_2, ..., d_K)(N,C,d1​,d2​,...,dK​) with K≥1K \\geq 1K≥1\nin the case of K-dimensional loss.\nTarget: (N)(N)(N) or ()()(), where each value is\n0≤targets[i]≤C−10 \\leq \\text{targets}[i] \\leq C-10≤targets[i]≤C−1, or\n(N,d1,d2,...,dK)(N, d_1, d_2, ..., d_K)(N,d1​,d2​,...,dK​) with K≥1K \\geq 1K≥1 in the case of\nK-dimensional loss.\nOutput: If reduction is 'none', shape (N)(N)(N) or\n(N,d1,d2,...,dK)(N, d_1, d_2, ..., d_K)(N,d1​,d2​,...,dK​) with K≥1K \\geq 1K≥1 in the case of K-dimensional loss.\nOtherwise, scalar.\n\n",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.no_grad",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.no_grad.html#torch.no_grad",
        "api_signature": "torch.no_grad(orig_func=None)",
        "api_description": "Context-manager that disables gradient calculation.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> x = torch.tensor([1.], requires_grad=True)\n>>> with torch.no_grad():\n...     y = x * 2\n>>> y.requires_grad\nFalse\n>>> @torch.no_grad()\n... def doubler(x):\n...     return x * 2\n>>> z = doubler(x)\n>>> z.requires_grad\nFalse\n>>> @torch.no_grad\n... def tripler(x):\n...     return x * 3\n>>> z = tripler(x)\n>>> z.requires_grad\nFalse\n>>> # factory function exception\n>>> with torch.no_grad():\n...     a = torch.nn.Parameter(torch.rand(10))\n>>> a.requires_grad\nTrue\n\n\n"
    },
    {
        "api_name": "torch.distributed.fsdp.FullyShardedDataParallel.no_sync",
        "api_url": "https://pytorch.org/docs/stable/fsdp.html#torch.distributed.fsdp.FullyShardedDataParallel.no_sync",
        "api_signature": "no_sync()",
        "api_description": "Disable gradient synchronizations across FSDP instances.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.parallel.DistributedDataParallel.no_sync",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel.no_sync",
        "api_signature": "no_sync()",
        "api_description": "Context manager to disable gradient synchronizations across DDP processes.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.Node",
        "api_url": "https://pytorch.org/docs/stable/fx.html#torch.fx.Node",
        "api_signature": "torch.fx.Node(graph, name, op, target, args, kwargs, return_type=None)",
        "api_description": "Node is the data structure that represents individual operations within\na Graph. For the most part, Nodes represent callsites to various entities,\nsuch as operators, methods, and Modules (some exceptions include nodes that\nspecify function inputs and outputs). Each Node has a function specified\nby its op property. The Node semantics for each value of op are as follows:",
        "return_value": "List of Nodes that appear in the args and kwargs of this\nNode, in that order.\n\nIf 1) we’re using format_node as an internal helperin the __str__ method of Graph, and 2) self\nis a placeholder Node, return None. Otherwise,\nreturn a  descriptive string representation of the\ncurrent Node.\n\n\n\nIf the op is impure or not.\nThe next Node in the linked list of Nodes.\nReturns NamedTuple ArgsKwargsPair, or None if not successful.\nThe previous Node in the linked list of Nodes.\nThe list of Nodes on which this change was made.\n",
        "parameters": "x (Node) – The node to put after this node. Must be a member of the same graph.\nplaceholder_names (Optional[List[str]]) – A list that will store formatted strings\nrepresenting the placeholders in the generated\nforward function. Internal use only.\nmaybe_return_typename (Optional[List[str]]) – A single-element list that will store\na formatted string representing the output of the\ngenerated forward function. Internal use only.\nidx (int) – The index of the element in self.args to be inserted before.\narg (Argument) – The new argument value to insert into args\nroot (torch.nn.Module) – Module upon which to resolve module targets.\narg_types (Optional[Tuple[Any]]) – Tuple of arg types for the args\nkwarg_types (Optional[Dict[str, Any]]) – Dict of arg types for the kwargs\nnormalize_to_only_use_kwargs (bool) – Whether to normalize to only use kwargs.\nx (Node) – The node to put before this node. Must be a member of the same graph.\nreplace_with (Node) – The node to replace all uses of self with.\ndelete_user_cb (Callable) – Callback that is called to determine\nwhether a given user of the self node should be removed.\npropagate_meta (bool) – Whether or not to copy all properties\non the .meta field of the original node onto the replacement node.\nFor safety, this is only valid to do if the replacement node\ndoesn’t already have an existing .meta field.\nold_input (Node) – The old input node to be replaced.\nnew_input (Node) – The new input node to replace old_input.\nidx (int) – The index into self.args of the element to update\narg (Argument) – The new argument value to write into args\nkey (str) – The key in self.kwargs of the element to update\narg (Argument) – The new argument value to write into kwargs",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.Graph.node_copy",
        "api_url": "https://pytorch.org/docs/stable/fx.html#torch.fx.Graph.node_copy",
        "api_signature": "node_copy(node, arg_transform=<function Graph.<lambda>>)",
        "api_description": "Copy a node from one graph into another. arg_transform needs to transform arguments from\nthe graph of node to the graph of self. Example:",
        "return_value": "",
        "parameters": "node (Node) – The node to copy into self.\narg_transform (Callable[[Node], Argument]) – A function that transforms\nNode arguments in node’s args and kwargs into the\nequivalent argument in self. In the simplest case, this should\nretrieve a value out of a table mapping Nodes in the original\ngraph to self.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.Graph.nodes",
        "api_url": "https://pytorch.org/docs/stable/fx.html#torch.fx.Graph.nodes",
        "api_signature": null,
        "api_description": "Get the list of Nodes that constitute this Graph.",
        "return_value": "A doubly-linked list of Nodes. Note that reversed can be called on\nthis list to switch iteration order.\n",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nonzero",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nonzero.html#torch.nonzero",
        "api_signature": "torch.nonzero(input, *, out=None, as_tuple=False)",
        "api_description": "Note",
        "return_value": "If as_tuple is False, the output\ntensor containing indices. If as_tuple is True, one 1-D tensor for\neach dimension, containing the indices of each nonzero element along that\ndimension.\n",
        "parameters": "input (Tensor) – the input tensor.\nout (LongTensor, optional) – the output tensor containing indices",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.nonzero",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.nonzero.html#torch.Tensor.nonzero",
        "api_signature": "Tensor.nonzero()",
        "api_description": "See torch.nonzero()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.algorithms.ddp_comm_hooks.debugging_hooks.noop_hook",
        "api_url": "https://pytorch.org/docs/stable/ddp_comm_hooks.html#torch.distributed.algorithms.ddp_comm_hooks.debugging_hooks.noop_hook",
        "api_signature": "torch.distributed.algorithms.ddp_comm_hooks.debugging_hooks.noop_hook(_, bucket)",
        "api_description": "Return a future that wraps the input, so it is a no-op that does not incur any communication overheads.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> ddp_model.register_comm_hook(None, noop_hook)\n\n\n"
    },
    {
        "api_name": "torch.ao.quantization.observer.NoopObserver",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.quantization.observer.NoopObserver.html#torch.ao.quantization.observer.NoopObserver",
        "api_signature": "torch.ao.quantization.observer.NoopObserver(dtype=torch.float16, custom_op_name='')",
        "api_description": "Observer that doesn’t do anything and just passes its configuration to the\nquantized module’s .from_float().",
        "return_value": "",
        "parameters": "dtype – Quantized data type\ncustom_op_name – (temporary) specify this observer for an operator that doesn’t require any observation\n(Can be used in Graph Mode Passes for special case ops).",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.norm",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.norm.html#torch.norm",
        "api_signature": "torch.norm(input, p='fro', dim=None, keepdim=False, out=None, dtype=None)",
        "api_description": "Returns the matrix norm or vector norm of a given tensor.",
        "return_value": "",
        "parameters": "input (Tensor) – The input tensor. Its data type must be either a floating\npoint or complex type. For complex inputs, the norm is calculated using the\nabsolute value of each element. If the input is complex and neither\ndtype nor out is specified, the result’s data type will\nbe the corresponding floating point type (e.g. float if input is\ncomplexfloat).\np (int, float, inf, -inf, 'fro', 'nuc', optional) – the order of norm. Default: 'fro'\nThe following norms can be calculated:\nord\nmatrix norm\nvector norm\n’fro’\nFrobenius norm\n–\n‘nuc’\nnuclear norm\n–\nNumber\n–\nsum(abs(x)**ord)**(1./ord)\nThe vector norm can be calculated across any number of dimensions.\nThe corresponding dimensions of input are flattened into\none dimension, and the norm is calculated on the flattened\ndimension.\nFrobenius norm produces the same result as p=2 in all cases\nexcept when dim is a list of three or more dims, in which\ncase Frobenius norm throws an error.\nNuclear norm can only be calculated across exactly two dimensions.\ndim (int, tuple of ints, list of ints, optional) – Specifies which dimension or dimensions of input to\ncalculate the norm across. If dim is None, the norm will\nbe calculated across all dimensions of input. If the norm\ntype indicated by p does not support the specified number of\ndimensions, an error will occur.\nkeepdim (bool, optional) – whether the output tensors have dim\nretained or not. Ignored if dim = None and\nout = None. Default: False\nout (Tensor, optional) – the output tensor. Ignored if\ndim = None and out = None.\ndtype (torch.dtype, optional) – the desired data type of\nreturned tensor. If specified, the input tensor is casted to\ndtype while performing the operation. Default: None.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.linalg.norm",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.linalg.norm.html#torch.linalg.norm",
        "api_signature": "torch.linalg.norm(A, ord=None, dim=None, keepdim=False, *, out=None, dtype=None)",
        "api_description": "Computes a vector or matrix norm.",
        "return_value": "A real-valued tensor, even when A is complex.\n",
        "parameters": "A (Tensor) – tensor of shape (*, n) or (*, m, n) where * is zero or more batch dimensions\nord (int, float, inf, -inf, 'fro', 'nuc', optional) – order of norm. Default: None\ndim (int, Tuple[int], optional) – dimensions over which to compute\nthe vector or matrix norm. See above for the behavior when dim= None.\nDefault: None\nkeepdim (bool, optional) – If set to True, the reduced dimensions are retained\nin the result as dimensions with size one. Default: False\nout (Tensor, optional) – output tensor. Ignored if None. Default: None.\ndtype (torch.dtype, optional) – If specified, the input tensor is cast to\ndtype before performing the operation, and the returned tensor’s type\nwill be dtype. Default: None",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.norm",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.norm.html#torch.Tensor.norm",
        "api_signature": "Tensor.norm(p='fro', dim=None, keepdim=False, dtype=None)",
        "api_description": "See torch.norm()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.normal.Normal",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.normal.Normal",
        "api_signature": "torch.distributions.normal.Normal(loc, scale, validate_args=None)",
        "api_description": "Bases: ExponentialFamily",
        "return_value": "",
        "parameters": "loc (float or Tensor) – mean of the distribution (often referred to as mu)\nscale (float or Tensor) – standard deviation of the distribution\n(often referred to as sigma)",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.normal",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.normal.html#torch.normal",
        "api_signature": "torch.normal(mean, std, *, generator=None, out=None)",
        "api_description": "Returns a tensor of random numbers drawn from separate normal distributions\nwhose mean and standard deviation are given.",
        "return_value": "",
        "parameters": "mean (Tensor) – the tensor of per-element means\nstd (Tensor) – the tensor of per-element standard deviations\ngenerator (torch.Generator, optional) – a pseudorandom number generator for sampling\nout (Tensor, optional) – the output tensor.\nmean (float, optional) – the mean for all distributions\nstd (Tensor) – the tensor of per-element standard deviations\nout (Tensor, optional) – the output tensor.\nmean (Tensor) – the tensor of per-element means\nstd (float, optional) – the standard deviation for all distributions\nout (Tensor, optional) – the output tensor\nmean (float) – the mean for all distributions\nstd (float) – the standard deviation for all distributions\nsize (int...) – a sequence of integers defining the shape of the output tensor.\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.init.normal_",
        "api_url": "https://pytorch.org/docs/stable/nn.init.html#torch.nn.init.normal_",
        "api_signature": "torch.nn.init.normal_(tensor, mean=0.0, std=1.0, generator=None)",
        "api_description": "Fill the input Tensor with values drawn from the normal distribution.",
        "return_value": "",
        "parameters": "tensor (Tensor) – an n-dimensional torch.Tensor\nmean (float) – the mean of the normal distribution\nstd (float) – the standard deviation of the normal distribution\ngenerator (Optional[Generator]) – the torch Generator to sample from (default: None)",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.normal_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.normal_.html#torch.Tensor.normal_",
        "api_signature": "Tensor.normal_(mean=0, std=1, *, generator=None)",
        "api_description": "Fills self tensor with elements samples from the normal distribution\nparameterized by mean and std.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.functional.normalize",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.functional.normalize.html#torch.nn.functional.normalize",
        "api_signature": "torch.nn.functional.normalize(input, p=2.0, dim=1, eps=1e-12, out=None)",
        "api_description": "Perform LpL_pLp​ normalization of inputs over specified dimension.",
        "return_value": "",
        "parameters": "input (Tensor) – input tensor of any shape\np (float) – the exponent value in the norm formulation. Default: 2\ndim (int or tuple of ints) – the dimension to reduce. Default: 1\neps (float) – small value to avoid division by zero. Default: 1e-12\nout (Tensor, optional) – the output tensor. If out is used, this\noperation won’t be differentiable.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.Node.normalized_arguments",
        "api_url": "https://pytorch.org/docs/stable/fx.html#torch.fx.Node.normalized_arguments",
        "api_signature": "normalized_arguments(root, arg_types=None, kwarg_types=None, normalize_to_only_use_kwargs=False)",
        "api_description": "Returns normalized arguments to Python targets. This means that\nargs/kwargs will be matched up to the module/functional’s\nsignature and return exclusively kwargs in positional order\nif normalize_to_only_use_kwargs is true.\nAlso populates default values. Does not support positional-only\nparameters or varargs parameters.",
        "return_value": "Returns NamedTuple ArgsKwargsPair, or None if not successful.\n",
        "parameters": "root (torch.nn.Module) – Module upon which to resolve module targets.\narg_types (Optional[Tuple[Any]]) – Tuple of arg types for the args\nkwarg_types (Optional[Dict[str, Any]]) – Dict of arg types for the kwargs\nnormalize_to_only_use_kwargs (bool) – Whether to normalize to only use kwargs.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.not_equal",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.not_equal.html#torch.not_equal",
        "api_signature": "torch.not_equal(input, other, *, out=None)",
        "api_description": "Alias for torch.ne().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.not_equal",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.not_equal.html#torch.Tensor.not_equal",
        "api_signature": "Tensor.not_equal(other)",
        "api_description": "See torch.not_equal().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.not_equal_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.not_equal_.html#torch.Tensor.not_equal_",
        "api_signature": "Tensor.not_equal_(other)",
        "api_description": "In-place version of not_equal().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.algorithms.Join.notify_join_context",
        "api_url": "https://pytorch.org/docs/stable/distributed.algorithms.join.html#torch.distributed.algorithms.Join.notify_join_context",
        "api_signature": "notify_join_context(joinable)",
        "api_description": "Notifies the join context manager that the calling process has not yet joined.",
        "return_value": "An async work handle for the all-reduce meant to notify the context\nmanager that the process has not yet joined if joinable is the\nfirst one passed into the context manager; None otherwise.\n",
        "parameters": "joinable (Joinable) – the Joinable object calling this\nmethod.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.ns._numeric_suite_fx.NSTracer",
        "api_url": "https://pytorch.org/docs/stable/torch.ao.ns._numeric_suite_fx.html#torch.ao.ns._numeric_suite_fx.NSTracer",
        "api_signature": "torch.ao.ns._numeric_suite_fx.NSTracer(skipped_module_names, skipped_module_classes)",
        "api_description": "Just like a regular FX quantization tracer, but treats observers and fake_quantize\nmodules as leaf modules.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.metrics.api.NullMetricHandler",
        "api_url": "https://pytorch.org/docs/stable/elastic/metrics.html#torch.distributed.elastic.metrics.api.NullMetricHandler",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.Store.num_keys",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#torch.distributed.Store.num_keys",
        "api_signature": "torch.distributed.Store.num_keys(self: torch._C._distributed_c10d.Store)",
        "api_description": "Returns the number of keys set in the store. Note that this number will typically\nbe one greater than the number of keys added by set()\nand add() since one key is used to coordinate all\nthe workers using the store.",
        "return_value": "The number of keys present in the store.\n",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> # Using TCPStore as an example, other store types can also be used\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n>>> store.set(\"first_key\", \"first_value\")\n>>> # This should return 2\n>>> store.num_keys()\n\n\n"
    },
    {
        "api_name": "torch.distributed.elastic.rendezvous.RendezvousHandler.num_nodes_waiting",
        "api_url": "https://pytorch.org/docs/stable/elastic/rendezvous.html#torch.distributed.elastic.rendezvous.RendezvousHandler.num_nodes_waiting",
        "api_signature": "num_nodes_waiting()",
        "api_description": "Return the number of nodes who arrived late at the rendezvous\nbarrier, hence were not included in the current worker group.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.rpc.TensorPipeRpcBackendOptions.num_worker_threads",
        "api_url": "https://pytorch.org/docs/stable/rpc.html#torch.distributed.rpc.TensorPipeRpcBackendOptions.num_worker_threads",
        "api_signature": null,
        "api_description": "The number of threads in the thread-pool used by\nTensorPipeAgent to execute\nrequests.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.numel",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.numel.html#torch.numel",
        "api_signature": "torch.numel(input)",
        "api_description": "Returns the total number of elements in the input tensor.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.numel",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.numel.html#torch.Tensor.numel",
        "api_signature": "Tensor.numel()",
        "api_description": "See torch.numel()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.numpy",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.numpy.html#torch.Tensor.numpy",
        "api_signature": "Tensor.numpy(*, force=False)",
        "api_description": "Returns the tensor as a NumPy ndarray.",
        "return_value": "",
        "parameters": "force (bool) – if True, the ndarray may be a copy of the tensor\ninstead of always sharing memory, defaults to False.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.signal.windows.nuttall",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.signal.windows.nuttall.html#torch.signal.windows.nuttall",
        "api_signature": "torch.signal.windows.nuttall(M, *, sym=True, dtype=None, layout=torch.strided, device=None, requires_grad=False)",
        "api_description": "Computes the minimum 4-term Blackman-Harris window according to Nuttall.",
        "return_value": "",
        "parameters": "M (int) – the length of the window.\nIn other words, the number of points of the returned window.\nsym (bool, optional) – If False, returns a periodic window suitable for use in spectral analysis.\nIf True, returns a symmetric window suitable for use in filter design. Default: True.\ndtype (torch.dtype, optional) – the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_dtype()).\nlayout (torch.layout, optional) – the desired layout of returned Tensor.\nDefault: torch.strided.\ndevice (torch.device, optional) – the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_device()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types.\nrequires_grad (bool, optional) – If autograd should record operations on the\nreturned tensor. Default: False.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.backend_config.ObservationType",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.quantization.backend_config.ObservationType.html#torch.ao.quantization.backend_config.ObservationType",
        "api_signature": "torch.ao.quantization.backend_config.ObservationType(value)",
        "api_description": "An enum that represents different ways of how an operator/operator pattern\nshould be observed",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.observer.ObserverBase",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.quantization.observer.ObserverBase.html#torch.ao.quantization.observer.ObserverBase",
        "api_signature": "torch.ao.quantization.observer.ObserverBase(dtype, is_dynamic=False)",
        "api_description": "Base observer Module.\nAny observer implementation should derive from this class.",
        "return_value": "",
        "parameters": "dtype – dtype argument to the quantize node needed to implement the\nreference model spec.\nis_dynamic – indicator for whether the observer is a placeholder for dynamic quantization\nquantization (or static) –",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.Graph.on_generate_code",
        "api_url": "https://pytorch.org/docs/stable/fx.html#torch.fx.Graph.on_generate_code",
        "api_signature": "on_generate_code(make_transformer)",
        "api_description": "Register a transformer function when python code is generated",
        "return_value": "a context manager that when used in a with statement, to automatically\nrestore the previously registered code transformer.\n",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.autograd.function.once_differentiable",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.autograd.function.once_differentiable.html#torch.autograd.function.once_differentiable",
        "api_signature": "torch.autograd.function.once_differentiable(fn)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.functional.one_hot",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.functional.one_hot.html#torch.nn.functional.one_hot",
        "api_signature": "torch.nn.functional.one_hot(tensor, num_classes=-1)",
        "api_description": "Takes LongTensor with index values of shape (*) and returns a tensor\nof shape (*, num_classes) that have zeros everywhere except where the\nindex of last dimension matches the corresponding value of the input tensor,\nin which case it will be 1.",
        "return_value": "LongTensor that has one more dimension with 1 values at the\nindex of last dimension indicated by the input, and 0 everywhere\nelse.\n",
        "parameters": "tensor (LongTensor) – class values of any shape.\nnum_classes (int) – Total number of classes. If set to -1, the number\nof classes will be inferred as one greater than the largest class\nvalue in the input tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.lr_scheduler.OneCycleLR",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.OneCycleLR.html#torch.optim.lr_scheduler.OneCycleLR",
        "api_signature": "torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, total_steps=None, epochs=None, steps_per_epoch=None, pct_start=0.3, anneal_strategy='cos', cycle_momentum=True, base_momentum=0.85, max_momentum=0.95, div_factor=25.0, final_div_factor=10000.0, three_phase=False, last_epoch=-1, verbose='deprecated')",
        "api_description": "Sets the learning rate of each parameter group according to the\n1cycle learning rate policy. The 1cycle policy anneals the learning\nrate from an initial learning rate to some maximum learning rate and then\nfrom that maximum learning rate to some minimum learning rate much lower\nthan the initial learning rate.\nThis policy was initially described in the paper Super-Convergence:\nVery Fast Training of Neural Networks Using Large Learning Rates.",
        "return_value": "",
        "parameters": "optimizer (Optimizer) – Wrapped optimizer.\nmax_lr (float or list) – Upper learning rate boundaries in the cycle\nfor each parameter group.\ntotal_steps (int) – The total number of steps in the cycle. Note that\nif a value is not provided here, then it must be inferred by providing\na value for epochs and steps_per_epoch.\nDefault: None\nepochs (int) – The number of epochs to train for. This is used along\nwith steps_per_epoch in order to infer the total number of steps in the cycle\nif a value for total_steps is not provided.\nDefault: None\nsteps_per_epoch (int) – The number of steps per epoch to train for. This is\nused along with epochs in order to infer the total number of steps in the\ncycle if a value for total_steps is not provided.\nDefault: None\npct_start (float) – The percentage of the cycle (in number of steps) spent\nincreasing the learning rate.\nDefault: 0.3\nanneal_strategy (str) – {‘cos’, ‘linear’}\nSpecifies the annealing strategy: “cos” for cosine annealing, “linear” for\nlinear annealing.\nDefault: ‘cos’\ncycle_momentum (bool) – If True, momentum is cycled inversely\nto learning rate between ‘base_momentum’ and ‘max_momentum’.\nDefault: True\nbase_momentum (float or list) – Lower momentum boundaries in the cycle\nfor each parameter group. Note that momentum is cycled inversely\nto learning rate; at the peak of a cycle, momentum is\n‘base_momentum’ and learning rate is ‘max_lr’.\nDefault: 0.85\nmax_momentum (float or list) – Upper momentum boundaries in the cycle\nfor each parameter group. Functionally,\nit defines the cycle amplitude (max_momentum - base_momentum).\nNote that momentum is cycled inversely\nto learning rate; at the start of a cycle, momentum is ‘max_momentum’\nand learning rate is ‘base_lr’\nDefault: 0.95\ndiv_factor (float) – Determines the initial learning rate via\ninitial_lr = max_lr/div_factor\nDefault: 25\nfinal_div_factor (float) – Determines the minimum learning rate via\nmin_lr = initial_lr/final_div_factor\nDefault: 1e4\nthree_phase (bool) – If True, use a third phase of the schedule to annihilate the\nlearning rate according to ‘final_div_factor’ instead of modifying the second\nphase (the first two phases will be symmetrical about the step indicated by\n‘pct_start’).\nlast_epoch (int) – The index of the last batch. This parameter is used when\nresuming a training job. Since step() should be invoked after each\nbatch instead of after each epoch, this number represents the total\nnumber of batches computed, not the total number of epochs computed.\nWhen last_epoch=-1, the schedule is started from the beginning.\nDefault: -1\nverbose (bool) – If True, prints a message to stdout for\neach update. Default: False.\nDeprecated since version 2.2: verbose is deprecated. Please use get_last_lr() to access the\nlearning rate.\nstate_dict (dict) – scheduler state. Should be an object returned\nfrom a call to state_dict().",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.jit.onednn_fusion_enabled",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.jit.onednn_fusion_enabled.html#torch.jit.onednn_fusion_enabled",
        "api_signature": "torch.jit.onednn_fusion_enabled()",
        "api_description": "Return whether onednn JIT fusion is enabled.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.one_hot_categorical.OneHotCategorical",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.one_hot_categorical.OneHotCategorical",
        "api_signature": "torch.distributions.one_hot_categorical.OneHotCategorical(probs=None, logits=None, validate_args=None)",
        "api_description": "Bases: Distribution",
        "return_value": "",
        "parameters": "probs (Tensor) – event probabilities\nlogits (Tensor) – event log probabilities (unnormalized)",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ones",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ones.html#torch.ones",
        "api_signature": "torch.ones(*size, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False)",
        "api_description": "Returns a tensor filled with the scalar value 1, with the shape defined\nby the variable argument size.",
        "return_value": "",
        "parameters": "size (int...) – a sequence of integers defining the shape of the output tensor.\nCan be a variable number of arguments or a collection like a list or tuple.\nout (Tensor, optional) – the output tensor.\ndtype (torch.dtype, optional) – the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_dtype()).\nlayout (torch.layout, optional) – the desired layout of returned Tensor.\nDefault: torch.strided.\ndevice (torch.device, optional) – the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_device()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types.\nrequires_grad (bool, optional) – If autograd should record operations on the\nreturned tensor. Default: False.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.init.ones_",
        "api_url": "https://pytorch.org/docs/stable/nn.init.html#torch.nn.init.ones_",
        "api_signature": "torch.nn.init.ones_(tensor)",
        "api_description": "Fill the input Tensor with the scalar value 1.",
        "return_value": "",
        "parameters": "tensor (Tensor) – an n-dimensional torch.Tensor",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ones_like",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ones_like.html#torch.ones_like",
        "api_signature": "torch.ones_like(input, *, dtype=None, layout=None, device=None, requires_grad=False, memory_format=torch.preserve_format)",
        "api_description": "Returns a tensor filled with the scalar value 1, with the same size as\ninput. torch.ones_like(input) is equivalent to\ntorch.ones(input.size(), dtype=input.dtype, layout=input.layout, device=input.device).",
        "return_value": "",
        "parameters": "input (Tensor) – the size of input will determine size of the output tensor.\ndtype (torch.dtype, optional) – the desired data type of returned Tensor.\nDefault: if None, defaults to the dtype of input.\nlayout (torch.layout, optional) – the desired layout of returned tensor.\nDefault: if None, defaults to the layout of input.\ndevice (torch.device, optional) – the desired device of returned tensor.\nDefault: if None, defaults to the device of input.\nrequires_grad (bool, optional) – If autograd should record operations on the\nreturned tensor. Default: False.\nmemory_format (torch.memory_format, optional) – the desired memory format of\nreturned Tensor. Default: torch.preserve_format.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.onnx.JitScalarType.onnx_compatible",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.onnx.JitScalarType.html#torch.onnx.JitScalarType.onnx_compatible",
        "api_signature": "onnx_compatible()",
        "api_description": "Return whether this JitScalarType is compatible with ONNX.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.onnx.JitScalarType.onnx_type",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.onnx.JitScalarType.html#torch.onnx.JitScalarType.onnx_type",
        "api_signature": "onnx_type()",
        "api_description": "Convert a JitScalarType to an ONNX data type.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.onnx.OnnxExporterError",
        "api_url": "https://pytorch.org/docs/stable/onnx_dynamo.html#torch.onnx.OnnxExporterError",
        "api_signature": "torch.onnx.OnnxExporterError(onnx_program, message)",
        "api_description": "Raised when an ONNX exporter error occurs.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.onnx.ONNXProgram",
        "api_url": "https://pytorch.org/docs/stable/onnx_dynamo.html#torch.onnx.ONNXProgram",
        "api_signature": "torch.onnx.ONNXProgram(model_proto, input_adapter, output_adapter, diagnostic_context, *, fake_context=None, export_exception=None, model_signature=None, model_torch=None)",
        "api_description": "An in-memory representation of a PyTorch model that has been exported to ONNX.",
        "return_value": "A sequence of tensors converted from PyTorch model inputs.\nPyTorch model outputs in exported ONNX model outputs format.\n",
        "parameters": "model_proto (onnx.ModelProto) – The exported ONNX model as an onnx.ModelProto.\ndiagnostic_context (diagnostics.DiagnosticContext) – Context object for the SARIF diagnostic system responsible for logging errors and metadata.\nfake_context (Optional[ONNXFakeContext]) – The fake context used for symbolic tracing.\nexport_exception (Optional[Exception]) – The exception that occurred during export, if any.\nmodel_signature (Optional[torch.export.ExportGraphSignature]) – The model signature for the exported ONNX graph.\nIf not specified, the model used during export is used.\nRequired when enable_fake_mode() is used to extract real initializers as needed by the ONNX graph.\nIf not specified, the model used during export is used.\nRequired when enable_fake_mode() is used to extract real initializers as needed by the ONNX graph.\ndestination (Union[str, BufferedIOBase]) – The destination to save the ONNX model. It can be either a string or a file-like object.\nWhen used with model_state, it must be a string with a full path to the destination.\nIf destination is a string, besides saving the ONNX model into a file, model weights are also stored\nin separate files in the same directory as the ONNX model. E.g. for destination=”/path/model.onnx”,\nthe initializers are saved in “/path/” folder along with “onnx.model”.\nIt can be either a string with the path to a checkpoint or a dictionary with the actual model state.\nThe supported file formats are the same as those supported by torch.load and safetensors.safe_open.\nRequired when enable_fake_mode() is used but real initializers are needed on the ONNX graph.\nserializer (Optional[ONNXProgramSerializer]) – The serializer to use. If not specified, the model will be serialized as Protobuf.\ndestination (str) – The destination to save the diagnostics SARIF log.\nIt must have a .sarif extension.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.onnx.ONNXProgramSerializer",
        "api_url": "https://pytorch.org/docs/stable/onnx_dynamo.html#torch.onnx.ONNXProgramSerializer",
        "api_signature": "torch.onnx.ONNXProgramSerializer(*args, **kwargs)",
        "api_description": "Protocol for serializing an ONNX graph into a specific format (e.g. Protobuf).\nNote that this is an advanced usage scenario.",
        "return_value": "",
        "parameters": "onnx_program (ONNXProgram) – Represents the in-memory exported ONNX model\ndestination (BufferedIOBase) – A binary IO stream or pre-allocated buffer into which\nthe serialized model should be written.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.onnx.OnnxRegistry",
        "api_url": "https://pytorch.org/docs/stable/onnx_dynamo.html#torch.onnx.OnnxRegistry",
        "api_signature": null,
        "api_description": "Registry for ONNX functions.",
        "return_value": "A list of ONNXFunctions corresponding to the given name, or None if\nthe name is not in the registry.\nTrue if the given op is registered, otherwise False.\n",
        "parameters": "namespace (str) – The namespace of the operator to get.\nop_name (str) – The name of the operator to get.\noverload (Optional[str]) – The overload of the operator to get. If it’s default overload,\nleave it to None.\nnamespace (str) – The namespace of the operator to check.\nop_name (str) – The name of the operator to check.\noverload (Optional[str]) – The overload of the operator to check. If it’s default overload,\nleave it to None.\nfunction (Union[onnxscript.OnnxFunction, onnxscript.TracedOnnxFunction]) – The onnx-sctip function to register.\nnamespace (str) – The namespace of the operator to register.\nop_name (str) – The name of the operator to register.\noverload (Optional[str]) – The overload of the operator to register. If it’s default overload,\nleave it to None.\nis_complex (bool) – Whether the function is a function that handles complex valued inputs.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.onnx.ONNXRuntimeOptions",
        "api_url": "https://pytorch.org/docs/stable/onnx_dynamo.html#torch.onnx.ONNXRuntimeOptions",
        "api_signature": "torch.onnx.ONNXRuntimeOptions(*, session_options=None, execution_providers=None, execution_provider_options=None)",
        "api_description": "Options to influence the execution of the ONNX model through ONNX Runtime.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.onnx.OnnxRegistry.opset_version",
        "api_url": "https://pytorch.org/docs/stable/onnx_dynamo.html#torch.onnx.OnnxRegistry.opset_version",
        "api_signature": null,
        "api_description": "The ONNX opset version the exporter should target. Defaults to the latest\nsupported ONNX opset version: 18. The default version will increment over time as\nONNX continues to evolve.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.fsdp.FullyShardedDataParallel.optim_state_dict",
        "api_url": "https://pytorch.org/docs/stable/fsdp.html#torch.distributed.fsdp.FullyShardedDataParallel.optim_state_dict",
        "api_signature": "optim_state_dict(model, optim, optim_state_dict=None, group=None)",
        "api_description": "Transform the state-dict of an optimizer corresponding to a sharded model.",
        "return_value": "A dict containing the optimizer state for\nmodel. The sharding of the optimizer state is based on\nstate_dict_type.\n",
        "parameters": "model (torch.nn.Module) – Root module (which may or may not be a\nFullyShardedDataParallel instance) whose parameters\nwere passed into the optimizer optim.\noptim (torch.optim.Optimizer) – Optimizer for model ‘s\nparameters.\noptim_state_dict (Dict[str, Any]) – the target optimizer state_dict to\ntransform. If the value is None, optim.state_dict() will be used. (\nDefault: None)\ngroup (dist.ProcessGroup) – Model’s process group across which parameters\nare sharded or None if using the default process group. (\nDefault: None)",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.fsdp.FullyShardedDataParallel.optim_state_dict_to_load",
        "api_url": "https://pytorch.org/docs/stable/fsdp.html#torch.distributed.fsdp.FullyShardedDataParallel.optim_state_dict_to_load",
        "api_signature": "optim_state_dict_to_load(model, optim, optim_state_dict, is_named_optimizer=False, load_directly=False, group=None)",
        "api_description": "Convert an optimizer state-dict so that it can be loaded into the optimizer associated with the FSDP model.",
        "return_value": "",
        "parameters": "model (torch.nn.Module) – Root module (which may or may not be a\nFullyShardedDataParallel instance) whose parameters\nwere passed into the optimizer optim.\noptim (torch.optim.Optimizer) – Optimizer for model ‘s\nparameters.\noptim_state_dict (Dict[str, Any]) – The optimizer states to be loaded.\nis_named_optimizer (bool) – Is this optimizer a NamedOptimizer or\nKeyedOptimizer. Only set to True if optim is TorchRec’s\nKeyedOptimizer or torch.distributed’s NamedOptimizer.\nload_directly (bool) – If this is set to True, this API will also\ncall optim.load_state_dict(result) before returning the result.\nOtherwise, users are responsible to call optim.load_state_dict()\n(Default: False)\ngroup (dist.ProcessGroup) – Model’s process group across which parameters\nare sharded or None if using the default process group. (\nDefault: None)",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.jit.optimize_for_inference",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.jit.optimize_for_inference.html#torch.jit.optimize_for_inference",
        "api_signature": "torch.jit.optimize_for_inference(mod, other_methods=None)",
        "api_description": "Perform a set of optimization passes to optimize a model for the purposes of inference.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.mobile_optimizer.optimize_for_mobile",
        "api_url": "https://pytorch.org/docs/stable/mobile_optimizer.html#torch.utils.mobile_optimizer.optimize_for_mobile",
        "api_signature": "torch.utils.mobile_optimizer.optimize_for_mobile(script_module, optimization_blocklist=None, preserved_methods=None, backend='CPU')",
        "api_description": "Optimize a torch script module for mobile deployment.",
        "return_value": "A new optimized torch script module\n",
        "parameters": "script_module (ScriptModule) – An instance of torch script module with type of ScriptModule.\noptimization_blocklist (Optional[Set[_MobileOptimizerType]]) – A set with type of MobileOptimizerType. When set is not passed,\noptimization method will run all the optimizer pass; otherwise, optimizer\nmethod will run the optimization pass that is not included inside optimization_blocklist.\npreserved_methods (Optional[List]) – A list of methods that needed to be preserved when freeze_module pass is invoked\nbackend (str) – Device type to use for running the result model (‘CPU’(default), ‘Vulkan’ or ‘Metal’).",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.Optimizer",
        "api_url": "https://pytorch.org/docs/stable/optim.html#torch.optim.Optimizer",
        "api_signature": "torch.optim.Optimizer(params, defaults)",
        "api_description": "Base class for all optimizers.",
        "return_value": "",
        "parameters": "params (iterable) – an iterable of torch.Tensor s or\ndict s. Specifies what Tensors should be optimized.\ndefaults (Dict[str, Any]) – (dict): a dict containing default values of optimization\noptions (used when a parameter group doesn’t specify them).",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.fsdp.OptimStateDictConfig",
        "api_url": "https://pytorch.org/docs/stable/fsdp.html#torch.distributed.fsdp.OptimStateDictConfig",
        "api_signature": "torch.distributed.fsdp.OptimStateDictConfig(offload_to_cpu=True)",
        "api_description": "OptimStateDictConfig is the base class for all optim_state_dict\nconfiguration classes.  Users should instantiate a child class (e.g.\nFullOptimStateDictConfig) in order to configure settings for the\ncorresponding optim_state_dict type supported by FSDP.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.orgqr",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.orgqr.html#torch.orgqr",
        "api_signature": "torch.orgqr(input, tau)",
        "api_description": "Alias for torch.linalg.householder_product().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.orgqr",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.orgqr.html#torch.Tensor.orgqr",
        "api_signature": "Tensor.orgqr(input2)",
        "api_description": "See torch.orgqr()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ormqr",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ormqr.html#torch.ormqr",
        "api_signature": "torch.ormqr(input, tau, other, left=True, transpose=False, *, out=None)",
        "api_description": "Computes the matrix-matrix multiplication of a product of Householder matrices with a general matrix.",
        "return_value": "",
        "parameters": "input (Tensor) – tensor of shape (*, mn, k) where * is zero or more batch dimensions\nand mn equals to m or n depending on the left.\ntau (Tensor) – tensor of shape (*, min(mn, k)) where * is zero or more batch dimensions.\nother (Tensor) – tensor of shape (*, m, n) where * is zero or more batch dimensions.\nleft (bool) – controls the order of multiplication.\ntranspose (bool) – controls whether the matrix Q is conjugate transposed or not.\nout (Tensor, optional) – the output Tensor. Ignored if None. Default: None.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.ormqr",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.ormqr.html#torch.Tensor.ormqr",
        "api_signature": "Tensor.ormqr(input2, input3, left=True, transpose=False)",
        "api_description": "See torch.ormqr()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.utils.parametrizations.orthogonal",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.utils.parametrizations.orthogonal.html#torch.nn.utils.parametrizations.orthogonal",
        "api_signature": "torch.nn.utils.parametrizations.orthogonal(module, name='weight', orthogonal_map=None, *, use_trivialization=True)",
        "api_description": "Apply an orthogonal or unitary parametrization to a matrix or a batch of matrices.",
        "return_value": "The original module with an orthogonal parametrization registered to the specified\nweight\n",
        "parameters": "module (nn.Module) – module on which to register the parametrization.\nname (str, optional) – name of the tensor to make orthogonal. Default: \"weight\".\northogonal_map (str, optional) – One of the following: \"matrix_exp\", \"cayley\", \"householder\".\nDefault: \"matrix_exp\" if the matrix is square or complex, \"householder\" otherwise.\nuse_trivialization (bool, optional) – whether to use the dynamic trivialization framework.\nDefault: True.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.init.orthogonal_",
        "api_url": "https://pytorch.org/docs/stable/nn.init.html#torch.nn.init.orthogonal_",
        "api_signature": "torch.nn.init.orthogonal_(tensor, gain=1, generator=None)",
        "api_description": "Fill the input Tensor with a (semi) orthogonal matrix.",
        "return_value": "",
        "parameters": "tensor – an n-dimensional torch.Tensor, where n≥2n \\geq 2n≥2\ngain – optional scaling factor\ngenerator (Optional[Generator]) – the torch Generator to sample from (default: None)",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.outer",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.outer.html#torch.outer",
        "api_signature": "torch.outer(input, vec2, *, out=None)",
        "api_description": "Outer product of input and vec2.\nIf input is a vector of size nnn and vec2 is a vector of\nsize mmm, then out must be a matrix of size (n×m)(n \\times m)(n×m).",
        "return_value": "",
        "parameters": "input (Tensor) – 1-D input vector\nvec2 (Tensor) – 1-D input vector\nout (Tensor, optional) – optional output matrix",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.outer",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.outer.html#torch.Tensor.outer",
        "api_signature": "Tensor.outer(vec2)",
        "api_description": "See torch.outer().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda.OutOfMemoryError",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.cuda.OutOfMemoryError.html#torch.cuda.OutOfMemoryError",
        "api_signature": null,
        "api_description": "Exception raised when CUDA is out of memory",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.Graph.output",
        "api_url": "https://pytorch.org/docs/stable/fx.html#torch.fx.Graph.output",
        "api_signature": "output(result, type_expr=None)",
        "api_description": "Insert an output Node into the Graph. An output node represents\na return statement in Python code. result is the value that should\nbe returned.",
        "return_value": "",
        "parameters": "result (Argument) – The value to be returned.\ntype_expr (Optional[Any]) – an optional type annotation representing the\nPython type the output of this node will have.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.Interpreter.output",
        "api_url": "https://pytorch.org/docs/stable/fx.html#torch.fx.Interpreter.output",
        "api_signature": "output(target, args, kwargs)",
        "api_description": "Execute an output node. This really just retrieves\nthe value referenced by the output node and returns it.",
        "return_value": "The return value referenced by the output node\n",
        "parameters": "target (Target) – The call target for this node. See\nNode for\ndetails on semantics\nargs (Tuple) – Tuple of positional args for this invocation\nkwargs (Dict) – Dict of keyword arguments for this invocation",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.backend_config.ObservationType.OUTPUT_SHARE_OBSERVER_WITH_INPUT",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.quantization.backend_config.ObservationType.html#torch.ao.quantization.backend_config.ObservationType.OUTPUT_SHARE_OBSERVER_WITH_INPUT",
        "api_signature": null,
        "api_description": "this means the output will use the same observer instance as input, based\non qconfig.activation\nexample: torch.cat, maxpool",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.backend_config.ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.quantization.backend_config.ObservationType.html#torch.ao.quantization.backend_config.ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT",
        "api_signature": null,
        "api_description": "this means input and output are observed with different observers, based\non qconfig.activation\nexample: conv, linear, softmax",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.ns._numeric_suite_fx.OutputComparisonLogger",
        "api_url": "https://pytorch.org/docs/stable/torch.ao.ns._numeric_suite_fx.html#torch.ao.ns._numeric_suite_fx.OutputComparisonLogger",
        "api_signature": "torch.ao.ns._numeric_suite_fx.OutputComparisonLogger(*args, **kwargs)",
        "api_description": "Same as OutputLogger, but also requires the original activation\nin order to calculate the comparison at calibration time",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.export.graph_signature.OutputKind",
        "api_url": "https://pytorch.org/docs/stable/export.html#torch.export.graph_signature.OutputKind",
        "api_signature": "torch.export.graph_signature.OutputKind(value)",
        "api_description": "An enumeration.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.ns._numeric_suite.OutputLogger",
        "api_url": "https://pytorch.org/docs/stable/torch.ao.ns._numeric_suite.html#torch.ao.ns._numeric_suite.OutputLogger",
        "api_signature": null,
        "api_description": "Class used to log the outputs of the module",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.ns._numeric_suite_fx.OutputLogger",
        "api_url": "https://pytorch.org/docs/stable/torch.ao.ns._numeric_suite_fx.html#torch.ao.ns._numeric_suite_fx.OutputLogger",
        "api_signature": "torch.ao.ns._numeric_suite_fx.OutputLogger(ref_node_name, prev_node_name, model_name, ref_name, prev_node_target_type, ref_node_target_type, results_type, index_within_arg, index_of_arg, fqn, qconfig_str='')",
        "api_description": "Base class for capturing intermediate values.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.export.graph_signature.OutputSpec",
        "api_url": "https://pytorch.org/docs/stable/export.html#torch.export.graph_signature.OutputSpec",
        "api_signature": "torch.export.graph_signature.OutputSpec(kind: torch.export.graph_signature.OutputKind, arg: Union[torch.export.graph_signature.TensorArgument, torch.export.graph_signature.SymIntArgument, torch.export.graph_signature.ConstantArgument, torch.export.graph_signature.CustomObjArgument], target: Union[str, NoneType])",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.rpc.PyRRef.owner",
        "api_url": "https://pytorch.org/docs/stable/rpc.html#torch.distributed.rpc.PyRRef.owner",
        "api_signature": "owner(self: torch._C._distributed_rpc.PyRRef)",
        "api_description": "Returns worker information of the node that owns this RRef.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.rpc.PyRRef.owner_name",
        "api_url": "https://pytorch.org/docs/stable/rpc.html#torch.distributed.rpc.PyRRef.owner_name",
        "api_signature": "owner_name(self: torch._C._distributed_rpc.PyRRef)",
        "api_description": "Returns worker name of the node that owns this RRef.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.P2POp",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#torch.distributed.P2POp",
        "api_signature": "torch.distributed.P2POp(op, tensor, peer, group=None, tag=0)",
        "api_description": "A class to build point-to-point operations for batch_isend_irecv.",
        "return_value": "",
        "parameters": "op (Callable) – A function to send data to or receive data from a peer process.\nThe type of op is either torch.distributed.isend or\ntorch.distributed.irecv.\ntensor (Tensor) – Tensor to send or receive.\npeer (int) – Destination or source rank.\ngroup (ProcessGroup, optional) – The process group to work on. If None,\nthe default process group will be used.\ntag (int, optional) – Tag to match send with recv.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.utils.rnn.pack_padded_sequence",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pack_padded_sequence.html#torch.nn.utils.rnn.pack_padded_sequence",
        "api_signature": "torch.nn.utils.rnn.pack_padded_sequence(input, lengths, batch_first=False, enforce_sorted=True)",
        "api_description": "Packs a Tensor containing padded sequences of variable length.",
        "return_value": "a PackedSequence object\n",
        "parameters": "input (Tensor) – padded batch of variable length sequences.\nlengths (Tensor or list(int)) – list of sequence lengths of each batch\nelement (must be on the CPU if provided as a tensor).\nbatch_first (bool, optional) – if True, the input is expected in B x T x *\nformat.\nenforce_sorted (bool, optional) – if True, the input is expected to\ncontain sequences sorted by length in a decreasing order. If\nFalse, the input will get sorted unconditionally. Default: True.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.utils.rnn.pack_sequence",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pack_sequence.html#torch.nn.utils.rnn.pack_sequence",
        "api_signature": "torch.nn.utils.rnn.pack_sequence(sequences, enforce_sorted=True)",
        "api_description": "Packs a list of variable length Tensors.",
        "return_value": "a PackedSequence object\n",
        "parameters": "sequences (list[Tensor]) – A list of sequences of decreasing length.\nenforce_sorted (bool, optional) – if True, checks that the input\ncontains sequences sorted by length in a decreasing order. If\nFalse, this condition is not checked. Default: True.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.package.PackageExporter",
        "api_url": "https://pytorch.org/docs/stable/package.html#torch.package.PackageExporter",
        "api_signature": "torch.package.PackageExporter(f, importer=<torch.package.importer._SysImporter object>, debug=False)",
        "api_description": "Exporters allow you to write packages of code, pickled Python data, and\narbitrary binary and text resources into a self-contained package.",
        "return_value": "A dot representation containing all paths from src to dst.\n(https://graphviz.org/doc/info/lang.html)\nA list containing the names of modules which will be\ndenied in this package.\nA string representation of dependencies in package.\nA list containing the names of modules which will be\nexterned in this package.\nA list containing the names of modules which depend on module_name.\nA list containing the names of modules which will be\ninterned in this package.\nA list containing the names of modules which will be\nmocked in this package.\nA handle that can be used to remove the added hook by calling\nhandle.remove().\nA handle that can be used to remove the added hook by calling\nhandle.remove().\nA handle that can be used to remove the added hook by calling\nhandle.remove().\n",
        "parameters": "f (Union[str, Path, BinaryIO]) – The location to export to. Can be a  string/Path object containing a filename\nor a binary I/O object.\nimporter (Union[Importer, Sequence[Importer]]) – If a single Importer is passed, use that to search for modules.\nIf a sequence of importers are passed, an OrderedImporter will be constructed out of them.\ndebug (bool) – If set to True, add path of broken modules to PackagingErrors.\ninclude (Union[List[str], str]) – A string e.g. \"my_package.my_subpackage\", or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as described in mock().\nexclude (Union[List[str], str]) – An optional pattern that excludes some patterns that match the include string.\ninclude (Union[List[str], str]) – A string e.g. \"my_package.my_subpackage\", or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as\ndescribed in mock().\nexclude (Union[List[str], str]) – An optional pattern that excludes some patterns that match the\ninclude string.\nallow_empty (bool) – An optional flag that specifies whether the extern modules specified by this call\nto the extern method must be matched to some module during packaging. If an extern module glob\npattern is added with allow_empty=False, and close() is called (either explicitly or via\n__exit__) before any modules match that pattern, an exception is thrown. If allow_empty=True,\nno such exception is thrown.\ninclude (Union[List[str], str]) – A string e.g. “my_package.my_subpackage”, or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as described in mock().\nexclude (Union[List[str], str]) – An optional pattern that excludes some patterns that match the include string.\nallow_empty (bool) – An optional flag that specifies whether the intern modules specified by this call\nto the intern method must be matched to some module during packaging. If an intern module glob\npattern is added with allow_empty=False, and close() is called (either explicitly or via __exit__)\nbefore any modules match that pattern, an exception is thrown. If allow_empty=True, no such exception is thrown.\ninclude (Union[List[str], str]) – A string e.g. \"my_package.my_subpackage\", or list of strings\nfor the names of the modules to be mocked out. Strings can also be a glob-style pattern\nstring that may match multiple modules. Any required dependencies that match this pattern\nstring will be mocked out automatically.\nExamples :'torch.**' – matches torch and all submodules of torch, e.g. 'torch.nn'\nand 'torch.nn.functional'\n'torch.*' – matches 'torch.nn' or 'torch.functional', but not\n'torch.nn.functional'\nexclude (Union[List[str], str]) – An optional pattern that excludes some patterns that match the include string.\ne.g. include='torch.**', exclude='torch.foo' will mock all torch packages except 'torch.foo',\nDefault: is [].\nallow_empty (bool) – An optional flag that specifies whether the mock implementation(s) specified by this call\nto the mock() method must be matched to some module during packaging. If a mock is added with\nallow_empty=False, and close() is called (either explicitly or via __exit__) and the mock has\nnot been matched to a module used by the package being exported, an exception is thrown.\nIf allow_empty=True, no such exception is thrown.\npackage (str) – The name of module package this resource should go it (e.g. \"my_package.my_subpackage\").\nresource (str) – A unique name for the resource, used to identify it to load.\nbinary (str) – The data to save.\nmodule_name (str) – e.g. my_package.my_subpackage, code will be saved to provide code\nfor this package.\ndependencies (bool, optional) – If True, we scan the source for dependencies.\npackage (str) – The name of module package this resource should go in (e.g. \"my_package.my_subpackage\").\nresource (str) – A unique name for the resource, used to identify it to load.\nobj (Any) – The object to save, must be picklable.\ndependencies (bool, optional) – If True, we scan the source for dependencies.\nmodule_name (str) – e.g. \"my_package.my_subpackage\", code will be saved to provide code for this package.\nfile_or_directory (str) – the path to a file or directory of code. When a directory, all python files in the directory\nare recursively copied using save_source_file(). If a file is named \"/__init__.py\" the code is treated\nas a package.\ndependencies (bool, optional) – If True, we scan the source for dependencies.\nmodule_name (str) – e.g. my_package.my_subpackage, code will be saved to provide code for this package.\nsrc (str) – The Python source code to save for this package.\nis_package (bool, optional) – If True, this module is treated as a package. Packages are allowed to have submodules\n(e.g. my_package.my_subpackage.my_subsubpackage), and resources can be saved inside them. Defaults to False.\ndependencies (bool, optional) – If True, we scan the source for dependencies.\npackage (str) – The name of module package this resource should go it (e.g. \"my_package.my_subpackage\").\nresource (str) – A unique name for the resource, used to identify it to load.\ntext (str) – The contents to save.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.package.PackageImporter",
        "api_url": "https://pytorch.org/docs/stable/package.html#torch.package.PackageImporter",
        "api_signature": "torch.package.PackageImporter(file_or_buffer, module_allowed=<function PackageImporter.<lambda>>)",
        "api_description": "Importers allow you to load code written to packages by PackageExporter.\nCode is loaded in a hermetic way, using files from the package\nrather than the normal python import system. This allows\nfor the packaging of PyTorch model code and data so that it can be run\non a server or used in the future for transfer learning.",
        "return_value": "Directory\nThe (possibly already) loaded module.\nThe loaded data.\nThe unpickled object.\nThe loaded text.\nOptional[str] a python version e.g. 3.8.9 or None if no version was stored with this package\n",
        "parameters": "a string, or an os.PathLike object containing a filename.\nmodule_allowed (Callable[[str], bool], optional) – A method to determine if a externally provided module\nshould be allowed. Can be used to ensure packages loaded do not depend on modules that the server\ndoes not support. Defaults to allowing anything.\ninclude (Union[List[str], str]) – An optional string e.g. \"my_package.my_subpackage\", or optional list of strings\nfor the names of the files to be included in the zipfile representation. This can also be\na glob-style pattern, as described in PackageExporter.mock()\nexclude (Union[List[str], str]) – An optional pattern that excludes files whose name match the pattern.\nname (str) – Fully qualified name of the module to load.\npackage ([type], optional) – Unused, but present to match the signature of importlib.import_module. Defaults to None.\npackage (str) – The name of module package (e.g. \"my_package.my_subpackage\").\nresource (str) – The unique name for the resource.\npackage (str) – The name of module package (e.g. \"my_package.my_subpackage\").\nresource (str) – The unique name for the resource.\nmap_location – Passed to torch.load to determine how tensors are mapped to devices. Defaults to None.\npackage (str) – The name of module package (e.g. \"my_package.my_subpackage\").\nresource (str) – The unique name for the resource.\nencoding (str, optional) – Passed to decode. Defaults to 'utf-8'.\nerrors (str, optional) – Passed to decode. Defaults to 'strict'.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.package.PackagingError",
        "api_url": "https://pytorch.org/docs/stable/package.html#torch.package.PackagingError",
        "api_signature": "torch.package.PackagingError(dependency_graph, debug=False)",
        "api_description": "This exception is raised when there is an issue with exporting a package.\nPackageExporter will attempt to gather up all the errors and present\nthem to you at once.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.utils.rnn.PackedSequence",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.PackedSequence.html#torch.nn.utils.rnn.PackedSequence",
        "api_signature": "torch.nn.utils.rnn.PackedSequence(data, batch_sizes=None, sorted_indices=None, unsorted_indices=None)",
        "api_description": "Holds the data and list of batch_sizes of a packed sequence.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.functional.pad",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.functional.pad.html#torch.nn.functional.pad",
        "api_signature": "torch.nn.functional.pad(input, pad, mode='constant', value=None)",
        "api_description": "Pads tensor.",
        "return_value": "",
        "parameters": "input (Tensor) – N-dimensional tensor\npad (tuple) – m-elements tuple, where\nm2≤\\frac{m}{2} \\leq2m​≤ input dimensions and mmm is even.\nmode (str) – 'constant', 'reflect', 'replicate' or 'circular'.\nDefault: 'constant'\nvalue (Optional[float]) – fill value for 'constant' padding. Default: 0",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.utils.rnn.pad_packed_sequence",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pad_packed_sequence.html#torch.nn.utils.rnn.pad_packed_sequence",
        "api_signature": "torch.nn.utils.rnn.pad_packed_sequence(sequence, batch_first=False, padding_value=0.0, total_length=None)",
        "api_description": "Pad a packed batch of variable length sequences.",
        "return_value": "Tuple of Tensor containing the padded sequence, and a Tensor\ncontaining the list of lengths of each sequence in the batch.\nBatch elements will be re-ordered as they were ordered originally when\nthe batch was passed to pack_padded_sequence or pack_sequence.\n",
        "parameters": "sequence (PackedSequence) – batch to pad\nbatch_first (bool, optional) – if True, the output will be in B x T x *\nformat.\npadding_value (float, optional) – values for padded elements.\ntotal_length (int, optional) – if not None, the output will be padded to\nhave length total_length. This method will throw ValueError\nif total_length is less than the max sequence length in\nsequence.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.utils.rnn.pad_sequence",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pad_sequence.html#torch.nn.utils.rnn.pad_sequence",
        "api_signature": "torch.nn.utils.rnn.pad_sequence(sequences, batch_first=False, padding_value=0.0)",
        "api_description": "Pad a list of variable length Tensors with padding_value.",
        "return_value": "Tensor of size T x B x * if batch_first is False.\nTensor of size B x T x * otherwise\n",
        "parameters": "sequences (list[Tensor]) – list of variable length sequences.\nbatch_first (bool, optional) – output will be in B x T x * if True, or in\nT x B x * otherwise. Default: False.\npadding_value (float, optional) – value for padded elements. Default: 0.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.functional.pairwise_distance",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.functional.pairwise_distance.html#torch.nn.functional.pairwise_distance",
        "api_signature": "torch.nn.functional.pairwise_distance(x1, x2, p=2.0, eps=1e-6, keepdim=False)",
        "api_description": "See torch.nn.PairwiseDistance for details",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.PairwiseDistance",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.PairwiseDistance.html#torch.nn.PairwiseDistance",
        "api_signature": "torch.nn.PairwiseDistance(p=2.0, eps=1e-06, keepdim=False)",
        "api_description": "Computes the pairwise distance between input vectors, or between columns of input matrices.",
        "return_value": "",
        "parameters": "p (real, optional) – the norm degree. Can be negative. Default: 2\neps (float, optional) – Small value to avoid division by zero.\nDefault: 1e-6\nkeepdim (bool, optional) – Determines whether or not to keep the vector dimension.\nDefault: False",
        "input_shape": "\nInput1: (N,D)(N, D)(N,D) or (D)(D)(D) where N = batch dimension and D = vector dimension\nInput2: (N,D)(N, D)(N,D) or (D)(D)(D), same shape as the Input1\nOutput: (N)(N)(N) or ()()() based on input dimension.\nIf keepdim is True, then (N,1)(N, 1)(N,1) or (1)(1)(1) based on input dimension.\n\n",
        "notes": "",
        "code_example": ">>> pdist = nn.PairwiseDistance(p=2)\n>>> input1 = torch.randn(100, 128)\n>>> input2 = torch.randn(100, 128)\n>>> output = pdist(input1, input2)\n\n\n"
    },
    {
        "api_name": "torch.fx.experimental.symbolic_shapes.parallel_and",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.fx.experimental.symbolic_shapes.parallel_and.html#torch.fx.experimental.symbolic_shapes.parallel_and",
        "api_signature": "torch.fx.experimental.symbolic_shapes.parallel_and(*args)",
        "api_description": "Evaluate the logical FALSE of several arguments, avoiding guarding on\nunbacked SymInts if another argument is definitely False.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.__config__.parallel_info",
        "api_url": "https://pytorch.org/docs/stable/config_mod.html#torch.__config__.parallel_info",
        "api_signature": "torch.__config__.parallel_info()",
        "api_description": "Returns detailed string with parallelization settings",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.symbolic_shapes.parallel_or",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.fx.experimental.symbolic_shapes.parallel_or.html#torch.fx.experimental.symbolic_shapes.parallel_or",
        "api_signature": "torch.fx.experimental.symbolic_shapes.parallel_or(*args)",
        "api_description": "Evaluate the logical OR of several arguments, avoiding guarding on\nunbacked SymInts if another argument is definitely True.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.tensor.parallel.parallelize_module",
        "api_url": "https://pytorch.org/docs/stable/distributed.tensor.parallel.html#torch.distributed.tensor.parallel.parallelize_module",
        "api_signature": "torch.distributed.tensor.parallel.parallelize_module(module, device_mesh, parallelize_plan)",
        "api_description": "Apply Tensor Parallelism in PyTorch by parallelizing modules or sub-modules based on a user-specified plan.",
        "return_value": "A nn.Module object parallelized.\n",
        "parameters": "module (nn.Module) – Module to be parallelized.\ndevice_mesh (DeviceMesh) – Object which describes the mesh topology\nof devices for the DTensor.\nparallelize_plan (Union[ParallelStyle, Dict[str, ParallelStyle]]) – The plan used to parallelize the module. It can be either a\nParallelStyle object which contains how\nwe prepare input/output for Tensor Parallelism or it can be a\ndict of module FQN and its corresponding ParallelStyle object.",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> from torch.distributed.tensor.parallel import parallelize_module, ColwiseParallel\n>>> from torch.distributed.device_mesh import init_device_mesh\n>>>\n>>> # Define the module.\n>>> m = Model(...)\n>>> tp_mesh = init_device_mesh(\"cuda\", (8,))\n>>> m = parallelize_module(m, tp_mesh, {\"w1\": ColwiseParallel(), \"w2\": RowwiseParallel()})\n>>>\n\n\n"
    },
    {
        "api_name": "torch.distributions.bernoulli.Bernoulli.param_shape",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.bernoulli.Bernoulli.param_shape",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.binomial.Binomial.param_shape",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.binomial.Binomial.param_shape",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.categorical.Categorical.param_shape",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.categorical.Categorical.param_shape",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.continuous_bernoulli.ContinuousBernoulli.param_shape",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.continuous_bernoulli.ContinuousBernoulli.param_shape",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.multinomial.Multinomial.param_shape",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.multinomial.Multinomial.param_shape",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.negative_binomial.NegativeBinomial.param_shape",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.negative_binomial.NegativeBinomial.param_shape",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.one_hot_categorical.OneHotCategorical.param_shape",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.one_hot_categorical.OneHotCategorical.param_shape",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.param_shape",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.param_shape",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.parameter.Parameter",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html#torch.nn.parameter.Parameter",
        "api_signature": "torch.nn.parameter.Parameter(data=None, requires_grad=True)",
        "api_description": "A kind of Tensor that is to be considered a module parameter.",
        "return_value": "",
        "parameters": "data (Tensor) – parameter tensor.\nrequires_grad (bool, optional) – if the parameter requires gradient. Note that\nthe torch.no_grad() context does NOT affect the default behavior of\nParameter creation–the Parameter will still have requires_grad=True in\nno_grad mode. See Locally disabling gradient computation for more\ndetails. Default: True",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.ParameterDict",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.ParameterDict.html#torch.nn.ParameterDict",
        "api_signature": "torch.nn.ParameterDict(parameters=None)",
        "api_description": "Holds parameters in a dictionary.",
        "return_value": "",
        "parameters": "values (iterable, optional) – a mapping (dictionary) of\n(string : Any) or an iterable of key-value pairs\nof type (string, Any)\nkeys (iterable, string) – keys to make the new ParameterDict from\ndefault (Parameter, optional) – value to set for all keys\nkey (str) – key to get from the ParameterDict\ndefault (Parameter, optional) – value to return if key not present\nkey (str) – key to pop from the ParameterDict\nkey (str) – key to set default for\ndefault (Any) – the parameter set to the key\nparameters (iterable) – a mapping (dictionary) from string to\nParameter, or an iterable of\nkey-value pairs of type (string, Parameter)",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.ParameterList",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.ParameterList.html#torch.nn.ParameterList",
        "api_signature": "torch.nn.ParameterList(values=None)",
        "api_description": "Holds parameters in a list.",
        "return_value": "",
        "parameters": "parameters (iterable, optional) – an iterable of elements to add to the list.\nvalue (Any) – value to append\nvalues (iterable) – iterable of values to append",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.GradBucket.parameters",
        "api_url": "https://pytorch.org/docs/stable/ddp_comm_hooks.html#torch.distributed.GradBucket.parameters",
        "api_signature": "torch.distributed.GradBucket.parameters(self: torch._C._distributed_c10d.GradBucket)",
        "api_description": "A list of torch.Tensor. Each tensor in the list corresponds to a model\nparameter.",
        "return_value": "A list of torch.Tensor. Each tensor in the list corresponds to a model\nparameter.\n",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.export.ExportedProgram.parameters",
        "api_url": "https://pytorch.org/docs/stable/export.html#torch.export.ExportedProgram.parameters",
        "api_signature": "parameters()",
        "api_description": "Returns an iterator over original module’s parameters.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.jit.ScriptModule.parameters",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.jit.ScriptModule.html#torch.jit.ScriptModule.parameters",
        "api_signature": "parameters(recurse=True)",
        "api_description": "Return an iterator over module parameters.",
        "return_value": "",
        "parameters": "recurse (bool) – if True, then yields parameters of this module\nand all submodules. Otherwise, yields only parameters that\nare direct members of this module.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.Module.parameters",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.parameters",
        "api_signature": "parameters(recurse=True)",
        "api_description": "Return an iterator over module parameters.",
        "return_value": "",
        "parameters": "recurse (bool) – if True, then yields parameters of this module\nand all submodules. Otherwise, yields only parameters that\nare direct members of this module.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.utils.parameters_to_vector",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.utils.parameters_to_vector.html#torch.nn.utils.parameters_to_vector",
        "api_signature": "torch.nn.utils.parameters_to_vector(parameters)",
        "api_description": "Flatten an iterable of parameters into a single vector.",
        "return_value": "The parameters represented by a single vector\n",
        "parameters": "parameters (Iterable[Tensor]) – an iterable of Tensors that are the\nparameters of a model.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.utils.parametrize.ParametrizationList",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.utils.parametrize.ParametrizationList.html#torch.nn.utils.parametrize.ParametrizationList",
        "api_signature": "torch.nn.utils.parametrize.ParametrizationList(modules, original, unsafe=False)",
        "api_description": "A sequential container that holds and manages the original parameters or buffers of a parametrized torch.nn.Module.",
        "return_value": "",
        "parameters": "modules (sequence) – sequence of modules representing the parametrizations\noriginal (Parameter or Tensor) – parameter or buffer that is parametrized\nunsafe (bool) – a boolean flag that denotes whether the parametrization\nmay change the dtype and shape of the tensor. Default: False\nWarning: the parametrization is not checked for consistency upon registration.\nEnable this flag at your own risk.\nvalue (Tensor) – Value to which initialize the module",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.pareto.Pareto",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.pareto.Pareto",
        "api_signature": "torch.distributions.pareto.Pareto(scale, alpha, validate_args=None)",
        "api_description": "Bases: TransformedDistribution",
        "return_value": "",
        "parameters": "scale (float or Tensor) – Scale parameter of the distribution\nalpha (float or Tensor) – Shape parameter of the distribution",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.autograd.profiler.parse_nvprof_trace",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.autograd.profiler.parse_nvprof_trace.html#torch.autograd.profiler.parse_nvprof_trace",
        "api_signature": "torch.autograd.profiler.parse_nvprof_trace(path)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.Tracer.path_of_module",
        "api_url": "https://pytorch.org/docs/stable/fx.html#torch.fx.Tracer.path_of_module",
        "api_signature": "path_of_module(mod)",
        "api_description": "Helper method to find the qualified name of mod in the Module hierarchy\nof root. For example, if root has a submodule named foo, which has\na submodule named bar, passing bar into this function will return\nthe string “foo.bar”.",
        "return_value": "",
        "parameters": "mod (str) – The Module to retrieve the qualified name for.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.pca_lowrank",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.pca_lowrank.html#torch.pca_lowrank",
        "api_signature": "torch.pca_lowrank(A, q=None, center=True, niter=2)",
        "api_description": "Performs linear Principal Component Analysis (PCA) on a low-rank\nmatrix, batches of such matrices, or sparse matrix.",
        "return_value": "",
        "parameters": "A (Tensor) – the input tensor of size (∗,m,n)(*, m, n)(∗,m,n)\nq (int, optional) – a slightly overestimated rank of\nAAA. By default, q = min(6, m,\nn).\ncenter (bool, optional) – if True, center the input tensor,\notherwise, assume that the input is\ncentered.\nniter (int, optional) – the number of subspace iterations to\nconduct; niter must be a nonnegative\ninteger, and defaults to 2.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.multiprocessing.api.PContext",
        "api_url": "https://pytorch.org/docs/stable/elastic/multiprocessing.html#torch.distributed.elastic.multiprocessing.api.PContext",
        "api_signature": "torch.distributed.elastic.multiprocessing.api.PContext(name, entrypoint, args, envs, logs_specs, log_line_prefixes=None)",
        "api_description": "The base class that standardizes operations over a set of processes that are launched via different mechanisms.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.functional.pdist",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.functional.pdist.html#torch.nn.functional.pdist",
        "api_signature": "torch.nn.functional.pdist(input, p=2)",
        "api_description": "Computes the p-norm distance between every pair of row vectors in the input.\nThis is identical to the upper triangular portion, excluding the diagonal, of\ntorch.norm(input[:, None] - input, dim=2, p=p). This function will be faster\nif the rows are contiguous.",
        "return_value": "",
        "parameters": "input – input tensor of shape N×MN \\times MN×M.\np – p value for the p-norm distance to calculate between each vector pair\n∈[0,∞]\\in [0, \\infty]∈[0,∞].",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.qconfig.per_channel_dynamic_qconfig",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.quantization.qconfig.per_channel_dynamic_qconfig.html#torch.ao.quantization.qconfig.per_channel_dynamic_qconfig",
        "api_signature": null,
        "api_description": "Dynamic qconfig with weights quantized per channel.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.observer.PerChannelMinMaxObserver",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.quantization.observer.PerChannelMinMaxObserver.html#torch.ao.quantization.observer.PerChannelMinMaxObserver",
        "api_signature": "torch.ao.quantization.observer.PerChannelMinMaxObserver(ch_axis=0, dtype=torch.quint8, qscheme=torch.per_channel_affine, reduce_range=False, quant_min=None, quant_max=None, factory_kwargs=None, eps=1.1920928955078125e-07, is_dynamic=False, **kwargs)",
        "api_description": "Observer module for computing the quantization parameters based on the\nrunning per channel min and max values.",
        "return_value": "",
        "parameters": "ch_axis – Channel axis\ndtype – dtype argument to the quantize node needed to implement the\nreference model spec.\nqscheme – Quantization scheme to be used\nreduce_range – Reduces the range of the quantized data type by 1 bit\nquant_min – Minimum quantization value. If unspecified, it will follow the 8-bit setup.\nquant_max – Maximum quantization value. If unspecified, it will follow the 8-bit setup.\neps (Tensor) – Epsilon value for float32, Defaults to torch.finfo(torch.float32).eps.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.permute",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.permute.html#torch.permute",
        "api_signature": "torch.permute(input, dims)",
        "api_description": "Returns a view of the original tensor input with its dimensions permuted.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.\ndims (tuple of int) – The desired ordering of dimensions",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.permute",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.permute.html#torch.Tensor.permute",
        "api_signature": "Tensor.permute(*dims)",
        "api_description": "See torch.permute()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.distribution.Distribution.perplexity",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.distribution.Distribution.perplexity",
        "api_signature": "perplexity()",
        "api_description": "Returns perplexity of distribution, batched over batch_shape.",
        "return_value": "Tensor of shape batch_shape.\n",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.TypedStorage.pickle_storage_type",
        "api_url": "https://pytorch.org/docs/stable/storage.html#torch.TypedStorage.pickle_storage_type",
        "api_signature": "pickle_storage_type()",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.pin_memory",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.pin_memory.html#torch.Tensor.pin_memory",
        "api_signature": "Tensor.pin_memory()",
        "api_description": "Copies the tensor to pinned memory, if it’s not already pinned.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.TypedStorage.pin_memory",
        "api_url": "https://pytorch.org/docs/stable/storage.html#torch.TypedStorage.pin_memory",
        "api_signature": "pin_memory(device='cuda')",
        "api_description": "Copy the CPU TypedStorage to pinned memory, if it’s not already pinned.",
        "return_value": "A pinned CPU storage.\n",
        "parameters": "device (str or torch.device) – The device to pin memory on. Default: 'cuda'.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.UntypedStorage.pin_memory",
        "api_url": "https://pytorch.org/docs/stable/storage.html#torch.UntypedStorage.pin_memory",
        "api_signature": "pin_memory(device='cuda')",
        "api_description": "Copy the CPU storage to pinned memory, if it’s not already pinned.",
        "return_value": "A pinned CPU storage.\n",
        "parameters": "device (str or torch.device) – The device to pin memory on. Default: 'cuda'.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.linalg.pinv",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.linalg.pinv.html#torch.linalg.pinv",
        "api_signature": "torch.linalg.pinv(A, *, atol=None, rtol=None, hermitian=False, out=None)",
        "api_description": "Computes the pseudoinverse (Moore-Penrose inverse) of a matrix.",
        "return_value": "",
        "parameters": "A (Tensor) – tensor of shape (*, m, n) where * is zero or more batch dimensions.\nrcond (float, Tensor, optional) – [NumPy Compat]. Alias for rtol. Default: None.\natol (float, Tensor, optional) – the absolute tolerance value. When None it’s considered to be zero.\nDefault: None.\nrtol (float, Tensor, optional) – the relative tolerance value. See above for the value it takes when None.\nDefault: None.\nhermitian (bool, optional) – indicates whether A is Hermitian if complex\nor symmetric if real. Default: False.\nout (Tensor, optional) – output tensor. Ignored if None. Default: None.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.pinverse",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.pinverse.html#torch.pinverse",
        "api_signature": "torch.pinverse(input, rcond=1e-15)",
        "api_description": "Alias for torch.linalg.pinv()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.pinverse",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.pinverse.html#torch.Tensor.pinverse",
        "api_signature": "Tensor.pinverse()",
        "api_description": "See torch.pinverse()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.pipeline.sync.Pipe",
        "api_url": "https://pytorch.org/docs/stable/pipeline.html#torch.distributed.pipeline.sync.Pipe",
        "api_signature": "torch.distributed.pipeline.sync.Pipe(module, chunks=1, checkpoint='except_last', deferred_batch_norm=False)",
        "api_description": "Wraps an arbitrary nn.Sequential module\nto train on using synchronous pipeline parallelism. If the module requires\nlots of memory and doesn’t fit on a single GPU, pipeline parallelism is a\nuseful technique to employ for training.",
        "return_value": "RRef to the output of the mini-batch\n",
        "parameters": "module (nn.Sequential) – sequential module to be parallelized using pipelining. Each module\nin the sequence has to have all of its parameters on a single\ndevice. Each module in the sequence has to either be an nn.Module\nor nn.Sequential (to combine multiple\nsequential modules on a single device)\nchunks (int) – number of micro-batches (default: 1)\ncheckpoint (str) – when to enable checkpointing, one of 'always',\n'except_last', or 'never' (default: 'except_last').\n'never' disables checkpointing completely, 'except_last'\nenables checkpointing for all micro-batches except the last one\nand 'always' enables checkpointing for all micro-batches.\ndeferred_batch_norm (bool) – whether to use deferred BatchNorm moving statistics (default:\nFalse). If set to True, we track statistics across\nmultiple micro-batches to update the running statistics per\nmini-batch.\ninputs – input mini-batch",
        "input_shape": "",
        "notes": "",
        "code_example": "Pipeline of two FC layers across GPUs 0 and 1.\n>>> # Need to initialize RPC framework first.\n>>> os.environ['MASTER_ADDR'] = 'localhost'\n>>> os.environ['MASTER_PORT'] = '29500'\n>>> torch.distributed.rpc.init_rpc('worker', rank=0, world_size=1)\n>>>\n>>> # Build pipe.\n>>> fc1 = nn.Linear(16, 8).cuda(0)\n>>> fc2 = nn.Linear(8, 4).cuda(1)\n>>> model = nn.Sequential(fc1, fc2)\n>>> model = Pipe(model, chunks=8)\n>>> input = torch.rand(16, 16).cuda(0)\n>>> output_rref = model(input)\n\n\n"
    },
    {
        "api_name": "torch.nn.functional.pixel_shuffle",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.functional.pixel_shuffle.html#torch.nn.functional.pixel_shuffle",
        "api_signature": "torch.nn.functional.pixel_shuffle(input, upscale_factor)",
        "api_description": "Rearranges elements in a tensor of shape (∗,C×r2,H,W)(*, C \\times r^2, H, W)(∗,C×r2,H,W) to a\ntensor of shape (∗,C,H×r,W×r)(*, C, H \\times r, W \\times r)(∗,C,H×r,W×r), where r is the upscale_factor.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor\nupscale_factor (int) – factor to increase spatial resolution by",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.functional.pixel_unshuffle",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.functional.pixel_unshuffle.html#torch.nn.functional.pixel_unshuffle",
        "api_signature": "torch.nn.functional.pixel_unshuffle(input, downscale_factor)",
        "api_description": "Reverses the PixelShuffle operation by rearranging elements in a\ntensor of shape (∗,C,H×r,W×r)(*, C, H \\times r, W \\times r)(∗,C,H×r,W×r) to a tensor of shape\n(∗,C×r2,H,W)(*, C \\times r^2, H, W)(∗,C×r2,H,W), where r is the downscale_factor.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor\ndownscale_factor (int) – factor to increase spatial resolution by",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.PixelShuffle",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.PixelShuffle.html#torch.nn.PixelShuffle",
        "api_signature": "torch.nn.PixelShuffle(upscale_factor)",
        "api_description": "Rearrange elements in a tensor according to an upscaling factor.",
        "return_value": "",
        "parameters": "upscale_factor (int) – factor to increase spatial resolution by",
        "input_shape": "\nInput: (∗,Cin,Hin,Win)(*, C_{in}, H_{in}, W_{in})(∗,Cin​,Hin​,Win​), where * is zero or more batch dimensions\nOutput: (∗,Cout,Hout,Wout)(*, C_{out}, H_{out}, W_{out})(∗,Cout​,Hout​,Wout​), where\n\n",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.PixelUnshuffle",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.PixelUnshuffle.html#torch.nn.PixelUnshuffle",
        "api_signature": "torch.nn.PixelUnshuffle(downscale_factor)",
        "api_description": "Reverse the PixelShuffle operation.",
        "return_value": "",
        "parameters": "downscale_factor (int) – factor to decrease spatial resolution by",
        "input_shape": "\nInput: (∗,Cin,Hin,Win)(*, C_{in}, H_{in}, W_{in})(∗,Cin​,Hin​,Win​), where * is zero or more batch dimensions\nOutput: (∗,Cout,Hout,Wout)(*, C_{out}, H_{out}, W_{out})(∗,Cout​,Hout​,Wout​), where\n\n",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.Graph.placeholder",
        "api_url": "https://pytorch.org/docs/stable/fx.html#torch.fx.Graph.placeholder",
        "api_signature": "placeholder(name, type_expr=None, default_value)",
        "api_description": "Insert a placeholder node into the Graph. A placeholder represents\na function input.",
        "return_value": "",
        "parameters": "name (str) – A name for the input value. This corresponds to the name\nof the positional argument to the function this Graph represents.\ntype_expr (Optional[Any]) – an optional type annotation representing the\nPython type the output of this node will have. This is needed in some\ncases for proper code generation (e.g. when the function is used\nsubsequently in TorchScript compilation).\ndefault_value (Any) – The default value this function argument should take\non. NOTE: to allow for None as a default value, inspect.Signature.empty\nshould be passed as this argument to specify that the parameter does _not_\nhave a default value.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.Interpreter.placeholder",
        "api_url": "https://pytorch.org/docs/stable/fx.html#torch.fx.Interpreter.placeholder",
        "api_signature": "placeholder(target, args, kwargs)",
        "api_description": "Execute a placeholder node. Note that this is stateful:\nInterpreter maintains an internal iterator over\narguments passed to run and this method returns\nnext() on that iterator.",
        "return_value": "The argument value that was retrieved.\n",
        "parameters": "target (Target) – The call target for this node. See\nNode for\ndetails on semantics\nargs (Tuple) – Tuple of positional args for this invocation\nkwargs (Dict) – Dict of keyword arguments for this invocation",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.Transformer.placeholder",
        "api_url": "https://pytorch.org/docs/stable/fx.html#torch.fx.Transformer.placeholder",
        "api_signature": "placeholder(target, args, kwargs)",
        "api_description": "Execute a placeholder node. In Transformer, this is\noverridden to insert a new placeholder into the output\ngraph.",
        "return_value": "",
        "parameters": "target (Target) – The call target for this node. See\nNode for\ndetails on semantics\nargs (Tuple) – Tuple of positional args for this invocation\nkwargs (Dict) – Dict of keyword arguments for this invocation",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.observer.PlaceholderObserver",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.quantization.observer.PlaceholderObserver.html#torch.ao.quantization.observer.PlaceholderObserver",
        "api_signature": "torch.ao.quantization.observer.PlaceholderObserver(dtype=torch.float32, custom_op_name='', compute_dtype=None, quant_min=None, quant_max=None, qscheme=None, eps=None, is_dynamic=False)",
        "api_description": "Observer that doesn’t do anything and just passes its configuration to the\nquantized module’s .from_float().",
        "return_value": "",
        "parameters": "dtype – dtype argument to the quantize node needed to implement the\nreference model spec.\nquant_min – minimum value in quantized domain (TODO: align behavior with other observers)\nquant_max – maximum value in quantized domain\ncustom_op_name – (temporary) specify this observer for an operator that doesn’t require any observation\n(Can be used in Graph Mode Passes for special case ops).\ncompute_dtype (deprecated) – if set, marks the future quantize function to use\ndynamic quantization instead of static quantization.\nThis field is deprecated, use is_dynamic=True instead.\nis_dynamic – if True, the quantize function in the reference model\nrepresentation taking stats from this observer instance will\nuse dynamic quantization.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.poisson.Poisson",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.poisson.Poisson",
        "api_signature": "torch.distributions.poisson.Poisson(rate, validate_args=None)",
        "api_description": "Bases: ExponentialFamily",
        "return_value": "",
        "parameters": "rate (Number, Tensor) – the rate parameter",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.poisson",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.poisson.html#torch.poisson",
        "api_signature": "torch.poisson(input, generator=None)",
        "api_description": "Returns a tensor of the same size as input with each element\nsampled from a Poisson distribution with rate parameter given by the corresponding\nelement in input i.e.,",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor containing the rates of the Poisson distribution\ngenerator (torch.Generator, optional) – a pseudorandom number generator for sampling",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.functional.poisson_nll_loss",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.functional.poisson_nll_loss.html#torch.nn.functional.poisson_nll_loss",
        "api_signature": "torch.nn.functional.poisson_nll_loss(input, target, log_input=True, full=False, size_average=None, eps=1e-08, reduce=None, reduction='mean')",
        "api_description": "Poisson negative log likelihood loss.",
        "return_value": "",
        "parameters": "input (Tensor) – expectation of underlying Poisson distribution.\ntarget (Tensor) – random sample target∼Poisson(input)target \\sim \\text{Poisson}(input)target∼Poisson(input).\nlog_input (bool) – if True the loss is computed as\nexp⁡(input)−target∗input\\exp(\\text{input}) - \\text{target} * \\text{input}exp(input)−target∗input, if False then loss is\ninput−target∗log⁡(input+eps)\\text{input} - \\text{target} * \\log(\\text{input}+\\text{eps})input−target∗log(input+eps). Default: True\nfull (bool) – whether to compute full loss, i. e. to add the Stirling\napproximation term. Default: False\ntarget∗log⁡(target)−target+0.5∗log⁡(2∗π∗target)\\text{target} * \\log(\\text{target}) - \\text{target} + 0.5 * \\log(2 * \\pi * \\text{target})target∗log(target)−target+0.5∗log(2∗π∗target).\nsize_average (bool, optional) – Deprecated (see reduction). By default,\nthe losses are averaged over each loss element in the batch. Note that for\nsome losses, there multiple elements per sample. If the field size_average\nis set to False, the losses are instead summed for each minibatch. Ignored\nwhen reduce is False. Default: True\neps (float, optional) – Small value to avoid evaluation of log⁡(0)\\log(0)log(0) when\nlog_input=False. Default: 1e-8\nreduce (bool, optional) – Deprecated (see reduction). By default, the\nlosses are averaged or summed over observations for each minibatch depending\non size_average. When reduce is False, returns a loss per\nbatch element instead and ignores size_average. Default: True\nreduction (str, optional) – Specifies the reduction to apply to the output:\n'none' | 'mean' | 'sum'. 'none': no reduction will be applied,\n'mean': the sum of the output will be divided by the number of\nelements in the output, 'sum': the output will be summed. Note: size_average\nand reduce are in the process of being deprecated, and in the meantime,\nspecifying either of those two args will override reduction. Default: 'mean'",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.PoissonNLLLoss",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.PoissonNLLLoss.html#torch.nn.PoissonNLLLoss",
        "api_signature": "torch.nn.PoissonNLLLoss(log_input=True, full=False, size_average=None, eps=1e-08, reduce=None, reduction='mean')",
        "api_description": "Negative log likelihood loss with Poisson distribution of target.",
        "return_value": "",
        "parameters": "log_input (bool, optional) – if True the loss is computed as\nexp⁡(input)−target∗input\\exp(\\text{input}) - \\text{target}*\\text{input}exp(input)−target∗input, if False the loss is\ninput−target∗log⁡(input+eps)\\text{input} - \\text{target}*\\log(\\text{input}+\\text{eps})input−target∗log(input+eps).\nfull (bool, optional) – whether to compute full loss, i. e. to add the\nStirling approximation term\ntarget∗log⁡(target)−target+0.5∗log⁡(2πtarget).\\text{target}*\\log(\\text{target}) - \\text{target} + 0.5 * \\log(2\\pi\\text{target}).\ntarget∗log(target)−target+0.5∗log(2πtarget).\nsize_average (bool, optional) – Deprecated (see reduction). By default,\nthe losses are averaged over each loss element in the batch. Note that for\nsome losses, there are multiple elements per sample. If the field size_average\nis set to False, the losses are instead summed for each minibatch. Ignored\nwhen reduce is False. Default: True\neps (float, optional) – Small value to avoid evaluation of log⁡(0)\\log(0)log(0) when\nlog_input = False. Default: 1e-8\nreduce (bool, optional) – Deprecated (see reduction). By default, the\nlosses are averaged or summed over observations for each minibatch depending\non size_average. When reduce is False, returns a loss per\nbatch element instead and ignores size_average. Default: True\nreduction (str, optional) – Specifies the reduction to apply to the output:\n'none' | 'mean' | 'sum'. 'none': no reduction will be applied,\n'mean': the sum of the output will be divided by the number of\nelements in the output, 'sum': the output will be summed. Note: size_average\nand reduce are in the process of being deprecated, and in the meantime,\nspecifying either of those two args will override reduction. Default: 'mean'",
        "input_shape": "\nInput: (∗)(*)(∗), where ∗*∗ means any number of dimensions.\nTarget: (∗)(*)(∗), same shape as the input.\nOutput: scalar by default. If reduction is 'none', then (∗)(*)(∗),\nthe same shape as the input.\n\n",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.polar",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.polar.html#torch.polar",
        "api_signature": "torch.polar(abs, angle, *, out=None)",
        "api_description": "Constructs a complex tensor whose elements are Cartesian coordinates\ncorresponding to the polar coordinates with absolute value abs and angle\nangle.",
        "return_value": "",
        "parameters": "abs (Tensor) – The absolute value the complex tensor. Must be float or double.\nangle (Tensor) – The angle of the complex tensor. Must be same dtype as\nabs.\nout (Tensor) – If the inputs are torch.float32, must be\ntorch.complex64. If the inputs are torch.float64, must be\ntorch.complex128.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.polygamma",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.polygamma.html#torch.polygamma",
        "api_signature": "torch.polygamma(n, input, *, out=None)",
        "api_description": "Alias for torch.special.polygamma().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.special.polygamma",
        "api_url": "https://pytorch.org/docs/stable/special.html#torch.special.polygamma",
        "api_signature": "torch.special.polygamma(n, input, *, out=None)",
        "api_description": "Computes the nthn^{th}nth derivative of the digamma function on input.\nn≥0n \\geq 0n≥0 is called the order of the polygamma function.",
        "return_value": "",
        "parameters": "n (int) – the order of the polygamma function\ninput (Tensor) – the input tensor.\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> a = torch.tensor([1, 0.5])\n>>> torch.special.polygamma(1, a)\ntensor([1.64493, 4.9348])\n>>> torch.special.polygamma(2, a)\ntensor([ -2.4041, -16.8288])\n>>> torch.special.polygamma(3, a)\ntensor([ 6.4939, 97.4091])\n>>> torch.special.polygamma(4, a)\ntensor([ -24.8863, -771.4742])\n\n\n"
    },
    {
        "api_name": "torch.Tensor.polygamma",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.polygamma.html#torch.Tensor.polygamma",
        "api_signature": "Tensor.polygamma(n)",
        "api_description": "See torch.polygamma()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.polygamma_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.polygamma_.html#torch.Tensor.polygamma_",
        "api_signature": "Tensor.polygamma_(n)",
        "api_description": "In-place version of polygamma()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.lr_scheduler.PolynomialLR",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.PolynomialLR.html#torch.optim.lr_scheduler.PolynomialLR",
        "api_signature": "torch.optim.lr_scheduler.PolynomialLR(optimizer, total_iters=5, power=1.0, last_epoch=-1, verbose='deprecated')",
        "api_description": "Decays the learning rate of each parameter group using a polynomial function\nin the given total_iters. When last_epoch=-1, sets initial lr as lr.",
        "return_value": "",
        "parameters": "optimizer (Optimizer) – Wrapped optimizer.\ntotal_iters (int) – The number of steps that the scheduler decays the learning rate. Default: 5.\npower (float) – The power of the polynomial. Default: 1.0.\nverbose (bool) – If True, prints a message to stdout for\neach update. Default: False.\nDeprecated since version 2.2: verbose is deprecated. Please use get_last_lr() to access the\nlearning rate.\nstate_dict (dict) – scheduler state. Should be an object returned\nfrom a call to state_dict().",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda.CUDAGraph.pool",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.cuda.CUDAGraph.html#torch.cuda.CUDAGraph.pool",
        "api_signature": "pool()",
        "api_description": "Return an opaque token representing the id of this graph’s memory pool.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.pipeline.sync.skip.skippable.pop",
        "api_url": "https://pytorch.org/docs/stable/pipeline.html#torch.distributed.pipeline.sync.skip.skippable.pop",
        "api_signature": "torch.distributed.pipeline.sync.skip.skippable.pop(name)",
        "api_description": "The command to pop a skip tensor.",
        "return_value": "the skip tensor previously stashed by another layer under the same name\n",
        "parameters": "name (str) – name of skip tensor",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.autograd.profiler_util.StringTable.pop",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.autograd.profiler_util.StringTable.html#torch.autograd.profiler_util.StringTable.pop",
        "api_signature": "pop(k[, d])",
        "api_description": "If key is not found, d is returned if given, otherwise KeyError is raised",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.ModuleDict.pop",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.ModuleDict.html#torch.nn.ModuleDict.pop",
        "api_signature": "pop(key)",
        "api_description": "Remove key from the ModuleDict and return its module.",
        "return_value": "",
        "parameters": "key (str) – key to pop from the ModuleDict",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.ParameterDict.pop",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.ParameterDict.html#torch.nn.ParameterDict.pop",
        "api_signature": "pop(key)",
        "api_description": "Remove key from the ParameterDict and return its parameter.",
        "return_value": "",
        "parameters": "key (str) – key to pop from the ParameterDict",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.autograd.profiler_util.StringTable.popitem",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.autograd.profiler_util.StringTable.html#torch.autograd.profiler_util.StringTable.popitem",
        "api_signature": "popitem()",
        "api_description": "Remove and return a (key, value) pair as a 2-tuple.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.ParameterDict.popitem",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.ParameterDict.html#torch.nn.ParameterDict.popitem",
        "api_signature": "popitem()",
        "api_description": "Remove and return the last inserted (key, parameter) pair from the ParameterDict.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.positive",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.positive.html#torch.positive",
        "api_signature": "torch.positive(input)",
        "api_description": "Returns input.\nThrows a runtime error if input is a bool tensor.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.positive",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.positive.html#torch.Tensor.positive",
        "api_signature": "Tensor.positive()",
        "api_description": "See torch.positive()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.transforms.PositiveDefiniteTransform",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.transforms.PositiveDefiniteTransform",
        "api_signature": "torch.distributions.transforms.PositiveDefiniteTransform(cache_size=0)",
        "api_description": "Transform from unconstrained matrices to positive-definite matrices.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.algorithms.JoinHook.post_hook",
        "api_url": "https://pytorch.org/docs/stable/distributed.algorithms.join.html#torch.distributed.algorithms.JoinHook.post_hook",
        "api_signature": "post_hook(is_last_joiner)",
        "api_description": "Call hook after all processes have joined.",
        "return_value": "",
        "parameters": "is_last_joiner (bool) – True if the rank is one of the last to\njoin; False otherwise.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.optim.PostLocalSGDOptimizer",
        "api_url": "https://pytorch.org/docs/stable/distributed.optim.html#torch.distributed.optim.PostLocalSGDOptimizer",
        "api_signature": "torch.distributed.optim.PostLocalSGDOptimizer(optim, averager)",
        "api_description": "Wraps an arbitrary torch.optim.Optimizer and runs post-local SGD,\nThis optimizer runs local optimizer at every step.\nAfter the warm-up stage, it averages parameters periodically afer the local optimizer is applied.",
        "return_value": "",
        "parameters": "optim (Optimizer) – The local optimizer.\naverager (ModelAverager) – A model averager instance to run post-localSGD algorithm.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.pow",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.pow.html#torch.pow",
        "api_signature": "torch.pow(input, exponent, *, out=None)",
        "api_description": "Takes the power of each element in input with exponent and\nreturns a tensor with the result.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.\nexponent (float or tensor) – the exponent value\nout (Tensor, optional) – the output tensor.\nself (float) – the scalar base value for the power operation\nexponent (Tensor) – the exponent tensor\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.pow",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.pow.html#torch.Tensor.pow",
        "api_signature": "Tensor.pow(exponent)",
        "api_description": "See torch.pow()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.pow_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.pow_.html#torch.Tensor.pow_",
        "api_signature": "Tensor.pow_(exponent)",
        "api_description": "In-place version of pow()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda.power_draw",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.cuda.power_draw.html#torch.cuda.power_draw",
        "api_signature": "torch.cuda.power_draw(device=None)",
        "api_description": "over the past sample period as given by nvidia-smi for Fermi or newer fully supported devices.",
        "return_value": "",
        "parameters": "device (torch.device or int, optional) – selected device. Returns\nstatistic for the current device, given by current_device(),\nif device is None (default).",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.powerSGD_hook",
        "api_url": "https://pytorch.org/docs/stable/ddp_comm_hooks.html#torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.powerSGD_hook",
        "api_signature": "torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.powerSGD_hook(state, bucket)",
        "api_description": "Implement PowerSGD algorithm.",
        "return_value": "Future handler of the communication, which updates the gradients in place.\n",
        "parameters": "state (PowerSGDState) – State information to configure the compression rate and support error feedback, warm start, etc.\nTo tune the compression configs, mainly need to tune matrix_approximation_rank, start_powerSGD_iter\nand min_compression_rate.\nbucket (dist.GradBucket) – Bucket that stores a 1D flattened gradient tensor that batches multiple per-variable tensors.\nNote that since DDP comm hook only supports single process single device mode,\nonly exactly one tensor is stored in this bucket.",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> state = PowerSGDState(process_group=process_group, matrix_approximation_rank=1,\n                          start_powerSGD_iter=10, min_compression_rate=0.5)\n>>> ddp_model.register_comm_hook(state, powerSGD_hook)\n\n\n"
    },
    {
        "api_name": "torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.PowerSGDState",
        "api_url": "https://pytorch.org/docs/stable/ddp_comm_hooks.html#torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.PowerSGDState",
        "api_signature": "torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.PowerSGDState(process_group, matrix_approximation_rank=1, start_powerSGD_iter=1000, min_compression_rate=2, use_error_feedback=True, warm_start=True, orthogonalization_epsilon=0, random_seed=0, compression_stats_logging_frequency=10000, batch_tensors_with_same_shape=False)",
        "api_description": "Store both the algorithm’s hyperparameters and internal state for all gradients during training.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.transforms.PowerTransform",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.transforms.PowerTransform",
        "api_signature": "torch.distributions.transforms.PowerTransform(exponent, cache_size=0)",
        "api_description": "Transform via the mapping y=xexponenty = x^{\\text{exponent}}y=xexponent.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.precision_matrix",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.precision_matrix",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.multivariate_normal.MultivariateNormal.precision_matrix",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.multivariate_normal.MultivariateNormal.precision_matrix",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.wishart.Wishart.precision_matrix",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.wishart.Wishart.precision_matrix",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.AdaptiveLogSoftmaxWithLoss.predict",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.AdaptiveLogSoftmaxWithLoss.html#torch.nn.AdaptiveLogSoftmaxWithLoss.predict",
        "api_signature": "predict(input)",
        "api_description": "Return the class with the highest probability for each example in the input minibatch.",
        "return_value": "a class with the highest probability for each example\n",
        "parameters": "input (Tensor) – a minibatch of examples",
        "input_shape": "\nInput: (N,in_features)(N, \\texttt{in\\_features})(N,in_features)\nOutput: (N)(N)(N)\n\n",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.backends.cuda.preferred_linalg_library",
        "api_url": "https://pytorch.org/docs/stable/backends.html#torch.backends.cuda.preferred_linalg_library",
        "api_signature": "torch.backends.cuda.preferred_linalg_library(backend=None)",
        "api_description": "Override the heuristic PyTorch uses to choose between cuSOLVER and MAGMA for CUDA linear algebra operations.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.PrefixStore",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#torch.distributed.PrefixStore",
        "api_signature": null,
        "api_description": "A wrapper around any of the 3 key-value stores (TCPStore,\nFileStore, and HashStore)\nthat adds a prefix to each key inserted to the store.",
        "return_value": "",
        "parameters": "prefix (str) – The prefix string that is prepended to each key before being inserted into the store.\nstore (torch.distributed.store) – A store object that forms the underlying key-value store.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.PReLU",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.PReLU.html#torch.nn.PReLU",
        "api_signature": "torch.nn.PReLU(num_parameters=1, init=0.25, device=None, dtype=None)",
        "api_description": "Applies the element-wise PReLU function.",
        "return_value": "",
        "parameters": "num_parameters (int) – number of aaa to learn.\nAlthough it takes an int as input, there is only two values are legitimate:\n1, or the number of channels at input. Default: 1\ninit (float) – the initial value of aaa. Default: 0.25",
        "input_shape": "\nInput: (∗)( *)(∗) where * means, any number of additional\ndimensions.\nOutput: (∗)(*)(∗), same shape as the input.\n\n",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.functional.prelu",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.functional.prelu.html#torch.nn.functional.prelu",
        "api_signature": "torch.nn.functional.prelu(input, weight)",
        "api_description": "Applies element-wise the function\nPReLU(x)=max⁡(0,x)+weight∗min⁡(0,x)\\text{PReLU}(x) = \\max(0,x) + \\text{weight} * \\min(0,x)PReLU(x)=max(0,x)+weight∗min(0,x) where weight is a\nlearnable parameter.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.prepare",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.quantization.prepare.html#torch.ao.quantization.prepare",
        "api_signature": "torch.ao.quantization.prepare(model, inplace=False, allow_list=None, observer_non_leaf_module_list=None, prepare_custom_config_dict=None)",
        "api_description": "Prepares a copy of the model for quantization calibration or quantization-aware training.",
        "return_value": "",
        "parameters": "model – input model to be modified in-place\ninplace – carry out model transformations in-place, the original module is mutated\nallow_list – list of quantizable modules\nobserver_non_leaf_module_list – list of non-leaf modules we want to add observer\nprepare_custom_config_dict – customization configuration dictionary for prepare function",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.quantize_fx.prepare_fx",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.quantization.quantize_fx.prepare_fx.html#torch.ao.quantization.quantize_fx.prepare_fx",
        "api_signature": "torch.ao.quantization.quantize_fx.prepare_fx(model, qconfig_mapping, example_inputs, prepare_custom_config=None, _equalization_config=None, backend_config=None)",
        "api_description": "Prepare a model for post training quantization",
        "return_value": "A GraphModule with observer (configured by qconfig_mapping), ready for calibration\n",
        "parameters": "model (*) – torch.nn.Module model\nqconfig_mapping (*) – QConfigMapping object to configure how a model is\nquantized, see QConfigMapping\nfor more details\nexample_inputs (*) – Example inputs for forward function of the model,\nTuple of positional args (keyword args can be passed as positional args as well)\nprepare_custom_config (*) – customization configuration for quantization tool.\nSee PrepareCustomConfig for more details\n_equalization_config (*) – config for specifying how to perform equalization on the model\nbackend_config (*) – config that specifies how operators are quantized\nin a backend, this includes how the operators are observed,\nsupported fusion patterns, how quantize/dequantize ops are\ninserted, supported dtypes etc. See BackendConfig for more details",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.checkpoint.format_utils.BroadcastingTorchSaveReader.prepare_global_plan",
        "api_url": "https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.format_utils.BroadcastingTorchSaveReader.prepare_global_plan",
        "api_signature": "prepare_global_plan(global_plan)",
        "api_description": "Implementation of the StorageReader method",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.checkpoint.StorageReader.prepare_global_plan",
        "api_url": "https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.StorageReader.prepare_global_plan",
        "api_signature": "prepare_global_plan(plans)",
        "api_description": "Perform centralized planning of storage loading.",
        "return_value": "A list of transformed LoadPlan after storage global planning\n",
        "parameters": "plans (List[LoadPlan]) – A list of LoadPlan instances, one for each rank.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.checkpoint.StorageWriter.prepare_global_plan",
        "api_url": "https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.StorageWriter.prepare_global_plan",
        "api_signature": "prepare_global_plan(plans)",
        "api_description": "Perform centralized planning of storage.",
        "return_value": "A list of transformed SavePlan after storage global planning\n",
        "parameters": "plans (List[SavePlan]) – A list of SavePlan instances, one for each rank.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.checkpoint.format_utils.BroadcastingTorchSaveReader.prepare_local_plan",
        "api_url": "https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.format_utils.BroadcastingTorchSaveReader.prepare_local_plan",
        "api_signature": "prepare_local_plan(plan)",
        "api_description": "Implementation of the StorageReader method",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.checkpoint.StorageReader.prepare_local_plan",
        "api_url": "https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.StorageReader.prepare_local_plan",
        "api_signature": "prepare_local_plan(plan)",
        "api_description": "Perform storage-specific local planning.",
        "return_value": "A transformed LoadPlan after storage local planning\n",
        "parameters": "plan (LoadPlan) – The local plan from the LoadPlan in use.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.checkpoint.StorageWriter.prepare_local_plan",
        "api_url": "https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.StorageWriter.prepare_local_plan",
        "api_signature": "prepare_local_plan(plan)",
        "api_description": "Perform storage-specific local planning.",
        "return_value": "A transformed SavePlan after storage local planning\n",
        "parameters": "plan (SavePlan) – The local plan from the SavePlanner in use.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.ns._numeric_suite.prepare_model_outputs",
        "api_url": "https://pytorch.org/docs/stable/torch.ao.ns._numeric_suite.html#torch.ao.ns._numeric_suite.prepare_model_outputs",
        "api_signature": "torch.ao.ns._numeric_suite.prepare_model_outputs(float_module, q_module, logger_cls=<class 'torch.ao.ns._numeric_suite.OutputLogger'>, allow_list=None)",
        "api_description": "Prepare the model by attaching the logger to both float module\nand quantized module if they are in the allow_list.",
        "return_value": "",
        "parameters": "float_module (Module) – float module used to generate the q_module\nq_module (Module) – module quantized from float_module\nlogger_cls – type of logger to be attached to float_module and q_module\nallow_list – list of module types to attach logger",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.ns._numeric_suite.prepare_model_with_stubs",
        "api_url": "https://pytorch.org/docs/stable/torch.ao.ns._numeric_suite.html#torch.ao.ns._numeric_suite.prepare_model_with_stubs",
        "api_signature": "torch.ao.ns._numeric_suite.prepare_model_with_stubs(float_module, q_module, module_swap_list, logger_cls)",
        "api_description": "Prepare the model by attaching the float module to its matching quantized\nmodule as the shadow if the float module type is in module_swap_list.",
        "return_value": "",
        "parameters": "float_module (Module) – float module used to generate the q_module\nq_module (Module) – module quantized from float_module\nmodule_swap_list (Set[type]) – list of float module types to attach the shadow\nlogger_cls (Callable) – type of logger to be used in shadow module to process the outputs of\nquantized module and its float shadow module",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.ns._numeric_suite_fx.prepare_n_shadows_model",
        "api_url": "https://pytorch.org/docs/stable/torch.ao.ns._numeric_suite_fx.html#torch.ao.ns._numeric_suite_fx.prepare_n_shadows_model",
        "api_signature": "torch.ao.ns._numeric_suite_fx.prepare_n_shadows_model(model, example_inputs, qconfig_multi_mapping, backend_config, custom_prepare_fn=None, custom_prepare_kwargs=None, custom_tracer=None)",
        "api_description": "Given a model with a graph with M ops such as",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.prepare_qat",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.quantization.prepare_qat.html#torch.ao.quantization.prepare_qat",
        "api_signature": "torch.ao.quantization.prepare_qat(model, mapping=None, inplace=False)",
        "api_description": "Prepares a copy of the model for quantization calibration or\nquantization-aware training and converts it to quantized version.",
        "return_value": "",
        "parameters": "model – input model to be modified in-place\nmapping – dictionary that maps float modules to quantized modules to be\nreplaced.\ninplace – carry out model transformations in-place, the original module\nis mutated",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.quantize_fx.prepare_qat_fx",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.quantization.quantize_fx.prepare_qat_fx.html#torch.ao.quantization.quantize_fx.prepare_qat_fx",
        "api_signature": "torch.ao.quantization.quantize_fx.prepare_qat_fx(model, qconfig_mapping, example_inputs, prepare_custom_config=None, backend_config=None)",
        "api_description": "Prepare a model for quantization aware training",
        "return_value": "A GraphModule with fake quant modules (configured by qconfig_mapping and backend_config), ready for\nquantization aware training\n",
        "parameters": "model (*) – torch.nn.Module model\nqconfig_mapping (*) – see prepare_fx()\nexample_inputs (*) – see prepare_fx()\nprepare_custom_config (*) – see prepare_fx()\nbackend_config (*) – see prepare_fx()",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.fx.custom_config.PrepareCustomConfig",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.quantization.fx.custom_config.PrepareCustomConfig.html#torch.ao.quantization.fx.custom_config.PrepareCustomConfig",
        "api_signature": null,
        "api_description": "Custom configuration for prepare_fx() and\nprepare_qat_fx().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.tensor.parallel.PrepareModuleInput",
        "api_url": "https://pytorch.org/docs/stable/distributed.tensor.parallel.html#torch.distributed.tensor.parallel.PrepareModuleInput",
        "api_signature": "torch.distributed.tensor.parallel.PrepareModuleInput(*, input_layouts, desired_input_layouts, use_local_output=False)",
        "api_description": "Configure the nn.Module’s inputs to convert the input tensors of the nn.Module to DTensors at runtime according to\ninput_layouts, and perform layout redistribution according to the desired_input_layouts.",
        "return_value": "A ParallelStyle object that prepares the sharding layouts of the nn.Module’s inputs.\n",
        "parameters": "input_layouts (Union[Placement, Tuple[Placement]]) – The DTensor layouts of input tensors for the nn.Module, this is used to convert the input tensors to\nDTensors. If some inputs are not torch.Tensor or no need to convert to DTensors, None need to be specified\nas a placeholder.\ndesired_input_layouts (Union[Placement, Tuple[Placement]]) – The desired DTensor layout of input tensors for the nn.Module, this is used to ensure the inputs of the nn.Module\nhave the desired DTensor layouts. This argument needs to have the same length with input_layouts.\nuse_local_output (bool, optional) – Whether to use local torch.Tensor instead of DTensor for the module inputs, default: False.",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> from torch.distributed.tensor.parallel import parallelize_module, PrepareModuleInput\n>>> from torch.distributed.device_mesh import init_device_mesh\n>>> ...\n>>> block = TransformerBlock(...)  # block is a nn.Module that contains an \"attn\" Attention submodule\n>>> tp_mesh = init_device_mesh(\"cuda\", (8,))\n>>>\n>>> # According to the style specified below, the first input of attn will be annotated to Sharded DTensor\n>>> # and then redistributed to Replicated DTensor.\n>>> parallelize_module(\n>>>     block, # this can be a submodule or module\n>>>     tp_mesh,\n>>>     parallelize_plan={\n>>>         \"attn\": PrepareModuleInput(\n>>>             input_layouts=(Shard(0), None, None, ...),\n>>>             desired_input_layouts=(Replicate(), None, None, ...)\n>>>         ),\n>>>     }\n>>> )\n\n\n"
    },
    {
        "api_name": "torch.distributed.tensor.parallel.PrepareModuleOutput",
        "api_url": "https://pytorch.org/docs/stable/distributed.tensor.parallel.html#torch.distributed.tensor.parallel.PrepareModuleOutput",
        "api_signature": "torch.distributed.tensor.parallel.PrepareModuleOutput(*, output_layouts, desired_output_layouts, use_local_output=True)",
        "api_description": "Configure the nn.Module’s outputs to convert the output tensors of the nn.Module to DTensors at runtime according to\noutput_layouts, and perform layout redistribution according to the desired_output_layouts.",
        "return_value": "A ParallelStyle object that prepares the sharding layouts of the nn.Module’s outputs.\n",
        "parameters": "output_layouts (Union[Placement, Tuple[Placement]]) – The DTensor layouts of output tensors for the nn.Module, this is used to convert the output tensors to\nDTensors if they are torch.Tensor. If some outputs are not torch.Tensor or no need to convert to DTensors,\nNone need to be specified as a placeholder.\ndesired_output_layouts (Union[Placement, Tuple[Placement]]) – The desired DTensor layouts of output tensors for the nn.Module, this is used to ensure the outputs of the nn.Module\nhave the desired DTensor layouts.\nuse_local_output (bool, optional) – Whether to use local torch.Tensor instead of DTensor for the module outputs, default: True.",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> from torch.distributed.tensor.parallel import parallelize_module, PrepareModuleOutput\n>>> from torch.distributed.device_mesh import init_device_mesh\n>>> ...\n>>> block = TransformerBlock(...)  # block is a nn.Module that contains an \"attn\" Attention submodule\n>>> tp_mesh = init_device_mesh(\"cuda\", (8,))\n>>>\n>>> # According to the style specified below, the output of the TransformerBlock will be converted to Replicated DTensor\n>>> # and then redistributed to Sharded DTensor.\n>>> parallelize_module(\n>>>     block, # this can be a submodule or module\n>>>     tp_mesh,\n>>>     parallelize_plan = PrepareModuleOutput(\n>>>         output_layouts=Replicate(),\n>>>         desired_output_layouts=Shard(0)\n>>>     )\n>>> )\n\n\n"
    },
    {
        "api_name": "torch.fx.Node.prepend",
        "api_url": "https://pytorch.org/docs/stable/fx.html#torch.fx.Node.prepend",
        "api_signature": "prepend(x)",
        "api_description": "Insert x before this node in the list of nodes in the graph. Example:",
        "return_value": "",
        "parameters": "x (Node) – The node to put before this node. Must be a member of the same graph.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.profiler._KinetoProfile.preset_metadata_json",
        "api_url": "https://pytorch.org/docs/stable/profiler.html#torch.profiler._KinetoProfile.preset_metadata_json",
        "api_signature": "preset_metadata_json(key, value)",
        "api_description": "Preset a user defined metadata when the profiler is not started\nand added into the trace file later.\nMetadata is in the format of a string key and a valid json value",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.symbolic_shapes.DimConstraints.prettify_results",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.fx.experimental.symbolic_shapes.DimConstraints.html#torch.fx.experimental.symbolic_shapes.DimConstraints.prettify_results",
        "api_signature": "prettify_results(original_signature, constraint_violation_error=None, forced_specializations=None)",
        "api_description": "Format a message for constraint violation erros",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.onnx.verification.GraphInfo.pretty_print_mismatch",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.onnx.verification.GraphInfo.html#torch.onnx.verification.GraphInfo.pretty_print_mismatch",
        "api_signature": "pretty_print_mismatch(graph=False)",
        "api_description": "Pretty print details of the mismatch between torch and ONNX.",
        "return_value": "",
        "parameters": "graph (bool) – If True, print the ATen JIT graph and ONNX graph.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.onnx.verification.GraphInfo.pretty_print_tree",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.onnx.verification.GraphInfo.html#torch.onnx.verification.GraphInfo.pretty_print_tree",
        "api_signature": "pretty_print_tree()",
        "api_description": "Pretty print GraphInfo tree.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.Node.prev",
        "api_url": "https://pytorch.org/docs/stable/fx.html#torch.fx.Node.prev",
        "api_signature": null,
        "api_description": "Returns the previous Node in the linked list of Nodes.",
        "return_value": "The previous Node in the linked list of Nodes.\n",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.autograd.forward_ad.UnpackedDualTensor.primal",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.autograd.forward_ad.UnpackedDualTensor.html#torch.autograd.forward_ad.UnpackedDualTensor.primal",
        "api_signature": null,
        "api_description": "Alias for field number 0",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.ns._numeric_suite_fx.print_comparisons_n_shadows_model",
        "api_url": "https://pytorch.org/docs/stable/torch.ao.ns._numeric_suite_fx.html#torch.ao.ns._numeric_suite_fx.print_comparisons_n_shadows_model",
        "api_signature": "torch.ao.ns._numeric_suite_fx.print_comparisons_n_shadows_model(results)",
        "api_description": "Prints a summary of extracted results.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.lr_scheduler.ChainedScheduler.print_lr",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.ChainedScheduler.html#torch.optim.lr_scheduler.ChainedScheduler.print_lr",
        "api_signature": "print_lr(is_verbose, group, lr, epoch=None)",
        "api_description": "Display the current learning rate.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.lr_scheduler.ConstantLR.print_lr",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.ConstantLR.html#torch.optim.lr_scheduler.ConstantLR.print_lr",
        "api_signature": "print_lr(is_verbose, group, lr, epoch=None)",
        "api_description": "Display the current learning rate.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.lr_scheduler.CosineAnnealingLR.print_lr",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.CosineAnnealingLR.html#torch.optim.lr_scheduler.CosineAnnealingLR.print_lr",
        "api_signature": "print_lr(is_verbose, group, lr, epoch=None)",
        "api_description": "Display the current learning rate.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.lr_scheduler.CosineAnnealingWarmRestarts.print_lr",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.CosineAnnealingWarmRestarts.html#torch.optim.lr_scheduler.CosineAnnealingWarmRestarts.print_lr",
        "api_signature": "print_lr(is_verbose, group, lr, epoch=None)",
        "api_description": "Display the current learning rate.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.lr_scheduler.CyclicLR.print_lr",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.CyclicLR.html#torch.optim.lr_scheduler.CyclicLR.print_lr",
        "api_signature": "print_lr(is_verbose, group, lr, epoch=None)",
        "api_description": "Display the current learning rate.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.lr_scheduler.ExponentialLR.print_lr",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.ExponentialLR.html#torch.optim.lr_scheduler.ExponentialLR.print_lr",
        "api_signature": "print_lr(is_verbose, group, lr, epoch=None)",
        "api_description": "Display the current learning rate.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.lr_scheduler.LambdaLR.print_lr",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.LambdaLR.html#torch.optim.lr_scheduler.LambdaLR.print_lr",
        "api_signature": "print_lr(is_verbose, group, lr, epoch=None)",
        "api_description": "Display the current learning rate.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.lr_scheduler.LinearLR.print_lr",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.LinearLR.html#torch.optim.lr_scheduler.LinearLR.print_lr",
        "api_signature": "print_lr(is_verbose, group, lr, epoch=None)",
        "api_description": "Display the current learning rate.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.lr_scheduler.MultiplicativeLR.print_lr",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.MultiplicativeLR.html#torch.optim.lr_scheduler.MultiplicativeLR.print_lr",
        "api_signature": "print_lr(is_verbose, group, lr, epoch=None)",
        "api_description": "Display the current learning rate.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.lr_scheduler.MultiStepLR.print_lr",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.MultiStepLR.html#torch.optim.lr_scheduler.MultiStepLR.print_lr",
        "api_signature": "print_lr(is_verbose, group, lr, epoch=None)",
        "api_description": "Display the current learning rate.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.lr_scheduler.OneCycleLR.print_lr",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.OneCycleLR.html#torch.optim.lr_scheduler.OneCycleLR.print_lr",
        "api_signature": "print_lr(is_verbose, group, lr, epoch=None)",
        "api_description": "Display the current learning rate.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.lr_scheduler.PolynomialLR.print_lr",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.PolynomialLR.html#torch.optim.lr_scheduler.PolynomialLR.print_lr",
        "api_signature": "print_lr(is_verbose, group, lr, epoch=None)",
        "api_description": "Display the current learning rate.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.lr_scheduler.ReduceLROnPlateau.print_lr",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.ReduceLROnPlateau.html#torch.optim.lr_scheduler.ReduceLROnPlateau.print_lr",
        "api_signature": "print_lr(is_verbose, group, lr, epoch=None)",
        "api_description": "Display the current learning rate.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.lr_scheduler.SequentialLR.print_lr",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.SequentialLR.html#torch.optim.lr_scheduler.SequentialLR.print_lr",
        "api_signature": "print_lr(is_verbose, group, lr, epoch=None)",
        "api_description": "Display the current learning rate.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.lr_scheduler.StepLR.print_lr",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.StepLR.html#torch.optim.lr_scheduler.StepLR.print_lr",
        "api_signature": "print_lr(is_verbose, group, lr, epoch=None)",
        "api_description": "Display the current learning rate.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.GraphModule.print_readable",
        "api_url": "https://pytorch.org/docs/stable/fx.html#torch.fx.GraphModule.print_readable",
        "api_signature": "print_readable(print_output=True)",
        "api_description": "Return the Python code generated for current GraphModule and its children GraphModules",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.Graph.print_tabular",
        "api_url": "https://pytorch.org/docs/stable/fx.html#torch.fx.Graph.print_tabular",
        "api_signature": "print_tabular()",
        "api_description": "Prints the intermediate representation of the graph in tabular\nformat. Note that this API requires the tabulate module to be\ninstalled.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.bernoulli.Bernoulli.probs",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.bernoulli.Bernoulli.probs",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.binomial.Binomial.probs",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.binomial.Binomial.probs",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.categorical.Categorical.probs",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.categorical.Categorical.probs",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.continuous_bernoulli.ContinuousBernoulli.probs",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.continuous_bernoulli.ContinuousBernoulli.probs",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.geometric.Geometric.probs",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.geometric.Geometric.probs",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.multinomial.Multinomial.probs",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.multinomial.Multinomial.probs",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.negative_binomial.NegativeBinomial.probs",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.negative_binomial.NegativeBinomial.probs",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.one_hot_categorical.OneHotCategorical.probs",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.one_hot_categorical.OneHotCategorical.probs",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.probs",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.probs",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.relaxed_bernoulli.RelaxedBernoulli.probs",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.relaxed_bernoulli.RelaxedBernoulli.probs",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.probs",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.probs",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.Graph.process_inputs",
        "api_url": "https://pytorch.org/docs/stable/fx.html#torch.fx.Graph.process_inputs",
        "api_signature": "process_inputs(*args)",
        "api_description": "Processes args so that they can be passed to the FX graph.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.Graph.process_outputs",
        "api_url": "https://pytorch.org/docs/stable/fx.html#torch.fx.Graph.process_outputs",
        "api_signature": "process_outputs(out)",
        "api_description": "Warning",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.multiprocessing.errors.ProcessFailure",
        "api_url": "https://pytorch.org/docs/stable/elastic/errors.html#torch.distributed.elastic.multiprocessing.errors.ProcessFailure",
        "api_signature": "torch.distributed.elastic.multiprocessing.errors.ProcessFailure(local_rank, pid, exitcode, error_file)",
        "api_description": "Represent the failed process result. When the worker process fails, it may record failure root cause into the file.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.prod",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.prod.html#torch.prod",
        "api_signature": "torch.prod(input, *, dtype=None)",
        "api_description": "Returns the product of all elements in the input tensor.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.\ndtype (torch.dtype, optional) – the desired data type of returned tensor.\nIf specified, the input tensor is casted to dtype before the operation\nis performed. This is useful for preventing data type overflows. Default: None.\ninput (Tensor) – the input tensor.\ndim (int) – the dimension to reduce.\nkeepdim (bool) – whether the output tensor has dim retained or not.\ndtype (torch.dtype, optional) – the desired data type of returned tensor.\nIf specified, the input tensor is casted to dtype before the operation\nis performed. This is useful for preventing data type overflows. Default: None.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.prod",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.prod.html#torch.Tensor.prod",
        "api_signature": "Tensor.prod(dim=None, keepdim=False, dtype=None)",
        "api_description": "See torch.prod()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.symbolic_shapes.ShapeEnv.produce_guards",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.fx.experimental.symbolic_shapes.ShapeEnv.html#torch.fx.experimental.symbolic_shapes.ShapeEnv.produce_guards",
        "api_signature": "produce_guards(placeholders, sources, source_ref=<function ShapeEnv.<lambda>>, *, input_contexts=None, equalities_inputs=None, _simplified=False, ignore_static=True)",
        "api_description": "Generates a list of guards strings which, when evaluated in a context that\ndefines tensors for all the sources, returns True or False depending\non if the guards in the list evaluated to True or not.  Primarily used by Dynamo,\nbut this is also helpful for manual testing of guards (see\nevaluate_guards_for_args)",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.symbolic_shapes.ShapeEnv.produce_guards_expression",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.fx.experimental.symbolic_shapes.ShapeEnv.html#torch.fx.experimental.symbolic_shapes.ShapeEnv.produce_guards_expression",
        "api_signature": "produce_guards_expression(placeholders, ignore_static=True)",
        "api_description": "Expected to be used with evaluate_guards_expression(). Produces the guards\nfor the given placeholders and returns a string expression to be evaluated\nby evaluate_guards_expression given concrete values for the placeholders.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.metrics.prof",
        "api_url": "https://pytorch.org/docs/stable/elastic/metrics.html#torch.distributed.elastic.metrics.prof",
        "api_signature": "torch.distributed.elastic.metrics.prof(fn=None, group='torchelastic')",
        "api_description": "@profile decorator publishes duration.ms, count, success, failure metrics for the function that it decorates.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.autograd.profiler.profile",
        "api_url": "https://pytorch.org/docs/stable/autograd.html#torch.autograd.profiler.profile",
        "api_signature": "torch.autograd.profiler.profile(enabled=True, *, use_cuda=False, use_device=None, record_shapes=False, with_flops=False, profile_memory=False, with_stack=False, with_modules=False, use_kineto=False, use_cpu=True, use_mtia=False, experimental_config=None)",
        "api_description": "Context manager that manages autograd profiler state and holds a summary of results.",
        "return_value": "",
        "parameters": "enabled (bool, optional) – Setting this to False makes this context manager a no-op.\nuse_cuda (bool, optional) – Enables timing of CUDA events as well using the cudaEvent API.\nAdds approximately 4us of overhead to each tensor operation.\nrecord_shapes (bool, optional) – If shapes recording is set, information\nabout input dimensions will be collected. This allows one to see which\ndimensions have been used under the hood and further group by them\nusing prof.key_averages(group_by_input_shape=True). Please note that\nshape recording might skew your profiling data. It is recommended to\nuse separate runs with and without shape recording to validate the timing.\nMost likely the skew will be negligible for bottom most events (in a case\nof nested function calls). But for higher level functions the total\nself cpu time might be artificially increased because of the shape\ncollection.\nwith_flops (bool, optional) – If with_flops is set, the profiler will estimate\nthe FLOPs (floating point operations) value using the operator’s input shape.\nThis allows one to estimate the hardware performance. Currently,\nthis option only works for the matrix multiplication and 2D convolution operators.\nprofile_memory (bool, optional) – track tensor memory allocation/deallocation.\nwith_stack (bool, optional) – record source information (file and line number) for the ops.\nwith_modules (bool) – record module hierarchy (including function names)\ncorresponding to the callstack of the op. e.g. If module A’s forward call’s\nmodule B’s forward which contains an aten::add op,\nthen aten::add’s module hierarchy is A.B\nNote that this support exist, at the moment, only for TorchScript models\nand not eager mode models.\nuse_kineto (bool, optional) – experimental, enable profiling with Kineto profiler.\nuse_cpu (bool, optional) – profile CPU events; setting to False requires\nuse_kineto=True and can be used to lower the overhead for GPU-only profiling.\nexperimental_config (_ExperimentalConfig) – A set of experimental options\nused by profiler libraries like Kineto. Note, backward compatibility is not guaranteed.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.profiler.profile",
        "api_url": "https://pytorch.org/docs/stable/profiler.html#torch.profiler.profile",
        "api_signature": "torch.profiler.profile(*, activities=None, schedule=None, on_trace_ready=None, record_shapes=False, profile_memory=False, with_stack=False, with_flops=False, with_modules=False, experimental_config=None, execution_trace_observer=None, use_cuda=None)",
        "api_description": "Profiler context manager.",
        "return_value": "",
        "parameters": "activities (iterable) – list of activity groups (CPU, CUDA) to use in profiling, supported values:\ntorch.profiler.ProfilerActivity.CPU, torch.profiler.ProfilerActivity.CUDA.\nDefault value: ProfilerActivity.CPU and (when available) ProfilerActivity.CUDA.\nschedule (Callable) – callable that takes step (int) as a single parameter and returns\nProfilerAction value that specifies the profiler action to perform at each step.\non_trace_ready (Callable) – callable that is called at each step when schedule\nreturns ProfilerAction.RECORD_AND_SAVE during the profiling.\nrecord_shapes (bool) – save information about operator’s input shapes.\nprofile_memory (bool) – track tensor memory allocation/deallocation.\nwith_stack (bool) – record source information (file and line number) for the ops.\nwith_flops (bool) – use formula to estimate the FLOPs (floating point operations) of specific operators\n(matrix multiplication and 2D convolution).\nwith_modules (bool) – record module hierarchy (including function names)\ncorresponding to the callstack of the op. e.g. If module A’s forward call’s\nmodule B’s forward which contains an aten::add op,\nthen aten::add’s module hierarchy is A.B\nNote that this support exist, at the moment, only for TorchScript models\nand not eager mode models.\nexperimental_config (_ExperimentalConfig) – A set of experimental options\nused for Kineto library features. Note, backward compatibility is not guaranteed.\nrepresentation of AI/ML workloads and enable replay benchmarks, simulators, and emulators.\nWhen this argument is included the observer start() and stop() will be called for the\nuse_cuda (bool) –\nDeprecated since version 1.8.1: use activities instead.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.mps.profiler.profile",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.mps.profiler.profile.html#torch.mps.profiler.profile",
        "api_signature": "torch.mps.profiler.profile(mode='interval', wait_until_completed=False)",
        "api_description": "Context Manager to enabling generating OS Signpost tracing from MPS backend.",
        "return_value": "",
        "parameters": "mode (str) – OS Signpost tracing mode could be “interval”, “event”,\nor both “interval,event”.\nThe interval mode traces the duration of execution of the operations,\nwhereas event mode marks the completion of executions.\nSee document Recording Performance Data for more info.\nwait_until_completed (bool) – Waits until the MPS Stream complete\nexecuting each encoded GPU operation. This helps generating single\ndispatches on the trace’s timeline.\nNote that enabling this option would affect the performance negatively.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.profiler.ProfilerAction",
        "api_url": "https://pytorch.org/docs/stable/profiler.html#torch.profiler.ProfilerAction",
        "api_signature": "torch.profiler.ProfilerAction(value)",
        "api_description": "Profiler actions that can be taken at the specified intervals",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.profiler.ProfilerActivity",
        "api_url": "https://pytorch.org/docs/stable/profiler.html#torch.profiler.ProfilerActivity",
        "api_signature": null,
        "api_description": "Members:",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.promote_types",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.promote_types.html#torch.promote_types",
        "api_signature": "torch.promote_types(type1, type2)",
        "api_description": "Returns the torch.dtype with the smallest size and scalar kind that is\nnot smaller nor of lower kind than either type1 or type2. See type promotion\ndocumentation for more information on the type\npromotion logic.",
        "return_value": "",
        "parameters": "type1 (torch.dtype) –\ntype2 (torch.dtype) –",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.propagate_qconfig_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.quantization.propagate_qconfig_.html#torch.ao.quantization.propagate_qconfig_",
        "api_signature": "torch.ao.quantization.propagate_qconfig_(module, qconfig_dict=None, prepare_custom_config_dict=None)",
        "api_description": "Propagate qconfig through the module hierarchy and assign qconfig\nattribute on each leaf module",
        "return_value": "None, module is modified inplace with qconfig attached\n",
        "parameters": "module – input module\nqconfig_dict – dictionary that maps from name or type of submodule to\nquantization configuration, qconfig applies to all submodules of a\ngiven module unless qconfig for the submodules are specified (when\nthe submodule already has qconfig attribute)\nprepare_custom_config_dict – dictionary for custom handling of modules\nsee docs for prepare_fx()",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.Proxy",
        "api_url": "https://pytorch.org/docs/stable/fx.html#torch.fx.Proxy",
        "api_signature": "torch.fx.Proxy(node, tracer=None)",
        "api_description": "Proxy objects are Node wrappers that flow through the\nprogram during symbolic tracing and record all the operations\n(torch function calls, method calls, operators) that they touch\ninto the growing FX Graph.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.Tracer.proxy",
        "api_url": "https://pytorch.org/docs/stable/fx.html#torch.fx.Tracer.proxy",
        "api_signature": "proxy(node)",
        "api_description": "Note",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.utils.prune.BasePruningMethod.prune",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.BasePruningMethod.html#torch.nn.utils.prune.BasePruningMethod.prune",
        "api_signature": "prune(t, default_mask=None, importance_scores=None)",
        "api_description": "Compute and returns a pruned version of input tensor t.",
        "return_value": "pruned version of tensor t.\n",
        "parameters": "t (torch.Tensor) – tensor to prune (of same dimensions as\ndefault_mask).\nimportance_scores (torch.Tensor) – tensor of importance scores (of\nsame shape as t) used to compute mask for pruning t.\nThe values in this tensor indicate the importance of the\ncorresponding elements in the t that is being pruned.\nIf unspecified or None, the tensor t will be used in its place.\ndefault_mask (torch.Tensor, optional) – mask from previous pruning\niteration, if any. To be considered when determining what\nportion of the tensor that pruning should act on. If None,\ndefault to a mask of ones.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.utils.prune.CustomFromMask.prune",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.CustomFromMask.html#torch.nn.utils.prune.CustomFromMask.prune",
        "api_signature": "prune(t, default_mask=None, importance_scores=None)",
        "api_description": "Compute and returns a pruned version of input tensor t.",
        "return_value": "pruned version of tensor t.\n",
        "parameters": "t (torch.Tensor) – tensor to prune (of same dimensions as\ndefault_mask).\nimportance_scores (torch.Tensor) – tensor of importance scores (of\nsame shape as t) used to compute mask for pruning t.\nThe values in this tensor indicate the importance of the\ncorresponding elements in the t that is being pruned.\nIf unspecified or None, the tensor t will be used in its place.\ndefault_mask (torch.Tensor, optional) – mask from previous pruning\niteration, if any. To be considered when determining what\nportion of the tensor that pruning should act on. If None,\ndefault to a mask of ones.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.utils.prune.Identity.prune",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.Identity.html#torch.nn.utils.prune.Identity.prune",
        "api_signature": "prune(t, default_mask=None, importance_scores=None)",
        "api_description": "Compute and returns a pruned version of input tensor t.",
        "return_value": "pruned version of tensor t.\n",
        "parameters": "t (torch.Tensor) – tensor to prune (of same dimensions as\ndefault_mask).\nimportance_scores (torch.Tensor) – tensor of importance scores (of\nsame shape as t) used to compute mask for pruning t.\nThe values in this tensor indicate the importance of the\ncorresponding elements in the t that is being pruned.\nIf unspecified or None, the tensor t will be used in its place.\ndefault_mask (torch.Tensor, optional) – mask from previous pruning\niteration, if any. To be considered when determining what\nportion of the tensor that pruning should act on. If None,\ndefault to a mask of ones.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.utils.prune.L1Unstructured.prune",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.L1Unstructured.html#torch.nn.utils.prune.L1Unstructured.prune",
        "api_signature": "prune(t, default_mask=None, importance_scores=None)",
        "api_description": "Compute and returns a pruned version of input tensor t.",
        "return_value": "pruned version of tensor t.\n",
        "parameters": "t (torch.Tensor) – tensor to prune (of same dimensions as\ndefault_mask).\nimportance_scores (torch.Tensor) – tensor of importance scores (of\nsame shape as t) used to compute mask for pruning t.\nThe values in this tensor indicate the importance of the\ncorresponding elements in the t that is being pruned.\nIf unspecified or None, the tensor t will be used in its place.\ndefault_mask (torch.Tensor, optional) – mask from previous pruning\niteration, if any. To be considered when determining what\nportion of the tensor that pruning should act on. If None,\ndefault to a mask of ones.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.utils.prune.LnStructured.prune",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.LnStructured.html#torch.nn.utils.prune.LnStructured.prune",
        "api_signature": "prune(t, default_mask=None, importance_scores=None)",
        "api_description": "Compute and returns a pruned version of input tensor t.",
        "return_value": "pruned version of tensor t.\n",
        "parameters": "t (torch.Tensor) – tensor to prune (of same dimensions as\ndefault_mask).\nimportance_scores (torch.Tensor) – tensor of importance scores (of\nsame shape as t) used to compute mask for pruning t.\nThe values in this tensor indicate the importance of the\ncorresponding elements in the t that is being pruned.\nIf unspecified or None, the tensor t will be used in its place.\ndefault_mask (torch.Tensor, optional) – mask from previous pruning\niteration, if any. To be considered when determining what\nportion of the tensor that pruning should act on. If None,\ndefault to a mask of ones.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.utils.prune.PruningContainer.prune",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.PruningContainer.html#torch.nn.utils.prune.PruningContainer.prune",
        "api_signature": "prune(t, default_mask=None, importance_scores=None)",
        "api_description": "Compute and returns a pruned version of input tensor t.",
        "return_value": "pruned version of tensor t.\n",
        "parameters": "t (torch.Tensor) – tensor to prune (of same dimensions as\ndefault_mask).\nimportance_scores (torch.Tensor) – tensor of importance scores (of\nsame shape as t) used to compute mask for pruning t.\nThe values in this tensor indicate the importance of the\ncorresponding elements in the t that is being pruned.\nIf unspecified or None, the tensor t will be used in its place.\ndefault_mask (torch.Tensor, optional) – mask from previous pruning\niteration, if any. To be considered when determining what\nportion of the tensor that pruning should act on. If None,\ndefault to a mask of ones.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.utils.prune.RandomStructured.prune",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.RandomStructured.html#torch.nn.utils.prune.RandomStructured.prune",
        "api_signature": "prune(t, default_mask=None, importance_scores=None)",
        "api_description": "Compute and returns a pruned version of input tensor t.",
        "return_value": "pruned version of tensor t.\n",
        "parameters": "t (torch.Tensor) – tensor to prune (of same dimensions as\ndefault_mask).\nimportance_scores (torch.Tensor) – tensor of importance scores (of\nsame shape as t) used to compute mask for pruning t.\nThe values in this tensor indicate the importance of the\ncorresponding elements in the t that is being pruned.\nIf unspecified or None, the tensor t will be used in its place.\ndefault_mask (torch.Tensor, optional) – mask from previous pruning\niteration, if any. To be considered when determining what\nportion of the tensor that pruning should act on. If None,\ndefault to a mask of ones.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.utils.prune.RandomUnstructured.prune",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.RandomUnstructured.html#torch.nn.utils.prune.RandomUnstructured.prune",
        "api_signature": "prune(t, default_mask=None, importance_scores=None)",
        "api_description": "Compute and returns a pruned version of input tensor t.",
        "return_value": "pruned version of tensor t.\n",
        "parameters": "t (torch.Tensor) – tensor to prune (of same dimensions as\ndefault_mask).\nimportance_scores (torch.Tensor) – tensor of importance scores (of\nsame shape as t) used to compute mask for pruning t.\nThe values in this tensor indicate the importance of the\ncorresponding elements in the t that is being pruned.\nIf unspecified or None, the tensor t will be used in its place.\ndefault_mask (torch.Tensor, optional) – mask from previous pruning\niteration, if any. To be considered when determining what\nportion of the tensor that pruning should act on. If None,\ndefault to a mask of ones.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.utils.prune.PruningContainer",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.PruningContainer.html#torch.nn.utils.prune.PruningContainer",
        "api_signature": "torch.nn.utils.prune.PruningContainer(*args)",
        "api_description": "Container holding a sequence of pruning methods for iterative pruning.",
        "return_value": "pruned version of the input tensor\nnew mask that combines the effects\nof the default_mask and the new mask from the current\npruning method (of same dimensions as default_mask and\nt).\npruned version of tensor t.\n",
        "parameters": "method (subclass of BasePruningMethod) – child pruning method\nto be added to the container.\nmodule (nn.Module) – module containing the tensor to prune\nname (str) – parameter name within module on which pruning\nwill act.\nargs – arguments passed on to a subclass of\nBasePruningMethod\nimportance_scores (torch.Tensor) – tensor of importance scores (of\nsame shape as module parameter) used to compute mask for pruning.\nThe values in this tensor indicate the importance of the\ncorresponding elements in the parameter being pruned.\nIf unspecified or None, the parameter will be used in its place.\nkwargs – keyword arguments passed on to a subclass of a\nBasePruningMethod\nmodule (nn.Module) – module containing the tensor to prune\nt (torch.Tensor) – tensor representing the parameter to prune\n(of same dimensions as default_mask).\ndefault_mask (torch.Tensor) – mask from previous pruning iteration.\nt (torch.Tensor) – tensor to prune (of same dimensions as\ndefault_mask).\nimportance_scores (torch.Tensor) – tensor of importance scores (of\nsame shape as t) used to compute mask for pruning t.\nThe values in this tensor indicate the importance of the\ncorresponding elements in the t that is being pruned.\nIf unspecified or None, the tensor t will be used in its place.\ndefault_mask (torch.Tensor, optional) – mask from previous pruning\niteration, if any. To be considered when determining what\nportion of the tensor that pruning should act on. If None,\ndefault to a mask of ones.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.special.psi",
        "api_url": "https://pytorch.org/docs/stable/special.html#torch.special.psi",
        "api_signature": "torch.special.psi(input, *, out=None)",
        "api_description": "Alias for torch.special.digamma().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.put_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.put_.html#torch.Tensor.put_",
        "api_signature": "Tensor.put_(index, source, accumulate=False)",
        "api_description": "Copies the elements from source into the positions specified by\nindex. For the purpose of indexing, the self tensor is treated as if\nit were a 1-D tensor.",
        "return_value": "",
        "parameters": "index (LongTensor) – the indices into self\nsource (Tensor) – the tensor containing values to copy from\naccumulate (bool) – whether to accumulate into self",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.metrics.put_metric",
        "api_url": "https://pytorch.org/docs/stable/elastic/metrics.html#torch.distributed.elastic.metrics.put_metric",
        "api_signature": "torch.distributed.elastic.metrics.put_metric(metric_name, metric_value, metric_group='torchelastic')",
        "api_description": "Publish a metric data point.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.rpc.PyRRef",
        "api_url": "https://pytorch.org/docs/stable/rpc.html#torch.distributed.rpc.PyRRef",
        "api_signature": "torch.distributed.rpc.PyRRef(RRef)",
        "api_description": "A class encapsulating a reference to a value of some type on a remote\nworker. This handle will keep the referenced remote value alive on the\nworker. A UserRRef will be deleted when 1) no references to it in\nboth the application code and in the local RRef context, or 2) the\napplication has called a graceful shutdown. Invoking methods on a\ndeleted RRef leads to undefined behaviors. RRef implementation only\noffers best-effort error detection, and applications should not use\nUserRRefs after rpc.shutdown().",
        "return_value": "",
        "parameters": "value (object) – The value to be wrapped by this RRef.\ntype_hint (Type, optional) – Python type that should be passed to\nTorchScript compiler as type hint for value.\ndist_autograd_ctx_id (int, optional) – The distributed\nautograd context id for which we should retrieve the\ngradients (default: -1).\nretain_graph (bool, optional) – If False, the graph used to\ncompute the grad will be freed. Note that in nearly all\ncases setting this option to True is not needed and\noften can be worked around in a much more efficient way.\nUsually, you need to set this to True to run backward\nmultiple times (default: False).\ntimeout (float, optional) – Timeout for rref.remote(). If\nthe creation of this RRef\nis not successfully completed within the timeout, then the\nnext time there is an attempt to use the RRef\n(such as to_here), a timeout will be raised. If not\nprovided, the default RPC timeout will be used. Please see\nrpc.remote() for specific timeout semantics for\nRRef.\ntimeout (float, optional) – Timeout for rref.rpc_async().\nIf the call does not complete within this timeframe, an\nexception indicating so will be raised. If this argument\nis not provided, the default RPC timeout will be used.\ntimeout (float, optional) – Timeout for rref.rpc_sync().\nIf the call does not complete within this timeframe, an\nexception indicating so will be raised. If this argument\nis not provided, the default RPC timeout will be used.\ntimeout (float, optional) – Timeout for to_here. If\nthe call does not complete within this timeframe, an\nexception indicating so will be raised. If this\nargument is not provided, the default RPC timeout\n(60s) will be used.",
        "input_shape": "",
        "notes": "",
        "code_example": "Following examples skip RPC initialization and shutdown code\nfor simplicity. Refer to RPC docs for those details.\n\nCreate an RRef using rpc.remote\n\n>>> import torch\n>>> import torch.distributed.rpc as rpc\n>>> rref = rpc.remote(\"worker1\", torch.add, args=(torch.ones(2), 3))\n>>> # get a copy of value from the RRef\n>>> x = rref.to_here()\n\n\n\nCreate an RRef from a local object\n\n>>> import torch\n>>> from torch.distributed.rpc import RRef\n>>> x = torch.zeros(2, 2)\n>>> rref = RRef(x)\n\n\n\nShare an RRef with other workers\n\n>>> # On both worker0 and worker1:\n>>> def f(rref):\n>>>   return rref.to_here() + 1\n\n\n>>> # On worker0:\n>>> import torch\n>>> import torch.distributed.rpc as rpc\n>>> from torch.distributed.rpc import RRef\n>>> rref = RRef(torch.zeros(2, 2))\n>>> # the following RPC shares the rref with worker1, reference\n>>> # count is automatically updated.\n>>> rpc.rpc_sync(\"worker1\", f, args=(rref,))\n\n\n>>> import torch.distributed.autograd as dist_autograd\n>>> with dist_autograd.context() as context_id:\n>>>     rref.backward(context_id)\n\n\n>>> from torch.distributed import rpc\n>>> rref = rpc.remote(\"worker1\", torch.add, args=(torch.zeros(2, 2), 1))\n>>> rref.remote().size().to_here()  # returns torch.Size([2, 2])\n>>> rref.remote().view(1, 4).to_here()  # returns tensor([[1., 1., 1., 1.]])\n\n\n>>> from torch.distributed import rpc\n>>> rref = rpc.remote(\"worker1\", torch.add, args=(torch.zeros(2, 2), 1))\n>>> rref.rpc_async().size().wait()  # returns torch.Size([2, 2])\n>>> rref.rpc_async().view(1, 4).wait()  # returns tensor([[1., 1., 1., 1.]])\n\n\n>>> from torch.distributed import rpc\n>>> rref = rpc.remote(\"worker1\", torch.add, args=(torch.zeros(2, 2), 1))\n>>> rref.rpc_sync().size()  # returns torch.Size([2, 2])\n>>> rref.rpc_sync().view(1, 4)  # returns tensor([[1., 1., 1., 1.]])\n\n\n"
    },
    {
        "api_name": "torch.fx.Graph.python_code",
        "api_url": "https://pytorch.org/docs/stable/fx.html#torch.fx.Graph.python_code",
        "api_signature": "python_code(root_module, *, verbose=False)",
        "api_description": "Turn this Graph into valid Python code.",
        "return_value": "src: the Python source code representing the object\nglobals: a dictionary of global names in src -> the objects that they reference.\n",
        "parameters": "root_module (str) – The name of the root module on which to look-up\nqualified name targets. This is usually ‘self’.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.package.PackageImporter.python_version",
        "api_url": "https://pytorch.org/docs/stable/package.html#torch.package.PackageImporter.python_version",
        "api_signature": "python_version()",
        "api_description": "Returns the version of python that was used to create this package.",
        "return_value": "Optional[str] a python version e.g. 3.8.9 or None if no version was stored with this package\n",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.q_per_channel_axis",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.q_per_channel_axis.html#torch.Tensor.q_per_channel_axis",
        "api_signature": "Tensor.q_per_channel_axis()",
        "api_description": "Given a Tensor quantized by linear (affine) per-channel quantization,\nreturns the index of dimension on which per-channel quantization is applied.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.q_per_channel_scales",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.q_per_channel_scales.html#torch.Tensor.q_per_channel_scales",
        "api_signature": "Tensor.q_per_channel_scales()",
        "api_description": "Given a Tensor quantized by linear (affine) per-channel quantization,\nreturns a Tensor of scales of the underlying quantizer. It has the number of\nelements that matches the corresponding dimensions (from q_per_channel_axis) of\nthe tensor.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.q_per_channel_zero_points",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.q_per_channel_zero_points.html#torch.Tensor.q_per_channel_zero_points",
        "api_signature": "Tensor.q_per_channel_zero_points()",
        "api_description": "Given a Tensor quantized by linear (affine) per-channel quantization,\nreturns a tensor of zero_points of the underlying quantizer. It has the number of\nelements that matches the corresponding dimensions (from q_per_channel_axis) of\nthe tensor.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.q_scale",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.q_scale.html#torch.Tensor.q_scale",
        "api_signature": "Tensor.q_scale()",
        "api_description": "Given a Tensor quantized by linear(affine) quantization,\nreturns the scale of the underlying quantizer().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.q_zero_point",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.q_zero_point.html#torch.Tensor.q_zero_point",
        "api_signature": "Tensor.q_zero_point()",
        "api_description": "Given a Tensor quantized by linear(affine) quantization,\nreturns the zero_point of the underlying quantizer().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.qconfig.QConfig",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.quantization.qconfig.QConfig.html#torch.ao.quantization.qconfig.QConfig",
        "api_signature": "torch.ao.quantization.qconfig.QConfig(activation, weight)",
        "api_description": "Describes how to quantize a layer or a part of the network by providing\nsettings (observer classes) for activations and weights respectively.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.qconfig_mapping.QConfigMapping",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.quantization.qconfig_mapping.QConfigMapping.html#torch.ao.quantization.qconfig_mapping.QConfigMapping",
        "api_signature": null,
        "api_description": "Mapping from model ops to torch.ao.quantization.QConfig s.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.quantized.QFunctional",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.nn.quantized.QFunctional.html#torch.ao.nn.quantized.QFunctional",
        "api_signature": null,
        "api_description": "Wrapper class for quantized operations.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.QInt32Storage",
        "api_url": "https://pytorch.org/docs/stable/storage.html#torch.QInt32Storage",
        "api_signature": "torch.QInt32Storage(*args, wrap_storage=None, dtype=None, device=None, _internal=False)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.QInt8Storage",
        "api_url": "https://pytorch.org/docs/stable/storage.html#torch.QInt8Storage",
        "api_signature": "torch.QInt8Storage(*args, wrap_storage=None, dtype=None, device=None, _internal=False)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.qr",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.qr.html#torch.qr",
        "api_signature": "torch.qr(input, some=True, *, out=None)",
        "api_description": "Computes the QR decomposition of a matrix or a batch of matrices input,\nand returns a namedtuple (Q, R) of tensors such that input=QR\\text{input} = Q Rinput=QR\nwith QQQ being an orthogonal matrix or batch of orthogonal matrices and\nRRR being an upper triangular matrix or batch of upper triangular matrices.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor of size (∗,m,n)(*, m, n)(∗,m,n) where * is zero or more\nbatch dimensions consisting of matrices of dimension m×nm \\times nm×n.\nsome (bool, optional) – Set to True for reduced QR decomposition and False for\ncomplete QR decomposition. If k = min(m, n) then:\nsome=True : returns (Q, R) with dimensions (m, k), (k, n) (default)\n'some=False': returns (Q, R) with dimensions (m, m), (m, n)\nout (tuple, optional) – tuple of Q and R tensors.\nThe dimensions of Q and R are detailed in the description of some above.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.linalg.qr",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.linalg.qr.html#torch.linalg.qr",
        "api_signature": "torch.linalg.qr(A, mode='reduced', *, out=None)",
        "api_description": "Computes the QR decomposition of a matrix.",
        "return_value": "A named tuple (Q, R).\n",
        "parameters": "A (Tensor) – tensor of shape (*, m, n) where * is zero or more batch dimensions.\nmode (str, optional) – one of ‘reduced’, ‘complete’, ‘r’.\nControls the shape of the returned tensors. Default: ‘reduced’.\nout (tuple, optional) – output tuple of two tensors. Ignored if None. Default: None.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.qr",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.qr.html#torch.Tensor.qr",
        "api_signature": "Tensor.qr(some=True)",
        "api_description": "See torch.qr()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.qscheme",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.qscheme.html#torch.Tensor.qscheme",
        "api_signature": "Tensor.qscheme()",
        "api_description": "Returns the quantization scheme of a given QTensor.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.quantile",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.quantile.html#torch.quantile",
        "api_signature": "torch.quantile(input, q, dim=None, keepdim=False, *, interpolation='linear', out=None)",
        "api_description": "Computes the q-th quantiles of each row of the input tensor along the dimension dim.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.\nq (float or Tensor) – a scalar or 1D tensor of values in the range [0, 1].\ndim (int) – the dimension to reduce.\nkeepdim (bool) – whether the output tensor has dim retained or not.\ninterpolation (str) – interpolation method to use when the desired quantile lies between two data points.\nCan be linear, lower, higher, midpoint and nearest.\nDefault is linear.\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.quantile",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.quantile.html#torch.Tensor.quantile",
        "api_signature": "Tensor.quantile(q, dim=None, keepdim=False, *, interpolation='linear')",
        "api_description": "See torch.quantile()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.quantize",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.quantization.quantize.html#torch.ao.quantization.quantize",
        "api_signature": "torch.ao.quantization.quantize(model, run_fn, run_args, mapping=None, inplace=False)",
        "api_description": "Quantize the input float model with post training static quantization.",
        "return_value": "Quantized model.\n",
        "parameters": "model – input float model\nrun_fn – a calibration function for calibrating the prepared model\nrun_args – positional arguments for run_fn\ninplace – carry out model transformations in-place, the original module is mutated\nmapping – correspondence between original module types and quantized counterparts",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.quantize_dynamic",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.quantization.quantize_dynamic.html#torch.ao.quantization.quantize_dynamic",
        "api_signature": "torch.ao.quantization.quantize_dynamic(model, qconfig_spec=None, dtype=torch.qint8, mapping=None, inplace=False)",
        "api_description": "Converts a float model to dynamic (i.e. weights-only) quantized model.",
        "return_value": "",
        "parameters": "model – input model\nqconfig_spec – Either:\nA dictionary that maps from name or type of submodule to quantization\nconfiguration, qconfig applies to all submodules of a given\nmodule unless qconfig for the submodules are specified (when the\nsubmodule already has qconfig attribute). Entries in the dictionary\nneed to be QConfig instances.\nA set of types and/or submodule names to apply dynamic quantization to,\nin which case the dtype argument is used to specify the bit-width\ninplace – carry out model transformations in-place, the original module is mutated\nmapping – maps type of a submodule to a type of corresponding dynamically quantized version\nwith which the submodule needs to be replaced",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.quantize_per_channel",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.quantize_per_channel.html#torch.quantize_per_channel",
        "api_signature": "torch.quantize_per_channel(input, scales, zero_points, axis, dtype)",
        "api_description": "Converts a float tensor to a per-channel quantized tensor with given scales and zero points.",
        "return_value": "A newly quantized tensor\n",
        "parameters": "input (Tensor) – float tensor to quantize\nscales (Tensor) – float 1D tensor of scales to use, size should match input.size(axis)\nzero_points (int) – integer 1D tensor of offset to use, size should match input.size(axis)\naxis (int) – dimension on which apply per-channel quantization\ndtype (torch.dtype) – the desired data type of returned tensor.\nHas to be one of the quantized dtypes: torch.quint8, torch.qint8, torch.qint32",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.quantize_per_tensor",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.quantize_per_tensor.html#torch.quantize_per_tensor",
        "api_signature": "torch.quantize_per_tensor(input, scale, zero_point, dtype)",
        "api_description": "Converts a float tensor to a quantized tensor with given scale and zero point.",
        "return_value": "A newly quantized tensor or list of quantized tensors.\n",
        "parameters": "input (Tensor) – float tensor or list of tensors to quantize\nscale (float or Tensor) – scale to apply in quantization formula\nzero_point (int or Tensor) – offset in integer value that maps to float zero\ndtype (torch.dtype) – the desired data type of returned tensor.\nHas to be one of the quantized dtypes: torch.quint8, torch.qint8, torch.qint32",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.quantize_qat",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.quantization.quantize_qat.html#torch.ao.quantization.quantize_qat",
        "api_signature": "torch.ao.quantization.quantize_qat(model, run_fn, run_args, inplace=False)",
        "api_description": "Do quantization aware training and output a quantized model",
        "return_value": "Quantized model.\n",
        "parameters": "model – input model\nrun_fn – a function for evaluating the prepared model, can be a\nfunction that simply runs the prepared model or a training\nloop\nrun_args – positional arguments for run_fn",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.quantized_batch_norm",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.quantized_batch_norm.html#torch.quantized_batch_norm",
        "api_signature": "torch.quantized_batch_norm(input, weight=None, bias=None, mean, var, eps, output_scale, output_zero_point)",
        "api_description": "Applies batch normalization on a 4D (NCHW) quantized tensor.",
        "return_value": "A quantized tensor with batch normalization applied.\n",
        "parameters": "input (Tensor) – quantized tensor\nweight (Tensor) – float tensor that corresponds to the gamma, size C\nbias (Tensor) – float tensor that corresponds to the beta, size C\nmean (Tensor) – float mean value in batch normalization, size C\nvar (Tensor) – float tensor for variance, size C\neps (float) – a value added to the denominator for numerical stability.\noutput_scale (float) – output quantized tensor scale\noutput_zero_point (int) – output quantized tensor zero_point",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.quantized_max_pool1d",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.quantized_max_pool1d.html#torch.quantized_max_pool1d",
        "api_signature": "torch.quantized_max_pool1d(input, kernel_size, stride=[], padding=0, dilation=1, ceil_mode=False)",
        "api_description": "Applies a 1D max pooling over an input quantized tensor composed of several input planes.",
        "return_value": "A quantized tensor with max_pool1d applied.\n",
        "parameters": "input (Tensor) – quantized tensor\nkernel_size (list of int) – the size of the sliding window\nstride (list of int, optional) – the stride of the sliding window\npadding (list of int, optional) – padding to be added on both sides, must be >= 0 and <= kernel_size / 2\ndilation (list of int, optional) – The stride between elements within a sliding window, must be > 0. Default 1\nceil_mode (bool, optional) – If True, will use ceil instead of floor to compute the output shape.\nDefaults to False.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.quantized_max_pool2d",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.quantized_max_pool2d.html#torch.quantized_max_pool2d",
        "api_signature": "torch.quantized_max_pool2d(input, kernel_size, stride=[], padding=0, dilation=1, ceil_mode=False)",
        "api_description": "Applies a 2D max pooling over an input quantized tensor composed of several input planes.",
        "return_value": "A quantized tensor with max_pool2d applied.\n",
        "parameters": "input (Tensor) – quantized tensor\nkernel_size (list of int) – the size of the sliding window\nstride (list of int, optional) – the stride of the sliding window\npadding (list of int, optional) – padding to be added on both sides, must be >= 0 and <= kernel_size / 2\ndilation (list of int, optional) – The stride between elements within a sliding window, must be > 0. Default 1\nceil_mode (bool, optional) – If True, will use ceil instead of floor to compute the output shape.\nDefaults to False.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.QuantStub",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.quantization.QuantStub.html#torch.ao.quantization.QuantStub",
        "api_signature": "torch.ao.quantization.QuantStub(qconfig=None)",
        "api_description": "Quantize stub module, before calibration, this is same as an observer,\nit will be swapped as nnq.Quantize in convert.",
        "return_value": "",
        "parameters": "qconfig – quantization configuration for the tensor,\nif qconfig is not provided, we will get qconfig from parent modules",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.QuantWrapper",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.quantization.QuantWrapper.html#torch.ao.quantization.QuantWrapper",
        "api_signature": "torch.ao.quantization.QuantWrapper(module)",
        "api_description": "A wrapper class that wraps the input module, adds QuantStub and\nDeQuantStub and surround the call to module with call to quant and dequant\nmodules.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda.Event.query",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.cuda.Event.html#torch.cuda.Event.query",
        "api_signature": "query()",
        "api_description": "Check if all work currently captured by event has completed.",
        "return_value": "A boolean indicating if all work currently captured by event has\ncompleted.\n",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda.ExternalStream.query",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.cuda.ExternalStream.html#torch.cuda.ExternalStream.query",
        "api_signature": "query()",
        "api_description": "Check if all the work submitted has been completed.",
        "return_value": "A boolean indicating if all kernels in this stream are completed.\n",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda.Stream.query",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.cuda.Stream.html#torch.cuda.Stream.query",
        "api_signature": "query()",
        "api_description": "Check if all the work submitted has been completed.",
        "return_value": "A boolean indicating if all kernels in this stream are completed.\n",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.mps.event.Event.query",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.mps.event.Event.html#torch.mps.event.Event.query",
        "api_signature": "query()",
        "api_description": "Returns True if all work currently captured by event has completed.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.xpu.Event.query",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.xpu.Event.html#torch.xpu.Event.query",
        "api_signature": "query()",
        "api_description": "Check if all work currently captured by event has completed.",
        "return_value": "A boolean indicating if all work currently captured by event has\ncompleted.\n",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.xpu.Stream.query",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.xpu.Stream.html#torch.xpu.Stream.query",
        "api_signature": "query()",
        "api_description": "Check if all the work submitted has been completed.",
        "return_value": "A boolean indicating if all kernels in this stream are completed.\n",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.QUInt2x4Storage",
        "api_url": "https://pytorch.org/docs/stable/storage.html#torch.QUInt2x4Storage",
        "api_signature": "torch.QUInt2x4Storage(*args, wrap_storage=None, dtype=None, device=None, _internal=False)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.QUInt4x2Storage",
        "api_url": "https://pytorch.org/docs/stable/storage.html#torch.QUInt4x2Storage",
        "api_signature": "torch.QUInt4x2Storage(*args, wrap_storage=None, dtype=None, device=None, _internal=False)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.QUInt8Storage",
        "api_url": "https://pytorch.org/docs/stable/storage.html#torch.QUInt8Storage",
        "api_signature": "torch.QUInt8Storage(*args, wrap_storage=None, dtype=None, device=None, _internal=False)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.rad2deg",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.rad2deg.html#torch.rad2deg",
        "api_signature": "torch.rad2deg(input, *, out=None)",
        "api_description": "Returns a new tensor with each of the elements of input\nconverted from angles in radians to degrees.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.rad2deg",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.rad2deg.html#torch.Tensor.rad2deg",
        "api_signature": "Tensor.rad2deg()",
        "api_description": "See torch.rad2deg()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.RAdam",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.RAdam.html#torch.optim.RAdam",
        "api_signature": "torch.optim.RAdam(params, lr=0.001, betas=(0.9, 0.999)",
        "api_description": "Implements RAdam algorithm.",
        "return_value": "a handle that can be used to remove the added hook by calling\nhandle.remove()\na handle that can be used to remove the added hook by calling\nhandle.remove()\na handle that can be used to remove the added hook by calling\nhandle.remove()\na handle that can be used to remove the added hook by calling\nhandle.remove()\na handle that can be used to remove the added hook by calling\nhandle.remove()\na handle that can be used to remove the added hook by calling\nhandle.remove()\n",
        "parameters": "params (iterable) – iterable of parameters to optimize or dicts defining\nparameter groups\nlr (float, optional) – learning rate (default: 1e-3)\nbetas (Tuple[float, float], optional) – coefficients used for computing\nrunning averages of gradient and its square (default: (0.9, 0.999))\neps (float, optional) – term added to the denominator to improve\nnumerical stability (default: 1e-8)\nweight_decay (float, optional) – weight decay (L2 penalty) (default: 0)\ndecoupled_weight_decay (bool, optional) – whether to use decoupled weight\ndecay as in AdamW to obtain RAdamW (default: False)\nforeach (bool, optional) – whether foreach implementation of optimizer\nis used. If unspecified by the user (so foreach is None), we will try to use\nforeach over the for-loop implementation on CUDA, since it is usually\nsignificantly more performant. Note that the foreach implementation uses\n~ sizeof(params) more peak memory than the for-loop version due to the intermediates\nbeing a tensorlist vs just one tensor. If memory is prohibitive, batch fewer\nparameters through the optimizer at a time or switch this flag to False (default: None)\ndifferentiable (bool, optional) – whether autograd should\noccur through the optimizer step in training. Otherwise, the step()\nfunction runs in a torch.no_grad() context. Setting to True can impair\nperformance, so leave it False if you don’t intend to run autograd\nthrough this instance (default: False)\ncapturable (bool, optional) – whether this instance is safe to\ncapture in a CUDA graph. Passing True can impair ungraphed performance,\nso if you don’t intend to graph capture this instance, leave it False\n(default: False)\nparam_group (dict) – Specifies what Tensors should be optimized along with group\nspecific optimization options.\nstate_dict (dict) – optimizer state. Should be an object returned\nfrom a call to state_dict().\nhook (Callable) – The user defined hook to be registered.\nprepend (bool) – If True, the provided post hook will be fired before\nall the already registered post-hooks on load_state_dict. Otherwise,\nthe provided hook will be fired after all the already registered\npost-hooks. (default: False)\nhook (Callable) – The user defined hook to be registered.\nprepend (bool) – If True, the provided pre hook will be fired before\nall the already registered pre-hooks on load_state_dict. Otherwise,\nthe provided hook will be fired after all the already registered\npre-hooks. (default: False)\nhook (Callable) – The user defined hook to be registered.\nprepend (bool) – If True, the provided post hook will be fired before\nall the already registered post-hooks on state_dict. Otherwise,\nthe provided hook will be fired after all the already registered\npost-hooks. (default: False)\nhook (Callable) – The user defined hook to be registered.\nprepend (bool) – If True, the provided pre hook will be fired before\nall the already registered pre-hooks on state_dict. Otherwise,\nthe provided hook will be fired after all the already registered\npre-hooks. (default: False)\nhook (Callable) – The user defined hook to be registered.\nhook (Callable) – The user defined hook to be registered.\nclosure (Callable, optional) – A closure that reevaluates the model\nand returns the loss.\nset_to_none (bool) – instead of setting to zero, set the grads to None.\nThis will in general have lower memory footprint, and can modestly improve performance.\nHowever, it changes certain behaviors. For example:\n1. When the user tries to access a gradient and perform manual ops on it,\na None attribute or a Tensor full of 0s will behave differently.\n2. If the user requests zero_grad(set_to_none=True) followed by a backward pass, .grads\nare guaranteed to be None for params that did not receive a gradient.\n3. torch.optim optimizers have a different behavior if the gradient is 0 or None\n(in one case it does the step with a gradient of 0 and in the other it skips\nthe step altogether).",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.rand",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.rand.html#torch.rand",
        "api_signature": "torch.rand(*size, *, generator=None, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False, pin_memory=False)",
        "api_description": "Returns a tensor filled with random numbers from a uniform distribution\non the interval [0,1)[0, 1)[0,1)",
        "return_value": "",
        "parameters": "size (int...) – a sequence of integers defining the shape of the output tensor.\nCan be a variable number of arguments or a collection like a list or tuple.\ngenerator (torch.Generator, optional) – a pseudorandom number generator for sampling\nout (Tensor, optional) – the output tensor.\ndtype (torch.dtype, optional) – the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_dtype()).\nlayout (torch.layout, optional) – the desired layout of returned Tensor.\nDefault: torch.strided.\ndevice (torch.device, optional) – the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_device()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types.\nrequires_grad (bool, optional) – If autograd should record operations on the\nreturned tensor. Default: False.\npin_memory (bool, optional) – If set, returned tensor would be allocated in\nthe pinned memory. Works only for CPU tensors. Default: False.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.rand_like",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.rand_like.html#torch.rand_like",
        "api_signature": "torch.rand_like(input, *, dtype=None, layout=None, device=None, requires_grad=False, memory_format=torch.preserve_format)",
        "api_description": "Returns a tensor with the same size as input that is filled with\nrandom numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1).\ntorch.rand_like(input) is equivalent to\ntorch.rand(input.size(), dtype=input.dtype, layout=input.layout, device=input.device).",
        "return_value": "",
        "parameters": "input (Tensor) – the size of input will determine size of the output tensor.\ndtype (torch.dtype, optional) – the desired data type of returned Tensor.\nDefault: if None, defaults to the dtype of input.\nlayout (torch.layout, optional) – the desired layout of returned tensor.\nDefault: if None, defaults to the layout of input.\ndevice (torch.device, optional) – the desired device of returned tensor.\nDefault: if None, defaults to the device of input.\nrequires_grad (bool, optional) – If autograd should record operations on the\nreturned tensor. Default: False.\nmemory_format (torch.memory_format, optional) – the desired memory format of\nreturned Tensor. Default: torch.preserve_format.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.randint",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.randint.html#torch.randint",
        "api_signature": "torch.randint(low=0, high, size, \\*, generator=None, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False)",
        "api_description": "Returns a tensor filled with random integers generated uniformly\nbetween low (inclusive) and high (exclusive).",
        "return_value": "",
        "parameters": "low (int, optional) – Lowest integer to be drawn from the distribution. Default: 0.\nhigh (int) – One above the highest integer to be drawn from the distribution.\nsize (tuple) – a tuple defining the shape of the output tensor.\ngenerator (torch.Generator, optional) – a pseudorandom number generator for sampling\nout (Tensor, optional) – the output tensor.\ndtype (torch.dtype, optional) – if None,\nthis function returns a tensor with dtype torch.int64.\nlayout (torch.layout, optional) – the desired layout of returned Tensor.\nDefault: torch.strided.\ndevice (torch.device, optional) – the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_device()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types.\nrequires_grad (bool, optional) – If autograd should record operations on the\nreturned tensor. Default: False.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.randint_like",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.randint_like.html#torch.randint_like",
        "api_signature": "torch.randint_like(input, low=0, high, \\*, dtype=None, layout=torch.strided, device=None, requires_grad=False, memory_format=torch.preserve_format)",
        "api_description": "Returns a tensor with the same shape as Tensor input filled with\nrandom integers generated uniformly between low (inclusive) and\nhigh (exclusive).",
        "return_value": "",
        "parameters": "input (Tensor) – the size of input will determine size of the output tensor.\nlow (int, optional) – Lowest integer to be drawn from the distribution. Default: 0.\nhigh (int) – One above the highest integer to be drawn from the distribution.\ndtype (torch.dtype, optional) – the desired data type of returned Tensor.\nDefault: if None, defaults to the dtype of input.\nlayout (torch.layout, optional) – the desired layout of returned tensor.\nDefault: if None, defaults to the layout of input.\ndevice (torch.device, optional) – the desired device of returned tensor.\nDefault: if None, defaults to the device of input.\nrequires_grad (bool, optional) – If autograd should record operations on the\nreturned tensor. Default: False.\nmemory_format (torch.memory_format, optional) – the desired memory format of\nreturned Tensor. Default: torch.preserve_format.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.randn",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.randn.html#torch.randn",
        "api_signature": "torch.randn(*size, *, generator=None, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False, pin_memory=False)",
        "api_description": "Returns a tensor filled with random numbers from a normal distribution\nwith mean 0 and variance 1 (also called the standard normal\ndistribution).",
        "return_value": "",
        "parameters": "size (int...) – a sequence of integers defining the shape of the output tensor.\nCan be a variable number of arguments or a collection like a list or tuple.\ngenerator (torch.Generator, optional) – a pseudorandom number generator for sampling\nout (Tensor, optional) – the output tensor.\ndtype (torch.dtype, optional) – the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_dtype()).\nlayout (torch.layout, optional) – the desired layout of returned Tensor.\nDefault: torch.strided.\ndevice (torch.device, optional) – the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_device()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types.\nrequires_grad (bool, optional) – If autograd should record operations on the\nreturned tensor. Default: False.\npin_memory (bool, optional) – If set, returned tensor would be allocated in\nthe pinned memory. Works only for CPU tensors. Default: False.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.randn_like",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.randn_like.html#torch.randn_like",
        "api_signature": "torch.randn_like(input, *, dtype=None, layout=None, device=None, requires_grad=False, memory_format=torch.preserve_format)",
        "api_description": "Returns a tensor with the same size as input that is filled with\nrandom numbers from a normal distribution with mean 0 and variance 1. Please refer to torch.randn() for the\nsampling process of complex dtypes. torch.randn_like(input) is equivalent to\ntorch.randn(input.size(), dtype=input.dtype, layout=input.layout, device=input.device).",
        "return_value": "",
        "parameters": "input (Tensor) – the size of input will determine size of the output tensor.\ndtype (torch.dtype, optional) – the desired data type of returned Tensor.\nDefault: if None, defaults to the dtype of input.\nlayout (torch.layout, optional) – the desired layout of returned tensor.\nDefault: if None, defaults to the layout of input.\ndevice (torch.device, optional) – the desired device of returned tensor.\nDefault: if None, defaults to the device of input.\nrequires_grad (bool, optional) – If autograd should record operations on the\nreturned tensor. Default: False.\nmemory_format (torch.memory_format, optional) – the desired memory format of\nreturned Tensor. Default: torch.preserve_format.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.random_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.random_.html#torch.Tensor.random_",
        "api_signature": "Tensor.random_(from=0, to=None, *, generator=None)",
        "api_description": "Fills self tensor with numbers sampled from the discrete uniform\ndistribution over [from, to - 1]. If not specified, the values are usually\nonly bounded by self tensor’s data type. However, for floating point\ntypes, if unspecified, range will be [0, 2^mantissa] to ensure that every\nvalue is representable. For example, torch.tensor(1, dtype=torch.double).random_()\nwill be uniform in [0, 2^53].",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.data.random_split",
        "api_url": "https://pytorch.org/docs/stable/data.html#torch.utils.data.random_split",
        "api_signature": "torch.utils.data.random_split(dataset, lengths, generator=<torch._C.Generator object>)",
        "api_description": "Randomly split a dataset into non-overlapping new datasets of given lengths.",
        "return_value": "",
        "parameters": "dataset (Dataset) – Dataset to be split\nlengths (sequence) – lengths or fractions of splits to be produced\ngenerator (Generator) – Generator used for the random permutation.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.utils.prune.random_structured",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.random_structured.html#torch.nn.utils.prune.random_structured",
        "api_signature": "torch.nn.utils.prune.random_structured(module, name, amount, dim)",
        "api_description": "Prune tensor by removing random channels along the specified dimension.",
        "return_value": "modified (i.e. pruned) version of the input module\n",
        "parameters": "module (nn.Module) – module containing the tensor to prune\nname (str) – parameter name within module on which pruning\nwill act.\namount (int or float) – quantity of parameters to prune.\nIf float, should be between 0.0 and 1.0 and represent the\nfraction of parameters to prune. If int, it represents the\nabsolute number of parameters to prune.\ndim (int) – index of the dim along which we define channels to prune.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.utils.prune.random_unstructured",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.random_unstructured.html#torch.nn.utils.prune.random_unstructured",
        "api_signature": "torch.nn.utils.prune.random_unstructured(module, name, amount)",
        "api_description": "Prune tensor by removing random (currently unpruned) units.",
        "return_value": "modified (i.e. pruned) version of the input module\n",
        "parameters": "module (nn.Module) – module containing the tensor to prune\nname (str) – parameter name within module on which pruning\nwill act.\namount (int or float) – quantity of parameters to prune.\nIf float, should be between 0.0 and 1.0 and represent the\nfraction of parameters to prune. If int, it represents the\nabsolute number of parameters to prune.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.data.RandomSampler",
        "api_url": "https://pytorch.org/docs/stable/data.html#torch.utils.data.RandomSampler",
        "api_signature": "torch.utils.data.RandomSampler(data_source, replacement=False, num_samples=None, generator=None)",
        "api_description": "Samples elements randomly. If without replacement, then sample from a shuffled dataset.",
        "return_value": "",
        "parameters": "data_source (Dataset) – dataset to sample from\nreplacement (bool) – samples are drawn on-demand with replacement if True, default=``False``\nnum_samples (int) – number of samples to draw, default=`len(dataset)`.\ngenerator (Generator) – Generator used in sampling.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.utils.prune.RandomStructured",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.RandomStructured.html#torch.nn.utils.prune.RandomStructured",
        "api_signature": "torch.nn.utils.prune.RandomStructured(amount, dim=-1)",
        "api_description": "Prune entire (currently unpruned) channels in a tensor at random.",
        "return_value": "pruned version of the input tensor\nmask to apply to t, of same dims as t\npruned version of tensor t.\n",
        "parameters": "amount (int or float) – quantity of parameters to prune.\nIf float, should be between 0.0 and 1.0 and represent the\nfraction of parameters to prune. If int, it represents the\nabsolute number of parameters to prune.\ndim (int, optional) – index of the dim along which we define\nchannels to prune. Default: -1.\nmodule (nn.Module) – module containing the tensor to prune\nname (str) – parameter name within module on which pruning\nwill act.\namount (int or float) – quantity of parameters to prune.\nIf float, should be between 0.0 and 1.0 and represent the\nfraction of parameters to prune. If int, it represents the\nabsolute number of parameters to prune.\ndim (int, optional) – index of the dim along which we define\nchannels to prune. Default: -1.\nmodule (nn.Module) – module containing the tensor to prune\nt (torch.Tensor) – tensor representing the parameter to prune\ndefault_mask (torch.Tensor) – Base mask from previous pruning\niterations, that need to be respected after the new mask is\napplied. Same dims as t.\nt (torch.Tensor) – tensor to prune (of same dimensions as\ndefault_mask).\nimportance_scores (torch.Tensor) – tensor of importance scores (of\nsame shape as t) used to compute mask for pruning t.\nThe values in this tensor indicate the importance of the\ncorresponding elements in the t that is being pruned.\nIf unspecified or None, the tensor t will be used in its place.\ndefault_mask (torch.Tensor, optional) – mask from previous pruning\niteration, if any. To be considered when determining what\nportion of the tensor that pruning should act on. If None,\ndefault to a mask of ones.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.utils.prune.RandomUnstructured",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.RandomUnstructured.html#torch.nn.utils.prune.RandomUnstructured",
        "api_signature": "torch.nn.utils.prune.RandomUnstructured(amount)",
        "api_description": "Prune (currently unpruned) units in a tensor at random.",
        "return_value": "pruned version of the input tensor\npruned version of tensor t.\n",
        "parameters": "name (str) – parameter name within module on which pruning\nwill act.\namount (int or float) – quantity of parameters to prune.\nIf float, should be between 0.0 and 1.0 and represent the\nfraction of parameters to prune. If int, it represents the\nabsolute number of parameters to prune.\nmodule (nn.Module) – module containing the tensor to prune\nname (str) – parameter name within module on which pruning\nwill act.\namount (int or float) – quantity of parameters to prune.\nIf float, should be between 0.0 and 1.0 and represent the\nfraction of parameters to prune. If int, it represents the\nabsolute number of parameters to prune.\nmodule (nn.Module) – module containing the tensor to prune\nt (torch.Tensor) – tensor to prune (of same dimensions as\ndefault_mask).\nimportance_scores (torch.Tensor) – tensor of importance scores (of\nsame shape as t) used to compute mask for pruning t.\nThe values in this tensor indicate the importance of the\ncorresponding elements in the t that is being pruned.\nIf unspecified or None, the tensor t will be used in its place.\ndefault_mask (torch.Tensor, optional) – mask from previous pruning\niteration, if any. To be considered when determining what\nportion of the tensor that pruning should act on. If None,\ndefault to a mask of ones.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.randperm",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.randperm.html#torch.randperm",
        "api_signature": "torch.randperm(n, *, generator=None, out=None, dtype=torch.int64, layout=torch.strided, device=None, requires_grad=False, pin_memory=False)",
        "api_description": "Returns a random permutation of integers from 0 to n - 1.",
        "return_value": "",
        "parameters": "n (int) – the upper bound (exclusive)\ngenerator (torch.Generator, optional) – a pseudorandom number generator for sampling\nout (Tensor, optional) – the output tensor.\ndtype (torch.dtype, optional) – the desired data type of returned tensor.\nDefault: torch.int64.\nlayout (torch.layout, optional) – the desired layout of returned Tensor.\nDefault: torch.strided.\ndevice (torch.device, optional) – the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_device()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types.\nrequires_grad (bool, optional) – If autograd should record operations on the\nreturned tensor. Default: False.\npin_memory (bool, optional) – If set, returned tensor would be allocated in\nthe pinned memory. Works only for CPU tensors. Default: False.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.range",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.range.html#torch.range",
        "api_signature": "torch.range(start=0, end, step=1, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False)",
        "api_description": "Returns a 1-D tensor of size ⌊end−startstep⌋+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1⌊stepend−start​⌋+1\nwith values from start to end with step step. Step is\nthe gap between two values in the tensor.",
        "return_value": "",
        "parameters": "start (float) – the starting value for the set of points. Default: 0.\nend (float) – the ending value for the set of points\nstep (float) – the gap between each pair of adjacent points. Default: 1.\nout (Tensor, optional) – the output tensor.\ndtype (torch.dtype, optional) – the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_dtype()). If dtype is not given, infer the data type from the other input\narguments. If any of start, end, or stop are floating-point, the\ndtype is inferred to be the default dtype, see\nget_default_dtype(). Otherwise, the dtype is inferred to\nbe torch.int64.\nlayout (torch.layout, optional) – the desired layout of returned Tensor.\nDefault: torch.strided.\ndevice (torch.device, optional) – the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_device()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types.\nrequires_grad (bool, optional) – If autograd should record operations on the\nreturned tensor. Default: False.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda.nvtx.range_pop",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.cuda.nvtx.range_pop.html#torch.cuda.nvtx.range_pop",
        "api_signature": "torch.cuda.nvtx.range_pop()",
        "api_description": "Pop a range off of a stack of nested range spans.  Returns the  zero-based depth of the range that is ended.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.profiler.itt.range_pop",
        "api_url": "https://pytorch.org/docs/stable/profiler.html#torch.profiler.itt.range_pop",
        "api_signature": "torch.profiler.itt.range_pop()",
        "api_description": "Pops a range off of a stack of nested range spans. Returns the\nzero-based depth of the range that is ended.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda.nvtx.range_push",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.cuda.nvtx.range_push.html#torch.cuda.nvtx.range_push",
        "api_signature": "torch.cuda.nvtx.range_push(msg)",
        "api_description": "Push a range onto a stack of nested range span.  Returns zero-based depth of the range that is started.",
        "return_value": "",
        "parameters": "msg (str) – ASCII message to associate with range",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.profiler.itt.range_push",
        "api_url": "https://pytorch.org/docs/stable/profiler.html#torch.profiler.itt.range_push",
        "api_signature": "torch.profiler.itt.range_push(msg)",
        "api_description": "Pushes a range onto a stack of nested range span.  Returns zero-based\ndepth of the range that is started.",
        "return_value": "",
        "parameters": "msg (str) – ASCII message to associate with range",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.inverse_gamma.InverseGamma.rate",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.inverse_gamma.InverseGamma.rate",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ravel",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ravel.html#torch.ravel",
        "api_signature": "torch.ravel(input)",
        "api_description": "Return a contiguous flattened tensor. A copy is made only if needed.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.ravel",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.ravel.html#torch.Tensor.ravel",
        "api_signature": "Tensor.ravel()",
        "api_description": "see torch.ravel()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.checkpoint.format_utils.BroadcastingTorchSaveReader.read_data",
        "api_url": "https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.format_utils.BroadcastingTorchSaveReader.read_data",
        "api_signature": "read_data(plan, planner)",
        "api_description": "Reads torch save data on the coordinator rank, and broadcast afterwards\nthis incurrs a communication cost, but avoids having to load\nthe entire checkpoint on each rank, hopefully preventing OOM issues",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.checkpoint.StorageReader.read_data",
        "api_url": "https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.StorageReader.read_data",
        "api_signature": "read_data(plan, planner)",
        "api_description": "Read all items from plan using planner to resolve the data.",
        "return_value": "A future that completes once all reads are finished.\n",
        "parameters": "plan (LoadPlan) – The local plan to execute on\nplanner (LoadPlanner) – The planner object to use to resolve items.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.checkpoint.format_utils.BroadcastingTorchSaveReader.read_metadata",
        "api_url": "https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.format_utils.BroadcastingTorchSaveReader.read_metadata",
        "api_signature": "read_metadata()",
        "api_description": "Extends the default StorageReader to support building the metadata file",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.checkpoint.StorageReader.read_metadata",
        "api_url": "https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.StorageReader.read_metadata",
        "api_signature": "read_metadata()",
        "api_description": "Read the checkpoint metadata.",
        "return_value": "The metadata object associated with the checkpoint being loaded.\n",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.checkpoint.ReadItem",
        "api_url": "https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.ReadItem",
        "api_signature": "torch.distributed.checkpoint.ReadItem(type: torch.distributed.checkpoint.planner.LoadItemType, dest_index: torch.distributed.checkpoint.metadata.MetadataIndex, dest_offsets: torch.Size, storage_index: torch.distributed.checkpoint.metadata.MetadataIndex, storage_offsets: torch.Size, lengths: torch.Size)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.real",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.real.html#torch.Tensor.real",
        "api_signature": null,
        "api_description": "Returns a new tensor containing real values of the self tensor for a complex-valued input tensor.\nThe returned tensor and self share the same underlying storage.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> x=torch.randn(4, dtype=torch.cfloat)\n>>> x\ntensor([(0.3100+0.3553j), (-0.5445-0.7896j), (-1.6492-0.0633j), (-0.0638-0.8119j)])\n>>> x.real\ntensor([ 0.3100, -0.5445, -1.6492, -0.0638])\n\n\n"
    },
    {
        "api_name": "torch.real",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.real.html#torch.real",
        "api_signature": "torch.real(input)",
        "api_description": "Returns a new tensor containing real values of the self tensor.\nThe returned tensor and self share the same underlying storage.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.reciprocal",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.reciprocal.html#torch.reciprocal",
        "api_signature": "torch.reciprocal(input, *, out=None)",
        "api_description": "Returns a new tensor with the reciprocal of the elements of input",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.reciprocal",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.reciprocal.html#torch.Tensor.reciprocal",
        "api_signature": "Tensor.reciprocal()",
        "api_description": "See torch.reciprocal()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.reciprocal_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.reciprocal_.html#torch.Tensor.reciprocal_",
        "api_signature": "Tensor.reciprocal_()",
        "api_description": "In-place version of reciprocal()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.GraphModule.recompile",
        "api_url": "https://pytorch.org/docs/stable/fx.html#torch.fx.GraphModule.recompile",
        "api_signature": "recompile()",
        "api_description": "Recompile this GraphModule from its graph attribute. This should be\ncalled after editing the contained graph, otherwise the generated\ncode of this GraphModule will be out of date.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.events.record",
        "api_url": "https://pytorch.org/docs/stable/elastic/events.html#torch.distributed.elastic.events.record",
        "api_signature": "torch.distributed.elastic.events.record(event, destination='null')",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.multiprocessing.errors.record",
        "api_url": "https://pytorch.org/docs/stable/elastic/errors.html#torch.distributed.elastic.multiprocessing.errors.record",
        "api_signature": "torch.distributed.elastic.multiprocessing.errors.record(fn, error_handler=None)",
        "api_description": "Syntactic sugar to record errors/exceptions that happened in the decorated\nfunction using the provided error_handler.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda.Event.record",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.cuda.Event.html#torch.cuda.Event.record",
        "api_signature": "record(stream=None)",
        "api_description": "Record the event in a given stream.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.mps.event.Event.record",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.mps.event.Event.html#torch.mps.event.Event.record",
        "api_signature": "record()",
        "api_description": "Records the event in the default stream.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.xpu.Event.record",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.xpu.Event.html#torch.xpu.Event.record",
        "api_signature": "record(stream=None)",
        "api_description": "Record the event in a given stream.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda.ExternalStream.record_event",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.cuda.ExternalStream.html#torch.cuda.ExternalStream.record_event",
        "api_signature": "record_event(event=None)",
        "api_description": "Record an event.",
        "return_value": "Recorded event.\n",
        "parameters": "event (torch.cuda.Event, optional) – event to record. If not given, a new one\nwill be allocated.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda.Stream.record_event",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.cuda.Stream.html#torch.cuda.Stream.record_event",
        "api_signature": "record_event(event=None)",
        "api_description": "Record an event.",
        "return_value": "Recorded event.\n",
        "parameters": "event (torch.cuda.Event, optional) – event to record. If not given, a new one\nwill be allocated.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.xpu.Stream.record_event",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.xpu.Stream.html#torch.xpu.Stream.record_event",
        "api_signature": "record_event(event=None)",
        "api_description": "Record an event.",
        "return_value": "Recorded event.\n",
        "parameters": "event (torch.xpu.Event, optional) – event to record. If not given, a new one\nwill be allocated.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.autograd.profiler.record_function",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.autograd.profiler.record_function.html#torch.autograd.profiler.record_function",
        "api_signature": "torch.autograd.profiler.record_function(name, args=None)",
        "api_description": "Context manager/function decorator that adds a label to a code block/function when running autograd profiler.",
        "return_value": "",
        "parameters": "name (str) – Label assigned to the block of code.\nnode_id (int) – ID of node, for distributed profiling. Unset in\ncases. (non-distributed) –",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.record_stream",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.record_stream.html#torch.Tensor.record_stream",
        "api_signature": "Tensor.record_stream(stream)",
        "api_description": "Marks the tensor as having been used by this stream.  When the tensor\nis deallocated, ensure the tensor memory is not reused for another tensor\nuntil all work queued on stream at the time of deallocation is\ncomplete.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.observer.RecordingObserver",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.quantization.observer.RecordingObserver.html#torch.ao.quantization.observer.RecordingObserver",
        "api_signature": "torch.ao.quantization.observer.RecordingObserver(dtype=torch.quint8)",
        "api_description": "The module is mainly for debug and records the tensor values during runtime.",
        "return_value": "",
        "parameters": "dtype – Quantized data type\nqscheme – Quantization scheme to be used\nreduce_range – Reduces the range of the quantized data type by 1 bit",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.recv",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#torch.distributed.recv",
        "api_signature": "torch.distributed.recv(tensor, src=None, group=None, tag=0)",
        "api_description": "Receives a tensor synchronously.",
        "return_value": "Sender rank\n-1, if not part of the group\n",
        "parameters": "tensor (Tensor) – Tensor to fill with received data.\nsrc (int, optional) – Source rank on global process group (regardless of group argument).\nWill receive from any process if unspecified.\ngroup (ProcessGroup, optional) – The process group to work on. If None,\nthe default process group will be used.\ntag (int, optional) – Tag to match recv with remote send",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.reduce",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#torch.distributed.reduce",
        "api_signature": "torch.distributed.reduce(tensor, dst, op=<RedOpType.SUM: 0>, group=None, async_op=False)",
        "api_description": "Reduces the tensor data across all machines.",
        "return_value": "Async work handle, if async_op is set to True.\nNone, if not async_op or if not part of the group\n",
        "parameters": "tensor (Tensor) – Input and output of the collective. The function\noperates in-place.\ndst (int) – Destination rank on global process group (regardless of group argument)\nop (optional) – One of the values from\ntorch.distributed.ReduceOp\nenum.  Specifies an operation used for element-wise reductions.\ngroup (ProcessGroup, optional) – The process group to work on. If None,\nthe default process group will be used.\nasync_op (bool, optional) – Whether this op should be an async op",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda.comm.reduce_add",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.cuda.comm.reduce_add.html#torch.cuda.comm.reduce_add",
        "api_signature": "torch.cuda.comm.reduce_add(inputs, destination=None)",
        "api_description": "Sum tensors from multiple GPUs.",
        "return_value": "A tensor containing an elementwise sum of all inputs, placed on the\ndestination device.\n",
        "parameters": "inputs (Iterable[Tensor]) – an iterable of tensors to add.\ndestination (int, optional) – a device on which the output will be\nplaced (default: current device).",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.reduce_op",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#torch.distributed.reduce_op",
        "api_signature": null,
        "api_description": "Deprecated enum-like class for reduction operations: SUM, PRODUCT,\nMIN, and MAX.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.reduce_scatter",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#torch.distributed.reduce_scatter",
        "api_signature": "torch.distributed.reduce_scatter(output, input_list, op=<RedOpType.SUM: 0>, group=None, async_op=False)",
        "api_description": "Reduces, then scatters a list of tensors to all processes in a group.",
        "return_value": "Async work handle, if async_op is set to True.\nNone, if not async_op or if not part of the group.\n",
        "parameters": "output (Tensor) – Output tensor.\ninput_list (list[Tensor]) – List of tensors to reduce and scatter.\nop (optional) – One of the values from\ntorch.distributed.ReduceOp\nenum.  Specifies an operation used for element-wise reductions.\ngroup (ProcessGroup, optional) – The process group to work on. If None,\nthe default process group will be used.\nasync_op (bool, optional) – Whether this op should be an async op.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.reduce_scatter_tensor",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#torch.distributed.reduce_scatter_tensor",
        "api_signature": "torch.distributed.reduce_scatter_tensor(output, input, op=<RedOpType.SUM: 0>, group=None, async_op=False)",
        "api_description": "Reduces, then scatters a tensor to all ranks in a group.",
        "return_value": "Async work handle, if async_op is set to True.\nNone, if not async_op or if not part of the group.\n",
        "parameters": "output (Tensor) – Output tensor. It should have the same size across all\nranks.\ninput (Tensor) – Input tensor to be reduced and scattered. Its size\nshould be output tensor size times the world size. The input tensor\ncan have one of the following shapes:\n(i) a concatenation of the output tensors along the primary\ndimension, or\n(ii) a stack of the output tensors along the primary dimension.\nFor definition of “concatenation”, see torch.cat().\nFor definition of “stack”, see torch.stack().\ngroup (ProcessGroup, optional) – The process group to work on. If None,\nthe default process group will be used.\nasync_op (bool, optional) – Whether this op should be an async op.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.lr_scheduler.ReduceLROnPlateau",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.ReduceLROnPlateau.html#torch.optim.lr_scheduler.ReduceLROnPlateau",
        "api_signature": "torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, threshold=0.0001, threshold_mode='rel', cooldown=0, min_lr=0, eps=1e-08, verbose='deprecated')",
        "api_description": "Reduce learning rate when a metric has stopped improving.\nModels often benefit from reducing the learning rate by a factor\nof 2-10 once learning stagnates. This scheduler reads a metrics\nquantity and if no improvement is seen for a ‘patience’ number\nof epochs, the learning rate is reduced.",
        "return_value": "",
        "parameters": "optimizer (Optimizer) – Wrapped optimizer.\nmode (str) – One of min, max. In min mode, lr will\nbe reduced when the quantity monitored has stopped\ndecreasing; in max mode it will be reduced when the\nquantity monitored has stopped increasing. Default: ‘min’.\nfactor (float) – Factor by which the learning rate will be\nreduced. new_lr = lr * factor. Default: 0.1.\npatience (int) – The number of allowed epochs with no improvement after\nwhich the learning rate will be reduced.\nFor example, consider the case of having no patience (patience = 0).\nIn the first epoch, a baseline is established and is always considered good as there’s no previous baseline.\nIn the second epoch, if the performance is worse than the baseline,\nwe have what is considered an intolerable epoch.\nSince the count of intolerable epochs (1) is greater than the patience level (0),\nthe learning rate is reduced at the end of this epoch.\nFrom the third epoch onwards, the learning rate continues to be reduced at the end of each epoch\nif the performance is worse than the baseline. If the performance improves or remains the same,\nthe learning rate is not adjusted.\nDefault: 10.\nthreshold (float) – Threshold for measuring the new optimum,\nto only focus on significant changes. Default: 1e-4.\nthreshold_mode (str) – One of rel, abs. In rel mode,\ndynamic_threshold = best * ( 1 + threshold ) in ‘max’\nmode or best * ( 1 - threshold ) in min mode.\nIn abs mode, dynamic_threshold = best + threshold in\nmax mode or best - threshold in min mode. Default: ‘rel’.\ncooldown (int) – Number of epochs to wait before resuming\nnormal operation after lr has been reduced. Default: 0.\nmin_lr (float or list) – A scalar or a list of scalars. A\nlower bound on the learning rate of all param groups\nor each group respectively. Default: 0.\neps (float) – Minimal decay applied to lr. If the difference\nbetween new and old lr is smaller than eps, the update is\nignored. Default: 1e-8.\nverbose (bool) – If True, prints a message to stdout for\neach update. Default: False.\nDeprecated since version 2.2: verbose is deprecated. Please use get_last_lr() to access the\nlearning rate.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.ReduceOp",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#torch.distributed.ReduceOp",
        "api_signature": null,
        "api_description": "An enum-like class for available reduction operations: SUM, PRODUCT,\nMIN, MAX, BAND, BOR, BXOR, and PREMUL_SUM.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.refine_names",
        "api_url": "https://pytorch.org/docs/stable/named_tensor.html#torch.Tensor.refine_names",
        "api_signature": "refine_names(*names)",
        "api_description": "Refines the dimension names of self according to names.",
        "return_value": "",
        "parameters": "names (iterable of str) – The desired names of the output tensor. May\ncontain up to one Ellipsis.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.ReflectionPad1d",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.ReflectionPad1d.html#torch.nn.ReflectionPad1d",
        "api_signature": "torch.nn.ReflectionPad1d(padding)",
        "api_description": "Pads the input tensor using the reflection of the input boundary.",
        "return_value": "",
        "parameters": "padding (int, tuple) – the size of the padding. If is int, uses the same\npadding in all boundaries. If a 2-tuple, uses\n(padding_left\\text{padding\\_left}padding_left, padding_right\\text{padding\\_right}padding_right)",
        "input_shape": "\nInput: (C,Win)(C, W_{in})(C,Win​) or (N,C,Win)(N, C, W_{in})(N,C,Win​).\nOutput: (C,Wout)(C, W_{out})(C,Wout​) or (N,C,Wout)(N, C, W_{out})(N,C,Wout​), where\nWout=Win+padding_left+padding_rightW_{out} = W_{in} + \\text{padding\\_left} + \\text{padding\\_right}Wout​=Win​+padding_left+padding_right\n\n\n",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.ReflectionPad2d",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.ReflectionPad2d.html#torch.nn.ReflectionPad2d",
        "api_signature": "torch.nn.ReflectionPad2d(padding)",
        "api_description": "Pads the input tensor using the reflection of the input boundary.",
        "return_value": "",
        "parameters": "padding (int, tuple) – the size of the padding. If is int, uses the same\npadding in all boundaries. If a 4-tuple, uses (padding_left\\text{padding\\_left}padding_left,\npadding_right\\text{padding\\_right}padding_right, padding_top\\text{padding\\_top}padding_top, padding_bottom\\text{padding\\_bottom}padding_bottom)\nNote that padding size should be less than the corresponding input dimension.",
        "input_shape": "\nInput: (N,C,Hin,Win)(N, C, H_{in}, W_{in})(N,C,Hin​,Win​) or (C,Hin,Win)(C, H_{in}, W_{in})(C,Hin​,Win​).\nOutput: (N,C,Hout,Wout)(N, C, H_{out}, W_{out})(N,C,Hout​,Wout​) or (C,Hout,Wout)(C, H_{out}, W_{out})(C,Hout​,Wout​) where\nHout=Hin+padding_top+padding_bottomH_{out} = H_{in} + \\text{padding\\_top} + \\text{padding\\_bottom}Hout​=Hin​+padding_top+padding_bottom\nWout=Win+padding_left+padding_rightW_{out} = W_{in} + \\text{padding\\_left} + \\text{padding\\_right}Wout​=Win​+padding_left+padding_right\n\n\n",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.ReflectionPad3d",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.ReflectionPad3d.html#torch.nn.ReflectionPad3d",
        "api_signature": "torch.nn.ReflectionPad3d(padding)",
        "api_description": "Pads the input tensor using the reflection of the input boundary.",
        "return_value": "",
        "parameters": "padding (int, tuple) – the size of the padding. If is int, uses the same\npadding in all boundaries. If a 6-tuple, uses\n(padding_left\\text{padding\\_left}padding_left, padding_right\\text{padding\\_right}padding_right,\npadding_top\\text{padding\\_top}padding_top, padding_bottom\\text{padding\\_bottom}padding_bottom,\npadding_front\\text{padding\\_front}padding_front, padding_back\\text{padding\\_back}padding_back)",
        "input_shape": "\nInput: (N,C,Din,Hin,Win)(N, C, D_{in}, H_{in}, W_{in})(N,C,Din​,Hin​,Win​) or (C,Din,Hin,Win)(C, D_{in}, H_{in}, W_{in})(C,Din​,Hin​,Win​).\nOutput: (N,C,Dout,Hout,Wout)(N, C, D_{out}, H_{out}, W_{out})(N,C,Dout​,Hout​,Wout​) or (C,Dout,Hout,Wout)(C, D_{out}, H_{out}, W_{out})(C,Dout​,Hout​,Wout​),\nwhere\nDout=Din+padding_front+padding_backD_{out} = D_{in} + \\text{padding\\_front} + \\text{padding\\_back}Dout​=Din​+padding_front+padding_back\nHout=Hin+padding_top+padding_bottomH_{out} = H_{in} + \\text{padding\\_top} + \\text{padding\\_bottom}Hout​=Hin​+padding_top+padding_bottom\nWout=Win+padding_left+padding_rightW_{out} = W_{in} + \\text{padding\\_left} + \\text{padding\\_right}Wout​=Win​+padding_left+padding_right\n\n\n",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.rendezvous.RendezvousHandlerRegistry.register",
        "api_url": "https://pytorch.org/docs/stable/elastic/rendezvous.html#torch.distributed.elastic.rendezvous.RendezvousHandlerRegistry.register",
        "api_signature": "register(backend, creator)",
        "api_description": "Register a new rendezvous backend.",
        "return_value": "",
        "parameters": "backend (str) – The name of the backend.\ncreator (Callable[[RendezvousParameters], RendezvousHandler]) – The callback to invoke to construct the\nRendezvousHandler.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.constraint_registry.ConstraintRegistry.register",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.constraint_registry.ConstraintRegistry.register",
        "api_signature": "register(constraint, factory=None)",
        "api_description": "Registers a Constraint\nsubclass in this registry. Usage:",
        "return_value": "",
        "parameters": "constraint (subclass of Constraint) – A subclass of Constraint, or\na singleton object of the desired class.\nfactory (Callable) – A callable that inputs a constraint object and returns\na  Transform object.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.Backend.register_backend",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#torch.distributed.Backend.register_backend",
        "api_signature": "register_backend(name, func, extended_api=False, devices=None)",
        "api_description": "Register a new backend with the given name and instantiating function.",
        "return_value": "",
        "parameters": "name (str) – Backend name of the ProcessGroup extension. It\nshould match the one in init_process_group().\nfunc (function) – Function handler that instantiates the backend.\nThe function should be implemented in the backend\nextension and takes four arguments, including\nstore, rank, world_size, and timeout.\nextended_api (bool, optional) – Whether the backend supports extended argument structure.\nDefault: False. If set to True, the backend\nwill get an instance of c10d::DistributedBackendOptions, and\na process group options object as defined by the backend implementation.\ndevice (str or list of str, optional) – device type this backend\nsupports, e.g. “cpu”, “cuda”, etc. If None,\nassuming both “cpu” and “cuda”",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.jit.ScriptModule.register_backward_hook",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.jit.ScriptModule.html#torch.jit.ScriptModule.register_backward_hook",
        "api_signature": "register_backward_hook(hook)",
        "api_description": "Register a backward hook on the module.",
        "return_value": "a handle that can be used to remove the added hook by calling\nhandle.remove()\n",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.Module.register_backward_hook",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.register_backward_hook",
        "api_signature": "register_backward_hook(hook)",
        "api_description": "Register a backward hook on the module.",
        "return_value": "a handle that can be used to remove the added hook by calling\nhandle.remove()\n",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.jit.ScriptModule.register_buffer",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.jit.ScriptModule.html#torch.jit.ScriptModule.register_buffer",
        "api_signature": "register_buffer(name, tensor, persistent=True)",
        "api_description": "Add a buffer to the module.",
        "return_value": "",
        "parameters": "name (str) – name of the buffer. The buffer can be accessed\nfrom this module using the given name\ntensor (Tensor or None) – buffer to be registered. If None, then operations\nthat run on buffers, such as cuda, are ignored. If None,\nthe buffer is not included in the module’s state_dict.\npersistent (bool) – whether the buffer is part of this module’s\nstate_dict.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.Module.register_buffer",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.register_buffer",
        "api_signature": "register_buffer(name, tensor, persistent=True)",
        "api_description": "Add a buffer to the module.",
        "return_value": "",
        "parameters": "name (str) – name of the buffer. The buffer can be accessed\nfrom this module using the given name\ntensor (Tensor or None) – buffer to be registered. If None, then operations\nthat run on buffers, such as cuda, are ignored. If None,\nthe buffer is not included in the module’s state_dict.\npersistent (bool) – whether the buffer is part of this module’s\nstate_dict.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.fsdp.FullyShardedDataParallel.register_comm_hook",
        "api_url": "https://pytorch.org/docs/stable/fsdp.html#torch.distributed.fsdp.FullyShardedDataParallel.register_comm_hook",
        "api_signature": "register_comm_hook(state, hook)",
        "api_description": "Register a communication hook.",
        "return_value": "",
        "parameters": "state (object) – Passed to the hook to maintain any state information during the training process.\nExamples include error feedback in gradient compression,\npeers to communicate with next in GossipGrad, etc.\nIt is locally stored by each worker\nand shared by all the gradient tensors on the worker.\nhook (Callable) – Callable, which has one of the following signatures:\n1) hook: Callable[torch.Tensor] -> None:\nThis function takes in a Python tensor, which represents\nthe full, flattened, unsharded gradient with respect to all variables\ncorresponding to the model this FSDP unit is wrapping\n(that are not wrapped by other FSDP sub-units).\nIt then performs all necessary processing and returns None;\n2) hook: Callable[torch.Tensor, torch.Tensor] -> None:\nThis function takes in two Python tensors, the first one represents\nthe full, flattened, unsharded gradient with respect to all variables\ncorresponding to the model this FSDP unit is wrapping\n(that are not wrapped by other FSDP sub-units). The latter\nrepresents a pre-sized tensor to store a chunk of a sharded gradient after\nreduction.\nIn both cases, callable performs all necessary processing and returns None.\nCallables with signature 1 are expected to handle gradient communication for a NO_SHARD case.\nCallables with signature 2 are expected to handle gradient communication for sharded cases.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.parallel.DistributedDataParallel.register_comm_hook",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel.register_comm_hook",
        "api_signature": "register_comm_hook(state, hook)",
        "api_description": "Register communication hook for user-defined DDP aggregation of gradients across multiple workers.",
        "return_value": "",
        "parameters": "state (object) – Passed to the hook to maintain any state information during the training process.\nExamples include error feedback in gradient compression,\npeers to communicate with next in GossipGrad, etc.\nIt is locally stored by each worker\nand shared by all the gradient tensors on the worker.\nhook (Callable) – Callable with the following signature:\nhook(state: object, bucket: dist.GradBucket) -> torch.futures.Future[torch.Tensor]:\nThis function is called once the bucket is ready. The\nhook can perform whatever processing is needed and return\na Future indicating completion of any async work (ex: allreduce).\nIf the hook doesn’t perform any communication, it still\nmust return a completed Future. The Future should hold the\nnew value of grad bucket’s tensors. Once a bucket is ready,\nc10d reducer would call this hook and use the tensors returned\nby the Future and copy grads to individual parameters.\nNote that the future’s return type must be a single tensor.\nWe also provide an API called get_future to retrieve a\nFuture associated with the completion of c10d.ProcessGroup.Work.\nget_future is currently supported for NCCL and also supported for most\noperations on GLOO and MPI, except for peer to peer operations (send/recv).",
        "input_shape": "",
        "notes": "",
        "code_example": "Below is an example of a noop hook that returns the same tensor.\n>>> def noop(state: object, bucket: dist.GradBucket) -> torch.futures.Future[torch.Tensor]:\n>>>     fut = torch.futures.Future()\n>>>     fut.set_result(bucket.buffer())\n>>>     return fut\n>>> ddp.register_comm_hook(state=None, hook=noop)\n\n\nBelow is an example of a Parallel SGD algorithm where gradients are encoded before\nallreduce, and then decoded after allreduce.\n>>> def encode_and_decode(state: object, bucket: dist.GradBucket) -> torch.futures.Future[torch.Tensor]:\n>>>     encoded_tensor = encode(bucket.buffer())  # encode gradients\n>>>     fut = torch.distributed.all_reduce(encoded_tensor).get_future()\n>>>     # Define the then callback to decode.\n>>>     def decode(fut):\n>>>         decoded_tensor = decode(fut.value()[0])  # decode gradients\n>>>         return decoded_tensor\n>>>     return fut.then(decode)\n>>> ddp.register_comm_hook(state=None, hook=encode_and_decode)\n\n\n"
    },
    {
        "api_name": "torch.onnx.register_custom_op_symbolic",
        "api_url": "https://pytorch.org/docs/stable/onnx_torchscript.html#torch.onnx.register_custom_op_symbolic",
        "api_signature": "torch.onnx.register_custom_op_symbolic(symbolic_name, symbolic_fn, opset_version)",
        "api_description": "Registers a symbolic function for a custom operator.",
        "return_value": "",
        "parameters": "symbolic_name (str) – The name of the custom operator in “<domain>::<op>”\nformat.\nsymbolic_fn (Callable) – A function that takes in the ONNX graph and\nthe input arguments to the current operator, and returns new\noperator nodes to add to the graph.\nopset_version (int) – The ONNX opset version in which to register.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.export.register_dataclass",
        "api_url": "https://pytorch.org/docs/stable/export.html#torch.export.register_dataclass",
        "api_signature": "torch.export.register_dataclass(cls, *, serialized_type_name=None)",
        "api_description": "Registers a dataclass as a valid input/output type for torch.export.export().",
        "return_value": "",
        "parameters": "cls (Type[Any]) – the dataclass type to register\nserialized_type_name (Optional[str]) – The serialized name for the dataclass. This is\nthis (required if you want to serialize the pytree TreeSpec containing) –\ndataclass. –",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.monitor.register_event_handler",
        "api_url": "https://pytorch.org/docs/stable/monitor.html#torch.monitor.register_event_handler",
        "api_signature": "torch.monitor.register_event_handler(callback: Callable[[torch._C._monitor.Event], None])",
        "api_description": "register_event_handler registers a callback to be called whenever an\nevent is logged via log_event. These handlers should avoid blocking\nthe main thread since that may interfere with training as they run\nduring the log_event call.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.package.PackageExporter.register_extern_hook",
        "api_url": "https://pytorch.org/docs/stable/package.html#torch.package.PackageExporter.register_extern_hook",
        "api_signature": "register_extern_hook(hook)",
        "api_description": "Registers an extern hook on the exporter.",
        "return_value": "A handle that can be used to remove the added hook by calling\nhandle.remove().\n",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.jit.ScriptModule.register_forward_hook",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.jit.ScriptModule.html#torch.jit.ScriptModule.register_forward_hook",
        "api_signature": "register_forward_hook(hook, *, prepend=False, with_kwargs=False, always_call=False)",
        "api_description": "Register a forward hook on the module.",
        "return_value": "a handle that can be used to remove the added hook by calling\nhandle.remove()\n",
        "parameters": "hook (Callable) – The user defined hook to be registered.\nprepend (bool) – If True, the provided hook will be fired\nbefore all existing forward hooks on this\ntorch.nn.modules.Module. Otherwise, the provided\nhook will be fired after all existing forward hooks on\nthis torch.nn.modules.Module. Note that global\nforward hooks registered with\nregister_module_forward_hook() will fire before all hooks\nregistered by this method.\nDefault: False\nwith_kwargs (bool) – If True, the hook will be passed the\nkwargs given to the forward function.\nDefault: False\nalways_call (bool) – If True the hook will be run regardless of\nwhether an exception is raised while calling the Module.\nDefault: False",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.Module.register_forward_hook",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.register_forward_hook",
        "api_signature": "register_forward_hook(hook, *, prepend=False, with_kwargs=False, always_call=False)",
        "api_description": "Register a forward hook on the module.",
        "return_value": "a handle that can be used to remove the added hook by calling\nhandle.remove()\n",
        "parameters": "hook (Callable) – The user defined hook to be registered.\nprepend (bool) – If True, the provided hook will be fired\nbefore all existing forward hooks on this\ntorch.nn.modules.Module. Otherwise, the provided\nhook will be fired after all existing forward hooks on\nthis torch.nn.modules.Module. Note that global\nforward hooks registered with\nregister_module_forward_hook() will fire before all hooks\nregistered by this method.\nDefault: False\nwith_kwargs (bool) – If True, the hook will be passed the\nkwargs given to the forward function.\nDefault: False\nalways_call (bool) – If True the hook will be run regardless of\nwhether an exception is raised while calling the Module.\nDefault: False",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.jit.ScriptModule.register_forward_pre_hook",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.jit.ScriptModule.html#torch.jit.ScriptModule.register_forward_pre_hook",
        "api_signature": "register_forward_pre_hook(hook, *, prepend=False, with_kwargs=False)",
        "api_description": "Register a forward pre-hook on the module.",
        "return_value": "a handle that can be used to remove the added hook by calling\nhandle.remove()\n",
        "parameters": "hook (Callable) – The user defined hook to be registered.\nprepend (bool) – If true, the provided hook will be fired before\nall existing forward_pre hooks on this\ntorch.nn.modules.Module. Otherwise, the provided\nhook will be fired after all existing forward_pre hooks\non this torch.nn.modules.Module. Note that global\nforward_pre hooks registered with\nregister_module_forward_pre_hook() will fire before all\nhooks registered by this method.\nDefault: False\nwith_kwargs (bool) – If true, the hook will be passed the kwargs\ngiven to the forward function.\nDefault: False",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.Module.register_forward_pre_hook",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.register_forward_pre_hook",
        "api_signature": "register_forward_pre_hook(hook, *, prepend=False, with_kwargs=False)",
        "api_description": "Register a forward pre-hook on the module.",
        "return_value": "a handle that can be used to remove the added hook by calling\nhandle.remove()\n",
        "parameters": "hook (Callable) – The user defined hook to be registered.\nprepend (bool) – If true, the provided hook will be fired before\nall existing forward_pre hooks on this\ntorch.nn.modules.Module. Otherwise, the provided\nhook will be fired after all existing forward_pre hooks\non this torch.nn.modules.Module. Note that global\nforward_pre hooks registered with\nregister_module_forward_pre_hook() will fire before all\nhooks registered by this method.\nDefault: False\nwith_kwargs (bool) – If true, the hook will be passed the kwargs\ngiven to the forward function.\nDefault: False",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.jit.ScriptModule.register_full_backward_hook",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.jit.ScriptModule.html#torch.jit.ScriptModule.register_full_backward_hook",
        "api_signature": "register_full_backward_hook(hook, prepend=False)",
        "api_description": "Register a backward hook on the module.",
        "return_value": "a handle that can be used to remove the added hook by calling\nhandle.remove()\n",
        "parameters": "hook (Callable) – The user-defined hook to be registered.\nprepend (bool) – If true, the provided hook will be fired before\nall existing backward hooks on this\ntorch.nn.modules.Module. Otherwise, the provided\nhook will be fired after all existing backward hooks on\nthis torch.nn.modules.Module. Note that global\nbackward hooks registered with\nregister_module_full_backward_hook() will fire before\nall hooks registered by this method.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.Module.register_full_backward_hook",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.register_full_backward_hook",
        "api_signature": "register_full_backward_hook(hook, prepend=False)",
        "api_description": "Register a backward hook on the module.",
        "return_value": "a handle that can be used to remove the added hook by calling\nhandle.remove()\n",
        "parameters": "hook (Callable) – The user-defined hook to be registered.\nprepend (bool) – If true, the provided hook will be fired before\nall existing backward hooks on this\ntorch.nn.modules.Module. Otherwise, the provided\nhook will be fired after all existing backward hooks on\nthis torch.nn.modules.Module. Note that global\nbackward hooks registered with\nregister_module_full_backward_hook() will fire before\nall hooks registered by this method.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.jit.ScriptModule.register_full_backward_pre_hook",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.jit.ScriptModule.html#torch.jit.ScriptModule.register_full_backward_pre_hook",
        "api_signature": "register_full_backward_pre_hook(hook, prepend=False)",
        "api_description": "Register a backward pre-hook on the module.",
        "return_value": "a handle that can be used to remove the added hook by calling\nhandle.remove()\n",
        "parameters": "hook (Callable) – The user-defined hook to be registered.\nprepend (bool) – If true, the provided hook will be fired before\nall existing backward_pre hooks on this\ntorch.nn.modules.Module. Otherwise, the provided\nhook will be fired after all existing backward_pre hooks\non this torch.nn.modules.Module. Note that global\nbackward_pre hooks registered with\nregister_module_full_backward_pre_hook() will fire before\nall hooks registered by this method.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.Module.register_full_backward_pre_hook",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.register_full_backward_pre_hook",
        "api_signature": "register_full_backward_pre_hook(hook, prepend=False)",
        "api_description": "Register a backward pre-hook on the module.",
        "return_value": "a handle that can be used to remove the added hook by calling\nhandle.remove()\n",
        "parameters": "hook (Callable) – The user-defined hook to be registered.\nprepend (bool) – If true, the provided hook will be fired before\nall existing backward_pre hooks on this\ntorch.nn.modules.Module. Otherwise, the provided\nhook will be fired after all existing backward_pre hooks\non this torch.nn.modules.Module. Note that global\nbackward_pre hooks registered with\nregister_module_full_backward_pre_hook() will fire before\nall hooks registered by this method.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.autograd.graph.Node.register_hook",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.autograd.graph.Node.register_hook.html#torch.autograd.graph.Node.register_hook",
        "api_signature": "Node.register_hook(fn)",
        "api_description": "Register a backward hook.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.register_hook",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.register_hook.html#torch.Tensor.register_hook",
        "api_signature": "Tensor.register_hook(hook)",
        "api_description": "Registers a backward hook.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.package.PackageExporter.register_intern_hook",
        "api_url": "https://pytorch.org/docs/stable/package.html#torch.package.PackageExporter.register_intern_hook",
        "api_signature": "register_intern_hook(hook)",
        "api_description": "Registers an intern hook on the exporter.",
        "return_value": "A handle that can be used to remove the added hook by calling\nhandle.remove().\n",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.kl.register_kl",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.kl.register_kl",
        "api_signature": "torch.distributions.kl.register_kl(type_p, type_q)",
        "api_description": "Decorator to register a pairwise function with kl_divergence().\nUsage:",
        "return_value": "",
        "parameters": "type_p (type) – A subclass of Distribution.\ntype_q (type) – A subclass of Distribution.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.jit.ScriptModule.register_load_state_dict_post_hook",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.jit.ScriptModule.html#torch.jit.ScriptModule.register_load_state_dict_post_hook",
        "api_signature": "register_load_state_dict_post_hook(hook)",
        "api_description": "Register a post hook to be run after module’s load_state_dict is called.",
        "return_value": "a handle that can be used to remove the added hook by calling\nhandle.remove()\n",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.Module.register_load_state_dict_post_hook",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.register_load_state_dict_post_hook",
        "api_signature": "register_load_state_dict_post_hook(hook)",
        "api_description": "Register a post hook to be run after module’s load_state_dict is called.",
        "return_value": "a handle that can be used to remove the added hook by calling\nhandle.remove()\n",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.Adadelta.register_load_state_dict_post_hook",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.Adadelta.html#torch.optim.Adadelta.register_load_state_dict_post_hook",
        "api_signature": "register_load_state_dict_post_hook(hook, prepend=False)",
        "api_description": "Register a load_state_dict post-hook which will be called after\nload_state_dict() is called. It should have the\nfollowing signature:",
        "return_value": "a handle that can be used to remove the added hook by calling\nhandle.remove()\n",
        "parameters": "hook (Callable) – The user defined hook to be registered.\nprepend (bool) – If True, the provided post hook will be fired before\nall the already registered post-hooks on load_state_dict. Otherwise,\nthe provided hook will be fired after all the already registered\npost-hooks. (default: False)",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.Adagrad.register_load_state_dict_post_hook",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.Adagrad.html#torch.optim.Adagrad.register_load_state_dict_post_hook",
        "api_signature": "register_load_state_dict_post_hook(hook, prepend=False)",
        "api_description": "Register a load_state_dict post-hook which will be called after\nload_state_dict() is called. It should have the\nfollowing signature:",
        "return_value": "a handle that can be used to remove the added hook by calling\nhandle.remove()\n",
        "parameters": "hook (Callable) – The user defined hook to be registered.\nprepend (bool) – If True, the provided post hook will be fired before\nall the already registered post-hooks on load_state_dict. Otherwise,\nthe provided hook will be fired after all the already registered\npost-hooks. (default: False)",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.Adam.register_load_state_dict_post_hook",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam.register_load_state_dict_post_hook",
        "api_signature": "register_load_state_dict_post_hook(hook, prepend=False)",
        "api_description": "Register a load_state_dict post-hook which will be called after\nload_state_dict() is called. It should have the\nfollowing signature:",
        "return_value": "a handle that can be used to remove the added hook by calling\nhandle.remove()\n",
        "parameters": "hook (Callable) – The user defined hook to be registered.\nprepend (bool) – If True, the provided post hook will be fired before\nall the already registered post-hooks on load_state_dict. Otherwise,\nthe provided hook will be fired after all the already registered\npost-hooks. (default: False)",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.Adamax.register_load_state_dict_post_hook",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.Adamax.html#torch.optim.Adamax.register_load_state_dict_post_hook",
        "api_signature": "register_load_state_dict_post_hook(hook, prepend=False)",
        "api_description": "Register a load_state_dict post-hook which will be called after\nload_state_dict() is called. It should have the\nfollowing signature:",
        "return_value": "a handle that can be used to remove the added hook by calling\nhandle.remove()\n",
        "parameters": "hook (Callable) – The user defined hook to be registered.\nprepend (bool) – If True, the provided post hook will be fired before\nall the already registered post-hooks on load_state_dict. Otherwise,\nthe provided hook will be fired after all the already registered\npost-hooks. (default: False)",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.AdamW.register_load_state_dict_post_hook",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html#torch.optim.AdamW.register_load_state_dict_post_hook",
        "api_signature": "register_load_state_dict_post_hook(hook, prepend=False)",
        "api_description": "Register a load_state_dict post-hook which will be called after\nload_state_dict() is called. It should have the\nfollowing signature:",
        "return_value": "a handle that can be used to remove the added hook by calling\nhandle.remove()\n",
        "parameters": "hook (Callable) – The user defined hook to be registered.\nprepend (bool) – If True, the provided post hook will be fired before\nall the already registered post-hooks on load_state_dict. Otherwise,\nthe provided hook will be fired after all the already registered\npost-hooks. (default: False)",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.ASGD.register_load_state_dict_post_hook",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.ASGD.html#torch.optim.ASGD.register_load_state_dict_post_hook",
        "api_signature": "register_load_state_dict_post_hook(hook, prepend=False)",
        "api_description": "Register a load_state_dict post-hook which will be called after\nload_state_dict() is called. It should have the\nfollowing signature:",
        "return_value": "a handle that can be used to remove the added hook by calling\nhandle.remove()\n",
        "parameters": "hook (Callable) – The user defined hook to be registered.\nprepend (bool) – If True, the provided post hook will be fired before\nall the already registered post-hooks on load_state_dict. Otherwise,\nthe provided hook will be fired after all the already registered\npost-hooks. (default: False)",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.LBFGS.register_load_state_dict_post_hook",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.LBFGS.html#torch.optim.LBFGS.register_load_state_dict_post_hook",
        "api_signature": "register_load_state_dict_post_hook(hook, prepend=False)",
        "api_description": "Register a load_state_dict post-hook which will be called after\nload_state_dict() is called. It should have the\nfollowing signature:",
        "return_value": "a handle that can be used to remove the added hook by calling\nhandle.remove()\n",
        "parameters": "hook (Callable) – The user defined hook to be registered.\nprepend (bool) – If True, the provided post hook will be fired before\nall the already registered post-hooks on load_state_dict. Otherwise,\nthe provided hook will be fired after all the already registered\npost-hooks. (default: False)",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.NAdam.register_load_state_dict_post_hook",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.NAdam.html#torch.optim.NAdam.register_load_state_dict_post_hook",
        "api_signature": "register_load_state_dict_post_hook(hook, prepend=False)",
        "api_description": "Register a load_state_dict post-hook which will be called after\nload_state_dict() is called. It should have the\nfollowing signature:",
        "return_value": "a handle that can be used to remove the added hook by calling\nhandle.remove()\n",
        "parameters": "hook (Callable) – The user defined hook to be registered.\nprepend (bool) – If True, the provided post hook will be fired before\nall the already registered post-hooks on load_state_dict. Otherwise,\nthe provided hook will be fired after all the already registered\npost-hooks. (default: False)",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.RAdam.register_load_state_dict_post_hook",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.RAdam.html#torch.optim.RAdam.register_load_state_dict_post_hook",
        "api_signature": "register_load_state_dict_post_hook(hook, prepend=False)",
        "api_description": "Register a load_state_dict post-hook which will be called after\nload_state_dict() is called. It should have the\nfollowing signature:",
        "return_value": "a handle that can be used to remove the added hook by calling\nhandle.remove()\n",
        "parameters": "hook (Callable) – The user defined hook to be registered.\nprepend (bool) – If True, the provided post hook will be fired before\nall the already registered post-hooks on load_state_dict. Otherwise,\nthe provided hook will be fired after all the already registered\npost-hooks. (default: False)",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.RMSprop.register_load_state_dict_post_hook",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.RMSprop.html#torch.optim.RMSprop.register_load_state_dict_post_hook",
        "api_signature": "register_load_state_dict_post_hook(hook, prepend=False)",
        "api_description": "Register a load_state_dict post-hook which will be called after\nload_state_dict() is called. It should have the\nfollowing signature:",
        "return_value": "a handle that can be used to remove the added hook by calling\nhandle.remove()\n",
        "parameters": "hook (Callable) – The user defined hook to be registered.\nprepend (bool) – If True, the provided post hook will be fired before\nall the already registered post-hooks on load_state_dict. Otherwise,\nthe provided hook will be fired after all the already registered\npost-hooks. (default: False)",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.Rprop.register_load_state_dict_post_hook",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.Rprop.html#torch.optim.Rprop.register_load_state_dict_post_hook",
        "api_signature": "register_load_state_dict_post_hook(hook, prepend=False)",
        "api_description": "Register a load_state_dict post-hook which will be called after\nload_state_dict() is called. It should have the\nfollowing signature:",
        "return_value": "a handle that can be used to remove the added hook by calling\nhandle.remove()\n",
        "parameters": "hook (Callable) – The user defined hook to be registered.\nprepend (bool) – If True, the provided post hook will be fired before\nall the already registered post-hooks on load_state_dict. Otherwise,\nthe provided hook will be fired after all the already registered\npost-hooks. (default: False)",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.SGD.register_load_state_dict_post_hook",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.SGD.html#torch.optim.SGD.register_load_state_dict_post_hook",
        "api_signature": "register_load_state_dict_post_hook(hook, prepend=False)",
        "api_description": "Register a load_state_dict post-hook which will be called after\nload_state_dict() is called. It should have the\nfollowing signature:",
        "return_value": "a handle that can be used to remove the added hook by calling\nhandle.remove()\n",
        "parameters": "hook (Callable) – The user defined hook to be registered.\nprepend (bool) – If True, the provided post hook will be fired before\nall the already registered post-hooks on load_state_dict. Otherwise,\nthe provided hook will be fired after all the already registered\npost-hooks. (default: False)",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.SparseAdam.register_load_state_dict_post_hook",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.SparseAdam.html#torch.optim.SparseAdam.register_load_state_dict_post_hook",
        "api_signature": "register_load_state_dict_post_hook(hook, prepend=False)",
        "api_description": "Register a load_state_dict post-hook which will be called after\nload_state_dict() is called. It should have the\nfollowing signature:",
        "return_value": "a handle that can be used to remove the added hook by calling\nhandle.remove()\n",
        "parameters": "hook (Callable) – The user defined hook to be registered.\nprepend (bool) – If True, the provided post hook will be fired before\nall the already registered post-hooks on load_state_dict. Otherwise,\nthe provided hook will be fired after all the already registered\npost-hooks. (default: False)",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.Adadelta.register_load_state_dict_pre_hook",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.Adadelta.html#torch.optim.Adadelta.register_load_state_dict_pre_hook",
        "api_signature": "register_load_state_dict_pre_hook(hook, prepend=False)",
        "api_description": "Register a load_state_dict pre-hook which will be called before\nload_state_dict() is called. It should have the\nfollowing signature:",
        "return_value": "a handle that can be used to remove the added hook by calling\nhandle.remove()\n",
        "parameters": "hook (Callable) – The user defined hook to be registered.\nprepend (bool) – If True, the provided pre hook will be fired before\nall the already registered pre-hooks on load_state_dict. Otherwise,\nthe provided hook will be fired after all the already registered\npre-hooks. (default: False)",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.Adagrad.register_load_state_dict_pre_hook",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.Adagrad.html#torch.optim.Adagrad.register_load_state_dict_pre_hook",
        "api_signature": "register_load_state_dict_pre_hook(hook, prepend=False)",
        "api_description": "Register a load_state_dict pre-hook which will be called before\nload_state_dict() is called. It should have the\nfollowing signature:",
        "return_value": "a handle that can be used to remove the added hook by calling\nhandle.remove()\n",
        "parameters": "hook (Callable) – The user defined hook to be registered.\nprepend (bool) – If True, the provided pre hook will be fired before\nall the already registered pre-hooks on load_state_dict. Otherwise,\nthe provided hook will be fired after all the already registered\npre-hooks. (default: False)",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.Adam.register_load_state_dict_pre_hook",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam.register_load_state_dict_pre_hook",
        "api_signature": "register_load_state_dict_pre_hook(hook, prepend=False)",
        "api_description": "Register a load_state_dict pre-hook which will be called before\nload_state_dict() is called. It should have the\nfollowing signature:",
        "return_value": "a handle that can be used to remove the added hook by calling\nhandle.remove()\n",
        "parameters": "hook (Callable) – The user defined hook to be registered.\nprepend (bool) – If True, the provided pre hook will be fired before\nall the already registered pre-hooks on load_state_dict. Otherwise,\nthe provided hook will be fired after all the already registered\npre-hooks. (default: False)",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.Adamax.register_load_state_dict_pre_hook",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.Adamax.html#torch.optim.Adamax.register_load_state_dict_pre_hook",
        "api_signature": "register_load_state_dict_pre_hook(hook, prepend=False)",
        "api_description": "Register a load_state_dict pre-hook which will be called before\nload_state_dict() is called. It should have the\nfollowing signature:",
        "return_value": "a handle that can be used to remove the added hook by calling\nhandle.remove()\n",
        "parameters": "hook (Callable) – The user defined hook to be registered.\nprepend (bool) – If True, the provided pre hook will be fired before\nall the already registered pre-hooks on load_state_dict. Otherwise,\nthe provided hook will be fired after all the already registered\npre-hooks. (default: False)",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.AdamW.register_load_state_dict_pre_hook",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html#torch.optim.AdamW.register_load_state_dict_pre_hook",
        "api_signature": "register_load_state_dict_pre_hook(hook, prepend=False)",
        "api_description": "Register a load_state_dict pre-hook which will be called before\nload_state_dict() is called. It should have the\nfollowing signature:",
        "return_value": "a handle that can be used to remove the added hook by calling\nhandle.remove()\n",
        "parameters": "hook (Callable) – The user defined hook to be registered.\nprepend (bool) – If True, the provided pre hook will be fired before\nall the already registered pre-hooks on load_state_dict. Otherwise,\nthe provided hook will be fired after all the already registered\npre-hooks. (default: False)",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.ASGD.register_load_state_dict_pre_hook",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.ASGD.html#torch.optim.ASGD.register_load_state_dict_pre_hook",
        "api_signature": "register_load_state_dict_pre_hook(hook, prepend=False)",
        "api_description": "Register a load_state_dict pre-hook which will be called before\nload_state_dict() is called. It should have the\nfollowing signature:",
        "return_value": "a handle that can be used to remove the added hook by calling\nhandle.remove()\n",
        "parameters": "hook (Callable) – The user defined hook to be registered.\nprepend (bool) – If True, the provided pre hook will be fired before\nall the already registered pre-hooks on load_state_dict. Otherwise,\nthe provided hook will be fired after all the already registered\npre-hooks. (default: False)",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.LBFGS.register_load_state_dict_pre_hook",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.LBFGS.html#torch.optim.LBFGS.register_load_state_dict_pre_hook",
        "api_signature": "register_load_state_dict_pre_hook(hook, prepend=False)",
        "api_description": "Register a load_state_dict pre-hook which will be called before\nload_state_dict() is called. It should have the\nfollowing signature:",
        "return_value": "a handle that can be used to remove the added hook by calling\nhandle.remove()\n",
        "parameters": "hook (Callable) – The user defined hook to be registered.\nprepend (bool) – If True, the provided pre hook will be fired before\nall the already registered pre-hooks on load_state_dict. Otherwise,\nthe provided hook will be fired after all the already registered\npre-hooks. (default: False)",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.NAdam.register_load_state_dict_pre_hook",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.NAdam.html#torch.optim.NAdam.register_load_state_dict_pre_hook",
        "api_signature": "register_load_state_dict_pre_hook(hook, prepend=False)",
        "api_description": "Register a load_state_dict pre-hook which will be called before\nload_state_dict() is called. It should have the\nfollowing signature:",
        "return_value": "a handle that can be used to remove the added hook by calling\nhandle.remove()\n",
        "parameters": "hook (Callable) – The user defined hook to be registered.\nprepend (bool) – If True, the provided pre hook will be fired before\nall the already registered pre-hooks on load_state_dict. Otherwise,\nthe provided hook will be fired after all the already registered\npre-hooks. (default: False)",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.RAdam.register_load_state_dict_pre_hook",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.RAdam.html#torch.optim.RAdam.register_load_state_dict_pre_hook",
        "api_signature": "register_load_state_dict_pre_hook(hook, prepend=False)",
        "api_description": "Register a load_state_dict pre-hook which will be called before\nload_state_dict() is called. It should have the\nfollowing signature:",
        "return_value": "a handle that can be used to remove the added hook by calling\nhandle.remove()\n",
        "parameters": "hook (Callable) – The user defined hook to be registered.\nprepend (bool) – If True, the provided pre hook will be fired before\nall the already registered pre-hooks on load_state_dict. Otherwise,\nthe provided hook will be fired after all the already registered\npre-hooks. (default: False)",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.RMSprop.register_load_state_dict_pre_hook",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.RMSprop.html#torch.optim.RMSprop.register_load_state_dict_pre_hook",
        "api_signature": "register_load_state_dict_pre_hook(hook, prepend=False)",
        "api_description": "Register a load_state_dict pre-hook which will be called before\nload_state_dict() is called. It should have the\nfollowing signature:",
        "return_value": "a handle that can be used to remove the added hook by calling\nhandle.remove()\n",
        "parameters": "hook (Callable) – The user defined hook to be registered.\nprepend (bool) – If True, the provided pre hook will be fired before\nall the already registered pre-hooks on load_state_dict. Otherwise,\nthe provided hook will be fired after all the already registered\npre-hooks. (default: False)",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.Rprop.register_load_state_dict_pre_hook",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.Rprop.html#torch.optim.Rprop.register_load_state_dict_pre_hook",
        "api_signature": "register_load_state_dict_pre_hook(hook, prepend=False)",
        "api_description": "Register a load_state_dict pre-hook which will be called before\nload_state_dict() is called. It should have the\nfollowing signature:",
        "return_value": "a handle that can be used to remove the added hook by calling\nhandle.remove()\n",
        "parameters": "hook (Callable) – The user defined hook to be registered.\nprepend (bool) – If True, the provided pre hook will be fired before\nall the already registered pre-hooks on load_state_dict. Otherwise,\nthe provided hook will be fired after all the already registered\npre-hooks. (default: False)",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.SGD.register_load_state_dict_pre_hook",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.SGD.html#torch.optim.SGD.register_load_state_dict_pre_hook",
        "api_signature": "register_load_state_dict_pre_hook(hook, prepend=False)",
        "api_description": "Register a load_state_dict pre-hook which will be called before\nload_state_dict() is called. It should have the\nfollowing signature:",
        "return_value": "a handle that can be used to remove the added hook by calling\nhandle.remove()\n",
        "parameters": "hook (Callable) – The user defined hook to be registered.\nprepend (bool) – If True, the provided pre hook will be fired before\nall the already registered pre-hooks on load_state_dict. Otherwise,\nthe provided hook will be fired after all the already registered\npre-hooks. (default: False)",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.SparseAdam.register_load_state_dict_pre_hook",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.SparseAdam.html#torch.optim.SparseAdam.register_load_state_dict_pre_hook",
        "api_signature": "register_load_state_dict_pre_hook(hook, prepend=False)",
        "api_description": "Register a load_state_dict pre-hook which will be called before\nload_state_dict() is called. It should have the\nfollowing signature:",
        "return_value": "a handle that can be used to remove the added hook by calling\nhandle.remove()\n",
        "parameters": "hook (Callable) – The user defined hook to be registered.\nprepend (bool) – If True, the provided pre hook will be fired before\nall the already registered pre-hooks on load_state_dict. Otherwise,\nthe provided hook will be fired after all the already registered\npre-hooks. (default: False)",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.package.PackageExporter.register_mock_hook",
        "api_url": "https://pytorch.org/docs/stable/package.html#torch.package.PackageExporter.register_mock_hook",
        "api_signature": "register_mock_hook(hook)",
        "api_description": "Registers a mock hook on the exporter.",
        "return_value": "A handle that can be used to remove the added hook by calling\nhandle.remove().\n",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.jit.ScriptModule.register_module",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.jit.ScriptModule.html#torch.jit.ScriptModule.register_module",
        "api_signature": "register_module(name, module)",
        "api_description": "Alias for add_module().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.Module.register_module",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.register_module",
        "api_signature": "register_module(name, module)",
        "api_description": "Alias for add_module().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.modules.module.register_module_backward_hook",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.modules.module.register_module_backward_hook.html#torch.nn.modules.module.register_module_backward_hook",
        "api_signature": "torch.nn.modules.module.register_module_backward_hook(hook)",
        "api_description": "Register a backward hook common to all the modules.",
        "return_value": "a handle that can be used to remove the added hook by calling\nhandle.remove()\n",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.modules.module.register_module_buffer_registration_hook",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.modules.module.register_module_buffer_registration_hook.html#torch.nn.modules.module.register_module_buffer_registration_hook",
        "api_signature": "torch.nn.modules.module.register_module_buffer_registration_hook(hook)",
        "api_description": "Register a buffer registration hook common to all modules.",
        "return_value": "a handle that can be used to remove the added hook by calling\nhandle.remove()\n",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.modules.module.register_module_forward_hook",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.modules.module.register_module_forward_hook.html#torch.nn.modules.module.register_module_forward_hook",
        "api_signature": "torch.nn.modules.module.register_module_forward_hook(hook, *, always_call=False)",
        "api_description": "Register a global forward hook for all the modules.",
        "return_value": "a handle that can be used to remove the added hook by calling\nhandle.remove()\n",
        "parameters": "hook (Callable) – The user defined hook to be registered.\nalways_call (bool) – If True the hook will be run regardless of\nwhether an exception is raised while calling the Module.\nDefault: False",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.modules.module.register_module_forward_pre_hook",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.modules.module.register_module_forward_pre_hook.html#torch.nn.modules.module.register_module_forward_pre_hook",
        "api_signature": "torch.nn.modules.module.register_module_forward_pre_hook(hook)",
        "api_description": "Register a forward pre-hook common to all modules.",
        "return_value": "a handle that can be used to remove the added hook by calling\nhandle.remove()\n",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.modules.module.register_module_full_backward_hook",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.modules.module.register_module_full_backward_hook.html#torch.nn.modules.module.register_module_full_backward_hook",
        "api_signature": "torch.nn.modules.module.register_module_full_backward_hook(hook)",
        "api_description": "Register a backward hook common to all the modules.",
        "return_value": "a handle that can be used to remove the added hook by calling\nhandle.remove()\n",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.modules.module.register_module_full_backward_pre_hook",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.modules.module.register_module_full_backward_pre_hook.html#torch.nn.modules.module.register_module_full_backward_pre_hook",
        "api_signature": "torch.nn.modules.module.register_module_full_backward_pre_hook(hook)",
        "api_description": "Register a backward pre-hook common to all the modules.",
        "return_value": "a handle that can be used to remove the added hook by calling\nhandle.remove()\n",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.modules.module.register_module_module_registration_hook",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.modules.module.register_module_module_registration_hook.html#torch.nn.modules.module.register_module_module_registration_hook",
        "api_signature": "torch.nn.modules.module.register_module_module_registration_hook(hook)",
        "api_description": "Register a module registration hook common to all modules.",
        "return_value": "a handle that can be used to remove the added hook by calling\nhandle.remove()\n",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.modules.module.register_module_parameter_registration_hook",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.modules.module.register_module_parameter_registration_hook.html#torch.nn.modules.module.register_module_parameter_registration_hook",
        "api_signature": "torch.nn.modules.module.register_module_parameter_registration_hook(hook)",
        "api_description": "Register a parameter registration hook common to all modules.",
        "return_value": "a handle that can be used to remove the added hook by calling\nhandle.remove()\n",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.autograd.graph.register_multi_grad_hook",
        "api_url": "https://pytorch.org/docs/stable/autograd.html#torch.autograd.graph.register_multi_grad_hook",
        "api_signature": "torch.autograd.graph.register_multi_grad_hook(tensors, fn, *, mode='all')",
        "api_description": "Register a multi-grad backward hook.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.onnx.OnnxRegistry.register_op",
        "api_url": "https://pytorch.org/docs/stable/onnx_dynamo.html#torch.onnx.OnnxRegistry.register_op",
        "api_signature": "register_op(function, namespace, op_name, overload=None, is_complex=False)",
        "api_description": "Registers a custom operator: torch.ops.<namespace>.<op_name>.<overload>.",
        "return_value": "",
        "parameters": "function (Union[onnxscript.OnnxFunction, onnxscript.TracedOnnxFunction]) – The onnx-sctip function to register.\nnamespace (str) – The namespace of the operator to register.\nop_name (str) – The name of the operator to register.\noverload (Optional[str]) – The overload of the operator to register. If it’s default overload,\nleave it to None.\nis_complex (bool) – Whether the function is a function that handles complex valued inputs.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.serialization.register_package",
        "api_url": "https://pytorch.org/docs/stable/notes/serialization.html#torch.serialization.register_package",
        "api_signature": "torch.serialization.register_package(priority, tagger, deserializer)",
        "api_description": "Registers callables for tagging and deserializing storage objects with an associated priority.\nTagging associates a device with a storage object at save time while deserializing moves a\nstorage object to an appropriate device at load time. tagger and deserializer\nare run in the order given by their priority until a tagger/deserializer returns a\nvalue that is not None.",
        "return_value": "None\n",
        "parameters": "priority (int) – Indicates the priority associated with the tagger and deserializer, where a lower\nvalue indicates higher priority.\ntagger (Callable[[Union[Storage, TypedStorage, UntypedStorage]], Optional[str]]) – Callable that takes in a storage object and returns its tagged device as a string\nor None.\ndeserializer (Callable[[Union[Storage, TypedStorage, UntypedStorage], str], Optional[Union[Storage, TypedStorage, UntypedStorage]]]) – Callable that takes in storage object and a device string and returns a storage\nobject on the appropriate device or None.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.jit.ScriptModule.register_parameter",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.jit.ScriptModule.html#torch.jit.ScriptModule.register_parameter",
        "api_signature": "register_parameter(name, param)",
        "api_description": "Add a parameter to the module.",
        "return_value": "",
        "parameters": "name (str) – name of the parameter. The parameter can be accessed\nfrom this module using the given name\nparam (Parameter or None) – parameter to be added to the module. If\nNone, then operations that run on parameters, such as cuda,\nare ignored. If None, the parameter is not included in the\nmodule’s state_dict.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.Module.register_parameter",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.register_parameter",
        "api_signature": "register_parameter(name, param)",
        "api_description": "Add a parameter to the module.",
        "return_value": "",
        "parameters": "name (str) – name of the parameter. The parameter can be accessed\nfrom this module using the given name\nparam (Parameter or None) – parameter to be added to the module. If\nNone, then operations that run on parameters, such as cuda,\nare ignored. If None, the parameter is not included in the\nmodule’s state_dict.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.utils.parametrize.register_parametrization",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.utils.parametrize.register_parametrization.html#torch.nn.utils.parametrize.register_parametrization",
        "api_signature": "torch.nn.utils.parametrize.register_parametrization(module, tensor_name, parametrization, *, unsafe=False)",
        "api_description": "Register a parametrization to a tensor in a module.",
        "return_value": "",
        "parameters": "module (nn.Module) – module on which to register the parametrization\ntensor_name (str) – name of the parameter or buffer on which to register\nthe parametrization\nparametrization (nn.Module) – the parametrization to register\nunsafe (bool) – a boolean flag that denotes whether the parametrization\nmay change the dtype and shape of the tensor. Default: False\nWarning: the parametrization is not checked for consistency upon registration.\nEnable this flag at your own risk.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.register_post_accumulate_grad_hook",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.register_post_accumulate_grad_hook.html#torch.Tensor.register_post_accumulate_grad_hook",
        "api_signature": "Tensor.register_post_accumulate_grad_hook(hook)",
        "api_description": "Registers a backward hook that runs after grad accumulation.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.autograd.graph.Node.register_prehook",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.autograd.graph.Node.register_prehook.html#torch.autograd.graph.Node.register_prehook",
        "api_signature": "Node.register_prehook(fn)",
        "api_description": "Register a backward pre-hook.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.Adadelta.register_state_dict_post_hook",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.Adadelta.html#torch.optim.Adadelta.register_state_dict_post_hook",
        "api_signature": "register_state_dict_post_hook(hook, prepend=False)",
        "api_description": "Register a state dict post-hook which will be called after\nstate_dict() is called. It should have the\nfollowing signature:",
        "return_value": "a handle that can be used to remove the added hook by calling\nhandle.remove()\n",
        "parameters": "hook (Callable) – The user defined hook to be registered.\nprepend (bool) – If True, the provided post hook will be fired before\nall the already registered post-hooks on state_dict. Otherwise,\nthe provided hook will be fired after all the already registered\npost-hooks. (default: False)",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.Adagrad.register_state_dict_post_hook",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.Adagrad.html#torch.optim.Adagrad.register_state_dict_post_hook",
        "api_signature": "register_state_dict_post_hook(hook, prepend=False)",
        "api_description": "Register a state dict post-hook which will be called after\nstate_dict() is called. It should have the\nfollowing signature:",
        "return_value": "a handle that can be used to remove the added hook by calling\nhandle.remove()\n",
        "parameters": "hook (Callable) – The user defined hook to be registered.\nprepend (bool) – If True, the provided post hook will be fired before\nall the already registered post-hooks on state_dict. Otherwise,\nthe provided hook will be fired after all the already registered\npost-hooks. (default: False)",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.Adam.register_state_dict_post_hook",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam.register_state_dict_post_hook",
        "api_signature": "register_state_dict_post_hook(hook, prepend=False)",
        "api_description": "Register a state dict post-hook which will be called after\nstate_dict() is called. It should have the\nfollowing signature:",
        "return_value": "a handle that can be used to remove the added hook by calling\nhandle.remove()\n",
        "parameters": "hook (Callable) – The user defined hook to be registered.\nprepend (bool) – If True, the provided post hook will be fired before\nall the already registered post-hooks on state_dict. Otherwise,\nthe provided hook will be fired after all the already registered\npost-hooks. (default: False)",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.Adamax.register_state_dict_post_hook",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.Adamax.html#torch.optim.Adamax.register_state_dict_post_hook",
        "api_signature": "register_state_dict_post_hook(hook, prepend=False)",
        "api_description": "Register a state dict post-hook which will be called after\nstate_dict() is called. It should have the\nfollowing signature:",
        "return_value": "a handle that can be used to remove the added hook by calling\nhandle.remove()\n",
        "parameters": "hook (Callable) – The user defined hook to be registered.\nprepend (bool) – If True, the provided post hook will be fired before\nall the already registered post-hooks on state_dict. Otherwise,\nthe provided hook will be fired after all the already registered\npost-hooks. (default: False)",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.AdamW.register_state_dict_post_hook",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html#torch.optim.AdamW.register_state_dict_post_hook",
        "api_signature": "register_state_dict_post_hook(hook, prepend=False)",
        "api_description": "Register a state dict post-hook which will be called after\nstate_dict() is called. It should have the\nfollowing signature:",
        "return_value": "a handle that can be used to remove the added hook by calling\nhandle.remove()\n",
        "parameters": "hook (Callable) – The user defined hook to be registered.\nprepend (bool) – If True, the provided post hook will be fired before\nall the already registered post-hooks on state_dict. Otherwise,\nthe provided hook will be fired after all the already registered\npost-hooks. (default: False)",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.ASGD.register_state_dict_post_hook",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.ASGD.html#torch.optim.ASGD.register_state_dict_post_hook",
        "api_signature": "register_state_dict_post_hook(hook, prepend=False)",
        "api_description": "Register a state dict post-hook which will be called after\nstate_dict() is called. It should have the\nfollowing signature:",
        "return_value": "a handle that can be used to remove the added hook by calling\nhandle.remove()\n",
        "parameters": "hook (Callable) – The user defined hook to be registered.\nprepend (bool) – If True, the provided post hook will be fired before\nall the already registered post-hooks on state_dict. Otherwise,\nthe provided hook will be fired after all the already registered\npost-hooks. (default: False)",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.LBFGS.register_state_dict_post_hook",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.LBFGS.html#torch.optim.LBFGS.register_state_dict_post_hook",
        "api_signature": "register_state_dict_post_hook(hook, prepend=False)",
        "api_description": "Register a state dict post-hook which will be called after\nstate_dict() is called. It should have the\nfollowing signature:",
        "return_value": "a handle that can be used to remove the added hook by calling\nhandle.remove()\n",
        "parameters": "hook (Callable) – The user defined hook to be registered.\nprepend (bool) – If True, the provided post hook will be fired before\nall the already registered post-hooks on state_dict. Otherwise,\nthe provided hook will be fired after all the already registered\npost-hooks. (default: False)",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.NAdam.register_state_dict_post_hook",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.NAdam.html#torch.optim.NAdam.register_state_dict_post_hook",
        "api_signature": "register_state_dict_post_hook(hook, prepend=False)",
        "api_description": "Register a state dict post-hook which will be called after\nstate_dict() is called. It should have the\nfollowing signature:",
        "return_value": "a handle that can be used to remove the added hook by calling\nhandle.remove()\n",
        "parameters": "hook (Callable) – The user defined hook to be registered.\nprepend (bool) – If True, the provided post hook will be fired before\nall the already registered post-hooks on state_dict. Otherwise,\nthe provided hook will be fired after all the already registered\npost-hooks. (default: False)",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.RAdam.register_state_dict_post_hook",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.RAdam.html#torch.optim.RAdam.register_state_dict_post_hook",
        "api_signature": "register_state_dict_post_hook(hook, prepend=False)",
        "api_description": "Register a state dict post-hook which will be called after\nstate_dict() is called. It should have the\nfollowing signature:",
        "return_value": "a handle that can be used to remove the added hook by calling\nhandle.remove()\n",
        "parameters": "hook (Callable) – The user defined hook to be registered.\nprepend (bool) – If True, the provided post hook will be fired before\nall the already registered post-hooks on state_dict. Otherwise,\nthe provided hook will be fired after all the already registered\npost-hooks. (default: False)",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.RMSprop.register_state_dict_post_hook",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.RMSprop.html#torch.optim.RMSprop.register_state_dict_post_hook",
        "api_signature": "register_state_dict_post_hook(hook, prepend=False)",
        "api_description": "Register a state dict post-hook which will be called after\nstate_dict() is called. It should have the\nfollowing signature:",
        "return_value": "a handle that can be used to remove the added hook by calling\nhandle.remove()\n",
        "parameters": "hook (Callable) – The user defined hook to be registered.\nprepend (bool) – If True, the provided post hook will be fired before\nall the already registered post-hooks on state_dict. Otherwise,\nthe provided hook will be fired after all the already registered\npost-hooks. (default: False)",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.Rprop.register_state_dict_post_hook",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.Rprop.html#torch.optim.Rprop.register_state_dict_post_hook",
        "api_signature": "register_state_dict_post_hook(hook, prepend=False)",
        "api_description": "Register a state dict post-hook which will be called after\nstate_dict() is called. It should have the\nfollowing signature:",
        "return_value": "a handle that can be used to remove the added hook by calling\nhandle.remove()\n",
        "parameters": "hook (Callable) – The user defined hook to be registered.\nprepend (bool) – If True, the provided post hook will be fired before\nall the already registered post-hooks on state_dict. Otherwise,\nthe provided hook will be fired after all the already registered\npost-hooks. (default: False)",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.SGD.register_state_dict_post_hook",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.SGD.html#torch.optim.SGD.register_state_dict_post_hook",
        "api_signature": "register_state_dict_post_hook(hook, prepend=False)",
        "api_description": "Register a state dict post-hook which will be called after\nstate_dict() is called. It should have the\nfollowing signature:",
        "return_value": "a handle that can be used to remove the added hook by calling\nhandle.remove()\n",
        "parameters": "hook (Callable) – The user defined hook to be registered.\nprepend (bool) – If True, the provided post hook will be fired before\nall the already registered post-hooks on state_dict. Otherwise,\nthe provided hook will be fired after all the already registered\npost-hooks. (default: False)",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.SparseAdam.register_state_dict_post_hook",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.SparseAdam.html#torch.optim.SparseAdam.register_state_dict_post_hook",
        "api_signature": "register_state_dict_post_hook(hook, prepend=False)",
        "api_description": "Register a state dict post-hook which will be called after\nstate_dict() is called. It should have the\nfollowing signature:",
        "return_value": "a handle that can be used to remove the added hook by calling\nhandle.remove()\n",
        "parameters": "hook (Callable) – The user defined hook to be registered.\nprepend (bool) – If True, the provided post hook will be fired before\nall the already registered post-hooks on state_dict. Otherwise,\nthe provided hook will be fired after all the already registered\npost-hooks. (default: False)",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.jit.ScriptModule.register_state_dict_pre_hook",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.jit.ScriptModule.html#torch.jit.ScriptModule.register_state_dict_pre_hook",
        "api_signature": "register_state_dict_pre_hook(hook)",
        "api_description": "Register a pre-hook for the state_dict() method.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.Module.register_state_dict_pre_hook",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.register_state_dict_pre_hook",
        "api_signature": "register_state_dict_pre_hook(hook)",
        "api_description": "Register a pre-hook for the state_dict() method.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.Adadelta.register_state_dict_pre_hook",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.Adadelta.html#torch.optim.Adadelta.register_state_dict_pre_hook",
        "api_signature": "register_state_dict_pre_hook(hook, prepend=False)",
        "api_description": "Register a state dict pre-hook which will be called before\nstate_dict() is called. It should have the\nfollowing signature:",
        "return_value": "a handle that can be used to remove the added hook by calling\nhandle.remove()\n",
        "parameters": "hook (Callable) – The user defined hook to be registered.\nprepend (bool) – If True, the provided pre hook will be fired before\nall the already registered pre-hooks on state_dict. Otherwise,\nthe provided hook will be fired after all the already registered\npre-hooks. (default: False)",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.Adagrad.register_state_dict_pre_hook",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.Adagrad.html#torch.optim.Adagrad.register_state_dict_pre_hook",
        "api_signature": "register_state_dict_pre_hook(hook, prepend=False)",
        "api_description": "Register a state dict pre-hook which will be called before\nstate_dict() is called. It should have the\nfollowing signature:",
        "return_value": "a handle that can be used to remove the added hook by calling\nhandle.remove()\n",
        "parameters": "hook (Callable) – The user defined hook to be registered.\nprepend (bool) – If True, the provided pre hook will be fired before\nall the already registered pre-hooks on state_dict. Otherwise,\nthe provided hook will be fired after all the already registered\npre-hooks. (default: False)",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.Adam.register_state_dict_pre_hook",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam.register_state_dict_pre_hook",
        "api_signature": "register_state_dict_pre_hook(hook, prepend=False)",
        "api_description": "Register a state dict pre-hook which will be called before\nstate_dict() is called. It should have the\nfollowing signature:",
        "return_value": "a handle that can be used to remove the added hook by calling\nhandle.remove()\n",
        "parameters": "hook (Callable) – The user defined hook to be registered.\nprepend (bool) – If True, the provided pre hook will be fired before\nall the already registered pre-hooks on state_dict. Otherwise,\nthe provided hook will be fired after all the already registered\npre-hooks. (default: False)",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.Adamax.register_state_dict_pre_hook",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.Adamax.html#torch.optim.Adamax.register_state_dict_pre_hook",
        "api_signature": "register_state_dict_pre_hook(hook, prepend=False)",
        "api_description": "Register a state dict pre-hook which will be called before\nstate_dict() is called. It should have the\nfollowing signature:",
        "return_value": "a handle that can be used to remove the added hook by calling\nhandle.remove()\n",
        "parameters": "hook (Callable) – The user defined hook to be registered.\nprepend (bool) – If True, the provided pre hook will be fired before\nall the already registered pre-hooks on state_dict. Otherwise,\nthe provided hook will be fired after all the already registered\npre-hooks. (default: False)",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.AdamW.register_state_dict_pre_hook",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html#torch.optim.AdamW.register_state_dict_pre_hook",
        "api_signature": "register_state_dict_pre_hook(hook, prepend=False)",
        "api_description": "Register a state dict pre-hook which will be called before\nstate_dict() is called. It should have the\nfollowing signature:",
        "return_value": "a handle that can be used to remove the added hook by calling\nhandle.remove()\n",
        "parameters": "hook (Callable) – The user defined hook to be registered.\nprepend (bool) – If True, the provided pre hook will be fired before\nall the already registered pre-hooks on state_dict. Otherwise,\nthe provided hook will be fired after all the already registered\npre-hooks. (default: False)",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.ASGD.register_state_dict_pre_hook",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.ASGD.html#torch.optim.ASGD.register_state_dict_pre_hook",
        "api_signature": "register_state_dict_pre_hook(hook, prepend=False)",
        "api_description": "Register a state dict pre-hook which will be called before\nstate_dict() is called. It should have the\nfollowing signature:",
        "return_value": "a handle that can be used to remove the added hook by calling\nhandle.remove()\n",
        "parameters": "hook (Callable) – The user defined hook to be registered.\nprepend (bool) – If True, the provided pre hook will be fired before\nall the already registered pre-hooks on state_dict. Otherwise,\nthe provided hook will be fired after all the already registered\npre-hooks. (default: False)",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.LBFGS.register_state_dict_pre_hook",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.LBFGS.html#torch.optim.LBFGS.register_state_dict_pre_hook",
        "api_signature": "register_state_dict_pre_hook(hook, prepend=False)",
        "api_description": "Register a state dict pre-hook which will be called before\nstate_dict() is called. It should have the\nfollowing signature:",
        "return_value": "a handle that can be used to remove the added hook by calling\nhandle.remove()\n",
        "parameters": "hook (Callable) – The user defined hook to be registered.\nprepend (bool) – If True, the provided pre hook will be fired before\nall the already registered pre-hooks on state_dict. Otherwise,\nthe provided hook will be fired after all the already registered\npre-hooks. (default: False)",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.NAdam.register_state_dict_pre_hook",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.NAdam.html#torch.optim.NAdam.register_state_dict_pre_hook",
        "api_signature": "register_state_dict_pre_hook(hook, prepend=False)",
        "api_description": "Register a state dict pre-hook which will be called before\nstate_dict() is called. It should have the\nfollowing signature:",
        "return_value": "a handle that can be used to remove the added hook by calling\nhandle.remove()\n",
        "parameters": "hook (Callable) – The user defined hook to be registered.\nprepend (bool) – If True, the provided pre hook will be fired before\nall the already registered pre-hooks on state_dict. Otherwise,\nthe provided hook will be fired after all the already registered\npre-hooks. (default: False)",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.RAdam.register_state_dict_pre_hook",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.RAdam.html#torch.optim.RAdam.register_state_dict_pre_hook",
        "api_signature": "register_state_dict_pre_hook(hook, prepend=False)",
        "api_description": "Register a state dict pre-hook which will be called before\nstate_dict() is called. It should have the\nfollowing signature:",
        "return_value": "a handle that can be used to remove the added hook by calling\nhandle.remove()\n",
        "parameters": "hook (Callable) – The user defined hook to be registered.\nprepend (bool) – If True, the provided pre hook will be fired before\nall the already registered pre-hooks on state_dict. Otherwise,\nthe provided hook will be fired after all the already registered\npre-hooks. (default: False)",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.RMSprop.register_state_dict_pre_hook",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.RMSprop.html#torch.optim.RMSprop.register_state_dict_pre_hook",
        "api_signature": "register_state_dict_pre_hook(hook, prepend=False)",
        "api_description": "Register a state dict pre-hook which will be called before\nstate_dict() is called. It should have the\nfollowing signature:",
        "return_value": "a handle that can be used to remove the added hook by calling\nhandle.remove()\n",
        "parameters": "hook (Callable) – The user defined hook to be registered.\nprepend (bool) – If True, the provided pre hook will be fired before\nall the already registered pre-hooks on state_dict. Otherwise,\nthe provided hook will be fired after all the already registered\npre-hooks. (default: False)",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.Rprop.register_state_dict_pre_hook",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.Rprop.html#torch.optim.Rprop.register_state_dict_pre_hook",
        "api_signature": "register_state_dict_pre_hook(hook, prepend=False)",
        "api_description": "Register a state dict pre-hook which will be called before\nstate_dict() is called. It should have the\nfollowing signature:",
        "return_value": "a handle that can be used to remove the added hook by calling\nhandle.remove()\n",
        "parameters": "hook (Callable) – The user defined hook to be registered.\nprepend (bool) – If True, the provided pre hook will be fired before\nall the already registered pre-hooks on state_dict. Otherwise,\nthe provided hook will be fired after all the already registered\npre-hooks. (default: False)",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.SGD.register_state_dict_pre_hook",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.SGD.html#torch.optim.SGD.register_state_dict_pre_hook",
        "api_signature": "register_state_dict_pre_hook(hook, prepend=False)",
        "api_description": "Register a state dict pre-hook which will be called before\nstate_dict() is called. It should have the\nfollowing signature:",
        "return_value": "a handle that can be used to remove the added hook by calling\nhandle.remove()\n",
        "parameters": "hook (Callable) – The user defined hook to be registered.\nprepend (bool) – If True, the provided pre hook will be fired before\nall the already registered pre-hooks on state_dict. Otherwise,\nthe provided hook will be fired after all the already registered\npre-hooks. (default: False)",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.SparseAdam.register_state_dict_pre_hook",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.SparseAdam.html#torch.optim.SparseAdam.register_state_dict_pre_hook",
        "api_signature": "register_state_dict_pre_hook(hook, prepend=False)",
        "api_description": "Register a state dict pre-hook which will be called before\nstate_dict() is called. It should have the\nfollowing signature:",
        "return_value": "a handle that can be used to remove the added hook by calling\nhandle.remove()\n",
        "parameters": "hook (Callable) – The user defined hook to be registered.\nprepend (bool) – If True, the provided pre hook will be fired before\nall the already registered pre-hooks on state_dict. Otherwise,\nthe provided hook will be fired after all the already registered\npre-hooks. (default: False)",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.Adadelta.register_step_post_hook",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.Adadelta.html#torch.optim.Adadelta.register_step_post_hook",
        "api_signature": "register_step_post_hook(hook)",
        "api_description": "Register an optimizer step post hook which will be called after optimizer step.\nIt should have the following signature:",
        "return_value": "a handle that can be used to remove the added hook by calling\nhandle.remove()\n",
        "parameters": "hook (Callable) – The user defined hook to be registered.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.Adagrad.register_step_post_hook",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.Adagrad.html#torch.optim.Adagrad.register_step_post_hook",
        "api_signature": "register_step_post_hook(hook)",
        "api_description": "Register an optimizer step post hook which will be called after optimizer step.\nIt should have the following signature:",
        "return_value": "a handle that can be used to remove the added hook by calling\nhandle.remove()\n",
        "parameters": "hook (Callable) – The user defined hook to be registered.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.Adam.register_step_post_hook",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam.register_step_post_hook",
        "api_signature": "register_step_post_hook(hook)",
        "api_description": "Register an optimizer step post hook which will be called after optimizer step.\nIt should have the following signature:",
        "return_value": "a handle that can be used to remove the added hook by calling\nhandle.remove()\n",
        "parameters": "hook (Callable) – The user defined hook to be registered.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.Adamax.register_step_post_hook",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.Adamax.html#torch.optim.Adamax.register_step_post_hook",
        "api_signature": "register_step_post_hook(hook)",
        "api_description": "Register an optimizer step post hook which will be called after optimizer step.\nIt should have the following signature:",
        "return_value": "a handle that can be used to remove the added hook by calling\nhandle.remove()\n",
        "parameters": "hook (Callable) – The user defined hook to be registered.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.AdamW.register_step_post_hook",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html#torch.optim.AdamW.register_step_post_hook",
        "api_signature": "register_step_post_hook(hook)",
        "api_description": "Register an optimizer step post hook which will be called after optimizer step.\nIt should have the following signature:",
        "return_value": "a handle that can be used to remove the added hook by calling\nhandle.remove()\n",
        "parameters": "hook (Callable) – The user defined hook to be registered.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.ASGD.register_step_post_hook",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.ASGD.html#torch.optim.ASGD.register_step_post_hook",
        "api_signature": "register_step_post_hook(hook)",
        "api_description": "Register an optimizer step post hook which will be called after optimizer step.\nIt should have the following signature:",
        "return_value": "a handle that can be used to remove the added hook by calling\nhandle.remove()\n",
        "parameters": "hook (Callable) – The user defined hook to be registered.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.LBFGS.register_step_post_hook",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.LBFGS.html#torch.optim.LBFGS.register_step_post_hook",
        "api_signature": "register_step_post_hook(hook)",
        "api_description": "Register an optimizer step post hook which will be called after optimizer step.\nIt should have the following signature:",
        "return_value": "a handle that can be used to remove the added hook by calling\nhandle.remove()\n",
        "parameters": "hook (Callable) – The user defined hook to be registered.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.NAdam.register_step_post_hook",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.NAdam.html#torch.optim.NAdam.register_step_post_hook",
        "api_signature": "register_step_post_hook(hook)",
        "api_description": "Register an optimizer step post hook which will be called after optimizer step.\nIt should have the following signature:",
        "return_value": "a handle that can be used to remove the added hook by calling\nhandle.remove()\n",
        "parameters": "hook (Callable) – The user defined hook to be registered.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.RAdam.register_step_post_hook",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.RAdam.html#torch.optim.RAdam.register_step_post_hook",
        "api_signature": "register_step_post_hook(hook)",
        "api_description": "Register an optimizer step post hook which will be called after optimizer step.\nIt should have the following signature:",
        "return_value": "a handle that can be used to remove the added hook by calling\nhandle.remove()\n",
        "parameters": "hook (Callable) – The user defined hook to be registered.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.RMSprop.register_step_post_hook",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.RMSprop.html#torch.optim.RMSprop.register_step_post_hook",
        "api_signature": "register_step_post_hook(hook)",
        "api_description": "Register an optimizer step post hook which will be called after optimizer step.\nIt should have the following signature:",
        "return_value": "a handle that can be used to remove the added hook by calling\nhandle.remove()\n",
        "parameters": "hook (Callable) – The user defined hook to be registered.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.Rprop.register_step_post_hook",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.Rprop.html#torch.optim.Rprop.register_step_post_hook",
        "api_signature": "register_step_post_hook(hook)",
        "api_description": "Register an optimizer step post hook which will be called after optimizer step.\nIt should have the following signature:",
        "return_value": "a handle that can be used to remove the added hook by calling\nhandle.remove()\n",
        "parameters": "hook (Callable) – The user defined hook to be registered.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.SGD.register_step_post_hook",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.SGD.html#torch.optim.SGD.register_step_post_hook",
        "api_signature": "register_step_post_hook(hook)",
        "api_description": "Register an optimizer step post hook which will be called after optimizer step.\nIt should have the following signature:",
        "return_value": "a handle that can be used to remove the added hook by calling\nhandle.remove()\n",
        "parameters": "hook (Callable) – The user defined hook to be registered.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.SparseAdam.register_step_post_hook",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.SparseAdam.html#torch.optim.SparseAdam.register_step_post_hook",
        "api_signature": "register_step_post_hook(hook)",
        "api_description": "Register an optimizer step post hook which will be called after optimizer step.\nIt should have the following signature:",
        "return_value": "a handle that can be used to remove the added hook by calling\nhandle.remove()\n",
        "parameters": "hook (Callable) – The user defined hook to be registered.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.Adadelta.register_step_pre_hook",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.Adadelta.html#torch.optim.Adadelta.register_step_pre_hook",
        "api_signature": "register_step_pre_hook(hook)",
        "api_description": "Register an optimizer step pre hook which will be called before\noptimizer step. It should have the following signature:",
        "return_value": "a handle that can be used to remove the added hook by calling\nhandle.remove()\n",
        "parameters": "hook (Callable) – The user defined hook to be registered.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.Adagrad.register_step_pre_hook",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.Adagrad.html#torch.optim.Adagrad.register_step_pre_hook",
        "api_signature": "register_step_pre_hook(hook)",
        "api_description": "Register an optimizer step pre hook which will be called before\noptimizer step. It should have the following signature:",
        "return_value": "a handle that can be used to remove the added hook by calling\nhandle.remove()\n",
        "parameters": "hook (Callable) – The user defined hook to be registered.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.Adam.register_step_pre_hook",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam.register_step_pre_hook",
        "api_signature": "register_step_pre_hook(hook)",
        "api_description": "Register an optimizer step pre hook which will be called before\noptimizer step. It should have the following signature:",
        "return_value": "a handle that can be used to remove the added hook by calling\nhandle.remove()\n",
        "parameters": "hook (Callable) – The user defined hook to be registered.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.Adamax.register_step_pre_hook",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.Adamax.html#torch.optim.Adamax.register_step_pre_hook",
        "api_signature": "register_step_pre_hook(hook)",
        "api_description": "Register an optimizer step pre hook which will be called before\noptimizer step. It should have the following signature:",
        "return_value": "a handle that can be used to remove the added hook by calling\nhandle.remove()\n",
        "parameters": "hook (Callable) – The user defined hook to be registered.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.AdamW.register_step_pre_hook",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html#torch.optim.AdamW.register_step_pre_hook",
        "api_signature": "register_step_pre_hook(hook)",
        "api_description": "Register an optimizer step pre hook which will be called before\noptimizer step. It should have the following signature:",
        "return_value": "a handle that can be used to remove the added hook by calling\nhandle.remove()\n",
        "parameters": "hook (Callable) – The user defined hook to be registered.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.ASGD.register_step_pre_hook",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.ASGD.html#torch.optim.ASGD.register_step_pre_hook",
        "api_signature": "register_step_pre_hook(hook)",
        "api_description": "Register an optimizer step pre hook which will be called before\noptimizer step. It should have the following signature:",
        "return_value": "a handle that can be used to remove the added hook by calling\nhandle.remove()\n",
        "parameters": "hook (Callable) – The user defined hook to be registered.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.LBFGS.register_step_pre_hook",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.LBFGS.html#torch.optim.LBFGS.register_step_pre_hook",
        "api_signature": "register_step_pre_hook(hook)",
        "api_description": "Register an optimizer step pre hook which will be called before\noptimizer step. It should have the following signature:",
        "return_value": "a handle that can be used to remove the added hook by calling\nhandle.remove()\n",
        "parameters": "hook (Callable) – The user defined hook to be registered.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.NAdam.register_step_pre_hook",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.NAdam.html#torch.optim.NAdam.register_step_pre_hook",
        "api_signature": "register_step_pre_hook(hook)",
        "api_description": "Register an optimizer step pre hook which will be called before\noptimizer step. It should have the following signature:",
        "return_value": "a handle that can be used to remove the added hook by calling\nhandle.remove()\n",
        "parameters": "hook (Callable) – The user defined hook to be registered.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.RAdam.register_step_pre_hook",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.RAdam.html#torch.optim.RAdam.register_step_pre_hook",
        "api_signature": "register_step_pre_hook(hook)",
        "api_description": "Register an optimizer step pre hook which will be called before\noptimizer step. It should have the following signature:",
        "return_value": "a handle that can be used to remove the added hook by calling\nhandle.remove()\n",
        "parameters": "hook (Callable) – The user defined hook to be registered.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.RMSprop.register_step_pre_hook",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.RMSprop.html#torch.optim.RMSprop.register_step_pre_hook",
        "api_signature": "register_step_pre_hook(hook)",
        "api_description": "Register an optimizer step pre hook which will be called before\noptimizer step. It should have the following signature:",
        "return_value": "a handle that can be used to remove the added hook by calling\nhandle.remove()\n",
        "parameters": "hook (Callable) – The user defined hook to be registered.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.Rprop.register_step_pre_hook",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.Rprop.html#torch.optim.Rprop.register_step_pre_hook",
        "api_signature": "register_step_pre_hook(hook)",
        "api_description": "Register an optimizer step pre hook which will be called before\noptimizer step. It should have the following signature:",
        "return_value": "a handle that can be used to remove the added hook by calling\nhandle.remove()\n",
        "parameters": "hook (Callable) – The user defined hook to be registered.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.SGD.register_step_pre_hook",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.SGD.html#torch.optim.SGD.register_step_pre_hook",
        "api_signature": "register_step_pre_hook(hook)",
        "api_description": "Register an optimizer step pre hook which will be called before\noptimizer step. It should have the following signature:",
        "return_value": "a handle that can be used to remove the added hook by calling\nhandle.remove()\n",
        "parameters": "hook (Callable) – The user defined hook to be registered.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.SparseAdam.register_step_pre_hook",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.SparseAdam.html#torch.optim.SparseAdam.register_step_pre_hook",
        "api_signature": "register_step_pre_hook(hook)",
        "api_description": "Register an optimizer step pre hook which will be called before\noptimizer step. It should have the following signature:",
        "return_value": "a handle that can be used to remove the added hook by calling\nhandle.remove()\n",
        "parameters": "hook (Callable) – The user defined hook to be registered.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.timer.TimerServer.register_timers",
        "api_url": "https://pytorch.org/docs/stable/elastic/timer.html#torch.distributed.elastic.timer.TimerServer.register_timers",
        "api_signature": "register_timers(timer_requests)",
        "api_description": "Processes the incoming timer requests and registers them with the server.\nThe timer request can either be a acquire-timer or release-timer request.\nTimer requests with a negative expiration_time should be interpreted\nas a release-timer request.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.multiprocessing.api.DefaultLogsSpecs.reify",
        "api_url": "https://pytorch.org/docs/stable/elastic/multiprocessing.html#torch.distributed.elastic.multiprocessing.api.DefaultLogsSpecs.reify",
        "api_signature": "reify(envs)",
        "api_description": "Uses following scheme to build log destination paths:",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.multiprocessing.api.LogsSpecs.reify",
        "api_url": "https://pytorch.org/docs/stable/elastic/multiprocessing.html#torch.distributed.elastic.multiprocessing.api.LogsSpecs.reify",
        "api_signature": "reify(envs)",
        "api_description": "Given the environment variables, builds destination of log files for each of the local ranks.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.fsdp.FullyShardedDataParallel.rekey_optim_state_dict",
        "api_url": "https://pytorch.org/docs/stable/fsdp.html#torch.distributed.fsdp.FullyShardedDataParallel.rekey_optim_state_dict",
        "api_signature": "rekey_optim_state_dict(optim_state_dict, optim_state_key_type, model, optim_input=None, optim=None)",
        "api_description": "Re-keys the optimizer state dict optim_state_dict to use the key type optim_state_key_type.",
        "return_value": "The optimizer state dict re-keyed using the\nparameter keys specified by optim_state_key_type.\n",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.relaxed_bernoulli.RelaxedBernoulli",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.relaxed_bernoulli.RelaxedBernoulli",
        "api_signature": "torch.distributions.relaxed_bernoulli.RelaxedBernoulli(temperature, probs=None, logits=None, validate_args=None)",
        "api_description": "Bases: TransformedDistribution",
        "return_value": "",
        "parameters": "temperature (Tensor) – relaxation temperature\nprobs (Number, Tensor) – the probability of sampling 1\nlogits (Number, Tensor) – the log-odds of sampling 1",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.relaxed_categorical.RelaxedOneHotCategorical",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.relaxed_categorical.RelaxedOneHotCategorical",
        "api_signature": "torch.distributions.relaxed_categorical.RelaxedOneHotCategorical(temperature, probs=None, logits=None, validate_args=None)",
        "api_description": "Bases: TransformedDistribution",
        "return_value": "",
        "parameters": "temperature (Tensor) – relaxation temperature\nprobs (Tensor) – event probabilities\nlogits (Tensor) – unnormalized log probability for each event",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.symbolic_shapes.RelaxedUnspecConstraint",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.fx.experimental.symbolic_shapes.RelaxedUnspecConstraint.html#torch.fx.experimental.symbolic_shapes.RelaxedUnspecConstraint",
        "api_signature": "torch.fx.experimental.symbolic_shapes.RelaxedUnspecConstraint(warn_only)",
        "api_description": "For clients: no explicit constraint; constraint is whatever is implicitly\ninferred by guards from tracing.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.timer.TimerClient.release",
        "api_url": "https://pytorch.org/docs/stable/elastic/timer.html#torch.distributed.elastic.timer.TimerClient.release",
        "api_signature": "release(scope_id)",
        "api_description": "Releases the timer for the scope_id on the worker this\nclient represents. After this method is\ncalled, the countdown timer on the scope is no longer in effect.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.ReLU",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html#torch.nn.ReLU",
        "api_signature": "torch.nn.ReLU(inplace=False)",
        "api_description": "Applies the rectified linear unit function element-wise.",
        "return_value": "",
        "parameters": "inplace (bool) – can optionally do the operation in-place. Default: False",
        "input_shape": "\nInput: (∗)(*)(∗), where ∗*∗ means any number of dimensions.\nOutput: (∗)(*)(∗), same shape as the input.\n\n",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.functional.relu",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.functional.relu.html#torch.nn.functional.relu",
        "api_signature": "torch.nn.functional.relu(input, inplace=False)",
        "api_description": "Applies the rectified linear unit function element-wise. See\nReLU for more details.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.quantized.ReLU6",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.nn.quantized.ReLU6.html#torch.ao.nn.quantized.ReLU6",
        "api_signature": "torch.ao.nn.quantized.ReLU6(inplace=False)",
        "api_description": "Applies the element-wise function:",
        "return_value": "",
        "parameters": "inplace (bool) – can optionally do the operation in-place. Default: False",
        "input_shape": "\nInput: (N,∗)(N, *)(N,∗) where * means, any number of additional\ndimensions\nOutput: (N,∗)(N, *)(N,∗), same shape as the input\n\n",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.ReLU6",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.ReLU6.html#torch.nn.ReLU6",
        "api_signature": "torch.nn.ReLU6(inplace=False)",
        "api_description": "Applies the ReLU6 function element-wise.",
        "return_value": "",
        "parameters": "inplace (bool) – can optionally do the operation in-place. Default: False",
        "input_shape": "\nInput: (∗)(*)(∗), where ∗*∗ means any number of dimensions.\nOutput: (∗)(*)(∗), same shape as the input.\n\n",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.functional.relu6",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.functional.relu6.html#torch.nn.functional.relu6",
        "api_signature": "torch.nn.functional.relu6(input, inplace=False)",
        "api_description": "Applies the element-wise function ReLU6(x)=min⁡(max⁡(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.functional.relu_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.functional.relu_.html#torch.nn.functional.relu_",
        "api_signature": "torch.nn.functional.relu_(input)",
        "api_description": "In-place version of relu().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.remainder",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.remainder.html#torch.remainder",
        "api_signature": "torch.remainder(input, other, *, out=None)",
        "api_description": "Computes\nPython’s modulus operation\nentrywise.  The result has the same sign as the divisor other and its absolute value\nis less than that of other.",
        "return_value": "",
        "parameters": "input (Tensor or Scalar) – the dividend\nother (Tensor or Scalar) – the divisor\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.remainder",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.remainder.html#torch.Tensor.remainder",
        "api_signature": "Tensor.remainder(divisor)",
        "api_description": "See torch.remainder()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.remainder_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.remainder_.html#torch.Tensor.remainder_",
        "api_signature": "Tensor.remainder_(divisor)",
        "api_description": "In-place version of remainder()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.rpc.remote",
        "api_url": "https://pytorch.org/docs/stable/rpc.html#torch.distributed.rpc.remote",
        "api_signature": "torch.distributed.rpc.remote(to, func, args=None, kwargs=None, timeout=-1.0)",
        "api_description": "Make a remote call to run func on worker to and return an\nRRef to the result value immediately.\nWorker to will be the owner of the returned\nRRef, and the worker calling remote is\na user. The owner manages the global reference count of its\nRRef, and the owner\nRRef is only destructed when globally there\nare no living references to it.",
        "return_value": "A user RRef instance to the result\nvalue. Use the blocking API torch.distributed.rpc.RRef.to_here()\nto retrieve the result value locally.\n",
        "parameters": "to (str or WorkerInfo or int) – name/rank/WorkerInfo of the destination worker.\nfunc (Callable) – a callable function, such as Python callables, builtin\noperators (e.g. add()) and annotated\nTorchScript functions.\nargs (tuple) – the argument tuple for the func invocation.\nkwargs (dict) – is a dictionary of keyword arguments for the func\ninvocation.\ntimeout (float, optional) – timeout in seconds for this remote call. If the\ncreation of this\nRRef on worker\nto is not successfully processed on this\nworker within this timeout, then the next time\nthere is an attempt to use the RRef (such as\nto_here()), a timeout will be raised\nindicating this failure. A value of 0 indicates\nan infinite timeout, i.e. a timeout error will\nnever be raised. If not provided, the default\nvalue set during initialization or with\n_set_rpc_timeout is used.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.rpc.PyRRef.remote",
        "api_url": "https://pytorch.org/docs/stable/rpc.html#torch.distributed.rpc.PyRRef.remote",
        "api_signature": "remote(self: torch._C._distributed_rpc.PyRRef, timeout: float = -1.0)",
        "api_description": "Create a helper proxy to easily launch a remote using\nthe owner of the RRef as the destination to run functions on\nthe object referenced by this RRef. More specifically,\nrref.remote().func_name(*args, **kwargs) is the same as\nthe following:",
        "return_value": "",
        "parameters": "timeout (float, optional) – Timeout for rref.remote(). If\nthe creation of this RRef\nis not successfully completed within the timeout, then the\nnext time there is an attempt to use the RRef\n(such as to_here), a timeout will be raised. If not\nprovided, the default RPC timeout will be used. Please see\nrpc.remote() for specific timeout semantics for\nRRef.",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> from torch.distributed import rpc\n>>> rref = rpc.remote(\"worker1\", torch.add, args=(torch.zeros(2, 2), 1))\n>>> rref.remote().size().to_here()  # returns torch.Size([2, 2])\n>>> rref.remote().view(1, 4).to_here()  # returns tensor([[1., 1., 1., 1.]])\n\n\n"
    },
    {
        "api_name": "torch.distributed.nn.api.remote_module.RemoteModule.remote_parameters",
        "api_url": "https://pytorch.org/docs/stable/rpc.html#torch.distributed.nn.api.remote_module.RemoteModule.remote_parameters",
        "api_signature": "remote_parameters(recurse=True)",
        "api_description": "Return a list of RRef pointing to the remote module’s parameters.",
        "return_value": "A list of RRef (List[RRef[nn.Parameter]])\nto remote module’s parameters.\n",
        "parameters": "recurse (bool) – if True, then returns parameters of the remote\nmodule and all submodules of the remote module. Otherwise,\nreturns only parameters that are direct members of the\nremote module.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.nn.api.remote_module.RemoteModule",
        "api_url": "https://pytorch.org/docs/stable/rpc.html#torch.distributed.nn.api.remote_module.RemoteModule",
        "api_signature": "torch.distributed.nn.api.remote_module.RemoteModule(*args, **kwargs)",
        "api_description": "A RemoteModule instance can only be created after RPC initialization.",
        "return_value": "A remote module instance which wraps the Module created by the\nuser-provided module_cls, it has a blocking forward method and an\nasynchronous forward_async method that returns a future of the forward call\non the user-provided module on the remote side.\nA list of RRef (List[RRef[nn.Parameter]])\nto remote module’s parameters.\n",
        "parameters": "remote_device (str) – Device on the destination worker where we’d like to place this module.\nThe format should be “<workername>/<device>”, where the device field can be parsed as torch.device type.\nE.g., “trainer0/cpu”, “trainer0”, “ps0/cuda:0”.\nIn addition, the device field can be optional and the default value is “cpu”.\nmodule_cls (nn.Module) – Class for the module to be created remotely. For example,\n>>> class MyModule(nn.Module):\n>>>     def forward(input):\n>>>         return input + 1\n>>>\n>>> module_cls = MyModule\nargs (Sequence, optional) – args to be passed to module_cls.\nkwargs (Dict, optional) – kwargs to be passed to module_cls.\nrecurse (bool) – if True, then returns parameters of the remote\nmodule and all submodules of the remote module. Otherwise,\nreturns only parameters that are direct members of the\nremote module.",
        "input_shape": "",
        "notes": "",
        "code_example": "Run the following code in two different processes:\n>>> # On worker 0:\n>>> import torch\n>>> import torch.distributed.rpc as rpc\n>>> from torch import nn, Tensor\n>>> from torch.distributed.nn.api.remote_module import RemoteModule\n>>>\n>>> rpc.init_rpc(\"worker0\", rank=0, world_size=2)\n>>> remote_linear_module = RemoteModule(\n>>>     \"worker1/cpu\", nn.Linear, args=(20, 30),\n>>> )\n>>> input = torch.randn(128, 20)\n>>> ret_fut = remote_linear_module.forward_async(input)\n>>> ret = ret_fut.wait()\n>>> rpc.shutdown()\n\n\n>>> # On worker 1:\n>>> import torch\n>>> import torch.distributed.rpc as rpc\n>>>\n>>> rpc.init_rpc(\"worker1\", rank=1, world_size=2)\n>>> rpc.shutdown()\n\n\nFurthermore, a more practical example that is combined with\nDistributedDataParallel (DDP)\ncan be found in this tutorial.\n"
    },
    {
        "api_name": "torch.nn.utils.prune.remove",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.remove.html#torch.nn.utils.prune.remove",
        "api_signature": "torch.nn.utils.prune.remove(module, name)",
        "api_description": "Remove the pruning reparameterization from a module and the pruning method from the forward hook.",
        "return_value": "",
        "parameters": "module (nn.Module) – module containing the tensor to prune\nname (str) – parameter name within module on which pruning\nwill act.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.utils.prune.BasePruningMethod.remove",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.BasePruningMethod.html#torch.nn.utils.prune.BasePruningMethod.remove",
        "api_signature": "remove(module)",
        "api_description": "Remove the pruning reparameterization from a module.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.utils.prune.CustomFromMask.remove",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.CustomFromMask.html#torch.nn.utils.prune.CustomFromMask.remove",
        "api_signature": "remove(module)",
        "api_description": "Remove the pruning reparameterization from a module.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.utils.prune.Identity.remove",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.Identity.html#torch.nn.utils.prune.Identity.remove",
        "api_signature": "remove(module)",
        "api_description": "Remove the pruning reparameterization from a module.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.utils.prune.L1Unstructured.remove",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.L1Unstructured.html#torch.nn.utils.prune.L1Unstructured.remove",
        "api_signature": "remove(module)",
        "api_description": "Remove the pruning reparameterization from a module.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.utils.prune.LnStructured.remove",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.LnStructured.html#torch.nn.utils.prune.LnStructured.remove",
        "api_signature": "remove(module)",
        "api_description": "Remove the pruning reparameterization from a module.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.utils.prune.PruningContainer.remove",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.PruningContainer.html#torch.nn.utils.prune.PruningContainer.remove",
        "api_signature": "remove(module)",
        "api_description": "Remove the pruning reparameterization from a module.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.utils.prune.RandomStructured.remove",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.RandomStructured.html#torch.nn.utils.prune.RandomStructured.remove",
        "api_signature": "remove(module)",
        "api_description": "Remove the pruning reparameterization from a module.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.utils.prune.RandomUnstructured.remove",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.RandomUnstructured.html#torch.nn.utils.prune.RandomUnstructured.remove",
        "api_signature": "remove(module)",
        "api_description": "Remove the pruning reparameterization from a module.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.utils.parametrize.remove_parametrizations",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.utils.parametrize.remove_parametrizations.html#torch.nn.utils.parametrize.remove_parametrizations",
        "api_signature": "torch.nn.utils.parametrize.remove_parametrizations(module, tensor_name, leave_parametrized=True)",
        "api_description": "Remove the parametrizations on a tensor in a module.",
        "return_value": "module\n",
        "parameters": "module (nn.Module) – module from which remove the parametrization\ntensor_name (str) – name of the parametrization to be removed\nleave_parametrized (bool, optional) – leave the attribute tensor_name parametrized.\nDefault: True",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.symbolic_shapes.DimConstraints.remove_redundant_dynamic_results",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.fx.experimental.symbolic_shapes.DimConstraints.html#torch.fx.experimental.symbolic_shapes.DimConstraints.remove_redundant_dynamic_results",
        "api_signature": "remove_redundant_dynamic_results()",
        "api_description": "Remove constraints of the form 2 <= dynamic_dim(…) as 2 is the default\nlower bound.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.utils.remove_spectral_norm",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.utils.remove_spectral_norm.html#torch.nn.utils.remove_spectral_norm",
        "api_signature": "torch.nn.utils.remove_spectral_norm(module, name='weight')",
        "api_description": "Remove the spectral normalization reparameterization from a module.",
        "return_value": "",
        "parameters": "module (Module) – containing module\nname (str, optional) – name of weight parameter",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.utils.remove_weight_norm",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.utils.remove_weight_norm.html#torch.nn.utils.remove_weight_norm",
        "api_signature": "torch.nn.utils.remove_weight_norm(module, name='weight')",
        "api_description": "Remove the weight normalization reparameterization from a module.",
        "return_value": "",
        "parameters": "module (Module) – containing module\nname (str, optional) – name of weight parameter",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.rename",
        "api_url": "https://pytorch.org/docs/stable/named_tensor.html#torch.Tensor.rename",
        "api_signature": "rename(*names, **rename_map)",
        "api_description": "Renames dimension names of self.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.rename_",
        "api_url": "https://pytorch.org/docs/stable/named_tensor.html#torch.Tensor.rename_",
        "api_signature": "rename_(*names, **rename_map)",
        "api_description": "In-place version of rename().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.rename_privateuse1_backend",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.utils.rename_privateuse1_backend.html#torch.utils.rename_privateuse1_backend",
        "api_signature": "torch.utils.rename_privateuse1_backend(backend_name)",
        "api_description": "Rename the privateuse1 backend device to make it more convenient to use as a device name within PyTorch APIs.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.symbolic_shapes.StrictMinMaxConstraint.render",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.fx.experimental.symbolic_shapes.StrictMinMaxConstraint.html#torch.fx.experimental.symbolic_shapes.StrictMinMaxConstraint.render",
        "api_signature": "render(source)",
        "api_description": "Format the constrain equation",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.rendezvous.dynamic_rendezvous.RendezvousBackend",
        "api_url": "https://pytorch.org/docs/stable/elastic/rendezvous.html#torch.distributed.elastic.rendezvous.dynamic_rendezvous.RendezvousBackend",
        "api_signature": null,
        "api_description": "Represent a backend that holds the rendezvous state.",
        "return_value": "A tuple of the encoded rendezvous state and its fencing token or\nNone if no state is found in the backend.\nA tuple of the serialized rendezvous state, its fencing token, and\na boolean value indicating whether our set attempt succeeded.\n",
        "parameters": "state (bytes) – The encoded rendezvous state.\ntoken (Optional[Any]) – An optional fencing token that was retrieved by a previous call\nto get_state() or set_state().",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.rendezvous.RendezvousClosedError",
        "api_url": "https://pytorch.org/docs/stable/elastic/rendezvous.html#torch.distributed.elastic.rendezvous.RendezvousClosedError",
        "api_signature": null,
        "api_description": "Raised when a rendezvous is closed.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.rendezvous.RendezvousConnectionError",
        "api_url": "https://pytorch.org/docs/stable/elastic/rendezvous.html#torch.distributed.elastic.rendezvous.RendezvousConnectionError",
        "api_signature": null,
        "api_description": "Raised when the connection to a rendezvous backend has failed.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.rendezvous.RendezvousError",
        "api_url": "https://pytorch.org/docs/stable/elastic/rendezvous.html#torch.distributed.elastic.rendezvous.RendezvousError",
        "api_signature": null,
        "api_description": "Represents the base type for rendezvous errors.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.rendezvous.RendezvousGracefulExitError",
        "api_url": "https://pytorch.org/docs/stable/elastic/rendezvous.html#torch.distributed.elastic.rendezvous.RendezvousGracefulExitError",
        "api_signature": null,
        "api_description": "Raised when node wasn’t not included in rendezvous and gracefully exits.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.rendezvous.RendezvousHandler",
        "api_url": "https://pytorch.org/docs/stable/elastic/rendezvous.html#torch.distributed.elastic.rendezvous.RendezvousHandler",
        "api_signature": null,
        "api_description": "Main rendezvous interface.",
        "return_value": "A tuple of torch.distributed.Store, rank, and\nworld size.\n",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.rendezvous.RendezvousHandlerRegistry",
        "api_url": "https://pytorch.org/docs/stable/elastic/rendezvous.html#torch.distributed.elastic.rendezvous.RendezvousHandlerRegistry",
        "api_signature": null,
        "api_description": "Represent a registry of RendezvousHandler backends.",
        "return_value": "",
        "parameters": "backend (str) – The name of the backend.\ncreator (Callable[[RendezvousParameters], RendezvousHandler]) – The callback to invoke to construct the\nRendezvousHandler.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.rendezvous.RendezvousParameters",
        "api_url": "https://pytorch.org/docs/stable/elastic/rendezvous.html#torch.distributed.elastic.rendezvous.RendezvousParameters",
        "api_signature": "torch.distributed.elastic.rendezvous.RendezvousParameters(backend, endpoint, run_id, min_nodes, max_nodes, local_addr=None, **kwargs)",
        "api_description": "Hold the parameters to construct a RendezvousHandler.",
        "return_value": "",
        "parameters": "Hold the parameters to construct a RendezvousHandler.\nParameters\nbackend (str) – The name of the backend to use to handle the rendezvous.\nendpoint (str) – The endpoint of the rendezvous, usually in form <hostname>[:<port>].\nrun_id (str) – The id of the rendezvous.\nmin_nodes (int) – The minimum number of nodes to admit to the rendezvous.\nmax_nodes (int) – The maximum number of nodes to admit to the rendezvous.\nlocal_addr (Optional[str]) – The address of the local node.\n**kwargs – Additional parameters for the specified backend.\nget(key, default=None)[source]¶\nReturn the value for key if key exists, else default.\nReturn type\nAny\nget_as_bool(key, default=None)[source]¶\nReturn the value for key as a bool.\nReturn type\nOptional[bool]\nget_as_int(key, default=None)[source]¶\nReturn the value for key as an int.\nReturn type\nOptional[int]\nbackend (str) – The name of the backend to use to handle the rendezvous.\nendpoint (str) – The endpoint of the rendezvous, usually in form <hostname>[:<port>].\nrun_id (str) – The id of the rendezvous.\nmin_nodes (int) – The minimum number of nodes to admit to the rendezvous.\nmax_nodes (int) – The maximum number of nodes to admit to the rendezvous.\nlocal_addr (Optional[str]) – The address of the local node.\n**kwargs – Additional parameters for the specified backend.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.rendezvous.RendezvousStateError",
        "api_url": "https://pytorch.org/docs/stable/elastic/rendezvous.html#torch.distributed.elastic.rendezvous.RendezvousStateError",
        "api_signature": null,
        "api_description": "Raised when the state of a rendezvous is corrupt.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.rendezvous.dynamic_rendezvous.RendezvousTimeout",
        "api_url": "https://pytorch.org/docs/stable/elastic/rendezvous.html#torch.distributed.elastic.rendezvous.dynamic_rendezvous.RendezvousTimeout",
        "api_signature": "torch.distributed.elastic.rendezvous.dynamic_rendezvous.RendezvousTimeout(join=None, last_call=None, close=None, heartbeat=None)",
        "api_description": "Hold the timeout configuration of a rendezvous.",
        "return_value": "",
        "parameters": "join (Optional[timedelta]) – The time within which the rendezvous is expected to complete.\nlast_call (Optional[timedelta]) – An additional wait amount before completing the rendezvous once the\nrendezvous has the minimum number of required participants.\nclose (Optional[timedelta]) – The time within which the rendezvous is expected to close after a\ncall to RendezvousHandler.set_closed() or\nRendezvousHandler.shutdown().\nkeep_alive – The time within which a keep-alive heartbeat is expected to\ncomplete.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.rendezvous.RendezvousTimeoutError",
        "api_url": "https://pytorch.org/docs/stable/elastic/rendezvous.html#torch.distributed.elastic.rendezvous.RendezvousTimeoutError",
        "api_signature": null,
        "api_description": "Raised when a rendezvous did not complete on time.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.renorm",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.renorm.html#torch.renorm",
        "api_signature": "torch.renorm(input, p, dim, maxnorm, *, out=None)",
        "api_description": "Returns a tensor where each sub-tensor of input along dimension\ndim is normalized such that the p-norm of the sub-tensor is lower\nthan the value maxnorm",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.\np (float) – the power for the norm computation\ndim (int) – the dimension to slice over to get the sub-tensors\nmaxnorm (float) – the maximum norm to keep each sub-tensor under\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.renorm",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.renorm.html#torch.Tensor.renorm",
        "api_signature": "Tensor.renorm(p, dim, maxnorm)",
        "api_description": "See torch.renorm()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.renorm_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.renorm_.html#torch.Tensor.renorm_",
        "api_signature": "Tensor.renorm_(p, dim, maxnorm)",
        "api_description": "In-place version of renorm()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.repeat",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.repeat.html#torch.Tensor.repeat",
        "api_signature": "Tensor.repeat(*sizes)",
        "api_description": "Repeats this tensor along the specified dimensions.",
        "return_value": "",
        "parameters": "sizes (torch.Size or int...) – The number of times to repeat this tensor along each\ndimension",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.repeat_interleave",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.repeat_interleave.html#torch.repeat_interleave",
        "api_signature": "torch.repeat_interleave(input, repeats, dim=None, *, output_size=None)",
        "api_description": "Repeat elements of a tensor.",
        "return_value": "Repeated tensor which has the same shape as input, except along the given axis.\nRepeated tensor of size sum(repeats).\n",
        "parameters": "input (Tensor) – the input tensor.\nrepeats (Tensor or int) – The number of repetitions for each element.\nrepeats is broadcasted to fit the shape of the given axis.\ndim (int, optional) – The dimension along which to repeat values.\nBy default, use the flattened input array, and return a flat output\narray.\noutput_size (int, optional) – Total output size for the given axis\n( e.g. sum of repeats). If given, it will avoid stream synchronization\nneeded to calculate output shape of the tensor.\nrepeats (Tensor) – The number of repetitions for each element.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.repeat_interleave",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.repeat_interleave.html#torch.Tensor.repeat_interleave",
        "api_signature": "Tensor.repeat_interleave(repeats, dim=None, *, output_size=None)",
        "api_description": "See torch.repeat_interleave().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.symbolic_shapes.ShapeEnv.replace",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.fx.experimental.symbolic_shapes.ShapeEnv.html#torch.fx.experimental.symbolic_shapes.ShapeEnv.replace",
        "api_signature": "replace(expr)",
        "api_description": "Apply symbol replacements to any symbols in the given expression",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.func.replace_all_batch_norm_modules_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.func.replace_all_batch_norm_modules_.html#torch.func.replace_all_batch_norm_modules_",
        "api_signature": "torch.func.replace_all_batch_norm_modules_(root)",
        "api_description": "In place updates root by setting the running_mean and running_var to be None and\nsetting track_running_stats to be False for any nn.BatchNorm module in root",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.export.graph_signature.ExportGraphSignature.replace_all_uses",
        "api_url": "https://pytorch.org/docs/stable/export.html#torch.export.graph_signature.ExportGraphSignature.replace_all_uses",
        "api_signature": "replace_all_uses(old, new)",
        "api_description": "Replace all uses of the old name with new name in the signature.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.Node.replace_all_uses_with",
        "api_url": "https://pytorch.org/docs/stable/fx.html#torch.fx.Node.replace_all_uses_with",
        "api_signature": "replace_all_uses_with(replace_with, delete_user_cb=<function Node.<lambda>>, *, propagate_meta=False)",
        "api_description": "Replace all uses of self in the Graph with the Node replace_with.",
        "return_value": "The list of Nodes on which this change was made.\n",
        "parameters": "replace_with (Node) – The node to replace all uses of self with.\ndelete_user_cb (Callable) – Callback that is called to determine\nwhether a given user of the self node should be removed.\npropagate_meta (bool) – Whether or not to copy all properties\non the .meta field of the original node onto the replacement node.\nFor safety, this is only valid to do if the replacement node\ndoesn’t already have an existing .meta field.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.Node.replace_input_with",
        "api_url": "https://pytorch.org/docs/stable/fx.html#torch.fx.Node.replace_input_with",
        "api_signature": "replace_input_with(old_input, new_input)",
        "api_description": "Loop through input nodes of self, and replace all instances of\nold_input with new_input.",
        "return_value": "",
        "parameters": "old_input (Node) – The old input node to be replaced.\nnew_input (Node) – The new input node to replace old_input.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.replace_pattern",
        "api_url": "https://pytorch.org/docs/stable/fx.html#torch.fx.replace_pattern",
        "api_signature": "torch.fx.replace_pattern(gm, pattern, replacement)",
        "api_description": "Matches all possible non-overlapping sets of operators and their\ndata dependencies (pattern) in the Graph of a GraphModule\n(gm), then replaces each of these matched subgraphs with another\nsubgraph (replacement).",
        "return_value": "A list of Match objects representing the places\nin the original graph that pattern was matched to. The list\nis empty if there are no matches. Match is defined as:\nclass Match(NamedTuple):\n    # Node from which the match was found\n    anchor: Node\n    # Maps nodes in the pattern subgraph to nodes in the larger graph\n    nodes_map: Dict[Node, Node]\n\n\n\n",
        "parameters": "gm (GraphModule) – The GraphModule that wraps the Graph to operate on\npattern (Union[Callable, GraphModule]) – The subgraph to match in gm for replacement\nreplacement (Union[Callable, GraphModule]) – The subgraph to replace pattern with",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda.CUDAGraph.replay",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.cuda.CUDAGraph.html#torch.cuda.CUDAGraph.replay",
        "api_signature": "replay()",
        "api_description": "Replay the CUDA work captured by this graph.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.ReplicationPad1d",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.ReplicationPad1d.html#torch.nn.ReplicationPad1d",
        "api_signature": "torch.nn.ReplicationPad1d(padding)",
        "api_description": "Pads the input tensor using replication of the input boundary.",
        "return_value": "",
        "parameters": "padding (int, tuple) – the size of the padding. If is int, uses the same\npadding in all boundaries. If a 2-tuple, uses\n(padding_left\\text{padding\\_left}padding_left, padding_right\\text{padding\\_right}padding_right)",
        "input_shape": "\nInput: (C,Win)(C, W_{in})(C,Win​) or (N,C,Win)(N, C, W_{in})(N,C,Win​).\nOutput: (C,Wout)(C, W_{out})(C,Wout​) or (N,C,Wout)(N, C, W_{out})(N,C,Wout​), where\nWout=Win+padding_left+padding_rightW_{out} = W_{in} + \\text{padding\\_left} + \\text{padding\\_right}Wout​=Win​+padding_left+padding_right\n\n\n",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.ReplicationPad2d",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.ReplicationPad2d.html#torch.nn.ReplicationPad2d",
        "api_signature": "torch.nn.ReplicationPad2d(padding)",
        "api_description": "Pads the input tensor using replication of the input boundary.",
        "return_value": "",
        "parameters": "padding (int, tuple) – the size of the padding. If is int, uses the same\npadding in all boundaries. If a 4-tuple, uses (padding_left\\text{padding\\_left}padding_left,\npadding_right\\text{padding\\_right}padding_right, padding_top\\text{padding\\_top}padding_top, padding_bottom\\text{padding\\_bottom}padding_bottom)",
        "input_shape": "\nInput: (N,C,Hin,Win)(N, C, H_{in}, W_{in})(N,C,Hin​,Win​) or (C,Hin,Win)(C, H_{in}, W_{in})(C,Hin​,Win​).\nOutput: (N,C,Hout,Wout)(N, C, H_{out}, W_{out})(N,C,Hout​,Wout​) or (C,Hout,Wout)(C, H_{out}, W_{out})(C,Hout​,Wout​), where\nHout=Hin+padding_top+padding_bottomH_{out} = H_{in} + \\text{padding\\_top} + \\text{padding\\_bottom}Hout​=Hin​+padding_top+padding_bottom\nWout=Win+padding_left+padding_rightW_{out} = W_{in} + \\text{padding\\_left} + \\text{padding\\_right}Wout​=Win​+padding_left+padding_right\n\n\n",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.ReplicationPad3d",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.ReplicationPad3d.html#torch.nn.ReplicationPad3d",
        "api_signature": "torch.nn.ReplicationPad3d(padding)",
        "api_description": "Pads the input tensor using replication of the input boundary.",
        "return_value": "",
        "parameters": "padding (int, tuple) – the size of the padding. If is int, uses the same\npadding in all boundaries. If a 6-tuple, uses\n(padding_left\\text{padding\\_left}padding_left, padding_right\\text{padding\\_right}padding_right,\npadding_top\\text{padding\\_top}padding_top, padding_bottom\\text{padding\\_bottom}padding_bottom,\npadding_front\\text{padding\\_front}padding_front, padding_back\\text{padding\\_back}padding_back)",
        "input_shape": "\nInput: (N,C,Din,Hin,Win)(N, C, D_{in}, H_{in}, W_{in})(N,C,Din​,Hin​,Win​) or (C,Din,Hin,Win)(C, D_{in}, H_{in}, W_{in})(C,Din​,Hin​,Win​).\nOutput: (N,C,Dout,Hout,Wout)(N, C, D_{out}, H_{out}, W_{out})(N,C,Dout​,Hout​,Wout​) or (C,Dout,Hout,Wout)(C, D_{out}, H_{out}, W_{out})(C,Dout​,Hout​,Wout​),\nwhere\nDout=Din+padding_front+padding_backD_{out} = D_{in} + \\text{padding\\_front} + \\text{padding\\_back}Dout​=Din​+padding_front+padding_back\nHout=Hin+padding_top+padding_bottomH_{out} = H_{in} + \\text{padding\\_top} + \\text{padding\\_bottom}Hout​=Hin​+padding_top+padding_bottom\nWout=Win+padding_left+padding_rightW_{out} = W_{in} + \\text{padding\\_left} + \\text{padding\\_right}Wout​=Win​+padding_left+padding_right\n\n\n",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.requires_grad",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.requires_grad.html#torch.Tensor.requires_grad",
        "api_signature": null,
        "api_description": "Is True if gradients need to be computed for this Tensor, False otherwise.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.jit.ScriptModule.requires_grad_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.jit.ScriptModule.html#torch.jit.ScriptModule.requires_grad_",
        "api_signature": "requires_grad_(requires_grad=True)",
        "api_description": "Change if autograd should record operations on parameters in this module.",
        "return_value": "self\n",
        "parameters": "requires_grad (bool) – whether autograd should record operations on\nparameters in this module. Default: True.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.Module.requires_grad_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.requires_grad_",
        "api_signature": "requires_grad_(requires_grad=True)",
        "api_description": "Change if autograd should record operations on parameters in this module.",
        "return_value": "self\n",
        "parameters": "requires_grad (bool) – whether autograd should record operations on\nparameters in this module. Default: True.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.requires_grad_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.requires_grad_.html#torch.Tensor.requires_grad_",
        "api_signature": "Tensor.requires_grad_(requires_grad=True)",
        "api_description": "Change if autograd should record operations on this tensor: sets this tensor’s\nrequires_grad attribute in-place. Returns this tensor.",
        "return_value": "",
        "parameters": "requires_grad (bool) – If autograd should record operations on this tensor.\nDefault: True.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.compiler.reset",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.compiler.reset.html#torch.compiler.reset",
        "api_signature": "torch.compiler.reset()",
        "api_description": "This function clears all compilation caches and restores the system to its initial state.\nIt is recommended to call this function, especially after using operations like torch.compile(…)\nto ensure a clean state before another unrelated compilation",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda.CUDAGraph.reset",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.cuda.CUDAGraph.html#torch.cuda.CUDAGraph.reset",
        "api_signature": "reset()",
        "api_description": "Delete the graph currently held by this instance.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.checkpoint.format_utils.BroadcastingTorchSaveReader.reset",
        "api_url": "https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.format_utils.BroadcastingTorchSaveReader.reset",
        "api_signature": "reset(checkpoint_id=None)",
        "api_description": "Implementation of the StorageReader method",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.checkpoint.StorageReader.reset",
        "api_url": "https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.StorageReader.reset",
        "api_signature": "reset(checkpoint_id=None)",
        "api_description": "Calls to indicates a brand new checkpoint read is going to happen.\nA checkpoint_id may be present if users set the checkpoint_id for\nthis checkpoint read. The meaning of the checkpiont_id is\nstorage-dependent. It can be a path to a folder/file or a key for\na key-value storage.",
        "return_value": "",
        "parameters": "checkpoint_id (Union[str, os.PathLike, None]) – The ID of this checkpoint instance. The meaning of the checkpoint_id\ndepends on the storage. It can be a path to a folder or to a file.\nIt can also be a key if the storage is more like a key-value store.\n(Default: None)",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.checkpoint.StorageWriter.reset",
        "api_url": "https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.StorageWriter.reset",
        "api_signature": "reset(checkpoint_id=None)",
        "api_description": "Calls to indicates a brand new checkpoint write is going to happen.\nA checkpoint_id may be present if users set the checkpoint_id for\nthis checkpoint write. The meaning of the checkpiont_id is\nstorage-dependent. It can be a path to a folder/file or a key for\na key-value storage.",
        "return_value": "",
        "parameters": "checkpoint_id (Union[str, os.PathLike, None]) – The ID of this checkpoint instance. The meaning of the checkpoint_id\ndepends on the storage. It can be a path to a folder or to a file.\nIt can also be a key if the storage is a key-value store.\n(Default: None)",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.quasirandom.SobolEngine.reset",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.quasirandom.SobolEngine.html#torch.quasirandom.SobolEngine.reset",
        "api_signature": "reset()",
        "api_description": "Function to reset the SobolEngine to base state.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda.reset_max_memory_allocated",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.cuda.reset_max_memory_allocated.html#torch.cuda.reset_max_memory_allocated",
        "api_signature": "torch.cuda.reset_max_memory_allocated(device=None)",
        "api_description": "Reset the starting point in tracking maximum GPU memory occupied by tensors for a given device.",
        "return_value": "",
        "parameters": "device (torch.device or int, optional) – selected device. Returns\nstatistic for the current device, given by current_device(),\nif device is None (default).",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda.reset_max_memory_cached",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.cuda.reset_max_memory_cached.html#torch.cuda.reset_max_memory_cached",
        "api_signature": "torch.cuda.reset_max_memory_cached(device=None)",
        "api_description": "Reset the starting point in tracking maximum GPU memory managed by the caching allocator for a given device.",
        "return_value": "",
        "parameters": "device (torch.device or int, optional) – selected device. Returns\nstatistic for the current device, given by current_device(),\nif device is None (default).",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.observer.MinMaxObserver.reset_min_max_vals",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.quantization.observer.MinMaxObserver.html#torch.ao.quantization.observer.MinMaxObserver.reset_min_max_vals",
        "api_signature": "reset_min_max_vals()",
        "api_description": "Resets the min/max values.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.observer.PerChannelMinMaxObserver.reset_min_max_vals",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.quantization.observer.PerChannelMinMaxObserver.html#torch.ao.quantization.observer.PerChannelMinMaxObserver.reset_min_max_vals",
        "api_signature": "reset_min_max_vals()",
        "api_description": "Resets the min/max values.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda.reset_peak_memory_stats",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.cuda.reset_peak_memory_stats.html#torch.cuda.reset_peak_memory_stats",
        "api_signature": "torch.cuda.reset_peak_memory_stats(device=None)",
        "api_description": "Reset the “peak” stats tracked by the CUDA memory allocator.",
        "return_value": "",
        "parameters": "device (torch.device or int, optional) – selected device. Returns\nstatistic for the current device, given by current_device(),\nif device is None (default).",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.reshape",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.reshape.html#torch.reshape",
        "api_signature": "torch.reshape(input, shape)",
        "api_description": "Returns a tensor with the same data and number of elements as input,\nbut with the specified shape. When possible, the returned tensor will be a view\nof input. Otherwise, it will be a copy. Contiguous inputs and inputs\nwith compatible strides can be reshaped without copying, but you should not\ndepend on the copying vs. viewing behavior.",
        "return_value": "",
        "parameters": "input (Tensor) – the tensor to be reshaped\nshape (tuple of int) – the new shape",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.reshape",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.reshape.html#torch.Tensor.reshape",
        "api_signature": "Tensor.reshape(*shape)",
        "api_description": "Returns a tensor with the same data and number of elements as self\nbut with the specified shape. This method returns a view if shape is\ncompatible with the current shape. See torch.Tensor.view() on when it is\npossible to return a view.",
        "return_value": "",
        "parameters": "shape (tuple of ints or int...) – the desired shape",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.reshape_as",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.reshape_as.html#torch.Tensor.reshape_as",
        "api_signature": "Tensor.reshape_as(other)",
        "api_description": "Returns this tensor as the same shape as other.\nself.reshape_as(other) is equivalent to self.reshape(other.sizes()).\nThis method returns a view if other.sizes() is compatible with the current\nshape. See torch.Tensor.view() on when it is possible to return a view.",
        "return_value": "",
        "parameters": "other (torch.Tensor) – The result tensor has the same shape\nas other.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.transforms.ReshapeTransform",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.transforms.ReshapeTransform",
        "api_signature": "torch.distributions.transforms.ReshapeTransform(in_shape, out_shape, cache_size=0)",
        "api_description": "Unit Jacobian transform to reshape the rightmost part of a tensor.",
        "return_value": "",
        "parameters": "in_shape (torch.Size) – The input event shape.\nout_shape (torch.Size) – The output event shape.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.TypedStorage.resizable",
        "api_url": "https://pytorch.org/docs/stable/storage.html#torch.TypedStorage.resizable",
        "api_signature": "resizable()",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.UntypedStorage.resizable",
        "api_url": "https://pytorch.org/docs/stable/storage.html#torch.UntypedStorage.resizable",
        "api_signature": "resizable()",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.resize_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.resize_.html#torch.Tensor.resize_",
        "api_signature": "Tensor.resize_(*sizes, memory_format=torch.contiguous_format)",
        "api_description": "Resizes self tensor to the specified size. If the number of elements is\nlarger than the current storage size, then the underlying storage is resized\nto fit the new number of elements. If the number of elements is smaller, the\nunderlying storage is not changed. Existing elements are preserved but any new\nmemory is uninitialized.",
        "return_value": "",
        "parameters": "sizes (torch.Size or int...) – the desired size\nmemory_format (torch.memory_format, optional) – the desired memory format of\nTensor. Default: torch.contiguous_format. Note that memory format of\nself is going to be unaffected if self.size() matches sizes.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.TypedStorage.resize_",
        "api_url": "https://pytorch.org/docs/stable/storage.html#torch.TypedStorage.resize_",
        "api_signature": "resize_(size)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.UntypedStorage.resize_",
        "api_url": "https://pytorch.org/docs/stable/storage.html#torch.UntypedStorage.resize_",
        "api_signature": "resize_()",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.resize_as_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.resize_as_.html#torch.Tensor.resize_as_",
        "api_signature": "Tensor.resize_as_(tensor, memory_format=torch.contiguous_format)",
        "api_description": "Resizes the self tensor to be the same size as the specified\ntensor. This is equivalent to self.resize_(tensor.size()).",
        "return_value": "",
        "parameters": "memory_format (torch.memory_format, optional) – the desired memory format of\nTensor. Default: torch.contiguous_format. Note that memory format of\nself is going to be unaffected if self.size() matches tensor.size().",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.resolve_conj",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.resolve_conj.html#torch.resolve_conj",
        "api_signature": "torch.resolve_conj(input)",
        "api_description": "Returns a new tensor with materialized conjugation if input’s conjugate bit is set to True,\nelse returns input. The output tensor will always have its conjugate bit set to False.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.resolve_conj",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.resolve_conj.html#torch.Tensor.resolve_conj",
        "api_signature": "Tensor.resolve_conj()",
        "api_description": "See torch.resolve_conj()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.checkpoint.SavePlanner.resolve_data",
        "api_url": "https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.SavePlanner.resolve_data",
        "api_signature": "resolve_data(write_item)",
        "api_description": "Transform and prepare write_item from state_dict for storage, ensuring idempotency and thread-safety.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.overrides.resolve_name",
        "api_url": "https://pytorch.org/docs/stable/torch.overrides.html#torch.overrides.resolve_name",
        "api_signature": "torch.overrides.resolve_name(f)",
        "api_description": "Get a human readable string name for a function passed to\n__torch_function__",
        "return_value": "Name of the function; if eval’ed it should give back the input\nfunction.\n",
        "parameters": "f (Callable) – Function to resolve the name of.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.resolve_neg",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.resolve_neg.html#torch.resolve_neg",
        "api_signature": "torch.resolve_neg(input)",
        "api_description": "Returns a new tensor with materialized negation if input’s negative bit is set to True,\nelse returns input. The output tensor will always have its negative bit set to False.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.resolve_neg",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.resolve_neg.html#torch.Tensor.resolve_neg",
        "api_signature": "Tensor.resolve_neg()",
        "api_description": "See torch.resolve_neg()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.checkpoint.LoadPlanner.resolve_tensor",
        "api_url": "https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.LoadPlanner.resolve_tensor",
        "api_signature": "resolve_tensor(read_item)",
        "api_description": "Return the tensor described by read_item to be used by the StorageReader to load read_item.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.result_type",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.result_type.html#torch.result_type",
        "api_signature": "torch.result_type(tensor1, tensor2)",
        "api_description": "Returns the torch.dtype that would result from performing an arithmetic\noperation on the provided input tensors. See type promotion documentation\nfor more information on the type promotion logic.",
        "return_value": "",
        "parameters": "tensor1 (Tensor or Number) – an input tensor or number\ntensor2 (Tensor or Number) – an input tensor or number",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.retain_grad",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.retain_grad.html#torch.Tensor.retain_grad",
        "api_signature": "Tensor.retain_grad()",
        "api_description": "Enables this Tensor to have their grad populated during\nbackward(). This is a no-op for leaf tensors.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.retains_grad",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.retains_grad.html#torch.Tensor.retains_grad",
        "api_signature": null,
        "api_description": "Is True if this Tensor is non-leaf and its grad is enabled to be\npopulated during backward(), False otherwise.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.symbolic_shapes.DimConstraints.rewrite_with_congruences",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.fx.experimental.symbolic_shapes.DimConstraints.html#torch.fx.experimental.symbolic_shapes.DimConstraints.rewrite_with_congruences",
        "api_signature": "rewrite_with_congruences(s, expr)",
        "api_description": "Eliminate expressions of the form b // d and b % d while adding congruences of the form b % d == k.\nThis leaves rational operators (in particular of the form b / d) that our inequality solver can handle.\nWe solve the added congruences separately (using our congruence solver, see below).",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fft.rfft",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.fft.rfft.html#torch.fft.rfft",
        "api_signature": "torch.fft.rfft(input, n=None, dim=-1, norm=None, *, out=None)",
        "api_description": "Computes the one dimensional Fourier transform of real-valued input.",
        "return_value": "",
        "parameters": "input (Tensor) – the real input tensor\nn (int, optional) – Signal length. If given, the input will either be zero-padded\nor trimmed to this length before computing the real FFT.\ndim (int, optional) – The dimension along which to take the one dimensional real FFT.\nnorm (str, optional) – Normalization mode. For the forward transform\n(rfft()), these correspond to:\n\"forward\" - normalize by 1/n\n\"backward\" - no normalization\n\"ortho\" - normalize by 1/sqrt(n) (making the FFT orthonormal)\nCalling the backward transform (irfft()) with the same\nnormalization mode will apply an overall normalization of 1/n between\nthe two transforms. This is required to make irfft()\nthe exact inverse.\nDefault is \"backward\" (no normalization).\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fft.rfft2",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.fft.rfft2.html#torch.fft.rfft2",
        "api_signature": "torch.fft.rfft2(input, s=None, dim=(-2, -1)",
        "api_description": "Computes the 2-dimensional discrete Fourier transform of real input.\nEquivalent to rfftn() but FFTs only the last two dimensions by default.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor\ns (Tuple[int], optional) – Signal size in the transformed dimensions.\nIf given, each dimension dim[i] will either be zero-padded or\ntrimmed to the length s[i] before computing the real FFT.\nIf a length -1 is specified, no padding is done in that dimension.\nDefault: s = [input.size(d) for d in dim]\ndim (Tuple[int], optional) – Dimensions to be transformed.\nDefault: last two dimensions.\nnorm (str, optional) – Normalization mode. For the forward transform\n(rfft2()), these correspond to:\n\"forward\" - normalize by 1/n\n\"backward\" - no normalization\n\"ortho\" - normalize by 1/sqrt(n) (making the real FFT orthonormal)\nWhere n = prod(s) is the logical FFT size.\nCalling the backward transform (irfft2()) with the same\nnormalization mode will apply an overall normalization of 1/n between\nthe two transforms. This is required to make irfft2()\nthe exact inverse.\nDefault is \"backward\" (no normalization).\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fft.rfftfreq",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.fft.rfftfreq.html#torch.fft.rfftfreq",
        "api_signature": "torch.fft.rfftfreq(n, d=1.0, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False)",
        "api_description": "Computes the sample frequencies for rfft() with a signal of size n.",
        "return_value": "",
        "parameters": "n (int) – the real FFT length\nd (float, optional) – The sampling length scale.\nThe spacing between individual samples of the FFT input.\nThe default assumes unit spacing, dividing that result by the actual\nspacing gives the result in physical frequency units.\nout (Tensor, optional) – the output tensor.\ndtype (torch.dtype, optional) – the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_dtype()).\nlayout (torch.layout, optional) – the desired layout of returned Tensor.\nDefault: torch.strided.\ndevice (torch.device, optional) – the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_device()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types.\nrequires_grad (bool, optional) – If autograd should record operations on the\nreturned tensor. Default: False.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fft.rfftn",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.fft.rfftn.html#torch.fft.rfftn",
        "api_signature": "torch.fft.rfftn(input, s=None, dim=None, norm=None, *, out=None)",
        "api_description": "Computes the N-dimensional discrete Fourier transform of real input.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor\ns (Tuple[int], optional) – Signal size in the transformed dimensions.\nIf given, each dimension dim[i] will either be zero-padded or\ntrimmed to the length s[i] before computing the real FFT.\nIf a length -1 is specified, no padding is done in that dimension.\nDefault: s = [input.size(d) for d in dim]\ndim (Tuple[int], optional) – Dimensions to be transformed.\nDefault: all dimensions, or the last len(s) dimensions if s is given.\nnorm (str, optional) – Normalization mode. For the forward transform\n(rfftn()), these correspond to:\n\"forward\" - normalize by 1/n\n\"backward\" - no normalization\n\"ortho\" - normalize by 1/sqrt(n) (making the real FFT orthonormal)\nWhere n = prod(s) is the logical FFT size.\nCalling the backward transform (irfftn()) with the same\nnormalization mode will apply an overall normalization of 1/n between\nthe two transforms. This is required to make irfftn()\nthe exact inverse.\nDefault is \"backward\" (no normalization).\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.utils.parametrize.ParametrizationList.right_inverse",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.utils.parametrize.ParametrizationList.html#torch.nn.utils.parametrize.ParametrizationList.right_inverse",
        "api_signature": "right_inverse(value)",
        "api_description": "Call the right_inverse methods of the parametrizations in the inverse registration order.",
        "return_value": "",
        "parameters": "value (Tensor) – Value to which initialize the module",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.RMSprop",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.RMSprop.html#torch.optim.RMSprop",
        "api_signature": "torch.optim.RMSprop(params, lr=0.01, alpha=0.99, eps=1e-08, weight_decay=0, momentum=0, centered=False, foreach=None, maximize=False, differentiable=False)",
        "api_description": "Implements RMSprop algorithm.",
        "return_value": "a handle that can be used to remove the added hook by calling\nhandle.remove()\na handle that can be used to remove the added hook by calling\nhandle.remove()\na handle that can be used to remove the added hook by calling\nhandle.remove()\na handle that can be used to remove the added hook by calling\nhandle.remove()\na handle that can be used to remove the added hook by calling\nhandle.remove()\na handle that can be used to remove the added hook by calling\nhandle.remove()\n",
        "parameters": "params (iterable) – iterable of parameters to optimize or dicts defining\nparameter groups\nlr (float, optional) – learning rate (default: 1e-2)\nmomentum (float, optional) – momentum factor (default: 0)\nalpha (float, optional) – smoothing constant (default: 0.99)\neps (float, optional) – term added to the denominator to improve\nnumerical stability (default: 1e-8)\ncentered (bool, optional) – if True, compute the centered RMSProp,\nthe gradient is normalized by an estimation of its variance\nweight_decay (float, optional) – weight decay (L2 penalty) (default: 0)\nforeach (bool, optional) – whether foreach implementation of optimizer\nis used. If unspecified by the user (so foreach is None), we will try to use\nforeach over the for-loop implementation on CUDA, since it is usually\nsignificantly more performant. Note that the foreach implementation uses\n~ sizeof(params) more peak memory than the for-loop version due to the intermediates\nbeing a tensorlist vs just one tensor. If memory is prohibitive, batch fewer\nparameters through the optimizer at a time or switch this flag to False (default: None)\nmaximize (bool, optional) – maximize the objective with respect to the\nparams, instead of minimizing (default: False)\ndifferentiable (bool, optional) – whether autograd should\noccur through the optimizer step in training. Otherwise, the step()\nfunction runs in a torch.no_grad() context. Setting to True can impair\nperformance, so leave it False if you don’t intend to run autograd\nthrough this instance (default: False)\nparam_group (dict) – Specifies what Tensors should be optimized along with group\nspecific optimization options.\nstate_dict (dict) – optimizer state. Should be an object returned\nfrom a call to state_dict().\nhook (Callable) – The user defined hook to be registered.\nprepend (bool) – If True, the provided post hook will be fired before\nall the already registered post-hooks on load_state_dict. Otherwise,\nthe provided hook will be fired after all the already registered\npost-hooks. (default: False)\nhook (Callable) – The user defined hook to be registered.\nprepend (bool) – If True, the provided pre hook will be fired before\nall the already registered pre-hooks on load_state_dict. Otherwise,\nthe provided hook will be fired after all the already registered\npre-hooks. (default: False)\nhook (Callable) – The user defined hook to be registered.\nprepend (bool) – If True, the provided post hook will be fired before\nall the already registered post-hooks on state_dict. Otherwise,\nthe provided hook will be fired after all the already registered\npost-hooks. (default: False)\nhook (Callable) – The user defined hook to be registered.\nprepend (bool) – If True, the provided pre hook will be fired before\nall the already registered pre-hooks on state_dict. Otherwise,\nthe provided hook will be fired after all the already registered\npre-hooks. (default: False)\nhook (Callable) – The user defined hook to be registered.\nhook (Callable) – The user defined hook to be registered.\nclosure (Callable, optional) – A closure that reevaluates the model\nand returns the loss.\nset_to_none (bool) – instead of setting to zero, set the grads to None.\nThis will in general have lower memory footprint, and can modestly improve performance.\nHowever, it changes certain behaviors. For example:\n1. When the user tries to access a gradient and perform manual ops on it,\na None attribute or a Tensor full of 0s will behave differently.\n2. If the user requests zero_grad(set_to_none=True) followed by a backward pass, .grads\nare guaranteed to be None for params that did not receive a gradient.\n3. torch.optim optimizers have a different behavior if the gradient is 0 or None\n(in one case it does the step with a gradient of 0 and in the other it skips\nthe step altogether).",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.RNN",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.RNN.html#torch.nn.RNN",
        "api_signature": "torch.nn.RNN(input_size, hidden_size, num_layers=1, nonlinearity='tanh', bias=True, batch_first=False, dropout=0.0, bidirectional=False, device=None, dtype=None)",
        "api_description": "Apply a multi-layer Elman RNN with tanh⁡\\tanhtanh or ReLU\\text{ReLU}ReLU\nnon-linearity to an input sequence. For each element in the input sequence,\neach layer computes the following function:",
        "return_value": "",
        "parameters": "input_size – The number of expected features in the input x\nhidden_size – The number of features in the hidden state h\nnum_layers – Number of recurrent layers. E.g., setting num_layers=2\nwould mean stacking two RNNs together to form a stacked RNN,\nwith the second RNN taking in outputs of the first RNN and\ncomputing the final results. Default: 1\nnonlinearity – The non-linearity to use. Can be either 'tanh' or 'relu'. Default: 'tanh'\nbias – If False, then the layer does not use bias weights b_ih and b_hh.\nDefault: True\nbatch_first – If True, then the input and output tensors are provided\nas (batch, seq, feature) instead of (seq, batch, feature).\nNote that this does not apply to hidden or cell states. See the\nInputs/Outputs sections below for details.  Default: False\ndropout – If non-zero, introduces a Dropout layer on the outputs of each\nRNN layer except the last layer, with dropout probability equal to\ndropout. Default: 0\nbidirectional – If True, becomes a bidirectional RNN. Default: False",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.RNNBase",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.RNNBase.html#torch.nn.RNNBase",
        "api_signature": "torch.nn.RNNBase(mode, input_size, hidden_size, num_layers=1, bias=True, batch_first=False, dropout=0.0, bidirectional=False, proj_size=0, device=None, dtype=None)",
        "api_description": "Base class for RNN modules (RNN, LSTM, GRU).",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.quantized.dynamic.RNNCell",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.nn.quantized.dynamic.RNNCell.html#torch.ao.nn.quantized.dynamic.RNNCell",
        "api_signature": "torch.ao.nn.quantized.dynamic.RNNCell(input_size, hidden_size, bias=True, nonlinearity='tanh', dtype=torch.qint8)",
        "api_description": "An Elman RNN cell with tanh or ReLU non-linearity.\nA dynamic quantized RNNCell module with floating point tensor as inputs and outputs.\nWeights are quantized to 8 bits. We adopt the same interface as torch.nn.RNNCell,\nplease see https://pytorch.org/docs/stable/nn.html#torch.nn.RNNCell for documentation.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.RNNCell",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.RNNCell.html#torch.nn.RNNCell",
        "api_signature": "torch.nn.RNNCell(input_size, hidden_size, bias=True, nonlinearity='tanh', device=None, dtype=None)",
        "api_description": "An Elman RNN cell with tanh or ReLU non-linearity.",
        "return_value": "",
        "parameters": "input_size (int) – The number of expected features in the input x\nhidden_size (int) – The number of features in the hidden state h\nbias (bool) – If False, then the layer does not use bias weights b_ih and b_hh.\nDefault: True\nnonlinearity (str) – The non-linearity to use. Can be either 'tanh' or 'relu'. Default: 'tanh'",
        "input_shape": "\ninput: (N,Hin)(N, H_{in})(N,Hin​) or (Hin)(H_{in})(Hin​) tensor containing input features where\nHinH_{in}Hin​ = input_size.\nhidden: (N,Hout)(N, H_{out})(N,Hout​) or (Hout)(H_{out})(Hout​) tensor containing the initial hidden\nstate where HoutH_{out}Hout​ = hidden_size. Defaults to zero if not provided.\noutput: (N,Hout)(N, H_{out})(N,Hout​) or (Hout)(H_{out})(Hout​) tensor containing the next hidden state.\n\n",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.roll",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.roll.html#torch.roll",
        "api_signature": "torch.roll(input, shifts, dims=None)",
        "api_description": "Roll the tensor input along the given dimension(s). Elements that are\nshifted beyond the last position are re-introduced at the first position. If\ndims is None, the tensor will be flattened before rolling and then\nrestored to the original shape.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.\nshifts (int or tuple of ints) – The number of places by which the elements\nof the tensor are shifted. If shifts is a tuple, dims must be a tuple of\nthe same size, and each dimension will be rolled by the corresponding\nvalue\ndims (int or tuple of ints) – Axis along which to roll",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.roll",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.roll.html#torch.Tensor.roll",
        "api_signature": "Tensor.roll(shifts, dims)",
        "api_description": "See torch.roll()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.rot90",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.rot90.html#torch.rot90",
        "api_signature": "torch.rot90(input, k=1, dims=[0, 1])",
        "api_description": "Rotate an n-D tensor by 90 degrees in the plane specified by dims axis.\nRotation direction is from the first towards the second axis if k > 0, and from the second towards the first for k < 0.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.\nk (int) – number of times to rotate. Default value is 1\ndims (a list or tuple) – axis to rotate. Default value is [0, 1]",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.rot90",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.rot90.html#torch.Tensor.rot90",
        "api_signature": "Tensor.rot90(k, dims)",
        "api_description": "See torch.rot90()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.round",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.round.html#torch.round",
        "api_signature": "torch.round(input, *, decimals=0, out=None)",
        "api_description": "Rounds elements of input to the nearest integer.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.\ndecimals (int) – Number of decimal places to round to (default: 0).\nIf decimals is negative, it specifies the number of positions\nto the left of the decimal point.\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.special.round",
        "api_url": "https://pytorch.org/docs/stable/special.html#torch.special.round",
        "api_signature": "torch.special.round(input, *, out=None)",
        "api_description": "Alias for torch.round().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.round",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.round.html#torch.Tensor.round",
        "api_signature": "Tensor.round(decimals=0)",
        "api_description": "See torch.round()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.round_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.round_.html#torch.Tensor.round_",
        "api_signature": "Tensor.round_(decimals=0)",
        "api_description": "In-place version of round()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.row_indices",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.row_indices.html#torch.Tensor.row_indices",
        "api_signature": "Tensor.row_indices()",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.row_stack",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.row_stack.html#torch.row_stack",
        "api_signature": "torch.row_stack(tensors, *, out=None)",
        "api_description": "Alias of torch.vstack().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.tensor.parallel.RowwiseParallel",
        "api_url": "https://pytorch.org/docs/stable/distributed.tensor.parallel.html#torch.distributed.tensor.parallel.RowwiseParallel",
        "api_signature": "torch.distributed.tensor.parallel.RowwiseParallel(*, input_layouts=None, output_layouts=None, use_local_output=True)",
        "api_description": "Partition a compatible nn.Module in a row-wise fashion. Currently supports nn.Linear and nn.Embedding.\nUsers can compose it with ColwiseParallel to achieve the sharding of more complicated modules.\n(i.e. MLP, Attention)",
        "return_value": "A ParallelStyle object that represents Rowwise sharding of the nn.Module.\n",
        "parameters": "input_layouts (Placement, optional) – The DTensor layout of input tensor for the nn.Module, this is used to annotate the input tensor to\nbecome a DTensor. If not specified, we assume the input tensor to be sharded on the last dimension.\noutput_layouts (Placement, optional) – The DTensor layout of the output for the nn.Module, this is used to ensure the output of the nn.Module\nwith the user desired layout. If not specified, the output tensor is replicated.\nuse_local_output (bool, optional) – Whether to use local torch.Tensor instead of DTensor for the module output, default: True.",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> from torch.distributed.tensor.parallel import parallelize_module, RowwiseParallel\n>>> from torch.distributed.device_mesh import init_device_mesh\n>>> ...\n>>> m = Model(...)  # m is a nn.Module that contains a \"w2\" nn.Linear submodule\n>>> tp_mesh = init_device_mesh(\"cuda\", (8,))\n>>>\n>>> # By default, the input of the \"w2\" Linear will be converted to DTensor that shards on the last dim\n>>> # and the output of \"w2\" will return a replicated :class:`torch.Tensor`.\n>>>\n>>> sharded_mod = parallelize_module(m, tp_mesh, {\"w2\": RowwiseParallel()}),\n>>> ...\n\n\n"
    },
    {
        "api_name": "torch.distributed.rpc.rpc_async",
        "api_url": "https://pytorch.org/docs/stable/rpc.html#torch.distributed.rpc.rpc_async",
        "api_signature": "torch.distributed.rpc.rpc_async(to, func, args=None, kwargs=None, timeout=-1.0)",
        "api_description": "Make a non-blocking RPC call to run function func on worker to. RPC\nmessages are sent and received in parallel to execution of Python code. This\nmethod is thread-safe. This method will immediately return a\nFuture that can be awaited on.",
        "return_value": "Returns a Future object that can be waited\non. When completed, the return value of func on args and\nkwargs can be retrieved from the Future\nobject.\n",
        "parameters": "to (str or WorkerInfo or int) – name/rank/WorkerInfo of the destination worker.\nfunc (Callable) – a callable function, such as Python callables, builtin\noperators (e.g. add()) and annotated\nTorchScript functions.\nargs (tuple) – the argument tuple for the func invocation.\nkwargs (dict) – is a dictionary of keyword arguments for the func\ninvocation.\ntimeout (float, optional) – timeout in seconds to use for this RPC. If\nthe RPC does not complete in this amount of\ntime, an exception indicating it has\ntimed out will be raised. A value of 0\nindicates an infinite timeout, i.e. a timeout\nerror will never be raised. If not provided,\nthe default value set during initialization\nor with _set_rpc_timeout is used.",
        "input_shape": "",
        "notes": "",
        "code_example": "Make sure that MASTER_ADDR and MASTER_PORT are set properly\non both workers. Refer to init_process_group()\nAPI for more details. For example,\nexport MASTER_ADDR=localhost\nexport MASTER_PORT=5678\nThen run the following code in two different processes:\n>>> # On worker 0:\n>>> import torch\n>>> import torch.distributed.rpc as rpc\n>>> rpc.init_rpc(\"worker0\", rank=0, world_size=2)\n>>> fut1 = rpc.rpc_async(\"worker1\", torch.add, args=(torch.ones(2), 3))\n>>> fut2 = rpc.rpc_async(\"worker1\", min, args=(1, 2))\n>>> result = fut1.wait() + fut2.wait()\n>>> rpc.shutdown()\n\n\n>>> # On worker 1:\n>>> import torch.distributed.rpc as rpc\n>>> rpc.init_rpc(\"worker1\", rank=1, world_size=2)\n>>> rpc.shutdown()\n\n\nBelow is an example of running a TorchScript function using RPC.\n>>> # On both workers:\n>>> @torch.jit.script\n>>> def my_script_add(tensor: torch.Tensor, scalar: int):\n>>>    return torch.add(tensor, scalar)\n\n\n>>> # On worker 0:\n>>> import torch.distributed.rpc as rpc\n>>> rpc.init_rpc(\"worker0\", rank=0, world_size=2)\n>>> fut = rpc.rpc_async(\"worker1\", my_script_add, args=(torch.ones(2), 3))\n>>> ret = fut.wait()\n>>> rpc.shutdown()\n\n\n>>> # On worker 1:\n>>> import torch.distributed.rpc as rpc\n>>> rpc.init_rpc(\"worker1\", rank=1, world_size=2)\n>>> rpc.shutdown()\n\n\n"
    },
    {
        "api_name": "torch.distributed.rpc.PyRRef.rpc_async",
        "api_url": "https://pytorch.org/docs/stable/rpc.html#torch.distributed.rpc.PyRRef.rpc_async",
        "api_signature": "rpc_async(self: torch._C._distributed_rpc.PyRRef, timeout: float = -1.0)",
        "api_description": "Create a helper proxy to easily launch an rpc_async using\nthe owner of the RRef as the destination to run functions on\nthe object referenced by this RRef. More specifically,\nrref.rpc_async().func_name(*args, **kwargs) is the same as\nthe following:",
        "return_value": "",
        "parameters": "timeout (float, optional) – Timeout for rref.rpc_async().\nIf the call does not complete within this timeframe, an\nexception indicating so will be raised. If this argument\nis not provided, the default RPC timeout will be used.",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> from torch.distributed import rpc\n>>> rref = rpc.remote(\"worker1\", torch.add, args=(torch.zeros(2, 2), 1))\n>>> rref.rpc_async().size().wait()  # returns torch.Size([2, 2])\n>>> rref.rpc_async().view(1, 4).wait()  # returns tensor([[1., 1., 1., 1.]])\n\n\n"
    },
    {
        "api_name": "torch.distributed.rpc.rpc_sync",
        "api_url": "https://pytorch.org/docs/stable/rpc.html#torch.distributed.rpc.rpc_sync",
        "api_signature": "torch.distributed.rpc.rpc_sync(to, func, args=None, kwargs=None, timeout=-1.0)",
        "api_description": "Make a blocking RPC call to run function func on worker to. RPC\nmessages are sent and received in parallel to execution of Python code. This\nmethod is thread-safe.",
        "return_value": "Returns the result of running func with args and kwargs.\n",
        "parameters": "to (str or WorkerInfo or int) – name/rank/WorkerInfo of the destination worker.\nfunc (Callable) – a callable function, such as Python callables, builtin\noperators (e.g. add()) and annotated\nTorchScript functions.\nargs (tuple) – the argument tuple for the func invocation.\nkwargs (dict) – is a dictionary of keyword arguments for the func\ninvocation.\ntimeout (float, optional) – timeout in seconds to use for this RPC. If\nthe RPC does not complete in this amount of\ntime, an exception indicating it has\ntimed out will be raised. A value of 0\nindicates an infinite timeout, i.e. a timeout\nerror will never be raised. If not provided,\nthe default value set during initialization\nor with _set_rpc_timeout is used.",
        "input_shape": "",
        "notes": "",
        "code_example": "Make sure that MASTER_ADDR and MASTER_PORT are set properly\non both workers. Refer to init_process_group()\nAPI for more details. For example,\nexport MASTER_ADDR=localhost\nexport MASTER_PORT=5678\nThen run the following code in two different processes:\n>>> # On worker 0:\n>>> import torch\n>>> import torch.distributed.rpc as rpc\n>>> rpc.init_rpc(\"worker0\", rank=0, world_size=2)\n>>> ret = rpc.rpc_sync(\"worker1\", torch.add, args=(torch.ones(2), 3))\n>>> rpc.shutdown()\n\n\n>>> # On worker 1:\n>>> import torch.distributed.rpc as rpc\n>>> rpc.init_rpc(\"worker1\", rank=1, world_size=2)\n>>> rpc.shutdown()\n\n\nBelow is an example of running a TorchScript function using RPC.\n>>> # On both workers:\n>>> @torch.jit.script\n>>> def my_script_add(tensor: torch.Tensor, scalar: int):\n>>>    return torch.add(tensor, scalar)\n\n\n>>> # On worker 0:\n>>> import torch.distributed.rpc as rpc\n>>> rpc.init_rpc(\"worker0\", rank=0, world_size=2)\n>>> ret = rpc.rpc_sync(\"worker1\", my_script_add, args=(torch.ones(2), 3))\n>>> rpc.shutdown()\n\n\n>>> # On worker 1:\n>>> import torch.distributed.rpc as rpc\n>>> rpc.init_rpc(\"worker1\", rank=1, world_size=2)\n>>> rpc.shutdown()\n\n\n"
    },
    {
        "api_name": "torch.distributed.rpc.PyRRef.rpc_sync",
        "api_url": "https://pytorch.org/docs/stable/rpc.html#torch.distributed.rpc.PyRRef.rpc_sync",
        "api_signature": "rpc_sync(self: torch._C._distributed_rpc.PyRRef, timeout: float = -1.0)",
        "api_description": "Create a helper proxy to easily launch an rpc_sync using\nthe owner of the RRef as the destination to run functions on\nthe object referenced by this RRef. More specifically,\nrref.rpc_sync().func_name(*args, **kwargs) is the same as\nthe following:",
        "return_value": "",
        "parameters": "timeout (float, optional) – Timeout for rref.rpc_sync().\nIf the call does not complete within this timeframe, an\nexception indicating so will be raised. If this argument\nis not provided, the default RPC timeout will be used.",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> from torch.distributed import rpc\n>>> rref = rpc.remote(\"worker1\", torch.add, args=(torch.zeros(2, 2), 1))\n>>> rref.rpc_sync().size()  # returns torch.Size([2, 2])\n>>> rref.rpc_sync().view(1, 4)  # returns tensor([[1., 1., 1., 1.]])\n\n\n"
    },
    {
        "api_name": "torch.distributed.rpc.RpcBackendOptions.rpc_timeout",
        "api_url": "https://pytorch.org/docs/stable/rpc.html#torch.distributed.rpc.RpcBackendOptions.rpc_timeout",
        "api_signature": null,
        "api_description": "A float indicating the timeout to use for all\nRPCs. If an RPC does not complete in this timeframe, it will\ncomplete with an exception indicating that it has timed out.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.rpc.TensorPipeRpcBackendOptions.rpc_timeout",
        "api_url": "https://pytorch.org/docs/stable/rpc.html#torch.distributed.rpc.TensorPipeRpcBackendOptions.rpc_timeout",
        "api_signature": null,
        "api_description": "A float indicating the timeout to use for all\nRPCs. If an RPC does not complete in this timeframe, it will\ncomplete with an exception indicating that it has timed out.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.rpc.RpcBackendOptions",
        "api_url": "https://pytorch.org/docs/stable/rpc.html#torch.distributed.rpc.RpcBackendOptions",
        "api_signature": null,
        "api_description": "An abstract structure encapsulating the options passed into the RPC\nbackend. An instance of this class can be passed in to\ninit_rpc() in order to initialize RPC\nwith specific configurations, such as the RPC timeout and\ninit_method to be used.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.Rprop",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.Rprop.html#torch.optim.Rprop",
        "api_signature": "torch.optim.Rprop(params, lr=0.01, etas=(0.5, 1.2)",
        "api_description": "Implements the resilient backpropagation algorithm.",
        "return_value": "a handle that can be used to remove the added hook by calling\nhandle.remove()\na handle that can be used to remove the added hook by calling\nhandle.remove()\na handle that can be used to remove the added hook by calling\nhandle.remove()\na handle that can be used to remove the added hook by calling\nhandle.remove()\na handle that can be used to remove the added hook by calling\nhandle.remove()\na handle that can be used to remove the added hook by calling\nhandle.remove()\n",
        "parameters": "params (iterable) – iterable of parameters to optimize or dicts defining\nparameter groups\nlr (float, optional) – learning rate (default: 1e-2)\netas (Tuple[float, float], optional) – pair of (etaminus, etaplus), that\nare multiplicative increase and decrease factors\n(default: (0.5, 1.2))\nstep_sizes (Tuple[float, float], optional) – a pair of minimal and\nmaximal allowed step sizes (default: (1e-6, 50))\nforeach (bool, optional) – whether foreach implementation of optimizer\nis used. If unspecified by the user (so foreach is None), we will try to use\nforeach over the for-loop implementation on CUDA, since it is usually\nsignificantly more performant. Note that the foreach implementation uses\n~ sizeof(params) more peak memory than the for-loop version due to the intermediates\nbeing a tensorlist vs just one tensor. If memory is prohibitive, batch fewer\nparameters through the optimizer at a time or switch this flag to False (default: None)\nmaximize (bool, optional) – maximize the objective with respect to the\nparams, instead of minimizing (default: False)\ndifferentiable (bool, optional) – whether autograd should\noccur through the optimizer step in training. Otherwise, the step()\nfunction runs in a torch.no_grad() context. Setting to True can impair\nperformance, so leave it False if you don’t intend to run autograd\nthrough this instance (default: False)\nparam_group (dict) – Specifies what Tensors should be optimized along with group\nspecific optimization options.\nstate_dict (dict) – optimizer state. Should be an object returned\nfrom a call to state_dict().\nhook (Callable) – The user defined hook to be registered.\nprepend (bool) – If True, the provided post hook will be fired before\nall the already registered post-hooks on load_state_dict. Otherwise,\nthe provided hook will be fired after all the already registered\npost-hooks. (default: False)\nhook (Callable) – The user defined hook to be registered.\nprepend (bool) – If True, the provided pre hook will be fired before\nall the already registered pre-hooks on load_state_dict. Otherwise,\nthe provided hook will be fired after all the already registered\npre-hooks. (default: False)\nhook (Callable) – The user defined hook to be registered.\nprepend (bool) – If True, the provided post hook will be fired before\nall the already registered post-hooks on state_dict. Otherwise,\nthe provided hook will be fired after all the already registered\npost-hooks. (default: False)\nhook (Callable) – The user defined hook to be registered.\nprepend (bool) – If True, the provided pre hook will be fired before\nall the already registered pre-hooks on state_dict. Otherwise,\nthe provided hook will be fired after all the already registered\npre-hooks. (default: False)\nhook (Callable) – The user defined hook to be registered.\nhook (Callable) – The user defined hook to be registered.\nclosure (Callable, optional) – A closure that reevaluates the model\nand returns the loss.\nset_to_none (bool) – instead of setting to zero, set the grads to None.\nThis will in general have lower memory footprint, and can modestly improve performance.\nHowever, it changes certain behaviors. For example:\n1. When the user tries to access a gradient and perform manual ops on it,\na None attribute or a Tensor full of 0s will behave differently.\n2. If the user requests zero_grad(set_to_none=True) followed by a backward pass, .grads\nare guaranteed to be None for params that did not receive a gradient.\n3. torch.optim optimizers have a different behavior if the gradient is 0 or None\n(in one case it does the step with a gradient of 0 and in the other it skips\nthe step altogether).",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.RReLU",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.RReLU.html#torch.nn.RReLU",
        "api_signature": "torch.nn.RReLU(lower=0.125, upper=0.3333333333333333, inplace=False)",
        "api_description": "Applies the randomized leaky rectified linear unit function, element-wise.",
        "return_value": "",
        "parameters": "lower (float) – lower bound of the uniform distribution. Default: 18\\frac{1}{8}81​\nupper (float) – upper bound of the uniform distribution. Default: 13\\frac{1}{3}31​\ninplace (bool) – can optionally do the operation in-place. Default: False",
        "input_shape": "\nInput: (∗)(*)(∗), where ∗*∗ means any number of dimensions.\nOutput: (∗)(*)(∗), same shape as the input.\n\n",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.functional.rrelu",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.functional.rrelu.html#torch.nn.functional.rrelu",
        "api_signature": "torch.nn.functional.rrelu(input, lower=1. / 8, upper=1. / 3, training=False, inplace=False)",
        "api_description": "Randomized leaky ReLU.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.functional.rrelu_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.functional.rrelu_.html#torch.nn.functional.rrelu_",
        "api_signature": "torch.nn.functional.rrelu_(input, lower=1. / 8, upper=1. / 3, training=False)",
        "api_description": "In-place version of rrelu().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.beta.Beta.rsample",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.beta.Beta.rsample",
        "api_signature": "rsample(sample_shape=()",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.cauchy.Cauchy.rsample",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.cauchy.Cauchy.rsample",
        "api_signature": "rsample(sample_shape=torch.Size([])",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.continuous_bernoulli.ContinuousBernoulli.rsample",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.continuous_bernoulli.ContinuousBernoulli.rsample",
        "api_signature": "rsample(sample_shape=torch.Size([])",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.dirichlet.Dirichlet.rsample",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.dirichlet.Dirichlet.rsample",
        "api_signature": "rsample(sample_shape=()",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.distribution.Distribution.rsample",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.distribution.Distribution.rsample",
        "api_signature": "rsample(sample_shape=torch.Size([])",
        "api_description": "Generates a sample_shape shaped reparameterized sample or sample_shape\nshaped batch of reparameterized samples if the distribution parameters\nare batched.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.exponential.Exponential.rsample",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.exponential.Exponential.rsample",
        "api_signature": "rsample(sample_shape=torch.Size([])",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.fishersnedecor.FisherSnedecor.rsample",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.fishersnedecor.FisherSnedecor.rsample",
        "api_signature": "rsample(sample_shape=torch.Size([])",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.gamma.Gamma.rsample",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.gamma.Gamma.rsample",
        "api_signature": "rsample(sample_shape=torch.Size([])",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.independent.Independent.rsample",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.independent.Independent.rsample",
        "api_signature": "rsample(sample_shape=torch.Size([])",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.laplace.Laplace.rsample",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.laplace.Laplace.rsample",
        "api_signature": "rsample(sample_shape=torch.Size([])",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.rsample",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.rsample",
        "api_signature": "rsample(sample_shape=torch.Size([])",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.multivariate_normal.MultivariateNormal.rsample",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.multivariate_normal.MultivariateNormal.rsample",
        "api_signature": "rsample(sample_shape=torch.Size([])",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.normal.Normal.rsample",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.normal.Normal.rsample",
        "api_signature": "rsample(sample_shape=torch.Size([])",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.rsample",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.rsample",
        "api_signature": "rsample(sample_shape=torch.Size([])",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.studentT.StudentT.rsample",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.studentT.StudentT.rsample",
        "api_signature": "rsample(sample_shape=torch.Size([])",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.transformed_distribution.TransformedDistribution.rsample",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.transformed_distribution.TransformedDistribution.rsample",
        "api_signature": "rsample(sample_shape=torch.Size([])",
        "api_description": "Generates a sample_shape shaped reparameterized sample or sample_shape\nshaped batch of reparameterized samples if the distribution parameters\nare batched. Samples first from base distribution and applies\ntransform() for every transform in the list.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.uniform.Uniform.rsample",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.uniform.Uniform.rsample",
        "api_signature": "rsample(sample_shape=torch.Size([])",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.wishart.Wishart.rsample",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.wishart.Wishart.rsample",
        "api_signature": "rsample(sample_shape=torch.Size([])",
        "api_description": "Warning",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.rsqrt",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.rsqrt.html#torch.rsqrt",
        "api_signature": "torch.rsqrt(input, *, out=None)",
        "api_description": "Returns a new tensor with the reciprocal of the square-root of each of\nthe elements of input.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.rsqrt",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.rsqrt.html#torch.Tensor.rsqrt",
        "api_signature": "Tensor.rsqrt()",
        "api_description": "See torch.rsqrt()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.rsqrt_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.rsqrt_.html#torch.Tensor.rsqrt_",
        "api_signature": "Tensor.rsqrt_()",
        "api_description": "In-place version of rsqrt()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.agent.server.ElasticAgent.run",
        "api_url": "https://pytorch.org/docs/stable/elastic/agent.html#torch.distributed.elastic.agent.server.ElasticAgent.run",
        "api_signature": "run(role='default')",
        "api_description": "Run the agent.",
        "return_value": "The result of the execution, containing the return values or\nfailure details for each worker mapped by the worker’s global rank.\n",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.Interpreter.run",
        "api_url": "https://pytorch.org/docs/stable/fx.html#torch.fx.Interpreter.run",
        "api_signature": "run(*args, initial_env=None, enable_io_processing=True)",
        "api_description": "Run module via interpretation and return the result.",
        "return_value": "The value returned from executing the Module\n",
        "parameters": "*args – The arguments to the Module to run, in positional order\ninitial_env (Optional[Dict[Node, Any]]) – An optional starting environment for execution.\nThis is a dict mapping Node to any value. This can be used, for example, to\npre-populate results for certain Nodes so as to do only partial evaluation within\nthe interpreter.\nenable_io_processing (bool) – If true, we process the inputs and outputs with graph’s process_inputs and\nprocess_outputs function first before using them.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.export.ExportedProgram.run_decompositions",
        "api_url": "https://pytorch.org/docs/stable/export.html#torch.export.ExportedProgram.run_decompositions",
        "api_signature": "run_decompositions(decomp_table=None)",
        "api_description": "Run a set of decompositions on the exported program and returns a new\nexported program. By default we will run the Core ATen decompositions to\nget operators in the\nCore ATen Operator Set.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.Interpreter.run_node",
        "api_url": "https://pytorch.org/docs/stable/fx.html#torch.fx.Interpreter.run_node",
        "api_signature": "run_node(n)",
        "api_description": "Run a specific node n and return the result.\nCalls into placeholder, get_attr, call_function,\ncall_method, call_module, or output depending\non node.op",
        "return_value": "The result of executing n\n",
        "parameters": "n (Node) – The Node to execute",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.multiprocessing.api.RunProcsResult",
        "api_url": "https://pytorch.org/docs/stable/elastic/multiprocessing.html#torch.distributed.elastic.multiprocessing.api.RunProcsResult",
        "api_signature": "torch.distributed.elastic.multiprocessing.api.RunProcsResult(return_values=<factory>, failures=<factory>, stdouts=<factory>, stderrs=<factory>)",
        "api_description": "Results of a completed run of processes started with start_processes(). Returned by PContext.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.agent.server.api.RunResult",
        "api_url": "https://pytorch.org/docs/stable/elastic/agent.html#torch.distributed.elastic.agent.server.api.RunResult",
        "api_signature": "torch.distributed.elastic.agent.server.api.RunResult(state, return_values=<factory>, failures=<factory>)",
        "api_description": "Return results of the worker executions.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.bernoulli.Bernoulli.sample",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.bernoulli.Bernoulli.sample",
        "api_signature": "sample(sample_shape=torch.Size([])",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.binomial.Binomial.sample",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.binomial.Binomial.sample",
        "api_signature": "sample(sample_shape=torch.Size([])",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.categorical.Categorical.sample",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.categorical.Categorical.sample",
        "api_signature": "sample(sample_shape=torch.Size([])",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.continuous_bernoulli.ContinuousBernoulli.sample",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.continuous_bernoulli.ContinuousBernoulli.sample",
        "api_signature": "sample(sample_shape=torch.Size([])",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.distribution.Distribution.sample",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.distribution.Distribution.sample",
        "api_signature": "sample(sample_shape=torch.Size([])",
        "api_description": "Generates a sample_shape shaped sample or sample_shape shaped batch of\nsamples if the distribution parameters are batched.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.geometric.Geometric.sample",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.geometric.Geometric.sample",
        "api_signature": "sample(sample_shape=torch.Size([])",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.independent.Independent.sample",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.independent.Independent.sample",
        "api_signature": "sample(sample_shape=torch.Size([])",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.lkj_cholesky.LKJCholesky.sample",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.lkj_cholesky.LKJCholesky.sample",
        "api_signature": "sample(sample_shape=torch.Size([])",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.mixture_same_family.MixtureSameFamily.sample",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.mixture_same_family.MixtureSameFamily.sample",
        "api_signature": "sample(sample_shape=torch.Size([])",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.multinomial.Multinomial.sample",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.multinomial.Multinomial.sample",
        "api_signature": "sample(sample_shape=torch.Size([])",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.negative_binomial.NegativeBinomial.sample",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.negative_binomial.NegativeBinomial.sample",
        "api_signature": "sample(sample_shape=torch.Size([])",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.normal.Normal.sample",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.normal.Normal.sample",
        "api_signature": "sample(sample_shape=torch.Size([])",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.one_hot_categorical.OneHotCategorical.sample",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.one_hot_categorical.OneHotCategorical.sample",
        "api_signature": "sample(sample_shape=torch.Size([])",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.poisson.Poisson.sample",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.poisson.Poisson.sample",
        "api_signature": "sample(sample_shape=torch.Size([])",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.transformed_distribution.TransformedDistribution.sample",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.transformed_distribution.TransformedDistribution.sample",
        "api_signature": "sample(sample_shape=torch.Size([])",
        "api_description": "Generates a sample_shape shaped sample or sample_shape shaped batch of\nsamples if the distribution parameters are batched. Samples first from\nbase distribution and applies transform() for every transform in the\nlist.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.von_mises.VonMises.sample",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.von_mises.VonMises.sample",
        "api_signature": "sample(sample_shape=torch.Size([])",
        "api_description": "The sampling algorithm for the von Mises distribution is based on the\nfollowing paper: D.J. Best and N.I. Fisher, “Efficient simulation of the\nvon Mises distribution.” Applied Statistics (1979): 152-157.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.distribution.Distribution.sample_n",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.distribution.Distribution.sample_n",
        "api_signature": "sample_n(n)",
        "api_description": "Generates n samples or n batches of samples if the distribution\nparameters are batched.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.sparse.sampled_addmm",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.sparse.sampled_addmm.html#torch.sparse.sampled_addmm",
        "api_signature": "torch.sparse.sampled_addmm(input, mat1, mat2, *, beta=1., alpha=1., out=None)",
        "api_description": "Performs a matrix multiplication of the dense matrices mat1 and mat2 at the locations\nspecified by the sparsity pattern of input. The matrix input is added to the final result.",
        "return_value": "",
        "parameters": "input (Tensor) – a sparse CSR matrix of shape (m, n) to be added and used to compute\nthe sampled matrix multiplication\nmat1 (Tensor) – a dense matrix of shape (m, k) to be multiplied\nmat2 (Tensor) – a dense matrix of shape (k, n) to be multiplied\nbeta (Number, optional) – multiplier for input (β\\betaβ)\nalpha (Number, optional) – multiplier for mat1@mat2mat1 @ mat2mat1@mat2 (α\\alphaα)\nout (Tensor, optional) – output tensor. Ignored if None. Default: None.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.data.Sampler",
        "api_url": "https://pytorch.org/docs/stable/data.html#torch.utils.data.Sampler",
        "api_signature": "torch.utils.data.Sampler(data_source=None)",
        "api_description": "Base class for all Samplers.",
        "return_value": "",
        "parameters": "data_source (Dataset) – This argument is not used and will be removed in 2.2.0.\nYou may still have custom implementation that utilizes it.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.save",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.save.html#torch.save",
        "api_signature": "torch.save(obj, f, pickle_module=pickle, pickle_protocol=DEFAULT_PROTOCOL, _use_new_zipfile_serialization=True)",
        "api_description": "Saves an object to a disk file.",
        "return_value": "",
        "parameters": "obj (object) – saved object\nf (Union[str, PathLike, BinaryIO, IO[bytes]]) – a file-like object (has to implement write and flush) or a string or\nos.PathLike object containing a file name\npickle_module (Any) – module used for pickling metadata and objects\npickle_protocol (int) – can be specified to override the default protocol",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.checkpoint.state_dict_saver.save",
        "api_url": "https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict_saver.save",
        "api_signature": "torch.distributed.checkpoint.state_dict_saver.save(state_dict, *, checkpoint_id=None, storage_writer=None, planner=None, process_group=None)",
        "api_description": "Save a distributed model in SPMD style.",
        "return_value": "Metadata object for the saved checkpoint.\n",
        "parameters": "state_dict (Dict[str, Any]) – The state_dict to save.\ncheckpoint_id (Union[str, os.PathLike, None]) – The ID of this checkpoint instance. The meaning of the checkpoint_id\ndepends on the storage. It can be a path to a folder or to a file.\nIt can also be a key if the storage is a key-value store.\n(Default: None)\nstorage_writer (Optional[StorageWriter]) – Instance of StorageWriter used to perform writes. If this is not\nspecified, DCP will automatically infer the writer based on the\ncheckpoint_id. If checkpoint_id is also None, an exception will\nbe raised. (Default: None)\nplanner (Optional[SavePlanner]) – Instance of SavePlanner. If this is not specificed, the default\nplanner will be used. (Default: None)\nprocess_group (Optional[ProcessGroup]) – ProcessGroup to be used for cross-rank synchronization.\n(Default: None)",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.export.save",
        "api_url": "https://pytorch.org/docs/stable/export.html#torch.export.save",
        "api_signature": "torch.export.save(ep, f, *, extra_files=None, opset_version=None)",
        "api_description": "Warning",
        "return_value": "",
        "parameters": "ep (ExportedProgram) – The exported program to save.\nf (Union[str, os.PathLike, io.BytesIO) – A file-like object (has to\nimplement write and flush) or a string containing a file name.\nextra_files (Optional[Dict[str, Any]]) – Map from filename to contents\nwhich will be stored as part of f.\nopset_version (Optional[Dict[str, int]]) – A map of opset names\nto the version of this opset",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.jit.save",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.jit.save.html#torch.jit.save",
        "api_signature": "torch.jit.save(m, f, _extra_files=None)",
        "api_description": "Save an offline version of this module for use in a separate process.",
        "return_value": "",
        "parameters": "m – A ScriptModule to save.\nf – A file-like object (has to implement write and flush) or a string\ncontaining a file name.\n_extra_files – Map from filename to contents which will be stored as part of f.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.jit.ScriptFunction.save",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.jit.ScriptFunction.html#torch.jit.ScriptFunction.save",
        "api_signature": "save(self: torch._C.ScriptFunction, filename: str, _extra_files: dict[str, str] = {})",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.jit.ScriptModule.save",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.jit.ScriptModule.html#torch.jit.ScriptModule.save",
        "api_signature": "save(f, **kwargs)",
        "api_description": "Save with a file-like object.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.onnx.ONNXProgram.save",
        "api_url": "https://pytorch.org/docs/stable/onnx_dynamo.html#torch.onnx.ONNXProgram.save",
        "api_signature": "save(destination, *, model_state=None, serializer=None)",
        "api_description": "Saves the in-memory ONNX model to destination using specified serializer.",
        "return_value": "",
        "parameters": "destination (Union[str, BufferedIOBase]) – The destination to save the ONNX model. It can be either a string or a file-like object.\nWhen used with model_state, it must be a string with a full path to the destination.\nIf destination is a string, besides saving the ONNX model into a file, model weights are also stored\nin separate files in the same directory as the ONNX model. E.g. for destination=”/path/model.onnx”,\nthe initializers are saved in “/path/” folder along with “onnx.model”.\nIt can be either a string with the path to a checkpoint or a dictionary with the actual model state.\nThe supported file formats are the same as those supported by torch.load and safetensors.safe_open.\nRequired when enable_fake_mode() is used but real initializers are needed on the ONNX graph.\nserializer (Optional[ONNXProgramSerializer]) – The serializer to use. If not specified, the model will be serialized as Protobuf.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.package.PackageExporter.save_binary",
        "api_url": "https://pytorch.org/docs/stable/package.html#torch.package.PackageExporter.save_binary",
        "api_signature": "save_binary(package, resource, binary)",
        "api_description": "Save raw bytes to the package.",
        "return_value": "",
        "parameters": "package (str) – The name of module package this resource should go it (e.g. \"my_package.my_subpackage\").\nresource (str) – A unique name for the resource, used to identify it to load.\nbinary (str) – The data to save.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.onnx.ONNXProgram.save_diagnostics",
        "api_url": "https://pytorch.org/docs/stable/onnx_dynamo.html#torch.onnx.ONNXProgram.save_diagnostics",
        "api_signature": "save_diagnostics(destination)",
        "api_description": "Saves the export diagnostics as a SARIF log to the specified destination path.",
        "return_value": "",
        "parameters": "destination (str) – The destination to save the diagnostics SARIF log.\nIt must have a .sarif extension.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.autograd.function.BackwardCFunction.save_for_backward",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.autograd.function.BackwardCFunction.html#torch.autograd.function.BackwardCFunction.save_for_backward",
        "api_signature": "save_for_backward(*tensors)",
        "api_description": "Save given tensors for a future call to backward().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> class Func(Function):\n>>>     @staticmethod\n>>>     def forward(ctx, x: torch.Tensor, y: torch.Tensor, z: int):\n>>>         w = x * z\n>>>         out = x * y + y * z + w * y\n>>>         ctx.save_for_backward(x, y, w, out)\n>>>         ctx.z = z  # z is not a tensor\n>>>         return out\n>>>\n>>>     @staticmethod\n>>>     @once_differentiable\n>>>     def backward(ctx, grad_out):\n>>>         x, y, w, out = ctx.saved_tensors\n>>>         z = ctx.z\n>>>         gx = grad_out * (y + y * z)\n>>>         gy = grad_out * (x + z + w)\n>>>         gz = None\n>>>         return gx, gy, gz\n>>>\n>>> a = torch.tensor(1., requires_grad=True, dtype=torch.double)\n>>> b = torch.tensor(2., requires_grad=True, dtype=torch.double)\n>>> c = 4\n>>> d = Func.apply(a, b, c)\n\n\n"
    },
    {
        "api_name": "torch.autograd.function.FunctionCtx.save_for_backward",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.autograd.function.FunctionCtx.save_for_backward.html#torch.autograd.function.FunctionCtx.save_for_backward",
        "api_signature": "FunctionCtx.save_for_backward(*tensors)",
        "api_description": "Save given tensors for a future call to backward().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> class Func(Function):\n>>>     @staticmethod\n>>>     def forward(ctx, x: torch.Tensor, y: torch.Tensor, z: int):\n>>>         w = x * z\n>>>         out = x * y + y * z + w * y\n>>>         ctx.save_for_backward(x, y, w, out)\n>>>         ctx.z = z  # z is not a tensor\n>>>         return out\n>>>\n>>>     @staticmethod\n>>>     @once_differentiable\n>>>     def backward(ctx, grad_out):\n>>>         x, y, w, out = ctx.saved_tensors\n>>>         z = ctx.z\n>>>         gx = grad_out * (y + y * z)\n>>>         gy = grad_out * (x + z + w)\n>>>         gz = None\n>>>         return gx, gy, gz\n>>>\n>>> a = torch.tensor(1., requires_grad=True, dtype=torch.double)\n>>> b = torch.tensor(2., requires_grad=True, dtype=torch.double)\n>>> c = 4\n>>> d = Func.apply(a, b, c)\n\n\n"
    },
    {
        "api_name": "torch.autograd.function.InplaceFunction.save_for_backward",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.autograd.function.InplaceFunction.html#torch.autograd.function.InplaceFunction.save_for_backward",
        "api_signature": "save_for_backward(*tensors)",
        "api_description": "Save given tensors for a future call to backward().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> class Func(Function):\n>>>     @staticmethod\n>>>     def forward(ctx, x: torch.Tensor, y: torch.Tensor, z: int):\n>>>         w = x * z\n>>>         out = x * y + y * z + w * y\n>>>         ctx.save_for_backward(x, y, w, out)\n>>>         ctx.z = z  # z is not a tensor\n>>>         return out\n>>>\n>>>     @staticmethod\n>>>     @once_differentiable\n>>>     def backward(ctx, grad_out):\n>>>         x, y, w, out = ctx.saved_tensors\n>>>         z = ctx.z\n>>>         gx = grad_out * (y + y * z)\n>>>         gy = grad_out * (x + z + w)\n>>>         gz = None\n>>>         return gx, gy, gz\n>>>\n>>> a = torch.tensor(1., requires_grad=True, dtype=torch.double)\n>>> b = torch.tensor(2., requires_grad=True, dtype=torch.double)\n>>> c = 4\n>>> d = Func.apply(a, b, c)\n\n\n"
    },
    {
        "api_name": "torch.autograd.function.NestedIOFunction.save_for_backward",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.autograd.function.NestedIOFunction.html#torch.autograd.function.NestedIOFunction.save_for_backward",
        "api_signature": "save_for_backward(*args)",
        "api_description": "See Function.save_for_backward().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.autograd.function.BackwardCFunction.save_for_forward",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.autograd.function.BackwardCFunction.html#torch.autograd.function.BackwardCFunction.save_for_forward",
        "api_signature": "save_for_forward(*tensors)",
        "api_description": "Save given tensors for a future call to jvp().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> class Func(torch.autograd.Function):\n>>>     @staticmethod\n>>>     def forward(ctx, x: torch.Tensor, y: torch.Tensor, z: int):\n>>>         ctx.save_for_backward(x, y)\n>>>         ctx.save_for_forward(x, y)\n>>>         ctx.z = z\n>>>         return x * y * z\n>>>\n>>>     @staticmethod\n>>>     def jvp(ctx, x_t, y_t, _):\n>>>         x, y = ctx.saved_tensors\n>>>         z = ctx.z\n>>>         return z * (y * x_t + x * y_t)\n>>>\n>>>     @staticmethod\n>>>     def vjp(ctx, grad_out):\n>>>         x, y = ctx.saved_tensors\n>>>         z = ctx.z\n>>>         return z * grad_out * y, z * grad_out * x, None\n>>>\n>>>     a = torch.tensor(1., requires_grad=True, dtype=torch.double)\n>>>     t = torch.tensor(1., dtype=torch.double)\n>>>     b = torch.tensor(2., requires_grad=True, dtype=torch.double)\n>>>     c = 4\n>>>\n>>>     with fwAD.dual_level():\n>>>         a_dual = fwAD.make_dual(a, t)\n>>>         d = Func.apply(a_dual, b, c)\n\n\n"
    },
    {
        "api_name": "torch.autograd.function.InplaceFunction.save_for_forward",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.autograd.function.InplaceFunction.html#torch.autograd.function.InplaceFunction.save_for_forward",
        "api_signature": "save_for_forward(*tensors)",
        "api_description": "Save given tensors for a future call to jvp().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> class Func(torch.autograd.Function):\n>>>     @staticmethod\n>>>     def forward(ctx, x: torch.Tensor, y: torch.Tensor, z: int):\n>>>         ctx.save_for_backward(x, y)\n>>>         ctx.save_for_forward(x, y)\n>>>         ctx.z = z\n>>>         return x * y * z\n>>>\n>>>     @staticmethod\n>>>     def jvp(ctx, x_t, y_t, _):\n>>>         x, y = ctx.saved_tensors\n>>>         z = ctx.z\n>>>         return z * (y * x_t + x * y_t)\n>>>\n>>>     @staticmethod\n>>>     def vjp(ctx, grad_out):\n>>>         x, y = ctx.saved_tensors\n>>>         z = ctx.z\n>>>         return z * grad_out * y, z * grad_out * x, None\n>>>\n>>>     a = torch.tensor(1., requires_grad=True, dtype=torch.double)\n>>>     t = torch.tensor(1., dtype=torch.double)\n>>>     b = torch.tensor(2., requires_grad=True, dtype=torch.double)\n>>>     c = 4\n>>>\n>>>     with fwAD.dual_level():\n>>>         a_dual = fwAD.make_dual(a, t)\n>>>         d = Func.apply(a_dual, b, c)\n\n\n"
    },
    {
        "api_name": "torch.autograd.function.NestedIOFunction.save_for_forward",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.autograd.function.NestedIOFunction.html#torch.autograd.function.NestedIOFunction.save_for_forward",
        "api_signature": "save_for_forward(*tensors)",
        "api_description": "Save given tensors for a future call to jvp().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> class Func(torch.autograd.Function):\n>>>     @staticmethod\n>>>     def forward(ctx, x: torch.Tensor, y: torch.Tensor, z: int):\n>>>         ctx.save_for_backward(x, y)\n>>>         ctx.save_for_forward(x, y)\n>>>         ctx.z = z\n>>>         return x * y * z\n>>>\n>>>     @staticmethod\n>>>     def jvp(ctx, x_t, y_t, _):\n>>>         x, y = ctx.saved_tensors\n>>>         z = ctx.z\n>>>         return z * (y * x_t + x * y_t)\n>>>\n>>>     @staticmethod\n>>>     def vjp(ctx, grad_out):\n>>>         x, y = ctx.saved_tensors\n>>>         z = ctx.z\n>>>         return z * grad_out * y, z * grad_out * x, None\n>>>\n>>>     a = torch.tensor(1., requires_grad=True, dtype=torch.double)\n>>>     t = torch.tensor(1., dtype=torch.double)\n>>>     b = torch.tensor(2., requires_grad=True, dtype=torch.double)\n>>>     c = 4\n>>>\n>>>     with fwAD.dual_level():\n>>>         a_dual = fwAD.make_dual(a, t)\n>>>         d = Func.apply(a_dual, b, c)\n\n\n"
    },
    {
        "api_name": "torch.package.PackageExporter.save_module",
        "api_url": "https://pytorch.org/docs/stable/package.html#torch.package.PackageExporter.save_module",
        "api_signature": "save_module(module_name, dependencies=True)",
        "api_description": "Save the code for module into the package. Code for the module is resolved using the importers path to find the\nmodule object, and then using its __file__ attribute to find the source code.",
        "return_value": "",
        "parameters": "module_name (str) – e.g. my_package.my_subpackage, code will be saved to provide code\nfor this package.\ndependencies (bool, optional) – If True, we scan the source for dependencies.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.autograd.graph.save_on_cpu",
        "api_url": "https://pytorch.org/docs/stable/autograd.html#torch.autograd.graph.save_on_cpu",
        "api_signature": "torch.autograd.graph.save_on_cpu(pin_memory=False, device_type='cuda')",
        "api_description": "Context manager under which tensors saved by the forward pass will be stored on cpu, then retrieved for backward.",
        "return_value": "",
        "parameters": "pin_memory (bool) – If True tensors will be saved to CPU pinned memory\nduring packing and copied to GPU asynchronously during unpacking.\nDefaults to False.\nAlso see Use pinned memory buffers.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.package.PackageExporter.save_pickle",
        "api_url": "https://pytorch.org/docs/stable/package.html#torch.package.PackageExporter.save_pickle",
        "api_signature": "save_pickle(package, resource, obj, dependencies=True, pickle_protocol=3)",
        "api_description": "Save a python object to the archive using pickle. Equivalent to torch.save() but saving into\nthe archive rather than a stand-alone file. Standard pickle does not save the code, only the objects.\nIf dependencies is true, this method will also scan the pickled objects for which modules are required\nto reconstruct them and save the relevant code.",
        "return_value": "",
        "parameters": "package (str) – The name of module package this resource should go in (e.g. \"my_package.my_subpackage\").\nresource (str) – A unique name for the resource, used to identify it to load.\nobj (Any) – The object to save, must be picklable.\ndependencies (bool, optional) – If True, we scan the source for dependencies.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.package.PackageExporter.save_source_file",
        "api_url": "https://pytorch.org/docs/stable/package.html#torch.package.PackageExporter.save_source_file",
        "api_signature": "save_source_file(module_name, file_or_directory, dependencies=True)",
        "api_description": "Adds the local file system file_or_directory to the source package to provide the code\nfor module_name.",
        "return_value": "",
        "parameters": "module_name (str) – e.g. \"my_package.my_subpackage\", code will be saved to provide code for this package.\nfile_or_directory (str) – the path to a file or directory of code. When a directory, all python files in the directory\nare recursively copied using save_source_file(). If a file is named \"/__init__.py\" the code is treated\nas a package.\ndependencies (bool, optional) – If True, we scan the source for dependencies.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.package.PackageExporter.save_source_string",
        "api_url": "https://pytorch.org/docs/stable/package.html#torch.package.PackageExporter.save_source_string",
        "api_signature": "save_source_string(module_name, src, is_package=False, dependencies=True)",
        "api_description": "Adds src as the source code for module_name in the exported package.",
        "return_value": "",
        "parameters": "module_name (str) – e.g. my_package.my_subpackage, code will be saved to provide code for this package.\nsrc (str) – The Python source code to save for this package.\nis_package (bool, optional) – If True, this module is treated as a package. Packages are allowed to have submodules\n(e.g. my_package.my_subpackage.my_subsubpackage), and resources can be saved inside them. Defaults to False.\ndependencies (bool, optional) – If True, we scan the source for dependencies.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.checkpoint.state_dict_saver.save_state_dict",
        "api_url": "https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict_saver.save_state_dict",
        "api_signature": "torch.distributed.checkpoint.state_dict_saver.save_state_dict(state_dict, storage_writer, process_group=None, coordinator_rank=0, no_dist=False, planner=None)",
        "api_description": "This method is deprecated. Please switch to ‘save’.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.package.PackageExporter.save_text",
        "api_url": "https://pytorch.org/docs/stable/package.html#torch.package.PackageExporter.save_text",
        "api_signature": "save_text(package, resource, text)",
        "api_description": "Save text data to the package.",
        "return_value": "",
        "parameters": "package (str) – The name of module package this resource should go it (e.g. \"my_package.my_subpackage\").\nresource (str) – A unique name for the resource, used to identify it to load.\ntext (str) – The contents to save.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.jit.ScriptFunction.save_to_buffer",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.jit.ScriptFunction.html#torch.jit.ScriptFunction.save_to_buffer",
        "api_signature": "save_to_buffer(self: torch._C.ScriptFunction, _extra_files: dict[str, str] = {})",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.autograd.function.NestedIOFunction.saved_tensors",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.autograd.function.NestedIOFunction.html#torch.autograd.function.NestedIOFunction.saved_tensors",
        "api_signature": null,
        "api_description": "See Function.saved_tensors().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.autograd.graph.saved_tensors_hooks",
        "api_url": "https://pytorch.org/docs/stable/autograd.html#torch.autograd.graph.saved_tensors_hooks",
        "api_signature": "torch.autograd.graph.saved_tensors_hooks(pack_hook, unpack_hook)",
        "api_description": "Context-manager that sets a pair of pack / unpack hooks for saved tensors.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.checkpoint.SavePlan",
        "api_url": "https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.SavePlan",
        "api_signature": "torch.distributed.checkpoint.SavePlan(items: List[torch.distributed.checkpoint.planner.WriteItem], storage_data: Any = None, planner_data: Any = None)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.checkpoint.SavePlanner",
        "api_url": "https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.SavePlanner",
        "api_signature": null,
        "api_description": "Abstract class defining the protocol used by save_state_dict to plan the save process.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.onnx.JitScalarType.scalar_name",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.onnx.JitScalarType.html#torch.onnx.JitScalarType.scalar_name",
        "api_signature": "scalar_name()",
        "api_description": "Convert a JitScalarType to a JIT scalar type name.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.half_cauchy.HalfCauchy.scale",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.half_cauchy.HalfCauchy.scale",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.half_normal.HalfNormal.scale",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.half_normal.HalfNormal.scale",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.log_normal.LogNormal.scale",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.log_normal.LogNormal.scale",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.scale_tril",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.scale_tril",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.multivariate_normal.MultivariateNormal.scale_tril",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.multivariate_normal.MultivariateNormal.scale_tril",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.wishart.Wishart.scale_tril",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.wishart.Wishart.scale_tril",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.functional.scaled_dot_product_attention",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html#torch.nn.functional.scaled_dot_product_attention",
        "api_signature": "torch.nn.functional.scaled_dot_product_attention(query, key, value, attn_mask=None, dropout_p=0.0, is_causal=False, scale=None)",
        "api_description": "Computes scaled dot product attention on query, key and value tensors, using\nan optional attention mask if passed, and applying dropout if a probability\ngreater than 0.0 is specified. The optional scale argument can only be specified as a keyword argument.",
        "return_value": "Attention output; shape (N,...,L,Ev)(N, ..., L, Ev)(N,...,L,Ev).\n",
        "parameters": "query (Tensor) – Query tensor; shape (N,...,L,E)(N, ..., L, E)(N,...,L,E).\nkey (Tensor) – Key tensor; shape (N,...,S,E)(N, ..., S, E)(N,...,S,E).\nvalue (Tensor) – Value tensor; shape (N,...,S,Ev)(N, ..., S, Ev)(N,...,S,Ev).\nattn_mask (optional Tensor) – Attention mask; shape must be broadcastable to the shape of attention weights,\nwhich is (N,...,L,S)(N,..., L, S)(N,...,L,S). Two types of masks are supported.\nA boolean mask where a value of True indicates that the element should take part in attention.\nA float mask of the same type as query, key, value that is added to the attention score.\ndropout_p (float) – Dropout probability; if greater than 0.0, dropout is applied\nis_causal (bool) – If true, assumes upper left causal attention masking and errors if both attn_mask and is_causal\nare set.\nscale (optional python:float, keyword-only) – Scaling factor applied prior to softmax. If None, the default value is set\nto 1E\\frac{1}{\\sqrt{E}}E​1​.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.special.scaled_modified_bessel_k0",
        "api_url": "https://pytorch.org/docs/stable/special.html#torch.special.scaled_modified_bessel_k0",
        "api_signature": "torch.special.scaled_modified_bessel_k0(input, *, out=None)",
        "api_description": "Scaled modified Bessel function of the second kind of order 000.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.special.scaled_modified_bessel_k1",
        "api_url": "https://pytorch.org/docs/stable/special.html#torch.special.scaled_modified_bessel_k1",
        "api_signature": "torch.special.scaled_modified_bessel_k1(input, *, out=None)",
        "api_description": "Scaled modified Bessel function of the second kind of order 111.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.scatter",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.scatter.html#torch.scatter",
        "api_signature": "torch.scatter(input, dim, index, src)",
        "api_description": "Out-of-place version of torch.Tensor.scatter_()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda.comm.scatter",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.cuda.comm.scatter.html#torch.cuda.comm.scatter",
        "api_signature": "torch.cuda.comm.scatter(tensor, devices=None, chunk_sizes=None, dim=0, streams=None, *, out=None)",
        "api_description": "Scatters tensor across multiple GPUs.",
        "return_value": "\n\nIf devices is specified,a tuple containing chunks of tensor, placed on\ndevices.\n\n\n\n\nIf out is specified,a tuple containing out tensors, each containing a chunk of\ntensor.\n\n\n\n\n\n",
        "parameters": "tensor (Tensor) – tensor to scatter. Can be on CPU or GPU.\ndevices (Iterable[torch.device, str or int], optional) – an iterable of\nGPU devices, among which to scatter.\nchunk_sizes (Iterable[int], optional) – sizes of chunks to be placed on\neach device. It should match devices in length and sums to\ntensor.size(dim). If not specified, tensor will be divided\ninto equal chunks.\ndim (int, optional) – A dimension along which to chunk tensor.\nDefault: 0.\nstreams (Iterable[torch.cuda.Stream], optional) – an iterable of Streams, among\nwhich to execute the scatter. If not specified, the default stream will\nbe utilized.\nout (Sequence[Tensor], optional, keyword-only) – the GPU tensors to\nstore output results. Sizes of these tensors must match that of\ntensor, except for dim, where the total size must\nsum to tensor.size(dim).",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.scatter",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#torch.distributed.scatter",
        "api_signature": "torch.distributed.scatter(tensor, scatter_list=None, src=0, group=None, async_op=False)",
        "api_description": "Scatters a list of tensors to all processes in a group.",
        "return_value": "Async work handle, if async_op is set to True.\nNone, if not async_op or if not part of the group\n",
        "parameters": "tensor (Tensor) – Output tensor.\nscatter_list (list[Tensor]) – List of tensors to scatter (default is\nNone, must be specified on the source rank)\nsrc (int) – Source rank on global process group (regardless of group argument).\nDefault is 0\ngroup (ProcessGroup, optional) – The process group to work on. If None,\nthe default process group will be used.\nasync_op (bool, optional) – Whether this op should be an async op",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> # Note: Process group initialization omitted on each rank.\n>>> import torch.distributed as dist\n>>> tensor_size = 2\n>>> t_ones = torch.ones(tensor_size)\n>>> t_fives = torch.ones(tensor_size) * 5\n>>> output_tensor = torch.zeros(tensor_size)\n>>> if dist.get_rank() == 0:\n>>>     # Assumes world_size of 2.\n>>>     # Only tensors, all of which must be the same size.\n>>>     scatter_list = [t_ones, t_fives]\n>>> else:\n>>>     scatter_list = None\n>>> dist.scatter(output_tensor, scatter_list, src=0)\n>>> # Rank i gets scatter_list[i]. For example, on rank 1:\n>>> output_tensor\ntensor([5., 5.])\n\n\n"
    },
    {
        "api_name": "torch.Tensor.scatter",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.scatter.html#torch.Tensor.scatter",
        "api_signature": "Tensor.scatter(dim, index, src)",
        "api_description": "Out-of-place version of torch.Tensor.scatter_()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.scatter_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.scatter_.html#torch.Tensor.scatter_",
        "api_signature": "Tensor.scatter_(dim, index, src, *, reduce=None)",
        "api_description": "Writes all values from the tensor src into self at the indices\nspecified in the index tensor. For each value in src, its output\nindex is specified by its index in src for dimension != dim and by\nthe corresponding value in index for dimension = dim.",
        "return_value": "",
        "parameters": "dim (int) – the axis along which to index\nindex (LongTensor) – the indices of elements to scatter, can be either empty\nor of the same dimensionality as src. When empty, the operation\nreturns self unchanged.\nsrc (Tensor) – the source element(s) to scatter.\nreduce (str, optional) – reduction operation to apply, can be either\n'add' or 'multiply'.\ndim (int) – the axis along which to index\nindex (LongTensor) – the indices of elements to scatter, can be either empty\nor of the same dimensionality as src. When empty, the operation\nreturns self unchanged.\nvalue (Scalar) – the value to scatter.\nreduce (str, optional) – reduction operation to apply, can be either\n'add' or 'multiply'.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.scatter_add",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.scatter_add.html#torch.scatter_add",
        "api_signature": "torch.scatter_add(input, dim, index, src)",
        "api_description": "Out-of-place version of torch.Tensor.scatter_add_()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.scatter_add",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.scatter_add.html#torch.Tensor.scatter_add",
        "api_signature": "Tensor.scatter_add(dim, index, src)",
        "api_description": "Out-of-place version of torch.Tensor.scatter_add_()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.scatter_add_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.scatter_add_.html#torch.Tensor.scatter_add_",
        "api_signature": "Tensor.scatter_add_(dim, index, src)",
        "api_description": "Adds all values from the tensor src into self at the indices\nspecified in the index tensor in a similar fashion as\nscatter_(). For each value in src, it is added to\nan index in self which is specified by its index in src\nfor dimension != dim and by the corresponding value in index for\ndimension = dim.",
        "return_value": "",
        "parameters": "dim (int) – the axis along which to index\nindex (LongTensor) – the indices of elements to scatter and add, can be\neither empty or of the same dimensionality as src. When empty, the\noperation returns self unchanged.\nsrc (Tensor) – the source elements to scatter and add",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.fsdp.FullyShardedDataParallel.scatter_full_optim_state_dict",
        "api_url": "https://pytorch.org/docs/stable/fsdp.html#torch.distributed.fsdp.FullyShardedDataParallel.scatter_full_optim_state_dict",
        "api_signature": "scatter_full_optim_state_dict(full_optim_state_dict, model, optim_input=None, optim=None, group=None)",
        "api_description": "Scatter the full optimizer state dict from rank 0 to all other ranks.",
        "return_value": "The full optimizer state dict now remapped to\nflattened parameters instead of unflattened parameters and\nrestricted to only include this rank’s part of the optimizer state.\n",
        "parameters": "full_optim_state_dict (Optional[Dict[str, Any]]) – Optimizer state\ndict corresponding to the unflattened parameters and holding\nthe full non-sharded optimizer state if on rank 0; the argument\nis ignored on nonzero ranks.\nmodel (torch.nn.Module) – Root module (which may or may not be a\nFullyShardedDataParallel instance) whose parameters\ncorrespond to the optimizer state in full_optim_state_dict.\noptim_input (Optional[Union[List[Dict[str, Any]], Iterable[torch.nn.Parameter]]]) – Input passed into the optimizer representing either a\nlist of parameter groups or an iterable of parameters;\nif None, then this method assumes the input was\nmodel.parameters(). This argument is deprecated, and there\nis no need to pass it in anymore. (Default: None)\noptim (Optional[torch.optim.Optimizer]) – Optimizer that will load\nthe state dict returned by this method. This is the preferred\nargument to use over optim_input. (Default: None)\ngroup (dist.ProcessGroup) – Model’s process group or None if\nusing the default process group. (Default: None)",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.scatter_object_list",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#torch.distributed.scatter_object_list",
        "api_signature": "torch.distributed.scatter_object_list(scatter_object_output_list, scatter_object_input_list, src=0, group=None)",
        "api_description": "Scatters picklable objects in scatter_object_input_list to the whole group.",
        "return_value": "None. If rank is part of the group, scatter_object_output_list\nwill have its first element set to the scattered object for this rank.\n",
        "parameters": "scatter_object_output_list (List[Any]) – Non-empty list whose first\nelement will store the object scattered to this rank.\nscatter_object_input_list (List[Any]) – List of input objects to scatter.\nEach object must be picklable. Only objects on the src rank will\nbe scattered, and the argument can be None for non-src ranks.\nsrc (int) – Source rank from which to scatter scatter_object_input_list.\nSource rank is based on global process group (regardless of group argument).\ngroup – (ProcessGroup, optional): The process group to work on. If None,\nthe default process group will be used. Default is None.",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> # Note: Process group initialization omitted on each rank.\n>>> import torch.distributed as dist\n>>> if dist.get_rank() == 0:\n>>>     # Assumes world_size of 3.\n>>>     objects = [\"foo\", 12, {1: 2}] # any picklable object\n>>> else:\n>>>     # Can be any list on non-src ranks, elements are not used.\n>>>     objects = [None, None, None]\n>>> output_list = [None]\n>>> dist.scatter_object_list(output_list, objects, src=0)\n>>> # Rank i gets objects[i]. For example, on rank 2:\n>>> output_list\n[{1: 2}]\n\n\n"
    },
    {
        "api_name": "torch.scatter_reduce",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.scatter_reduce.html#torch.scatter_reduce",
        "api_signature": "torch.scatter_reduce(input, dim, index, src, reduce, *, include_self=True)",
        "api_description": "Out-of-place version of torch.Tensor.scatter_reduce_()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.scatter_reduce",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.scatter_reduce.html#torch.Tensor.scatter_reduce",
        "api_signature": "Tensor.scatter_reduce(dim, index, src, reduce, *, include_self=True)",
        "api_description": "Out-of-place version of torch.Tensor.scatter_reduce_()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.scatter_reduce_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.scatter_reduce_.html#torch.Tensor.scatter_reduce_",
        "api_signature": "Tensor.scatter_reduce_(dim, index, src, reduce, *, include_self=True)",
        "api_description": "Reduces all values from the src tensor to the indices specified in\nthe index tensor in the self tensor using the applied reduction\ndefined via the reduce argument (\"sum\", \"prod\", \"mean\",\n\"amax\", \"amin\"). For each value in src, it is reduced to an\nindex in self which is specified by its index in src for\ndimension != dim and by the corresponding value in index for\ndimension = dim. If include_self=\"True\", the values in the self\ntensor are included in the reduction.",
        "return_value": "",
        "parameters": "dim (int) – the axis along which to index\nindex (LongTensor) – the indices of elements to scatter and reduce.\nsrc (Tensor) – the source elements to scatter and reduce\nreduce (str) – the reduction operation to apply for non-unique indices\n(\"sum\", \"prod\", \"mean\", \"amax\", \"amin\")\ninclude_self (bool) – whether elements from the self tensor are\nincluded in the reduction",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.profiler.schedule",
        "api_url": "https://pytorch.org/docs/stable/profiler.html#torch.profiler.schedule",
        "api_signature": "torch.profiler.schedule(*, wait, warmup, active, repeat=0, skip_first=0)",
        "api_description": "Returns a callable that can be used as profiler schedule argument. The profiler will skip\nthe first skip_first steps, then wait for wait steps, then do the warmup for the next warmup steps,\nthen do the active recording for the next active steps and then repeat the cycle starting with wait steps.\nThe optional number of cycles is specified with the repeat parameter, the zero value means that\nthe cycles will continue until the profiling is finished.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.jit.script",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.jit.script.html#torch.jit.script",
        "api_signature": "torch.jit.script(obj, optimize=None, _frames_up=0, _rcb=None, example_inputs=None)",
        "api_description": "Script the function.",
        "return_value": "If obj is nn.Module, script returns\na ScriptModule object. The returned ScriptModule will\nhave the same set of sub-modules and parameters as the\noriginal nn.Module. If obj is a standalone function,\na ScriptFunction will be returned. If obj is a dict, then\nscript returns an instance of torch._C.ScriptDict. If obj is a list,\nthen script returns an instance of torch._C.ScriptList.\n",
        "parameters": "obj (Callable, class, or nn.Module) – The nn.Module, function, class type,\ndictionary, or list to compile.\nexample_inputs (Union[List[Tuple], Dict[Callable, List[Tuple]], None]) – Provide example inputs\nto annotate the arguments for a function or nn.Module.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.jit.script_if_tracing",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.jit.script_if_tracing.html#torch.jit.script_if_tracing",
        "api_signature": "torch.jit.script_if_tracing(fn)",
        "api_description": "Compiles fn when it is first called during tracing.",
        "return_value": "If called during tracing, a ScriptFunction created by torch.jit.script is returned.\nOtherwise, the original function fn is returned.\n",
        "parameters": "fn – A function to compile.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.jit.ScriptFunction",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.jit.ScriptFunction.html#torch.jit.ScriptFunction",
        "api_signature": null,
        "api_description": "Functionally equivalent to a ScriptModule, but represents a single\nfunction and does not have any attributes or Parameters.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.jit.ScriptModule",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.jit.ScriptModule.html#torch.jit.ScriptModule",
        "api_signature": null,
        "api_description": "Wrapper for C++ torch::jit::Module with methods, attributes, and parameters.",
        "return_value": "self\nself\nself\nself\nself\nself\nself\nThe buffer referenced by target\nAny extra state to store in the module’s state_dict\nThe Parameter referenced by target\nThe submodule referenced by target\nself\nself\n\nmissing_keys is a list of str containing the missing keys\nunexpected_keys is a list of str containing the unexpected keys\n\n\na handle that can be used to remove the added hook by calling\nhandle.remove()\na handle that can be used to remove the added hook by calling\nhandle.remove()\na handle that can be used to remove the added hook by calling\nhandle.remove()\na handle that can be used to remove the added hook by calling\nhandle.remove()\na handle that can be used to remove the added hook by calling\nhandle.remove()\na handle that can be used to remove the added hook by calling\nhandle.remove()\nself\na dictionary containing a whole state of the module\nself\nself\nself\nself\nself\n",
        "parameters": "name (str) – name of the child module. The child module can be\naccessed from this module using the given name\nmodule (Module) – child module to be added to the module.\nfn (Module -> None) – function to be applied to each submodule\nrecurse (bool) – if True, then yields buffers of this module\nand all submodules. Otherwise, yields only buffers that\nare direct members of this module.\ndevice (int, optional) – if specified, all parameters will be\ncopied to that device\ntarget (str) – The fully-qualified string name of the buffer\nto look for. (See get_submodule for how to specify a\nfully-qualified string.)\ntarget (str) – The fully-qualified string name of the Parameter\nto look for. (See get_submodule for how to specify a\nfully-qualified string.)\ntarget (str) – The fully-qualified string name of the submodule\nto look for. (See above example for how to specify a\nfully-qualified string.)\ndevice (int, optional) – if specified, all parameters will be\ncopied to that device\nstate_dict (dict) – a dict containing parameters and\npersistent buffers.\nstrict (bool, optional) – whether to strictly enforce that the keys\nin state_dict match the keys returned by this module’s\nstate_dict() function. Default: True\nassign (bool, optional) – When False, the properties of the tensors\nin the current module are preserved while when True, the\nproperties of the Tensors in the state dict are preserved. The only\nexception is the requires_grad field of\nDefault: ``False`\nprefix (str) – prefix to prepend to all buffer names.\nrecurse (bool, optional) – if True, then yields buffers of this module\nand all submodules. Otherwise, yields only buffers that\nare direct members of this module. Defaults to True.\nremove_duplicate (bool, optional) – whether to remove the duplicated buffers in the result. Defaults to True.\nmemo (Optional[Set[Module]]) – a memo to store the set of modules already added to the result\nprefix (str) – a prefix that will be added to the name of the module\nremove_duplicate (bool) – whether to remove the duplicated module instances in the result\nor not\nprefix (str) – prefix to prepend to all parameter names.\nrecurse (bool) – if True, then yields parameters of this module\nand all submodules. Otherwise, yields only parameters that\nare direct members of this module.\nremove_duplicate (bool, optional) – whether to remove the duplicated\nparameters in the result. Defaults to True.\nrecurse (bool) – if True, then yields parameters of this module\nand all submodules. Otherwise, yields only parameters that\nare direct members of this module.\nname (str) – name of the buffer. The buffer can be accessed\nfrom this module using the given name\ntensor (Tensor or None) – buffer to be registered. If None, then operations\nthat run on buffers, such as cuda, are ignored. If None,\nthe buffer is not included in the module’s state_dict.\npersistent (bool) – whether the buffer is part of this module’s\nstate_dict.\nhook (Callable) – The user defined hook to be registered.\nprepend (bool) – If True, the provided hook will be fired\nbefore all existing forward hooks on this\ntorch.nn.modules.Module. Otherwise, the provided\nhook will be fired after all existing forward hooks on\nthis torch.nn.modules.Module. Note that global\nforward hooks registered with\nregister_module_forward_hook() will fire before all hooks\nregistered by this method.\nDefault: False\nwith_kwargs (bool) – If True, the hook will be passed the\nkwargs given to the forward function.\nDefault: False\nalways_call (bool) – If True the hook will be run regardless of\nwhether an exception is raised while calling the Module.\nDefault: False\nhook (Callable) – The user defined hook to be registered.\nprepend (bool) – If true, the provided hook will be fired before\nall existing forward_pre hooks on this\ntorch.nn.modules.Module. Otherwise, the provided\nhook will be fired after all existing forward_pre hooks\non this torch.nn.modules.Module. Note that global\nforward_pre hooks registered with\nregister_module_forward_pre_hook() will fire before all\nhooks registered by this method.\nDefault: False\nwith_kwargs (bool) – If true, the hook will be passed the kwargs\ngiven to the forward function.\nDefault: False\nhook (Callable) – The user-defined hook to be registered.\nprepend (bool) – If true, the provided hook will be fired before\nall existing backward hooks on this\ntorch.nn.modules.Module. Otherwise, the provided\nhook will be fired after all existing backward hooks on\nthis torch.nn.modules.Module. Note that global\nbackward hooks registered with\nregister_module_full_backward_hook() will fire before\nall hooks registered by this method.\nhook (Callable) – The user-defined hook to be registered.\nprepend (bool) – If true, the provided hook will be fired before\nall existing backward_pre hooks on this\ntorch.nn.modules.Module. Otherwise, the provided\nhook will be fired after all existing backward_pre hooks\non this torch.nn.modules.Module. Note that global\nbackward_pre hooks registered with\nregister_module_full_backward_pre_hook() will fire before\nall hooks registered by this method.\nname (str) – name of the parameter. The parameter can be accessed\nfrom this module using the given name\nparam (Parameter or None) – parameter to be added to the module. If\nNone, then operations that run on parameters, such as cuda,\nare ignored. If None, the parameter is not included in the\nmodule’s state_dict.\nrequires_grad (bool) – whether autograd should record operations on\nparameters in this module. Default: True.\nstate (dict) – Extra state from the state_dict\ndestination (dict, optional) – If provided, the state of module will\nbe updated into the dict and the same object is returned.\nOtherwise, an OrderedDict will be created and returned.\nDefault: None.\nprefix (str, optional) – a prefix added to parameter and buffer\nnames to compose the keys in state_dict. Default: ''.\nkeep_vars (bool, optional) – by default the Tensor s\nreturned in the state dict are detached from autograd. If it’s\nset to True, detaching will not be performed.\nDefault: False.\ndevice (torch.device) – the desired device of the parameters\nand buffers in this module\ndtype (torch.dtype) – the desired floating point or complex dtype of\nthe parameters and buffers in this module\ntensor (torch.Tensor) – Tensor whose dtype and device are the desired\ndtype and device for all parameters and buffers in this module\nmemory_format (torch.memory_format) – the desired memory\nformat for 4D parameters and buffers in this module (keyword\nonly argument)\ndevice (torch.device) – The desired device of the parameters\nand buffers in this module.\nrecurse (bool) – Whether parameters and buffers of submodules should\nbe recursively moved to the specified device.\nmode (bool) – whether to set training mode (True) or evaluation\nmode (False). Default: True.\ndst_type (type or string) – the desired type\ndevice (int, optional) – if specified, all parameters will be\ncopied to that device\nset_to_none (bool) – instead of setting to zero, set the grads to None.\nSee torch.optim.Optimizer.zero_grad() for details.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.backends.cuda.sdp_kernel",
        "api_url": "https://pytorch.org/docs/stable/backends.html#torch.backends.cuda.sdp_kernel",
        "api_signature": "torch.backends.cuda.sdp_kernel(enable_flash=True, enable_math=True, enable_mem_efficient=True, enable_cudnn=True)",
        "api_description": "Warning",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.attention.sdpa_kernel",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.attention.sdpa_kernel.html#torch.nn.attention.sdpa_kernel",
        "api_signature": "torch.nn.attention.sdpa_kernel(backends)",
        "api_description": "Context manager to select which backend to use for scaled dot product attention.",
        "return_value": "",
        "parameters": "backend (Union[List[SDPBackend], SDPBackend]) – A backend or list of backends for scaled dot product attention.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.backends.cuda.SDPAParams",
        "api_url": "https://pytorch.org/docs/stable/backends.html#torch.backends.cuda.SDPAParams",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.attention.SDPBackend",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.attention.SDPBackend.html#torch.nn.attention.SDPBackend",
        "api_signature": null,
        "api_description": "An enum-like class that contains the different backends for scaled dot product attention.\nThis backend class is designed to be used with the sdpa_kernel context manager.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.searchsorted",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.searchsorted.html#torch.searchsorted",
        "api_signature": "torch.searchsorted(sorted_sequence, values, *, out_int32=False, right=False, side=None, out=None, sorter=None)",
        "api_description": "Find the indices from the innermost dimension of sorted_sequence such that, if the\ncorresponding values in values were inserted before the indices, when sorted, the order\nof the corresponding innermost dimension within sorted_sequence would be preserved.\nReturn a new tensor with the same size as values. More formally,\nthe returned index satisfies the following rules:",
        "return_value": "",
        "parameters": "sorted_sequence (Tensor) – N-D or 1-D tensor, containing monotonically increasing sequence on the innermost\ndimension unless sorter is provided, in which case the sequence does not\nneed to be sorted\nvalues (Tensor or Scalar) – N-D tensor or a Scalar containing the search value(s).\nout_int32 (bool, optional) – indicate the output data type. torch.int32 if True, torch.int64 otherwise.\nDefault value is False, i.e. default output data type is torch.int64.\nright (bool, optional) – if False, return the first suitable location that is found. If True, return the\nlast such index. If no suitable index found, return 0 for non-numerical value\n(eg. nan, inf) or the size of innermost dimension within sorted_sequence\n(one pass the last index of the innermost dimension). In other words, if False,\ngets the lower bound index for each value in values on the corresponding\ninnermost dimension of the sorted_sequence. If True, gets the upper\nbound index instead. Default value is False. side does the same and is\npreferred. It will error if side is set to “left” while this is True.\nside (str, optional) – the same as right but preferred. “left” corresponds to False for right\nand “right” corresponds to True for right. It will error if this is set to\n“left” while right is True. Default value is None.\nout (Tensor, optional) – the output tensor, must be the same size as values if provided.\nsorter (LongTensor, optional) – if provided, a tensor matching the shape of the unsorted\nsorted_sequence containing a sequence of indices that sort it in the\nascending order on the innermost dimension",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.autograd.profiler.EnforceUnique.see",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.autograd.profiler.EnforceUnique.html#torch.autograd.profiler.EnforceUnique.see",
        "api_signature": "see(*key)",
        "api_description": "Observe a key and raise an error if it is seen multiple times.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.seed",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.seed.html#torch.seed",
        "api_signature": "torch.seed()",
        "api_description": "Sets the seed for generating random numbers to a non-deterministic\nrandom number on all devices. Returns a 64 bit number used to seed the RNG.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda.seed",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.cuda.seed.html#torch.cuda.seed",
        "api_signature": "torch.cuda.seed()",
        "api_description": "Set the seed for generating random numbers to a random number for the current GPU.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.mps.seed",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.mps.seed.html#torch.mps.seed",
        "api_signature": "torch.mps.seed()",
        "api_description": "Sets the seed for generating random numbers to a random number.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.random.seed",
        "api_url": "https://pytorch.org/docs/stable/random.html#torch.random.seed",
        "api_signature": "torch.random.seed()",
        "api_description": "Sets the seed for generating random numbers to a non-deterministic\nrandom number on all devices. Returns a 64 bit number used to seed the RNG.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.xpu.seed",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.xpu.seed.html#torch.xpu.seed",
        "api_signature": "torch.xpu.seed()",
        "api_description": "Set the seed for generating random numbers to a random number for the current GPU.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Generator.seed",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator.seed",
        "api_signature": "seed()",
        "api_description": "Gets a non-deterministic random number from std::random_device or the current\ntime and uses it to seed a Generator.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda.seed_all",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.cuda.seed_all.html#torch.cuda.seed_all",
        "api_signature": "torch.cuda.seed_all()",
        "api_description": "Set the seed for generating random numbers to a random number on all GPUs.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.xpu.seed_all",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.xpu.seed_all.html#torch.xpu.seed_all",
        "api_signature": "torch.xpu.seed_all()",
        "api_description": "Set the seed for generating random numbers to a random number on all GPUs.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.select",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.select.html#torch.select",
        "api_signature": "torch.select(input, dim, index)",
        "api_description": "Slices the input tensor along the selected dimension at the given index.\nThis function returns a view of the original tensor with the given dimension removed.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.\ndim (int) – the dimension to slice\nindex (int) – the index to select with",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.select",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.select.html#torch.Tensor.select",
        "api_signature": "Tensor.select(dim, index)",
        "api_description": "See torch.select()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.onnx.select_model_mode_for_export",
        "api_url": "https://pytorch.org/docs/stable/onnx_torchscript.html#torch.onnx.select_model_mode_for_export",
        "api_signature": "torch.onnx.select_model_mode_for_export(model, mode)",
        "api_description": "A context manager to temporarily set the training mode of model\nto mode, resetting it when we exit the with-block.",
        "return_value": "",
        "parameters": "model – Same type and meaning as model arg to export().\nmode (TrainingMode) – Same type and meaning as training arg to export().",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.select_scatter",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.select_scatter.html#torch.select_scatter",
        "api_signature": "torch.select_scatter(input, src, dim, index)",
        "api_description": "Embeds the values of the src tensor into input at the given index.\nThis function returns a tensor with fresh storage; it does not create a view.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.\nsrc (Tensor) – The tensor to embed into input\ndim (int) – the dimension to insert the slice into.\nindex (int) – the index to select with",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.select_scatter",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.select_scatter.html#torch.Tensor.select_scatter",
        "api_signature": "Tensor.select_scatter(src, dim, index)",
        "api_description": "See torch.select_scatter()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.autograd.profiler.profile.self_cpu_time_total",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.autograd.profiler.profile.self_cpu_time_total.html#torch.autograd.profiler.profile.self_cpu_time_total",
        "api_signature": null,
        "api_description": "Returns total time spent on CPU.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.SELU",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.SELU.html#torch.nn.SELU",
        "api_signature": "torch.nn.SELU(inplace=False)",
        "api_description": "Applies the SELU function element-wise.",
        "return_value": "",
        "parameters": "inplace (bool, optional) – can optionally do the operation in-place. Default: False",
        "input_shape": "\nInput: (∗)(*)(∗), where ∗*∗ means any number of dimensions.\nOutput: (∗)(*)(∗), same shape as the input.\n\n",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.functional.selu",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.functional.selu.html#torch.nn.functional.selu",
        "api_signature": "torch.nn.functional.selu(input, inplace=False)",
        "api_description": "Applies element-wise,\nSELU(x)=scale∗(max⁡(0,x)+min⁡(0,α∗(exp⁡(x)−1)))\\text{SELU}(x) = scale * (\\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1)))SELU(x)=scale∗(max(0,x)+min(0,α∗(exp(x)−1))),\nwith α=1.6732632423543772848170429916717\\alpha=1.6732632423543772848170429916717α=1.6732632423543772848170429916717 and\nscale=1.0507009873554804934193349852946scale=1.0507009873554804934193349852946scale=1.0507009873554804934193349852946.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.send",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#torch.distributed.send",
        "api_signature": "torch.distributed.send(tensor, dst, group=None, tag=0)",
        "api_description": "Send a tensor synchronously.",
        "return_value": "",
        "parameters": "tensor (Tensor) – Tensor to send.\ndst (int) – Destination rank on global process group (regardless of group argument).\nDestination rank should not be the same as the rank of the current process.\ngroup (ProcessGroup, optional) – The process group to work on. If None,\nthe default process group will be used.\ntag (int, optional) – Tag to match send with remote recv",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.tensor.parallel.SequenceParallel",
        "api_url": "https://pytorch.org/docs/stable/distributed.tensor.parallel.html#torch.distributed.tensor.parallel.SequenceParallel",
        "api_signature": "torch.distributed.tensor.parallel.SequenceParallel(*, sequence_dim=1, use_local_output=False)",
        "api_description": "SequenceParallel replicates a compatible nn.Module parameters and runs the sharded computation with\ninput sharded on the sequence dimension. This currently supports nn.LayerNorm, nn.Dropout, and the\nRMSNorm python implementation",
        "return_value": "A ParallelStyle object that represents Sequence Parallel of the nn.Module.\n",
        "parameters": "sequence_dim (int, optional) – The sequence dimension of the input tensor for the nn.Module, this is used to annotate the input tensor to\nbecome a DTensor that is sharded on the sequence dimension, default: 1.\nuse_local_output (bool, optional) – Whether to use local torch.Tensor instead of DTensor for the module output, default: False.",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> from torch.distributed.tensor.parallel import parallelize_module, SequenceParallel\n>>> from torch.distributed.device_mesh import init_device_mesh\n>>> ...\n>>> m = Model(...)  # m is a nn.Module that contains a \"norm\" nn.LayerNorm submodule\n>>> tp_mesh = init_device_mesh(\"cuda\", (8,))\n>>>\n>>> # By default, the input of the \"norm\" will be converted to DTensor that shards on the sequence dim\n>>> # and the output of \"norm\" will return a sharded on sequence dimension :class:`DTensor`.\n>>>\n>>> sharded_mod = parallelize_module(m, tp_mesh, {\"norm\": SequenceParallel()}),\n>>> ...\n\n\n"
    },
    {
        "api_name": "torch.nn.Sequential",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html#torch.nn.Sequential",
        "api_signature": "torch.nn.Sequential(*args: Module)",
        "api_description": "",
        "return_value": "",
        "parameters": "module (nn.Module) – module to append",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.lr_scheduler.SequentialLR",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.SequentialLR.html#torch.optim.lr_scheduler.SequentialLR",
        "api_signature": "torch.optim.lr_scheduler.SequentialLR(optimizer, schedulers, milestones, last_epoch=-1, verbose='deprecated')",
        "api_description": "Receives the list of schedulers that is expected to be called sequentially during\noptimization process and milestone points that provides exact intervals to reflect\nwhich scheduler is supposed to be called at a given epoch.",
        "return_value": "",
        "parameters": "optimizer (Optimizer) – Wrapped optimizer.\nschedulers (list) – List of chained schedulers.\nmilestones (list) – List of integers that reflects milestone points.\nlast_epoch (int) – The index of last epoch. Default: -1.\nverbose (bool) – Does nothing.\nDeprecated since version 2.2: verbose is deprecated. Please use get_last_lr() to access the\nlearning rate.\nstate_dict (dict) – scheduler state. Should be an object returned\nfrom a call to state_dict().",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.data.SequentialSampler",
        "api_url": "https://pytorch.org/docs/stable/data.html#torch.utils.data.SequentialSampler",
        "api_signature": "torch.utils.data.SequentialSampler(data_source)",
        "api_description": "Samples elements sequentially, always in the same order.",
        "return_value": "",
        "parameters": "data_source (Dataset) – dataset to sample from",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.onnx.ONNXProgramSerializer.serialize",
        "api_url": "https://pytorch.org/docs/stable/onnx_dynamo.html#torch.onnx.ONNXProgramSerializer.serialize",
        "api_signature": "serialize(onnx_program, destination)",
        "api_description": "Protocol method that must be implemented for serialization.",
        "return_value": "",
        "parameters": "onnx_program (ONNXProgram) – Represents the in-memory exported ONNX model\ndestination (BufferedIOBase) – A binary IO stream or pre-allocated buffer into which\nthe serialized model should be written.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.Store.set",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#torch.distributed.Store.set",
        "api_signature": "torch.distributed.Store.set(self: torch._C._distributed_c10d.Store, arg0: str, arg1: str)",
        "api_description": "Inserts the key-value pair into the store based on the supplied key and\nvalue. If key already exists in the store, it will overwrite the old\nvalue with the new supplied value.",
        "return_value": "",
        "parameters": "key (str) – The key to be added to the store.\nvalue (str) – The value associated with key to be added to the store.",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n>>> store.set(\"first_key\", \"first_value\")\n>>> # Should return \"first_value\"\n>>> store.get(\"first_key\")\n\n\n"
    },
    {
        "api_name": "torch.distributed.elastic.rendezvous.etcd_store.EtcdStore.set",
        "api_url": "https://pytorch.org/docs/stable/elastic/rendezvous.html#torch.distributed.elastic.rendezvous.etcd_store.EtcdStore.set",
        "api_signature": "set(key, value)",
        "api_description": "Write a key/value pair into EtcdStore.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.set_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.set_.html#torch.Tensor.set_",
        "api_signature": "Tensor.set_(source=None, storage_offset=0, size=None, stride=None)",
        "api_description": "Sets the underlying storage, size, and strides. If source is a tensor,\nself tensor will share the same storage and have the same size and\nstrides as source. Changes to elements in one tensor will be reflected\nin the other.",
        "return_value": "",
        "parameters": "source (Tensor or Storage) – the tensor or storage to use\nstorage_offset (int, optional) – the offset in the storage\nsize (torch.Size, optional) – the desired size. Defaults to the size of the source.\nstride (tuple, optional) – the desired stride. Defaults to C-contiguous strides.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.backend_config.BackendConfig.set_backend_pattern_config",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.quantization.backend_config.BackendConfig.html#torch.ao.quantization.backend_config.BackendConfig.set_backend_pattern_config",
        "api_signature": "set_backend_pattern_config(config)",
        "api_description": "Set the config for an pattern that can be run on the target backend.\nThis overrides any existing config for the given pattern.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.backend_config.BackendConfig.set_backend_pattern_configs",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.quantization.backend_config.BackendConfig.html#torch.ao.quantization.backend_config.BackendConfig.set_backend_pattern_configs",
        "api_signature": "set_backend_pattern_configs(configs)",
        "api_description": "Set the configs for patterns that can be run on the target backend.\nThis overrides any existing config for a given pattern if it was previously registered already.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.GradBucket.set_buffer",
        "api_url": "https://pytorch.org/docs/stable/ddp_comm_hooks.html#torch.distributed.GradBucket.set_buffer",
        "api_signature": "torch.distributed.GradBucket.set_buffer(self: torch._C._distributed_c10d.GradBucket, buffer: torch.Tensor)",
        "api_description": "Replaces the tensor in the bucket with the input tensor buffer.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.checkpoint.set_checkpoint_debug_enabled",
        "api_url": "https://pytorch.org/docs/stable/checkpoint.html#torch.utils.checkpoint.set_checkpoint_debug_enabled",
        "api_signature": "torch.utils.checkpoint.set_checkpoint_debug_enabled(enabled)",
        "api_description": "Context manager that sets whether checkpoint should print additional debug\ninformation when running. See the debug flag for\ncheckpoint() for more information. Note that\nwhen set, this context manager overrides the value of debug passed to\ncheckpoint. To defer to the local setting, pass None to this context.",
        "return_value": "",
        "parameters": "enabled (bool) – Whether checkpoint should print debug information.\nDefault is ‘None’.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.rendezvous.RendezvousHandler.set_closed",
        "api_url": "https://pytorch.org/docs/stable/elastic/rendezvous.html#torch.distributed.elastic.rendezvous.RendezvousHandler.set_closed",
        "api_signature": "set_closed()",
        "api_description": "Mark the rendezvous as closed.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.Graph.set_codegen",
        "api_url": "https://pytorch.org/docs/stable/fx.html#torch.fx.Graph.set_codegen",
        "api_signature": "set_codegen(codegen)",
        "api_description": "Warning",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.set_default_device",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.set_default_device.html#torch.set_default_device",
        "api_signature": "torch.set_default_device(device)",
        "api_description": "Sets the default torch.Tensor to be allocated on device.  This\ndoes not affect factory function calls which are called with an explicit\ndevice argument.  Factory calls will be performed as if they\nwere passed device as an argument.",
        "return_value": "",
        "parameters": "device (device or string) – the device to set as default",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.set_default_dtype",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.set_default_dtype.html#torch.set_default_dtype",
        "api_signature": "torch.set_default_dtype(d)",
        "api_description": "Sets the default floating point dtype to d. Supports torch.float32\nand torch.float64 as inputs. Other dtypes may be accepted without complaint\nbut are not supported and are unlikely to work as expected.",
        "return_value": "",
        "parameters": "d (torch.dtype) – the floating point dtype to make the default.\nEither torch.float32 or torch.float64.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.serialization.set_default_load_endianness",
        "api_url": "https://pytorch.org/docs/stable/notes/serialization.html#torch.serialization.set_default_load_endianness",
        "api_signature": "torch.serialization.set_default_load_endianness(endianness)",
        "api_description": "Set fallback byte order for loading files",
        "return_value": "",
        "parameters": "endianness – the new fallback byte order",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.set_default_tensor_type",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.set_default_tensor_type.html#torch.set_default_tensor_type",
        "api_signature": "torch.set_default_tensor_type(t)",
        "api_description": "Warning",
        "return_value": "",
        "parameters": "t (type or string) – the floating point tensor type or its name",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.distribution.Distribution.set_default_validate_args",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.distribution.Distribution.set_default_validate_args",
        "api_signature": "set_default_validate_args(value)",
        "api_description": "Sets whether validation is enabled or disabled.",
        "return_value": "",
        "parameters": "value (bool) – Whether to enable validation.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.autograd.set_detect_anomaly",
        "api_url": "https://pytorch.org/docs/stable/autograd.html#torch.autograd.set_detect_anomaly",
        "api_signature": "torch.autograd.set_detect_anomaly(mode, check_nan=True)",
        "api_description": "Context-manager that sets the anomaly detection for the autograd engine on or off.",
        "return_value": "",
        "parameters": "mode (bool) – Flag whether to enable anomaly detection (True),\nor disable (False).\ncheck_nan (bool) – Flag whether to raise an error when the backward\ngenerate “nan”",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.set_deterministic_debug_mode",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.set_deterministic_debug_mode.html#torch.set_deterministic_debug_mode",
        "api_signature": "torch.set_deterministic_debug_mode(debug_mode)",
        "api_description": "Sets the debug mode for deterministic operations.",
        "return_value": "",
        "parameters": "debug_mode (str or int) – If “default” or 0, don’t error or warn on\nnondeterministic operations. If “warn” or 1, warn on\nnondeterministic operations. If “error” or 2, error on\nnondeterministic operations.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cpu.set_device",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.cpu.set_device.html#torch.cpu.set_device",
        "api_signature": "torch.cpu.set_device(device)",
        "api_description": "Sets the current device, in CPU we do nothing.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda.set_device",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.cuda.set_device.html#torch.cuda.set_device",
        "api_signature": "torch.cuda.set_device(device)",
        "api_description": "Set the current device.",
        "return_value": "",
        "parameters": "device (torch.device or int) – selected device. This function is a no-op\nif this argument is negative.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.xpu.set_device",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.xpu.set_device.html#torch.xpu.set_device",
        "api_signature": "torch.xpu.set_device(device)",
        "api_description": "Set the current device.",
        "return_value": "",
        "parameters": "device (torch.device or int or str) – selected device. This function is a\nno-op if this argument is negative.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.rpc.TensorPipeRpcBackendOptions.set_device_map",
        "api_url": "https://pytorch.org/docs/stable/rpc.html#torch.distributed.rpc.TensorPipeRpcBackendOptions.set_device_map",
        "api_signature": "set_device_map(to, device_map)",
        "api_description": "Set device mapping between each RPC caller and callee pair. This\nfunction can be called multiple times to incrementally add\ndevice placement configurations.",
        "return_value": "",
        "parameters": "to (str) – Callee name.\ndevice_map (Dict of int, str, or torch.device) – Device placement\nmappings from this worker to the callee. This map must be\ninvertible.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.rpc.TensorPipeRpcBackendOptions.set_devices",
        "api_url": "https://pytorch.org/docs/stable/rpc.html#torch.distributed.rpc.TensorPipeRpcBackendOptions.set_devices",
        "api_signature": "set_devices(devices)",
        "api_description": "Set local devices used by the TensorPipe RPC agent. When processing\nCUDA RPC requests, the TensorPipe RPC agent will properly synchronize\nCUDA streams for all devices in this List.",
        "return_value": "",
        "parameters": "devices (List of int, str, or torch.device) – local devices used by\nthe TensorPipe RPC agent.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.hub.set_dir",
        "api_url": "https://pytorch.org/docs/stable/hub.html#torch.hub.set_dir",
        "api_signature": "torch.hub.set_dir(d)",
        "api_description": "Optionally set the Torch Hub directory used to save downloaded models & weights.",
        "return_value": "",
        "parameters": "d (str) – path to a local folder to save downloaded models & weights.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.backend_config.BackendPatternConfig.set_dtype_configs",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.quantization.backend_config.BackendPatternConfig.html#torch.ao.quantization.backend_config.BackendPatternConfig.set_dtype_configs",
        "api_signature": "set_dtype_configs(dtype_configs)",
        "api_description": "Set the supported data types passed as arguments to quantize ops in the\nreference model spec, overriding all previously registered data types.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.futures.Future.set_exception",
        "api_url": "https://pytorch.org/docs/stable/futures.html#torch.futures.Future.set_exception",
        "api_signature": "set_exception(result)",
        "api_description": "Set an exception for this Future, which will mark this Future as\ncompleted with an error and trigger all attached callbacks. Note that\nwhen calling wait()/value() on this Future, the exception set here\nwill be raised inline.",
        "return_value": "",
        "parameters": "result (BaseException) – the exception for this Future.",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> fut = torch.futures.Future()\n>>> fut.set_exception(ValueError(\"foo\"))\n>>> fut.wait()\nTraceback (most recent call last):\n...\nValueError: foo\n\n\n"
    },
    {
        "api_name": "torch.jit.ScriptModule.set_extra_state",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.jit.ScriptModule.html#torch.jit.ScriptModule.set_extra_state",
        "api_signature": "set_extra_state(state)",
        "api_description": "Set extra state contained in the loaded state_dict.",
        "return_value": "",
        "parameters": "state (dict) – Extra state from the state_dict",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.Module.set_extra_state",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.set_extra_state",
        "api_signature": "set_extra_state(state)",
        "api_description": "Set extra state contained in the loaded state_dict.",
        "return_value": "",
        "parameters": "state (dict) – Extra state from the state_dict",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.backends.mha.set_fastpath_enabled",
        "api_url": "https://pytorch.org/docs/stable/backends.html#torch.backends.mha.set_fastpath_enabled",
        "api_signature": "torch.backends.mha.set_fastpath_enabled(value)",
        "api_description": "Sets whether fast path is enabled",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.backends.nnpack.set_flags",
        "api_url": "https://pytorch.org/docs/stable/backends.html#torch.backends.nnpack.set_flags",
        "api_signature": "torch.backends.nnpack.set_flags(_enabled)",
        "api_description": "Set if nnpack is enabled globally",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.set_float32_matmul_precision",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision",
        "api_signature": "torch.set_float32_matmul_precision(precision)",
        "api_description": "Sets the internal precision of float32 matrix multiplications.",
        "return_value": "",
        "parameters": "precision (str) – can be set to “highest” (default), “high”, or “medium” (see above).",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.fx.custom_config.PrepareCustomConfig.set_float_to_observed_mapping",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.quantization.fx.custom_config.PrepareCustomConfig.html#torch.ao.quantization.fx.custom_config.PrepareCustomConfig.set_float_to_observed_mapping",
        "api_signature": "set_float_to_observed_mapping(float_class, observed_class, quant_type=QuantType.STATIC)",
        "api_description": "Set the mapping from a custom float module class to a custom observed module class.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.set_flush_denormal",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.set_flush_denormal.html#torch.set_flush_denormal",
        "api_signature": "torch.set_flush_denormal(mode)",
        "api_description": "Disables denormal floating numbers on CPU.",
        "return_value": "",
        "parameters": "mode (bool) – Controls whether to enable flush denormal mode or not",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.backend_config.BackendPatternConfig.set_fused_module",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.quantization.backend_config.BackendPatternConfig.html#torch.ao.quantization.backend_config.BackendPatternConfig.set_fused_module",
        "api_signature": "set_fused_module(fused_module)",
        "api_description": "Set the module that represents the fused implementation for this pattern.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.backend_config.BackendPatternConfig.set_fuser_method",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.quantization.backend_config.BackendPatternConfig.html#torch.ao.quantization.backend_config.BackendPatternConfig.set_fuser_method",
        "api_signature": "set_fuser_method(fuser_method)",
        "api_description": "Set the function that specifies how to fuse this BackendPatternConfig’s pattern.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.jit.set_fusion_strategy",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.jit.set_fusion_strategy.html#torch.jit.set_fusion_strategy",
        "api_signature": "torch.jit.set_fusion_strategy(strategy)",
        "api_description": "Set the type and number of specializations that can occur during fusion.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.qconfig_mapping.QConfigMapping.set_global",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.quantization.qconfig_mapping.QConfigMapping.html#torch.ao.quantization.qconfig_mapping.QConfigMapping.set_global",
        "api_signature": "set_global(global_qconfig)",
        "api_description": "Set the global (default) QConfig.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.autograd.grad_mode.set_grad_enabled",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.autograd.grad_mode.set_grad_enabled.html#torch.autograd.grad_mode.set_grad_enabled",
        "api_signature": "torch.autograd.grad_mode.set_grad_enabled(mode)",
        "api_description": "Context-manager that sets gradient calculation on or off.",
        "return_value": "",
        "parameters": "mode (bool) – Flag whether to enable grad (True), or disable\n(False). This can be used to conditionally enable\ngradients.",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> x = torch.tensor([1.], requires_grad=True)\n>>> is_train = False\n>>> with torch.set_grad_enabled(is_train):\n...     y = x * 2\n>>> y.requires_grad\nFalse\n>>> _ = torch.set_grad_enabled(True)\n>>> y = x * 2\n>>> y.requires_grad\nTrue\n>>> _ = torch.set_grad_enabled(False)\n>>> y = x * 2\n>>> y.requires_grad\nFalse\n\n\n"
    },
    {
        "api_name": "torch.ao.quantization.fx.custom_config.PrepareCustomConfig.set_input_quantized_indexes",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.quantization.fx.custom_config.PrepareCustomConfig.html#torch.ao.quantization.fx.custom_config.PrepareCustomConfig.set_input_quantized_indexes",
        "api_signature": "set_input_quantized_indexes(indexes)",
        "api_description": "Set the indexes of the inputs of the graph that should be quantized.\nInputs are otherwise assumed to be in fp32 by default instead.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch._logging.set_logs",
        "api_url": "https://pytorch.org/docs/stable/generated/torch._logging.set_logs.html#torch._logging.set_logs",
        "api_signature": "torch._logging.set_logs(*, all=None, dynamo=None, aot=None, autograd=None, dynamic=None, inductor=None, distributed=None, dist_c10d=None, dist_ddp=None, dist_fsdp=None, onnx=None, bytecode=False, aot_graphs=False, aot_joint_graph=False, ddp_graphs=False, graph=False, graph_code=False, graph_breaks=False, graph_sizes=False, guards=False, recompiles=False, recompiles_verbose=False, trace_source=False, trace_call=False, output_code=False, schedule=False, perf_hints=False, post_grad_graphs=False, onnx_diagnostics=False, fusion=False, overlap=False, export=None, modules=None, cudagraphs=False, sym_node=False)",
        "api_description": "Sets the log level for individual components and toggles individual log\nartifact types.",
        "return_value": "",
        "parameters": "all (Optional[int]) – The default log level for all components. Default: logging.WARN\ndynamo (Optional[int]) – The log level for the TorchDynamo component. Default: logging.WARN\naot (Optional[int]) – The log level for the AOTAutograd component. Default: logging.WARN\nautograd (Optional[int]) – The log level for autograd. Default: logging.WARN\ninductor (Optional[int]) – The log level for the TorchInductor component. Default: logging.WARN\ndynamic (Optional[int]) – The log level for dynamic shapes. Default: logging.WARN\nDefault: logging.WARN\nDefault: logging.WARN\nDefault: ``logging.WARN\nDefault: ``logging.WARN\nonnx (Optional[int]) – The log level for the ONNX exporter component. Default: logging.WARN\nbytecode (bool) – Whether to emit the original and generated bytecode from TorchDynamo.\nDefault: False\naot_graphs (bool) – Whether to emit the graphs generated by AOTAutograd. Default: False\naot_joint_graph (bool) – Whether to emit the joint forward-backward graph generated by AOTAutograd. Default: False\ninductor – Whether to log information from inductor cudagraphs. Default: logging.WARN\nddp_graphs (bool) – Whether to emit graphs generated by DDPOptimizer. Default: False\ngraph (bool) – Whether to emit the graph captured by TorchDynamo in tabular format.\nDefault: False\ngraph_code (bool) – Whether to emit the python source of the graph captured by TorchDynamo.\nDefault: False\ngraph_breaks (bool) – Whether to emit the graph breaks encountered by TorchDynamo.\nDefault: False\ngraph_sizes (bool) – Whether to emit tensor sizes of the graph captured by TorchDynamo.\nDefault: False\nguards (bool) – Whether to emit the guards generated by TorchDynamo for each compiled\nfunction. Default: False\nrecompiles (bool) – Whether to emit a guard failure reason and message every time\nTorchDynamo recompiles a function. Default: False\nrecompiles_verbose (bool) – Whether to emit all guard failure reasons when TorchDynamo recompiles\na function, even those that are not actually run. Default: False\ntrace_source (bool) – Whether to emit when TorchDynamo begins tracing a new line. Default: False\ntrace_call (bool) – Whether to emit detailed line location when TorchDynamo creates an FX node\ncorresponding to function call. Python 3.11+ only. Default: False\noutput_code (bool) – Whether to emit the TorchInductor output code. Default: False\nschedule (bool) – Whether to emit the TorchInductor schedule. Default: False\nperf_hints (bool) – Whether to emit the TorchInductor perf hints. Default: False\npost_grad_graphs (bool) – Whether to emit the graphs generated by after post grad passes. Default: False\nonnx_diagnostics (bool) – Whether to emit the ONNX exporter diagnostics in logging. Default: False\nfusion (bool) – Whether to emit detailed Inductor fusion decisions. Default: False\noverlap (bool) – Whether to emit detailed Inductor compute/comm overlap decisions. Default: False\nsym_node (bool) – Whether to emit debug info for various SymNode opterations. Default: False\nexport (Optional[int]) – The log level for export. Default: logging.WARN\nmodules (dict) – This argument provides an alternate way to specify the above log\ncomponent and artifact settings, in the format of a keyword args\ndictionary given as a single argument. There are two cases\nwhere this is useful (1) if a new log component or artifact has\nbeen registered but a keyword argument for it has not been added\nto this function and (2) if the log level for an unregistered module\nneeds to be set. This can be done by providing the fully-qualified module\nname as the key, with the log level as the value. Default: None",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.autograd.function.BackwardCFunction.set_materialize_grads",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.autograd.function.BackwardCFunction.html#torch.autograd.function.BackwardCFunction.set_materialize_grads",
        "api_signature": "set_materialize_grads(value)",
        "api_description": "Set whether to materialize grad tensors. Default is True.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> class SimpleFunc(Function):\n>>>     @staticmethod\n>>>     def forward(ctx, x):\n>>>         return x.clone(), x.clone()\n>>>\n>>>     @staticmethod\n>>>     @once_differentiable\n>>>     def backward(ctx, g1, g2):\n>>>         return g1 + g2  # No check for None necessary\n>>>\n>>> # We modify SimpleFunc to handle non-materialized grad outputs\n>>> class Func(Function):\n>>>     @staticmethod\n>>>     def forward(ctx, x):\n>>>         ctx.set_materialize_grads(False)\n>>>         ctx.save_for_backward(x)\n>>>         return x.clone(), x.clone()\n>>>\n>>>     @staticmethod\n>>>     @once_differentiable\n>>>     def backward(ctx, g1, g2):\n>>>         x, = ctx.saved_tensors\n>>>         grad_input = torch.zeros_like(x)\n>>>         if g1 is not None:  # We must check for None now\n>>>             grad_input += g1\n>>>         if g2 is not None:\n>>>             grad_input += g2\n>>>         return grad_input\n>>>\n>>> a = torch.tensor(1., requires_grad=True)\n>>> b, _ = Func.apply(a)  # induces g2 to be undefined\n\n\n"
    },
    {
        "api_name": "torch.autograd.function.FunctionCtx.set_materialize_grads",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.autograd.function.FunctionCtx.set_materialize_grads.html#torch.autograd.function.FunctionCtx.set_materialize_grads",
        "api_signature": "FunctionCtx.set_materialize_grads(value)",
        "api_description": "Set whether to materialize grad tensors. Default is True.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> class SimpleFunc(Function):\n>>>     @staticmethod\n>>>     def forward(ctx, x):\n>>>         return x.clone(), x.clone()\n>>>\n>>>     @staticmethod\n>>>     @once_differentiable\n>>>     def backward(ctx, g1, g2):\n>>>         return g1 + g2  # No check for None necessary\n>>>\n>>> # We modify SimpleFunc to handle non-materialized grad outputs\n>>> class Func(Function):\n>>>     @staticmethod\n>>>     def forward(ctx, x):\n>>>         ctx.set_materialize_grads(False)\n>>>         ctx.save_for_backward(x)\n>>>         return x.clone(), x.clone()\n>>>\n>>>     @staticmethod\n>>>     @once_differentiable\n>>>     def backward(ctx, g1, g2):\n>>>         x, = ctx.saved_tensors\n>>>         grad_input = torch.zeros_like(x)\n>>>         if g1 is not None:  # We must check for None now\n>>>             grad_input += g1\n>>>         if g2 is not None:\n>>>             grad_input += g2\n>>>         return grad_input\n>>>\n>>> a = torch.tensor(1., requires_grad=True)\n>>> b, _ = Func.apply(a)  # induces g2 to be undefined\n\n\n"
    },
    {
        "api_name": "torch.autograd.function.InplaceFunction.set_materialize_grads",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.autograd.function.InplaceFunction.html#torch.autograd.function.InplaceFunction.set_materialize_grads",
        "api_signature": "set_materialize_grads(value)",
        "api_description": "Set whether to materialize grad tensors. Default is True.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> class SimpleFunc(Function):\n>>>     @staticmethod\n>>>     def forward(ctx, x):\n>>>         return x.clone(), x.clone()\n>>>\n>>>     @staticmethod\n>>>     @once_differentiable\n>>>     def backward(ctx, g1, g2):\n>>>         return g1 + g2  # No check for None necessary\n>>>\n>>> # We modify SimpleFunc to handle non-materialized grad outputs\n>>> class Func(Function):\n>>>     @staticmethod\n>>>     def forward(ctx, x):\n>>>         ctx.set_materialize_grads(False)\n>>>         ctx.save_for_backward(x)\n>>>         return x.clone(), x.clone()\n>>>\n>>>     @staticmethod\n>>>     @once_differentiable\n>>>     def backward(ctx, g1, g2):\n>>>         x, = ctx.saved_tensors\n>>>         grad_input = torch.zeros_like(x)\n>>>         if g1 is not None:  # We must check for None now\n>>>             grad_input += g1\n>>>         if g2 is not None:\n>>>             grad_input += g2\n>>>         return grad_input\n>>>\n>>> a = torch.tensor(1., requires_grad=True)\n>>> b, _ = Func.apply(a)  # induces g2 to be undefined\n\n\n"
    },
    {
        "api_name": "torch.autograd.function.NestedIOFunction.set_materialize_grads",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.autograd.function.NestedIOFunction.html#torch.autograd.function.NestedIOFunction.set_materialize_grads",
        "api_signature": "set_materialize_grads(value)",
        "api_description": "Set whether to materialize grad tensors. Default is True.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> class SimpleFunc(Function):\n>>>     @staticmethod\n>>>     def forward(ctx, x):\n>>>         return x.clone(), x.clone()\n>>>\n>>>     @staticmethod\n>>>     @once_differentiable\n>>>     def backward(ctx, g1, g2):\n>>>         return g1 + g2  # No check for None necessary\n>>>\n>>> # We modify SimpleFunc to handle non-materialized grad outputs\n>>> class Func(Function):\n>>>     @staticmethod\n>>>     def forward(ctx, x):\n>>>         ctx.set_materialize_grads(False)\n>>>         ctx.save_for_backward(x)\n>>>         return x.clone(), x.clone()\n>>>\n>>>     @staticmethod\n>>>     @once_differentiable\n>>>     def backward(ctx, g1, g2):\n>>>         x, = ctx.saved_tensors\n>>>         grad_input = torch.zeros_like(x)\n>>>         if g1 is not None:  # We must check for None now\n>>>             grad_input += g1\n>>>         if g2 is not None:\n>>>             grad_input += g2\n>>>         return grad_input\n>>>\n>>> a = torch.tensor(1., requires_grad=True)\n>>> b, _ = Func.apply(a)  # induces g2 to be undefined\n\n\n"
    },
    {
        "api_name": "torch.distributed.checkpoint.state_dict.set_model_state_dict",
        "api_url": "https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.set_model_state_dict",
        "api_signature": "torch.distributed.checkpoint.state_dict.set_model_state_dict(model, model_state_dict, *, options=None)",
        "api_description": "Load the model state_dict.",
        "return_value": "\nmissing_keys is a list of str containing the missing keys\nunexpected_keys is a list of str containing the unexpected keys\n\n\n",
        "parameters": "model (nn.Module) – the nn.Module to the model.\nmodel_state_dict (Dict[str, ValueType]) – (Dict[str, ValueType]):\nthe model state_dict to load. If the key of the model_state_dict\nis nn.Module, the key is a submodule of model and the value should\nbe the state_dict of the submodule. When loading the state_dict,\nthe prefix of the submodule will be append to the state_dict.\noptions (StateDictOptions) – the options to control how\nmodel state_dict and optimizer state_dict should be loaded. See\nStateDictOptions for the details.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.set_module",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.utils.set_module.html#torch.utils.set_module",
        "api_signature": "torch.utils.set_module(obj, mod)",
        "api_description": "Set the module attribute on a python object for a given object for nicer printing",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.qconfig_mapping.QConfigMapping.set_module_name",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.quantization.qconfig_mapping.QConfigMapping.html#torch.ao.quantization.qconfig_mapping.QConfigMapping.set_module_name",
        "api_signature": "set_module_name(module_name, qconfig)",
        "api_description": "Set the QConfig for modules matching the given module name.\nIf the QConfig for an existing module name was already set, the new QConfig will override the old one.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.qconfig_mapping.QConfigMapping.set_module_name_object_type_order",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.quantization.qconfig_mapping.QConfigMapping.html#torch.ao.quantization.qconfig_mapping.QConfigMapping.set_module_name_object_type_order",
        "api_signature": "set_module_name_object_type_order(module_name, object_type, index, qconfig)",
        "api_description": "Set the QConfig for modules matching a combination of the given module name, object type,\nand the index at which the module appears.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.qconfig_mapping.QConfigMapping.set_module_name_regex",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.quantization.qconfig_mapping.QConfigMapping.html#torch.ao.quantization.qconfig_mapping.QConfigMapping.set_module_name_regex",
        "api_signature": "set_module_name_regex(module_name_regex, qconfig)",
        "api_description": "Set the QConfig for modules matching the given regex string.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.autograd.grad_mode.set_multithreading_enabled",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.autograd.grad_mode.set_multithreading_enabled.html#torch.autograd.grad_mode.set_multithreading_enabled",
        "api_signature": "torch.autograd.grad_mode.set_multithreading_enabled(mode)",
        "api_description": "Context-manager that sets multithreaded backwards on or off.",
        "return_value": "",
        "parameters": "mode (bool) – Flag whether to enable multithreaded backwards (True), or disable\n(False).",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.backend_config.BackendConfig.set_name",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.quantization.backend_config.BackendConfig.html#torch.ao.quantization.backend_config.BackendConfig.set_name",
        "api_signature": "set_name(name)",
        "api_description": "Set the name of the target backend.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.fx.custom_config.PrepareCustomConfig.set_non_traceable_module_classes",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.quantization.fx.custom_config.PrepareCustomConfig.html#torch.ao.quantization.fx.custom_config.PrepareCustomConfig.set_non_traceable_module_classes",
        "api_signature": "set_non_traceable_module_classes(module_classes)",
        "api_description": "Set the modules that are not symbolically traceable, identified by class.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.fx.custom_config.PrepareCustomConfig.set_non_traceable_module_names",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.quantization.fx.custom_config.PrepareCustomConfig.html#torch.ao.quantization.fx.custom_config.PrepareCustomConfig.set_non_traceable_module_names",
        "api_signature": "set_non_traceable_module_names(module_names)",
        "api_description": "Set the modules that are not symbolically traceable, identified by name.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.set_num_interop_threads",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.set_num_interop_threads.html#torch.set_num_interop_threads",
        "api_signature": "torch.set_num_interop_threads(int)",
        "api_description": "Sets the number of threads used for interop parallelism\n(e.g. in JIT interpreter) on CPU.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.set_num_threads",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.set_num_threads.html#torch.set_num_threads",
        "api_signature": "torch.set_num_threads(int)",
        "api_description": "Sets the number of threads used for intraop parallelism on CPU.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.qconfig_mapping.QConfigMapping.set_object_type",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.quantization.qconfig_mapping.QConfigMapping.html#torch.ao.quantization.qconfig_mapping.QConfigMapping.set_object_type",
        "api_signature": "set_object_type(object_type, qconfig)",
        "api_description": "Set the QConfig for a given module type, function, or method name.\nIf the QConfig for an existing object type was already set, the new QConfig will override the old one.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.backend_config.BackendPatternConfig.set_observation_type",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.quantization.backend_config.BackendPatternConfig.html#torch.ao.quantization.backend_config.BackendPatternConfig.set_observation_type",
        "api_signature": "set_observation_type(observation_type)",
        "api_description": "Set how observers should be inserted in the graph for this pattern.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.fx.custom_config.ConvertCustomConfig.set_observed_to_quantized_mapping",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.quantization.fx.custom_config.ConvertCustomConfig.html#torch.ao.quantization.fx.custom_config.ConvertCustomConfig.set_observed_to_quantized_mapping",
        "api_signature": "set_observed_to_quantized_mapping(observed_class, quantized_class, quant_type=QuantType.STATIC)",
        "api_description": "Set the mapping from a custom observed module class to a custom quantized module class.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.checkpoint.state_dict.set_optimizer_state_dict",
        "api_url": "https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.set_optimizer_state_dict",
        "api_signature": "torch.distributed.checkpoint.state_dict.set_optimizer_state_dict(model, optimizers, *, optim_state_dict, options=None)",
        "api_description": "Load the optimizers state_dict.",
        "return_value": "None\n",
        "parameters": "model (nn.Module) – the nn.Module to the model.\noptimizers (Union[Optimizer, Iterable[Optimizer]]) – The optimizers that are used to optimize model.\noptim_state_dict (OptimizerStateType) – OptimizerStateType:\nthe optimizer state_dict to load.\noptions (StateDictOptions) – the options to control how\nmodel state_dict and optimizer state_dict should be loaded. See\nStateDictOptions for the details.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.fx.custom_config.PrepareCustomConfig.set_output_quantized_indexes",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.quantization.fx.custom_config.PrepareCustomConfig.html#torch.ao.quantization.fx.custom_config.PrepareCustomConfig.set_output_quantized_indexes",
        "api_signature": "set_output_quantized_indexes(indexes)",
        "api_description": "Set the indexes of the outputs of the graph that should be quantized.\nOutputs are otherwise assumed to be in fp32 by default instead.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.__future__.set_overwrite_module_params_on_conversion",
        "api_url": "https://pytorch.org/docs/stable/future_mod.html#torch.__future__.set_overwrite_module_params_on_conversion",
        "api_signature": "torch.__future__.set_overwrite_module_params_on_conversion(value)",
        "api_description": "Sets whether to assign new tensors to the parameters instead of changing the\nexisting parameters in-place when converting an nn.Module.",
        "return_value": "",
        "parameters": "value (bool) – Whether to assign new tensors or not.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.backend_config.BackendPatternConfig.set_pattern",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.quantization.backend_config.BackendPatternConfig.html#torch.ao.quantization.backend_config.BackendPatternConfig.set_pattern",
        "api_signature": "set_pattern(pattern)",
        "api_description": "Set the pattern to configure.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda.set_per_process_memory_fraction",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.cuda.set_per_process_memory_fraction.html#torch.cuda.set_per_process_memory_fraction",
        "api_signature": "torch.cuda.set_per_process_memory_fraction(fraction, device=None)",
        "api_description": "Set memory fraction for a process.",
        "return_value": "",
        "parameters": "fraction (float) – Range: 0~1. Allowed memory equals total_memory * fraction.\ndevice (torch.device or int, optional) – selected device. If it is\nNone the default CUDA device is used.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.mps.set_per_process_memory_fraction",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.mps.set_per_process_memory_fraction.html#torch.mps.set_per_process_memory_fraction",
        "api_signature": "torch.mps.set_per_process_memory_fraction(fraction)",
        "api_description": "Set memory fraction for limiting process’s memory allocation on MPS device.\nThe allowed value equals the fraction multiplied by recommended maximum device memory\n(obtained from Metal API device.recommendedMaxWorkingSetSize).\nIf trying to allocate more than the allowed value in a process, it will raise an out of\nmemory error in allocator.",
        "return_value": "",
        "parameters": "fraction (float) – Range: 0~2. Allowed memory equals total_memory * fraction.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.fx.custom_config.ConvertCustomConfig.set_preserved_attributes",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.quantization.fx.custom_config.ConvertCustomConfig.html#torch.ao.quantization.fx.custom_config.ConvertCustomConfig.set_preserved_attributes",
        "api_signature": "set_preserved_attributes(attributes)",
        "api_description": "Set the names of the attributes that will persist in the graph module even if they are not used in\nthe model’s forward method.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.fx.custom_config.FuseCustomConfig.set_preserved_attributes",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.quantization.fx.custom_config.FuseCustomConfig.html#torch.ao.quantization.fx.custom_config.FuseCustomConfig.set_preserved_attributes",
        "api_signature": "set_preserved_attributes(attributes)",
        "api_description": "Set the names of the attributes that will persist in the graph module even if they are not used in\nthe model’s forward method.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.fx.custom_config.PrepareCustomConfig.set_preserved_attributes",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.quantization.fx.custom_config.PrepareCustomConfig.html#torch.ao.quantization.fx.custom_config.PrepareCustomConfig.set_preserved_attributes",
        "api_signature": "set_preserved_attributes(attributes)",
        "api_description": "Set the names of the attributes that will persist in the graph module even if they are not used in\nthe model’s forward method.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.set_printoptions",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.set_printoptions.html#torch.set_printoptions",
        "api_signature": "torch.set_printoptions(precision=None, threshold=None, edgeitems=None, linewidth=None, profile=None, sci_mode=None)",
        "api_description": "Set options for printing. Items shamelessly taken from NumPy",
        "return_value": "",
        "parameters": "precision – Number of digits of precision for floating point output\n(default = 4).\nthreshold – Total number of array elements which trigger summarization\nrather than full repr (default = 1000).\nedgeitems – Number of array items in summary at beginning and end of\neach dimension (default = 3).\nlinewidth – The number of characters per line for the purpose of\ninserting line breaks (default = 80). Thresholded matrices will\nignore this parameter.\nprofile – Sane defaults for pretty printing. Can override with any of\nthe above options. (any one of default, short, full)\nsci_mode – Enable (True) or disable (False) scientific notation. If\nNone (default) is specified, the value is defined by\ntorch._tensor_str._Formatter. This value is automatically chosen\nby the framework.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.backend_config.BackendPatternConfig.set_qat_module",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.quantization.backend_config.BackendPatternConfig.html#torch.ao.quantization.backend_config.BackendPatternConfig.set_qat_module",
        "api_signature": "set_qat_module(qat_module)",
        "api_description": "Set the module that represents the QAT implementation for this pattern.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.backend_config.BackendPatternConfig.set_reference_quantized_module",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.quantization.backend_config.BackendPatternConfig.html#torch.ao.quantization.backend_config.BackendPatternConfig.set_reference_quantized_module",
        "api_signature": "set_reference_quantized_module(reference_quantized_module)",
        "api_description": "Set the module that represents the reference quantized implementation for\nthis pattern’s root module.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.futures.Future.set_result",
        "api_url": "https://pytorch.org/docs/stable/futures.html#torch.futures.Future.set_result",
        "api_signature": "set_result(result)",
        "api_description": "Set the result for this Future, which will mark this Future as\ncompleted and trigger all attached callbacks. Note that a Future\ncannot be marked completed twice.",
        "return_value": "",
        "parameters": "result (object) – the result object of this Future.",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import threading\n>>> import time\n>>> def slow_set_future(fut, value):\n...     time.sleep(0.5)\n...     fut.set_result(value)\n>>> fut = torch.futures.Future()\n>>> t = threading.Thread(\n...     target=slow_set_future,\n...     args=(fut, torch.ones(2) * 3)\n... )\n>>> t.start()\n>>> print(fut.wait())\ntensor([3., 3.])\n>>> t.join()\n\n\n"
    },
    {
        "api_name": "torch.set_rng_state",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.set_rng_state.html#torch.set_rng_state",
        "api_signature": "torch.set_rng_state(new_state)",
        "api_description": "Sets the random number generator state.",
        "return_value": "",
        "parameters": "new_state (torch.ByteTensor) – The desired state",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda.set_rng_state",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.cuda.set_rng_state.html#torch.cuda.set_rng_state",
        "api_signature": "torch.cuda.set_rng_state(new_state, device='cuda')",
        "api_description": "Set the random number generator state of the specified GPU.",
        "return_value": "",
        "parameters": "new_state (torch.ByteTensor) – The desired state\ndevice (torch.device or int, optional) – The device to set the RNG state.\nDefault: 'cuda' (i.e., torch.device('cuda'), the current CUDA device).",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.mps.set_rng_state",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.mps.set_rng_state.html#torch.mps.set_rng_state",
        "api_signature": "torch.mps.set_rng_state(new_state)",
        "api_description": "Sets the random number generator state.",
        "return_value": "",
        "parameters": "new_state (torch.ByteTensor) – The desired state",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.random.set_rng_state",
        "api_url": "https://pytorch.org/docs/stable/random.html#torch.random.set_rng_state",
        "api_signature": "torch.random.set_rng_state(new_state)",
        "api_description": "Sets the random number generator state.",
        "return_value": "",
        "parameters": "new_state (torch.ByteTensor) – The desired state",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.xpu.set_rng_state",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.xpu.set_rng_state.html#torch.xpu.set_rng_state",
        "api_signature": "torch.xpu.set_rng_state(new_state, device='xpu')",
        "api_description": "Set the random number generator state of the specified GPU.",
        "return_value": "",
        "parameters": "new_state (torch.ByteTensor) – The desired state\ndevice (torch.device or int, optional) – The device to set the RNG state.\nDefault: 'xpu' (i.e., torch.device('xpu'), the current XPU device).",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda.set_rng_state_all",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.cuda.set_rng_state_all.html#torch.cuda.set_rng_state_all",
        "api_signature": "torch.cuda.set_rng_state_all(new_states)",
        "api_description": "Set the random number generator state of all devices.",
        "return_value": "",
        "parameters": "new_states (Iterable of torch.ByteTensor) – The desired state for each device.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.xpu.set_rng_state_all",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.xpu.set_rng_state_all.html#torch.xpu.set_rng_state_all",
        "api_signature": "torch.xpu.set_rng_state_all(new_states)",
        "api_description": "Set the random number generator state of all devices.",
        "return_value": "",
        "parameters": "new_states (Iterable of torch.ByteTensor) – The desired state for each device.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.backend_config.BackendPatternConfig.set_root_module",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.quantization.backend_config.BackendPatternConfig.html#torch.ao.quantization.backend_config.BackendPatternConfig.set_root_module",
        "api_signature": "set_root_module(root_module)",
        "api_description": "Set the module that represents the root for this pattern.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.multiprocessing.set_sharing_strategy",
        "api_url": "https://pytorch.org/docs/stable/multiprocessing.html#torch.multiprocessing.set_sharing_strategy",
        "api_signature": "torch.multiprocessing.set_sharing_strategy(new_strategy)",
        "api_description": "Set the strategy for sharing CPU tensors.",
        "return_value": "",
        "parameters": "new_strategy (str) – Name of the selected strategy. Should be one of\nthe values returned by get_all_sharing_strategies().",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.fx.custom_config.PrepareCustomConfig.set_standalone_module_class",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.quantization.fx.custom_config.PrepareCustomConfig.html#torch.ao.quantization.fx.custom_config.PrepareCustomConfig.set_standalone_module_class",
        "api_signature": "set_standalone_module_class(module_class, qconfig_mapping, example_inputs, prepare_custom_config, backend_config)",
        "api_description": "Set the configuration for running a standalone module identified by module_class.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.fx.custom_config.PrepareCustomConfig.set_standalone_module_name",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.quantization.fx.custom_config.PrepareCustomConfig.html#torch.ao.quantization.fx.custom_config.PrepareCustomConfig.set_standalone_module_name",
        "api_signature": "set_standalone_module_name(module_name, qconfig_mapping, example_inputs, prepare_custom_config, backend_config)",
        "api_description": "Set the configuration for running a standalone module identified by module_name.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.rendezvous.c10d_rendezvous_backend.C10dRendezvousBackend.set_state",
        "api_url": "https://pytorch.org/docs/stable/elastic/rendezvous.html#torch.distributed.elastic.rendezvous.c10d_rendezvous_backend.C10dRendezvousBackend.set_state",
        "api_signature": "set_state(state, token=None)",
        "api_description": "See base class.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.rendezvous.dynamic_rendezvous.RendezvousBackend.set_state",
        "api_url": "https://pytorch.org/docs/stable/elastic/rendezvous.html#torch.distributed.elastic.rendezvous.dynamic_rendezvous.RendezvousBackend.set_state",
        "api_signature": "set_state(state, token=None)",
        "api_description": "Set the rendezvous state.",
        "return_value": "A tuple of the serialized rendezvous state, its fencing token, and\na boolean value indicating whether our set attempt succeeded.\n",
        "parameters": "state (bytes) – The encoded rendezvous state.\ntoken (Optional[Any]) – An optional fencing token that was retrieved by a previous call\nto get_state() or set_state().",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.rendezvous.etcd_rendezvous_backend.EtcdRendezvousBackend.set_state",
        "api_url": "https://pytorch.org/docs/stable/elastic/rendezvous.html#torch.distributed.elastic.rendezvous.etcd_rendezvous_backend.EtcdRendezvousBackend.set_state",
        "api_signature": "set_state(state, token=None)",
        "api_description": "See base class.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Generator.set_state",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator.set_state",
        "api_signature": "set_state(new_state)",
        "api_description": "Sets the Generator state.",
        "return_value": "",
        "parameters": "new_state (torch.ByteTensor) – The desired state.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.checkpoint.state_dict.set_state_dict",
        "api_url": "https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.set_state_dict",
        "api_signature": "torch.distributed.checkpoint.state_dict.set_state_dict(model, optimizers, *, model_state_dict, optim_state_dict, options=None)",
        "api_description": "Load the model state_dict and optimizers state_dict.",
        "return_value": "\nmissing_keys is a list of str containing the missing keys of the model state_dict.\nunexpected_keys is a list of str containing the unexpected keys of the model state_dict.\n\n\n",
        "parameters": "model (nn.Module) – the nn.Module to the model.\noptimizers (Union[Optimizer, Iterable[Optimizer]]) – The optimizers that are used to optimize model.\nmodel_state_dict (Dict[str, ValueType]) – (Union[Dict[nn.Module, Dict[str, ValueType]], Dict[str, ValueType]]):\nthe model state_dict to load. If the key of the model_state_dict\nis nn.Module, the key is a submodule of model and the value should\nbe the state_dict of the submodule. When loading the state_dict,\nthe prefix of the submodule will be append to the state_dict.\noptim_state_dict (OptimizerStateType) – OptimizerStateType:\nthe optimizer state_dict to load.\noptions (StateDictOptions) – the options to control how\nmodel state_dict and optimizer state_dict should be loaded. See\nStateDictOptions for the details.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.fsdp.FullyShardedDataParallel.set_state_dict_type",
        "api_url": "https://pytorch.org/docs/stable/fsdp.html#torch.distributed.fsdp.FullyShardedDataParallel.set_state_dict_type",
        "api_signature": "set_state_dict_type(module, state_dict_type, state_dict_config=None, optim_state_dict_config=None)",
        "api_description": "Set the state_dict_type of all the descendant FSDP modules of the target module.",
        "return_value": "A StateDictSettings that include the previous state_dict type and\nconfiguration for the module.\n",
        "parameters": "module (torch.nn.Module) – Root module.\nstate_dict_type (StateDictType) – the desired state_dict_type to set.\nstate_dict_config (Optional[StateDictConfig]) – the configuration for the\ntarget state_dict_type.\noptim_state_dict_config (Optional[OptimStateDictConfig]) – the configuration\nfor the optimizer state dict.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda.set_stream",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.cuda.set_stream.html#torch.cuda.set_stream",
        "api_signature": "torch.cuda.set_stream(stream)",
        "api_description": "Usage of this function is discouraged in favor of the stream\ncontext manager.",
        "return_value": "",
        "parameters": "stream (Stream) – selected stream. This function is a no-op\nif this argument is None.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.xpu.set_stream",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.xpu.set_stream.html#torch.xpu.set_stream",
        "api_signature": "torch.xpu.set_stream(stream)",
        "api_description": "Usage of this function is discouraged in favor of the stream\ncontext manager.",
        "return_value": "",
        "parameters": "stream (Stream) – selected stream. This function is a no-op\nif this argument is None.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.__future__.set_swap_module_params_on_conversion",
        "api_url": "https://pytorch.org/docs/stable/future_mod.html#torch.__future__.set_swap_module_params_on_conversion",
        "api_signature": "torch.__future__.set_swap_module_params_on_conversion(value)",
        "api_description": "Sets whether to use swap_tensors() instead of setting .data to\nchange the existing parameters in-place when converting an nn.Module and instead\nof param.copy_(state_dict[key]) when loading a state dict into an nn.Module.",
        "return_value": "",
        "parameters": "value (bool) – Whether to use swap_tensors() or not.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda.set_sync_debug_mode",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.cuda.set_sync_debug_mode.html#torch.cuda.set_sync_debug_mode",
        "api_signature": "torch.cuda.set_sync_debug_mode(debug_mode)",
        "api_description": "Set the debug mode for cuda synchronizing operations.",
        "return_value": "",
        "parameters": "debug_mode (str or int) – if “default” or 0, don’t error or warn on synchronizing operations,\nif “warn” or 1, warn on synchronizing operations, if “error” or 2, error out synchronizing operations.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.Store.set_timeout",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#torch.distributed.Store.set_timeout",
        "api_signature": "torch.distributed.Store.set_timeout(self: torch._C._distributed_c10d.Store, arg0: datetime.timedelta)",
        "api_description": "Sets the store’s default timeout. This timeout is used during initialization and in\nwait() and get().",
        "return_value": "",
        "parameters": "timeout (timedelta) – timeout to be set in the store.",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> # Using TCPStore as an example, other store types can also be used\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n>>> store.set_timeout(timedelta(seconds=10))\n>>> # This will throw an exception after 10 seconds\n>>> store.wait([\"bad_key\"])\n\n\n"
    },
    {
        "api_name": "torch.distributed.checkpoint.format_utils.DynamicMetaLoadPlanner.set_up_planner",
        "api_url": "https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.format_utils.DynamicMetaLoadPlanner.set_up_planner",
        "api_signature": "set_up_planner(state_dict, metadata, is_coordinator)",
        "api_description": "Setups of the planner, extnding default behavior by creating the Metadata object from the state dict",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.checkpoint.LoadPlanner.set_up_planner",
        "api_url": "https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.LoadPlanner.set_up_planner",
        "api_signature": "set_up_planner(state_dict, metadata, is_coordinator)",
        "api_description": "Initialize this instance to load data into state_dict.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.checkpoint.SavePlanner.set_up_planner",
        "api_url": "https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.SavePlanner.set_up_planner",
        "api_signature": "set_up_planner(state_dict, is_coordinator)",
        "api_description": "Initialize this planner to save state_dict.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.checkpoint.format_utils.BroadcastingTorchSaveReader.set_up_storage_reader",
        "api_url": "https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.format_utils.BroadcastingTorchSaveReader.set_up_storage_reader",
        "api_signature": "set_up_storage_reader(metadata, is_coordinator)",
        "api_description": "Implementation of the StorageReader method",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.checkpoint.StorageReader.set_up_storage_reader",
        "api_url": "https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.StorageReader.set_up_storage_reader",
        "api_signature": "set_up_storage_reader(metadata, is_coordinator)",
        "api_description": "Initialize this instance.",
        "return_value": "",
        "parameters": "metadata (Metadata) – The metadata schema to use.\nis_coordinator (bool) – Whether this instance is responsible for coordinating\nthe checkpoint.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.checkpoint.StorageWriter.set_up_storage_writer",
        "api_url": "https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.StorageWriter.set_up_storage_writer",
        "api_signature": "set_up_storage_writer(is_coordinator)",
        "api_description": "Initialize this instance.",
        "return_value": "",
        "parameters": "is_coordinator (bool) – Whether this instance is responsible for coordinating\nthe checkpoint.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.set_warn_always",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.set_warn_always.html#torch.set_warn_always",
        "api_signature": "torch.set_warn_always(b)",
        "api_description": "When this flag is False (default) then some PyTorch warnings may only\nappear once per process. This helps avoid excessive warning information.\nSetting it to True causes these warnings to always appear, which may be\nhelpful when debugging.",
        "return_value": "",
        "parameters": "b (bool) – If True, force warnings to always be emitted\nIf False, set to the default behaviour",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.autograd.profiler_util.StringTable.setdefault",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.autograd.profiler_util.StringTable.html#torch.autograd.profiler_util.StringTable.setdefault",
        "api_signature": "setdefault(key, default=None, /)",
        "api_description": "Insert key with a value of default if key is not in the dictionary.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.ParameterDict.setdefault",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.ParameterDict.html#torch.nn.ParameterDict.setdefault",
        "api_signature": "setdefault(key, default=None)",
        "api_description": "Set the default for a key in the Parameterdict.",
        "return_value": "",
        "parameters": "key (str) – key to set default for\ndefault (Any) – the parameter set to the key",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.autograd.function.InplaceFunction.setup_context",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.autograd.function.InplaceFunction.html#torch.autograd.function.InplaceFunction.setup_context",
        "api_signature": "setup_context(ctx, inputs, output)",
        "api_description": "There are two ways to define the forward pass of an autograd.Function.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.autograd.function.NestedIOFunction.setup_context",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.autograd.function.NestedIOFunction.html#torch.autograd.function.NestedIOFunction.setup_context",
        "api_signature": "setup_context(ctx, inputs, output)",
        "api_description": "There are two ways to define the forward pass of an autograd.Function.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.SGD",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.SGD.html#torch.optim.SGD",
        "api_signature": "torch.optim.SGD(params, lr=0.001, momentum=0, dampening=0, weight_decay=0, nesterov=False, *, maximize=False, foreach=None, differentiable=False, fused=None)",
        "api_description": "Implements stochastic gradient descent (optionally with momentum).",
        "return_value": "a handle that can be used to remove the added hook by calling\nhandle.remove()\na handle that can be used to remove the added hook by calling\nhandle.remove()\na handle that can be used to remove the added hook by calling\nhandle.remove()\na handle that can be used to remove the added hook by calling\nhandle.remove()\na handle that can be used to remove the added hook by calling\nhandle.remove()\na handle that can be used to remove the added hook by calling\nhandle.remove()\n",
        "parameters": "params (iterable) – iterable of parameters to optimize or dicts defining\nparameter groups\nlr (float, optional) – learning rate (default: 1e-3)\nmomentum (float, optional) – momentum factor (default: 0)\nweight_decay (float, optional) – weight decay (L2 penalty) (default: 0)\ndampening (float, optional) – dampening for momentum (default: 0)\nnesterov (bool, optional) – enables Nesterov momentum (default: False)\nmaximize (bool, optional) – maximize the objective with respect to the\nparams, instead of minimizing (default: False)\nforeach (bool, optional) – whether foreach implementation of optimizer\nis used. If unspecified by the user (so foreach is None), we will try to use\nforeach over the for-loop implementation on CUDA, since it is usually\nsignificantly more performant. Note that the foreach implementation uses\n~ sizeof(params) more peak memory than the for-loop version due to the intermediates\nbeing a tensorlist vs just one tensor. If memory is prohibitive, batch fewer\nparameters through the optimizer at a time or switch this flag to False (default: None)\ndifferentiable (bool, optional) – whether autograd should\noccur through the optimizer step in training. Otherwise, the step()\nfunction runs in a torch.no_grad() context. Setting to True can impair\nperformance, so leave it False if you don’t intend to run autograd\nthrough this instance (default: False)\nfused (bool, optional) – whether the fused implementation (CUDA only) is used.\nCurrently, torch.float64, torch.float32, torch.float16, and torch.bfloat16\nare supported. (default: None)\nparam_group (dict) – Specifies what Tensors should be optimized along with group\nspecific optimization options.\nstate_dict (dict) – optimizer state. Should be an object returned\nfrom a call to state_dict().\nhook (Callable) – The user defined hook to be registered.\nprepend (bool) – If True, the provided post hook will be fired before\nall the already registered post-hooks on load_state_dict. Otherwise,\nthe provided hook will be fired after all the already registered\npost-hooks. (default: False)\nhook (Callable) – The user defined hook to be registered.\nprepend (bool) – If True, the provided pre hook will be fired before\nall the already registered pre-hooks on load_state_dict. Otherwise,\nthe provided hook will be fired after all the already registered\npre-hooks. (default: False)\nhook (Callable) – The user defined hook to be registered.\nprepend (bool) – If True, the provided post hook will be fired before\nall the already registered post-hooks on state_dict. Otherwise,\nthe provided hook will be fired after all the already registered\npost-hooks. (default: False)\nhook (Callable) – The user defined hook to be registered.\nprepend (bool) – If True, the provided pre hook will be fired before\nall the already registered pre-hooks on state_dict. Otherwise,\nthe provided hook will be fired after all the already registered\npre-hooks. (default: False)\nhook (Callable) – The user defined hook to be registered.\nhook (Callable) – The user defined hook to be registered.\nclosure (Callable, optional) – A closure that reevaluates the model\nand returns the loss.\nset_to_none (bool) – instead of setting to zero, set the grads to None.\nThis will in general have lower memory footprint, and can modestly improve performance.\nHowever, it changes certain behaviors. For example:\n1. When the user tries to access a gradient and perform manual ops on it,\na None attribute or a Tensor full of 0s will behave differently.\n2. If the user requests zero_grad(set_to_none=True) followed by a backward pass, .grads\nare guaranteed to be None for params that did not receive a gradient.\n3. torch.optim optimizers have a different behavior if the gradient is 0 or None\n(in one case it does the step with a gradient of 0 and in the other it skips\nthe step altogether).",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.sgn",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.sgn.html#torch.sgn",
        "api_signature": "torch.sgn(input, *, out=None)",
        "api_description": "This function is an extension of torch.sign() to complex tensors.\nIt computes a new tensor whose elements have\nthe same angles as the corresponding elements of input and\nabsolute values (i.e. magnitudes) of one for complex tensors and\nis equivalent to torch.sign() for non-complex tensors.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.sgn",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.sgn.html#torch.Tensor.sgn",
        "api_signature": "Tensor.sgn()",
        "api_description": "See torch.sgn()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.sgn_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.sgn_.html#torch.Tensor.sgn_",
        "api_signature": "Tensor.sgn_()",
        "api_description": "In-place version of sgn()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.ns._numeric_suite.Shadow",
        "api_url": "https://pytorch.org/docs/stable/torch.ao.ns._numeric_suite.html#torch.ao.ns._numeric_suite.Shadow",
        "api_signature": "torch.ao.ns._numeric_suite.Shadow(q_module, float_module, logger_cls)",
        "api_description": "Shadow module attaches the float module to its matching quantized module\nas the shadow. Then it uses Logger module to process the outputs of both\nmodules.",
        "return_value": "",
        "parameters": "q_module – module quantized from float_module that we want to shadow\nfloat_module – float module used to shadow q_module\nlogger_cls – type of logger used to process the outputs of q_module and\nfloat_module. ShadowLogger or custom loggers can be used.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.ns._numeric_suite.ShadowLogger",
        "api_url": "https://pytorch.org/docs/stable/torch.ao.ns._numeric_suite.html#torch.ao.ns._numeric_suite.ShadowLogger",
        "api_signature": null,
        "api_description": "Class used in Shadow module to record the outputs of the original and\nshadow modules.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.shape",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.shape.html#torch.Tensor.shape",
        "api_signature": null,
        "api_description": "Returns the size of the self tensor. Alias for size.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.symbolic_shapes.ShapeEnv",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.fx.experimental.symbolic_shapes.ShapeEnv.html#torch.fx.experimental.symbolic_shapes.ShapeEnv",
        "api_signature": "torch.fx.experimental.symbolic_shapes.ShapeEnv(*, should_record_events=None, tracked_fakes=None, **kwargs)",
        "api_description": "Given a paired list of placeholders (fake tensors with\nsymbolic sizes) and concrete arguments (regular tensors\nwith real sizes), returns a dictionary mapping each\nsymbol to its real value.  So for example, if you\nhave a placeholder with size (s0, s1), binding\n(2, 4) to it will give you {s0: 2, s1: 4}.  This is\nnot guaranteed to bind ALL symbols in the ShapeEnv;\nwe can’t bind a symbol if it doesn’t occur in any placeholder,\nand symbols that already have replacements won’t get bindings.",
        "return_value": "",
        "parameters": "orig_expr (sympy.Expr) – Boolean expression to assert is true\nmsg (str) – Message to display on assertion failure\nfx_node (Optional, torch.fx.Node) – node in self.graph corresponding\nto the expression, if applicable",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.fsdp.FullyShardedDataParallel.shard_full_optim_state_dict",
        "api_url": "https://pytorch.org/docs/stable/fsdp.html#torch.distributed.fsdp.FullyShardedDataParallel.shard_full_optim_state_dict",
        "api_signature": "shard_full_optim_state_dict(full_optim_state_dict, model, optim_input=None, optim=None)",
        "api_description": "Shard a full optimizer state-dict.",
        "return_value": "The full optimizer state dict now remapped to\nflattened parameters instead of unflattened parameters and\nrestricted to only include this rank’s part of the optimizer state.\n",
        "parameters": "full_optim_state_dict (Dict[str, Any]) – Optimizer state dict\ncorresponding to the unflattened parameters and holding the\nfull non-sharded optimizer state.\nmodel (torch.nn.Module) – Root module (which may or may not be a\nFullyShardedDataParallel instance) whose parameters\ncorrespond to the optimizer state in full_optim_state_dict.\noptim_input (Optional[Union[List[Dict[str, Any]], Iterable[torch.nn.Parameter]]]) – Input passed into the optimizer representing either a\nlist of parameter groups or an iterable of parameters;\nif None, then this method assumes the input was\nmodel.parameters(). This argument is deprecated, and there\nis no need to pass it in anymore. (Default: None)\noptim (Optional[torch.optim.Optimizer]) – Optimizer that will load\nthe state dict returned by this method. This is the preferred\nargument to use over optim_input. (Default: None)",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.fsdp.FullyShardedDataParallel.sharded_optim_state_dict",
        "api_url": "https://pytorch.org/docs/stable/fsdp.html#torch.distributed.fsdp.FullyShardedDataParallel.sharded_optim_state_dict",
        "api_signature": "sharded_optim_state_dict(model, optim, group=None)",
        "api_description": "Return the optimizer state-dict in its sharded form.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.fsdp.ShardedOptimStateDictConfig",
        "api_url": "https://pytorch.org/docs/stable/fsdp.html#torch.distributed.fsdp.ShardedOptimStateDictConfig",
        "api_signature": "torch.distributed.fsdp.ShardedOptimStateDictConfig(offload_to_cpu=True, _use_dtensor=False)",
        "api_description": "ShardedOptimStateDictConfig is a config class meant to be used with\nStateDictType.SHARDED_STATE_DICT.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.fsdp.ShardedStateDictConfig",
        "api_url": "https://pytorch.org/docs/stable/fsdp.html#torch.distributed.fsdp.ShardedStateDictConfig",
        "api_signature": "torch.distributed.fsdp.ShardedStateDictConfig(offload_to_cpu=False, _use_dtensor=False)",
        "api_description": "ShardedStateDictConfig is a config class meant to be used with\nStateDictType.SHARDED_STATE_DICT.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.fsdp.ShardingStrategy",
        "api_url": "https://pytorch.org/docs/stable/fsdp.html#torch.distributed.fsdp.ShardingStrategy",
        "api_signature": "torch.distributed.fsdp.ShardingStrategy(value)",
        "api_description": "This specifies the sharding strategy to be used for distributed training by\nFullyShardedDataParallel.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.jit.ScriptModule.share_memory",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.jit.ScriptModule.html#torch.jit.ScriptModule.share_memory",
        "api_signature": "share_memory()",
        "api_description": "See torch.Tensor.share_memory_().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.Module.share_memory",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.share_memory",
        "api_signature": "share_memory()",
        "api_description": "See torch.Tensor.share_memory_().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.share_memory_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.share_memory_.html#torch.Tensor.share_memory_",
        "api_signature": "Tensor.share_memory_()",
        "api_description": "Moves the underlying storage to shared memory.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.TypedStorage.share_memory_",
        "api_url": "https://pytorch.org/docs/stable/storage.html#torch.TypedStorage.share_memory_",
        "api_signature": "share_memory_()",
        "api_description": "See torch.UntypedStorage.share_memory_()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.UntypedStorage.share_memory_",
        "api_url": "https://pytorch.org/docs/stable/storage.html#torch.UntypedStorage.share_memory_",
        "api_signature": "share_memory_(*args, **kwargs)",
        "api_description": "Moves the storage to shared memory.",
        "return_value": "self\n",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.short",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.short.html#torch.Tensor.short",
        "api_signature": "Tensor.short(memory_format=torch.preserve_format)",
        "api_description": "self.short() is equivalent to self.to(torch.int16). See to().",
        "return_value": "",
        "parameters": "memory_format (torch.memory_format, optional) – the desired memory format of\nreturned Tensor. Default: torch.preserve_format.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.TypedStorage.short",
        "api_url": "https://pytorch.org/docs/stable/storage.html#torch.TypedStorage.short",
        "api_signature": "short()",
        "api_description": "Casts this storage to short type.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.UntypedStorage.short",
        "api_url": "https://pytorch.org/docs/stable/storage.html#torch.UntypedStorage.short",
        "api_signature": "short()",
        "api_description": "Casts this storage to short type.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ShortStorage",
        "api_url": "https://pytorch.org/docs/stable/storage.html#torch.ShortStorage",
        "api_signature": "torch.ShortStorage(*args, wrap_storage=None, dtype=None, device=None, _internal=False)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.__config__.show",
        "api_url": "https://pytorch.org/docs/stable/config_mod.html#torch.__config__.show",
        "api_signature": "torch.__config__.show()",
        "api_description": "Return a human-readable string with descriptions of the\nconfiguration of PyTorch.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.rpc.shutdown",
        "api_url": "https://pytorch.org/docs/stable/rpc.html#torch.distributed.rpc.shutdown",
        "api_signature": "torch.distributed.rpc.shutdown(graceful=True, timeout=0)",
        "api_description": "Perform a shutdown of the RPC agent, and then destroy the RPC agent. This\nstops the local agent from accepting outstanding requests, and shuts\ndown the RPC framework by terminating all RPC threads. If graceful=True,\nthis will block until all local and remote RPC processes reach this method\nand wait for all outstanding work to complete. Otherwise, if\ngraceful=False, this is a local shutdown, and it does not wait for other\nRPC processes to reach this method.",
        "return_value": "",
        "parameters": "graceful (bool) – Whether to do a graceful shutdown or not. If True,\nthis will 1) wait until there is no pending system\nmessages for UserRRefs and delete them; 2) block\nuntil all local and remote RPC processes have reached\nthis method and wait for all outstanding work to\ncomplete.",
        "input_shape": "",
        "notes": "",
        "code_example": "Make sure that MASTER_ADDR and MASTER_PORT are set properly\non both workers. Refer to init_process_group()\nAPI for more details. For example,\nexport MASTER_ADDR=localhost\nexport MASTER_PORT=5678\nThen run the following code in two different processes:\n>>> # On worker 0:\n>>> import torch\n>>> import torch.distributed.rpc as rpc\n>>> rpc.init_rpc(\"worker0\", rank=0, world_size=2)\n>>> # do some work\n>>> result = rpc.rpc_sync(\"worker1\", torch.add, args=(torch.ones(1), 1))\n>>> # ready to shutdown\n>>> rpc.shutdown()\n\n\n>>> # On worker 1:\n>>> import torch.distributed.rpc as rpc\n>>> rpc.init_rpc(\"worker1\", rank=1, world_size=2)\n>>> # wait for worker 0 to finish work, and then shutdown.\n>>> rpc.shutdown()\n\n\n"
    },
    {
        "api_name": "torch.distributed.elastic.rendezvous.RendezvousHandler.shutdown",
        "api_url": "https://pytorch.org/docs/stable/elastic/rendezvous.html#torch.distributed.elastic.rendezvous.RendezvousHandler.shutdown",
        "api_signature": "shutdown()",
        "api_description": "Close all resources that were open for the rendezvous.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.quantized.Sigmoid",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.nn.quantized.Sigmoid.html#torch.ao.nn.quantized.Sigmoid",
        "api_signature": "torch.ao.nn.quantized.Sigmoid(output_scale, output_zero_point)",
        "api_description": "This is the quantized equivalent of Sigmoid.",
        "return_value": "",
        "parameters": "scale – quantization scale of the output tensor\nzero_point – quantization zero point of the output tensor",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.Sigmoid",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.Sigmoid.html#torch.nn.Sigmoid",
        "api_signature": "torch.nn.Sigmoid(*args, **kwargs)",
        "api_description": "Applies the Sigmoid function element-wise.",
        "return_value": "",
        "parameters": "",
        "input_shape": "\nInput: (∗)(*)(∗), where ∗*∗ means any number of dimensions.\nOutput: (∗)(*)(∗), same shape as the input.\n\n",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.sigmoid",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.sigmoid.html#torch.sigmoid",
        "api_signature": "torch.sigmoid(input, *, out=None)",
        "api_description": "Alias for torch.special.expit().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.functional.sigmoid",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.functional.sigmoid.html#torch.nn.functional.sigmoid",
        "api_signature": "torch.nn.functional.sigmoid(input)",
        "api_description": "Applies the element-wise function Sigmoid(x)=11+exp⁡(−x)\\text{Sigmoid}(x) = \\frac{1}{1 + \\exp(-x)}Sigmoid(x)=1+exp(−x)1​",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.sigmoid",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.sigmoid.html#torch.Tensor.sigmoid",
        "api_signature": "Tensor.sigmoid()",
        "api_description": "See torch.sigmoid()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.sigmoid_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.sigmoid_.html#torch.Tensor.sigmoid_",
        "api_signature": "Tensor.sigmoid_()",
        "api_description": "In-place version of sigmoid()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.transforms.SigmoidTransform",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.transforms.SigmoidTransform",
        "api_signature": "torch.distributions.transforms.SigmoidTransform(cache_size=0)",
        "api_description": "Transform via the mapping y=11+exp⁡(−x)y = \\frac{1}{1 + \\exp(-x)}y=1+exp(−x)1​ and x=logit(y)x = \\text{logit}(y)x=logit(y).",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.transforms.Transform.sign",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.transforms.Transform.sign",
        "api_signature": null,
        "api_description": "Returns the sign of the determinant of the Jacobian, if applicable.\nIn general this only makes sense for bijective transforms.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.sign",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.sign.html#torch.sign",
        "api_signature": "torch.sign(input, *, out=None)",
        "api_description": "Returns a new tensor with the signs of the elements of input.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.sign",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.sign.html#torch.Tensor.sign",
        "api_signature": "Tensor.sign()",
        "api_description": "See torch.sign()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.sign_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.sign_.html#torch.Tensor.sign_",
        "api_signature": "Tensor.sign_()",
        "api_description": "In-place version of sign()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.signbit",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.signbit.html#torch.signbit",
        "api_signature": "torch.signbit(input, *, out=None)",
        "api_description": "Tests if each element of input has its sign bit set or not.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.signbit",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.signbit.html#torch.Tensor.signbit",
        "api_signature": "Tensor.signbit()",
        "api_description": "See torch.signbit()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.benchmark.Measurement.significant_figures",
        "api_url": "https://pytorch.org/docs/stable/benchmark_utils.html#torch.utils.benchmark.Measurement.significant_figures",
        "api_signature": null,
        "api_description": "Approximate significant figure estimate.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.SiLU",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.SiLU.html#torch.nn.SiLU",
        "api_signature": "torch.nn.SiLU(inplace=False)",
        "api_description": "Applies the Sigmoid Linear Unit (SiLU) function, element-wise.",
        "return_value": "",
        "parameters": "",
        "input_shape": "\nInput: (∗)(*)(∗), where ∗*∗ means any number of dimensions.\nOutput: (∗)(*)(∗), same shape as the input.\n\n",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.functional.silu",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.functional.silu.html#torch.nn.functional.silu",
        "api_signature": "torch.nn.functional.silu(input, inplace=False)",
        "api_description": "Apply the Sigmoid Linear Unit (SiLU) function, element-wise.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.agent.server.SimpleElasticAgent",
        "api_url": "https://pytorch.org/docs/stable/elastic/agent.html#torch.distributed.elastic.agent.server.SimpleElasticAgent",
        "api_signature": "torch.distributed.elastic.agent.server.SimpleElasticAgent(spec, exit_barrier_timeout=300)",
        "api_description": "An ElasticAgent that manages one particular type of worker role.",
        "return_value": "",
        "parameters": "death_sig (Signals) – Signal to send to the child process, SIGTERM is default",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.symbolic_shapes.ShapeEnv.simplify",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.fx.experimental.symbolic_shapes.ShapeEnv.html#torch.fx.experimental.symbolic_shapes.ShapeEnv.simplify",
        "api_signature": "simplify(expr)",
        "api_description": "Use known constraints and replacements to simplify the given expr",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.sin",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.sin.html#torch.sin",
        "api_signature": "torch.sin(input, *, out=None)",
        "api_description": "Returns a new tensor with the sine of the elements of input.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.sin",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.sin.html#torch.Tensor.sin",
        "api_signature": "Tensor.sin()",
        "api_description": "See torch.sin()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.sin_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.sin_.html#torch.Tensor.sin_",
        "api_signature": "Tensor.sin_()",
        "api_description": "In-place version of sin()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.sinc",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.sinc.html#torch.sinc",
        "api_signature": "torch.sinc(input, *, out=None)",
        "api_description": "Alias for torch.special.sinc().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.special.sinc",
        "api_url": "https://pytorch.org/docs/stable/special.html#torch.special.sinc",
        "api_signature": "torch.special.sinc(input, *, out=None)",
        "api_description": "Computes the normalized sinc of input.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> t = torch.randn(4)\n>>> t\ntensor([ 0.2252, -0.2948,  1.0267, -1.1566])\n>>> torch.special.sinc(t)\ntensor([ 0.9186,  0.8631, -0.0259, -0.1300])\n\n\n"
    },
    {
        "api_name": "torch.Tensor.sinc",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.sinc.html#torch.Tensor.sinc",
        "api_signature": "Tensor.sinc()",
        "api_description": "See torch.sinc()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.sinc_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.sinc_.html#torch.Tensor.sinc_",
        "api_signature": "Tensor.sinc_()",
        "api_description": "In-place version of sinc()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.sinh",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.sinh.html#torch.sinh",
        "api_signature": "torch.sinh(input, *, out=None)",
        "api_description": "Returns a new tensor with the hyperbolic sine of the elements of\ninput.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.sinh",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.sinh.html#torch.Tensor.sinh",
        "api_signature": "Tensor.sinh()",
        "api_description": "See torch.sinh()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.sinh_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.sinh_.html#torch.Tensor.sinh_",
        "api_signature": "Tensor.sinh_()",
        "api_description": "In-place version of sinh()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.backends.cuda.cufft_plan_cache.size",
        "api_url": "https://pytorch.org/docs/stable/backends.html#torch.backends.cuda.cufft_plan_cache.size",
        "api_signature": null,
        "api_description": "A readonly int that shows the number of plans currently in a cuFFT plan cache.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.size",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.size.html#torch.Tensor.size",
        "api_signature": "Tensor.size(dim=None)",
        "api_description": "Returns the size of the self tensor. If dim is not specified,\nthe returned value is a torch.Size, a subclass of tuple.\nIf dim is specified, returns an int holding the size of that dimension.",
        "return_value": "",
        "parameters": "dim (int, optional) – The dimension for which to retrieve the size.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.TypedStorage.size",
        "api_url": "https://pytorch.org/docs/stable/storage.html#torch.TypedStorage.size",
        "api_signature": "size()",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.UntypedStorage.size",
        "api_url": "https://pytorch.org/docs/stable/storage.html#torch.UntypedStorage.size",
        "api_signature": "size()",
        "api_description": "int",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.symbolic_shapes.ShapeEnv.size_hint",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.fx.experimental.symbolic_shapes.ShapeEnv.html#torch.fx.experimental.symbolic_shapes.ShapeEnv.size_hint",
        "api_signature": "size_hint(expr, *, allow_none=False)",
        "api_description": "Gets a size hint for a given expression from the underlying shapes we had.\nDoes not introduce a guard, so only use this when you can guarantee that\nyour code is still valid for arbitrary shapes (such as optimization decisions)",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.utils.skip_init",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.utils.skip_init.html#torch.nn.utils.skip_init",
        "api_signature": "torch.nn.utils.skip_init(module_cls, *args, **kwargs)",
        "api_description": "Given a module class object and args / kwargs, instantiate the module without initializing parameters / buffers.",
        "return_value": "Instantiated module with uninitialized parameters / buffers\n",
        "parameters": "module_cls – Class object; should be a subclass of torch.nn.Module\nargs – args to pass to the module’s constructor\nkwargs – kwargs to pass to the module’s constructor",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.pipeline.sync.skip.skippable.skippable",
        "api_url": "https://pytorch.org/docs/stable/pipeline.html#torch.distributed.pipeline.sync.skip.skippable.skippable",
        "api_signature": "torch.distributed.pipeline.sync.skip.skippable.skippable(stash=()",
        "api_description": "Define a decorator to create nn.Module with skip connections.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.slice_scatter",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.slice_scatter.html#torch.slice_scatter",
        "api_signature": "torch.slice_scatter(input, src, dim=0, start=None, end=None, step=1)",
        "api_description": "Embeds the values of the src tensor into input at the given\ndimension.\nThis function returns a tensor with fresh storage; it does not create a view.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.\nsrc (Tensor) – The tensor to embed into input\ndim (int) – the dimension to insert the slice into\nstart (Optional[int]) – the start index of where to insert the slice\nend (Optional[int]) – the end index of where to insert the slice\nstep (int) – the how many elements to skip in",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.slice_scatter",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.slice_scatter.html#torch.Tensor.slice_scatter",
        "api_signature": "Tensor.slice_scatter(src, dim=0, start=None, end=None, step=1)",
        "api_description": "See torch.slice_scatter()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.slogdet",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.slogdet.html#torch.slogdet",
        "api_signature": "torch.slogdet(input)",
        "api_description": "Alias for torch.linalg.slogdet()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.linalg.slogdet",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.linalg.slogdet.html#torch.linalg.slogdet",
        "api_signature": "torch.linalg.slogdet(A, *, out=None)",
        "api_description": "Computes the sign and natural logarithm of the absolute value of the determinant of a square matrix.",
        "return_value": "A named tuple (sign, logabsdet).\nsign will have the same dtype as A.\nlogabsdet will always be real-valued, even when A is complex.\n\n",
        "parameters": "A (Tensor) – tensor of shape (*, n, n) where * is zero or more batch dimensions.\nout (tuple, optional) – output tuple of two tensors. Ignored if None. Default: None.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.slogdet",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.slogdet.html#torch.Tensor.slogdet",
        "api_signature": "Tensor.slogdet()",
        "api_description": "See torch.slogdet()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.smm",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.smm.html#torch.smm",
        "api_signature": "torch.smm(input, mat)",
        "api_description": "Performs a matrix multiplication of the sparse matrix input\nwith the dense matrix mat.",
        "return_value": "",
        "parameters": "input (Tensor) – a sparse matrix to be matrix multiplied\nmat (Tensor) – a dense matrix to be matrix multiplied",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.smm",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.smm.html#torch.Tensor.smm",
        "api_signature": "Tensor.smm(mat)",
        "api_description": "See torch.smm()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.functional.smooth_l1_loss",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.functional.smooth_l1_loss.html#torch.nn.functional.smooth_l1_loss",
        "api_signature": "torch.nn.functional.smooth_l1_loss(input, target, size_average=None, reduce=None, reduction='mean', beta=1.0)",
        "api_description": "Compute the Smooth L1 loss.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.SmoothL1Loss",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.SmoothL1Loss.html#torch.nn.SmoothL1Loss",
        "api_signature": "torch.nn.SmoothL1Loss(size_average=None, reduce=None, reduction='mean', beta=1.0)",
        "api_description": "Creates a criterion that uses a squared term if the absolute\nelement-wise error falls below beta and an L1 term otherwise.\nIt is less sensitive to outliers than torch.nn.MSELoss and in some cases\nprevents exploding gradients (e.g. see the paper Fast R-CNN by Ross Girshick).",
        "return_value": "",
        "parameters": "size_average (bool, optional) – Deprecated (see reduction). By default,\nthe losses are averaged over each loss element in the batch. Note that for\nsome losses, there are multiple elements per sample. If the field size_average\nis set to False, the losses are instead summed for each minibatch. Ignored\nwhen reduce is False. Default: True\nreduce (bool, optional) – Deprecated (see reduction). By default, the\nlosses are averaged or summed over observations for each minibatch depending\non size_average. When reduce is False, returns a loss per\nbatch element instead and ignores size_average. Default: True\nreduction (str, optional) – Specifies the reduction to apply to the output:\n'none' | 'mean' | 'sum'. 'none': no reduction will be applied,\n'mean': the sum of the output will be divided by the number of\nelements in the output, 'sum': the output will be summed. Note: size_average\nand reduce are in the process of being deprecated, and in the meantime,\nspecifying either of those two args will override reduction. Default: 'mean'\nbeta (float, optional) – Specifies the threshold at which to change between L1 and L2 loss.\nThe value must be non-negative. Default: 1.0",
        "input_shape": "\nInput: (∗)(*)(∗), where ∗*∗ means any number of dimensions.\nTarget: (∗)(*)(∗), same shape as the input.\nOutput: scalar. If reduction is 'none', then (∗)(*)(∗), same shape as the input.\n\n",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.quasirandom.SobolEngine",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.quasirandom.SobolEngine.html#torch.quasirandom.SobolEngine",
        "api_signature": "torch.quasirandom.SobolEngine(dimension, scramble=False, seed=None)",
        "api_description": "The torch.quasirandom.SobolEngine is an engine for generating\n(scrambled) Sobol sequences. Sobol sequences are an example of low\ndiscrepancy quasi-random sequences.",
        "return_value": "",
        "parameters": "dimension (Int) – The dimensionality of the sequence to be drawn\nscramble (bool, optional) – Setting this to True will produce\nscrambled Sobol sequences. Scrambling is\ncapable of producing better Sobol\nsequences. Default: False.\nseed (Int, optional) – This is the seed for the scrambling. The seed\nof the random number generator is set to this,\nif specified. Otherwise, it uses a random seed.\nDefault: None\nn (Int, optional) – The length of sequence of points to draw.\nDefault: 1\nout (Tensor, optional) – The output tensor\ndtype (torch.dtype, optional) – the desired data type of the\nreturned tensor.\nDefault: torch.float32\nm (Int) – The (base2) exponent of the number of points to draw.\nout (Tensor, optional) – The output tensor\ndtype (torch.dtype, optional) – the desired data type of the\nreturned tensor.\nDefault: torch.float32\nn (Int) – The number of steps to fast-forward by.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.functional.soft_margin_loss",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.functional.soft_margin_loss.html#torch.nn.functional.soft_margin_loss",
        "api_signature": "torch.nn.functional.soft_margin_loss(input, target, size_average=None, reduce=None, reduction='mean')",
        "api_description": "See SoftMarginLoss for details.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.SoftMarginLoss",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.SoftMarginLoss.html#torch.nn.SoftMarginLoss",
        "api_signature": "torch.nn.SoftMarginLoss(size_average=None, reduce=None, reduction='mean')",
        "api_description": "Creates a criterion that optimizes a two-class classification\nlogistic loss between input tensor xxx and target tensor yyy\n(containing 1 or -1).",
        "return_value": "",
        "parameters": "size_average (bool, optional) – Deprecated (see reduction). By default,\nthe losses are averaged over each loss element in the batch. Note that for\nsome losses, there are multiple elements per sample. If the field size_average\nis set to False, the losses are instead summed for each minibatch. Ignored\nwhen reduce is False. Default: True\nreduce (bool, optional) – Deprecated (see reduction). By default, the\nlosses are averaged or summed over observations for each minibatch depending\non size_average. When reduce is False, returns a loss per\nbatch element instead and ignores size_average. Default: True\nreduction (str, optional) – Specifies the reduction to apply to the output:\n'none' | 'mean' | 'sum'. 'none': no reduction will be applied,\n'mean': the sum of the output will be divided by the number of\nelements in the output, 'sum': the output will be summed. Note: size_average\nand reduce are in the process of being deprecated, and in the meantime,\nspecifying either of those two args will override reduction. Default: 'mean'",
        "input_shape": "\nInput: (∗)(*)(∗), where ∗*∗ means any number of dimensions.\nTarget: (∗)(*)(∗), same shape as the input.\nOutput: scalar. If reduction is 'none', then (∗)(*)(∗), same\nshape as input.\n\n",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.Softmax",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html#torch.nn.Softmax",
        "api_signature": "torch.nn.Softmax(dim=None)",
        "api_description": "Applies the Softmax function to an n-dimensional input Tensor.",
        "return_value": "a Tensor of the same dimension and shape as the input with\nvalues in the range [0, 1]\n",
        "parameters": "dim (int) – A dimension along which Softmax will be computed (so every slice\nalong dim will sum to 1).",
        "input_shape": "\nInput: (∗)(*)(∗) where * means, any number of additional\ndimensions\nOutput: (∗)(*)(∗), same shape as the input\n\n",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.softmax",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.softmax.html#torch.softmax",
        "api_signature": "torch.softmax(input, dim, *, dtype=None)",
        "api_description": "Alias for torch.nn.functional.softmax().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.functional.softmax",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.functional.softmax.html#torch.nn.functional.softmax",
        "api_signature": "torch.nn.functional.softmax(input, dim=None, _stacklevel=3, dtype=None)",
        "api_description": "Apply a softmax function.",
        "return_value": "",
        "parameters": "input (Tensor) – input\ndim (int) – A dimension along which softmax will be computed.\ndtype (torch.dtype, optional) – the desired data type of returned tensor.\nIf specified, the input tensor is casted to dtype before the operation\nis performed. This is useful for preventing data type overflows. Default: None.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.sparse.softmax",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.sparse.softmax.html#torch.sparse.softmax",
        "api_signature": "torch.sparse.softmax(input, dim, *, dtype=None)",
        "api_description": "Applies a softmax function.",
        "return_value": "",
        "parameters": "input (Tensor) – input\ndim (int) – A dimension along which softmax will be computed.\ndtype (torch.dtype, optional) – the desired data type\nof returned tensor.  If specified, the input tensor is\ncasted to dtype before the operation is\nperformed. This is useful for preventing data type\noverflows. Default: None",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.special.softmax",
        "api_url": "https://pytorch.org/docs/stable/special.html#torch.special.softmax",
        "api_signature": "torch.special.softmax(input, dim, *, dtype=None)",
        "api_description": "Computes the softmax function.",
        "return_value": "",
        "parameters": "input (Tensor) – input\ndim (int) – A dimension along which softmax will be computed.\ndtype (torch.dtype, optional) – the desired data type of returned tensor.\nIf specified, the input tensor is cast to dtype before the operation\nis performed. This is useful for preventing data type overflows. Default: None.",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> t = torch.ones(2, 2)\n>>> torch.special.softmax(t, 0)\ntensor([[0.5000, 0.5000],\n        [0.5000, 0.5000]])\n\n\n"
    },
    {
        "api_name": "torch.Tensor.softmax",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.softmax.html#torch.Tensor.softmax",
        "api_signature": "Tensor.softmax(dim)",
        "api_description": "Alias for torch.nn.functional.softmax().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.Softmax2d",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.Softmax2d.html#torch.nn.Softmax2d",
        "api_signature": "torch.nn.Softmax2d(*args, **kwargs)",
        "api_description": "Applies SoftMax over features to each spatial location.",
        "return_value": "a Tensor of the same dimension and shape as the input with\nvalues in the range [0, 1]\n",
        "parameters": "",
        "input_shape": "\nInput: (N,C,H,W)(N, C, H, W)(N,C,H,W) or (C,H,W)(C, H, W)(C,H,W).\nOutput: (N,C,H,W)(N, C, H, W)(N,C,H,W) or (C,H,W)(C, H, W)(C,H,W) (same shape as input)\n\n",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.transforms.SoftmaxTransform",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.transforms.SoftmaxTransform",
        "api_signature": "torch.distributions.transforms.SoftmaxTransform(cache_size=0)",
        "api_description": "Transform from unconstrained space to the simplex via y=exp⁡(x)y = \\exp(x)y=exp(x) then\nnormalizing.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.Softmin",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.Softmin.html#torch.nn.Softmin",
        "api_signature": "torch.nn.Softmin(dim=None)",
        "api_description": "Applies the Softmin function to an n-dimensional input Tensor.",
        "return_value": "a Tensor of the same dimension and shape as the input, with\nvalues in the range [0, 1]\n",
        "parameters": "dim (int) – A dimension along which Softmin will be computed (so every slice\nalong dim will sum to 1).",
        "input_shape": "\nInput: (∗)(*)(∗) where * means, any number of additional\ndimensions\nOutput: (∗)(*)(∗), same shape as the input\n\n",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.functional.softmin",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.functional.softmin.html#torch.nn.functional.softmin",
        "api_signature": "torch.nn.functional.softmin(input, dim=None, _stacklevel=3, dtype=None)",
        "api_description": "Apply a softmin function.",
        "return_value": "",
        "parameters": "input (Tensor) – input\ndim (int) – A dimension along which softmin will be computed (so every slice\nalong dim will sum to 1).\ndtype (torch.dtype, optional) – the desired data type of returned tensor.\nIf specified, the input tensor is casted to dtype before the operation\nis performed. This is useful for preventing data type overflows. Default: None.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.Softplus",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.Softplus.html#torch.nn.Softplus",
        "api_signature": "torch.nn.Softplus(beta=1.0, threshold=20.0)",
        "api_description": "Applies the Softplus function element-wise.",
        "return_value": "",
        "parameters": "beta (float) – the β\\betaβ value for the Softplus formulation. Default: 1\nthreshold (float) – values above this revert to a linear function. Default: 20",
        "input_shape": "\nInput: (∗)(*)(∗), where ∗*∗ means any number of dimensions.\nOutput: (∗)(*)(∗), same shape as the input.\n\n",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.functional.softplus",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.functional.softplus.html#torch.nn.functional.softplus",
        "api_signature": "torch.nn.functional.softplus(input, beta=1, threshold=20)",
        "api_description": "Applies element-wise, the function Softplus(x)=1β∗log⁡(1+exp⁡(β∗x))\\text{Softplus}(x) = \\frac{1}{\\beta} * \\log(1 + \\exp(\\beta * x))Softplus(x)=β1​∗log(1+exp(β∗x)).",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.transforms.SoftplusTransform",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.transforms.SoftplusTransform",
        "api_signature": "torch.distributions.transforms.SoftplusTransform(cache_size=0)",
        "api_description": "Transform via the mapping Softplus(x)=log⁡(1+exp⁡(x))\\text{Softplus}(x) = \\log(1 + \\exp(x))Softplus(x)=log(1+exp(x)).\nThe implementation reverts to the linear function when x>20x > 20x>20.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.Softshrink",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.Softshrink.html#torch.nn.Softshrink",
        "api_signature": "torch.nn.Softshrink(lambd=0.5)",
        "api_description": "Applies the soft shrinkage function element-wise.",
        "return_value": "",
        "parameters": "lambd (float) – the λ\\lambdaλ (must be no less than zero) value for the Softshrink formulation. Default: 0.5",
        "input_shape": "\nInput: (∗)(*)(∗), where ∗*∗ means any number of dimensions.\nOutput: (∗)(*)(∗), same shape as the input.\n\n",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.functional.softshrink",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.functional.softshrink.html#torch.nn.functional.softshrink",
        "api_signature": "torch.nn.functional.softshrink(input, lambd=0.5)",
        "api_description": "Applies the soft shrinkage function elementwise",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.Softsign",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.Softsign.html#torch.nn.Softsign",
        "api_signature": "torch.nn.Softsign(*args, **kwargs)",
        "api_description": "Applies the element-wise Softsign function.",
        "return_value": "",
        "parameters": "",
        "input_shape": "\nInput: (∗)(*)(∗), where ∗*∗ means any number of dimensions.\nOutput: (∗)(*)(∗), same shape as the input.\n\n",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.functional.softsign",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.functional.softsign.html#torch.nn.functional.softsign",
        "api_signature": "torch.nn.functional.softsign(input)",
        "api_description": "Applies element-wise, the function SoftSign(x)=x1+∣x∣\\text{SoftSign}(x) = \\frac{x}{1 + |x|}SoftSign(x)=1+∣x∣x​",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.linalg.solve",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.linalg.solve.html#torch.linalg.solve",
        "api_signature": "torch.linalg.solve(A, B, *, left=True, out=None)",
        "api_description": "Computes the solution of a square system of linear equations with a unique solution.",
        "return_value": "",
        "parameters": "A (Tensor) – tensor of shape (*, n, n) where * is zero or more batch dimensions.\nB (Tensor) – right-hand side tensor of shape (*, n) or  (*, n, k) or (n,) or (n, k)\naccording to the rules described above\nleft (bool, optional) – whether to solve the system AX=BAX=BAX=B or XA=BXA = BXA=B. Default: True.\nout (Tensor, optional) – output tensor. Ignored if None. Default: None.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.symbolic_shapes.DimConstraints.solve",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.fx.experimental.symbolic_shapes.DimConstraints.html#torch.fx.experimental.symbolic_shapes.DimConstraints.solve",
        "api_signature": "solve(disable_congruences=True, disable_equivalences=True)",
        "api_description": "Solve the system of constraint equations to find simplified constraints",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.linalg.solve_ex",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.linalg.solve_ex.html#torch.linalg.solve_ex",
        "api_signature": "torch.linalg.solve_ex(A, B, *, left=True, check_errors=False, out=None)",
        "api_description": "A version of solve() that does not perform error checks unless check_errors= True.\nIt also returns the info tensor returned by LAPACK’s getrf.",
        "return_value": "A named tuple (result, info).\n",
        "parameters": "A (Tensor) – tensor of shape (*, n, n) where * is zero or more batch dimensions.\nleft (bool, optional) – whether to solve the system AX=BAX=BAX=B or XA=BXA = BXA=B. Default: True.\ncheck_errors (bool, optional) – controls whether to check the content of infos and raise\nan error if it is non-zero. Default: False.\nout (tuple, optional) – tuple of two tensors to write the output to. Ignored if None. Default: None.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.linalg.solve_triangular",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.linalg.solve_triangular.html#torch.linalg.solve_triangular",
        "api_signature": "torch.linalg.solve_triangular(A, B, *, upper, left=True, unitriangular=False, out=None)",
        "api_description": "Computes the solution of a triangular system of linear equations with a unique solution.",
        "return_value": "",
        "parameters": "A (Tensor) – tensor of shape (*, n, n) (or (*, k, k) if left= True)\nwhere * is zero or more batch dimensions.\nB (Tensor) – right-hand side tensor of shape (*, n, k).\nupper (bool) – whether A is an upper or lower triangular matrix.\nleft (bool, optional) – whether to solve the system AX=BAX=BAX=B or XA=BXA = BXA=B. Default: True.\nunitriangular (bool, optional) – if True, the diagonal elements of A are assumed to be\nall equal to 1. Default: False.\nout (Tensor, optional) – output tensor. B may be passed as out and the result is computed in-place on B.\nIgnored if None. Default: None.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.sort",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.sort.html#torch.sort",
        "api_signature": "torch.sort(input, dim=-1, descending=False, stable=False, *, out=None)",
        "api_description": "Sorts the elements of the input tensor along a given dimension\nin ascending order by value.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.\ndim (int, optional) – the dimension to sort along\ndescending (bool, optional) – controls the sorting order (ascending or descending)\nstable (bool, optional) – makes the sorting routine stable, which guarantees that the order\nof equivalent elements is preserved.\nout (tuple, optional) – the output tuple of (Tensor, LongTensor) that can\nbe optionally given to be used as output buffers",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.sort",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.sort.html#torch.Tensor.sort",
        "api_signature": "Tensor.sort(dim=-1, descending=False)",
        "api_description": "See torch.sort()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.utils.rnn.PackedSequence.sorted_indices",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.PackedSequence.html#torch.nn.utils.rnn.PackedSequence.sorted_indices",
        "api_signature": null,
        "api_description": "Alias for field number 2",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.init.sparse_",
        "api_url": "https://pytorch.org/docs/stable/nn.init.html#torch.nn.init.sparse_",
        "api_signature": "torch.nn.init.sparse_(tensor, sparsity, std=0.01, generator=None)",
        "api_description": "Fill the 2D input Tensor as a sparse matrix.",
        "return_value": "",
        "parameters": "tensor – an n-dimensional torch.Tensor\nsparsity – The fraction of elements in each column to be set to zero\nstd – the standard deviation of the normal distribution used to generate\nthe non-zero values\ngenerator (Optional[Generator]) – the torch Generator to sample from (default: None)",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.sparse_bsc_tensor",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.sparse_bsc_tensor.html#torch.sparse_bsc_tensor",
        "api_signature": "torch.sparse_bsc_tensor(ccol_indices, row_indices, values, size=None, *, dtype=None, device=None, requires_grad=False, check_invariants=None)",
        "api_description": "Constructs a sparse tensor in BSC (Block Compressed Sparse\nColumn)) with specified 2-dimensional blocks at the\ngiven ccol_indices and row_indices. Sparse matrix\nmultiplication operations in BSC format are typically faster than that\nfor sparse tensors in COO format. Make you have a look at the\nnote on the data type of the indices.",
        "return_value": "",
        "parameters": "ccol_indices (array_like) – (B+1)-dimensional array of size\n(*batchsize, ncolblocks + 1). The last element of each\nbatch is the number of non-zeros. This tensor encodes the\nindex in values and row_indices depending on where the given\ncolumn starts. Each successive number in the tensor subtracted\nby the number before it denotes the number of elements in a\ngiven column.\nrow_indices (array_like) – Row block co-ordinates of each block in\nvalues. (B+1)-dimensional tensor with the same length\nas values.\nvalues (array_list) – Initial blocks for the tensor. Can be a list,\ntuple, NumPy ndarray, and other types that\nrepresents a (1 + 2 + K)-dimensional tensor where K is the\nnumber of dense dimensions.\nsize (list, tuple, torch.Size, optional) – Size of the\nsparse tensor: (*batchsize, nrows * blocksize[0], ncols *\nblocksize[1], *densesize) If not provided, the size will be\ninferred as the minimum size big enough to hold all non-zero\nblocks.\ndtype (torch.dtype, optional) – the desired data type of\nreturned tensor.  Default: if None, infers data type from\nvalues.\ndevice (torch.device, optional) – the desired device of\nreturned tensor.  Default: if None, uses the current device\nfor the default tensor type (see\ntorch.set_default_device()). device will be\nthe CPU for CPU tensor types and the current CUDA device for\nCUDA tensor types.\nrequires_grad (bool, optional) – If autograd should record operations on the\nreturned tensor. Default: False.\ncheck_invariants (bool, optional) – If sparse tensor invariants are checked.\nDefault: as returned by torch.sparse.check_sparse_tensor_invariants.is_enabled(),\ninitially False.",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> ccol_indices = [0, 1, 2]\n>>> row_indices = [0, 1]\n>>> values = [[[1, 2], [3, 4]], [[5, 6], [7, 8]]]\n>>> torch.sparse_bsc_tensor(torch.tensor(ccol_indices, dtype=torch.int64),\n...                         torch.tensor(row_indices, dtype=torch.int64),\n...                         torch.tensor(values), dtype=torch.double)\ntensor(ccol_indices=tensor([0, 1, 2]),\n       row_indices=tensor([0, 1]),\n       values=tensor([[[1., 2.],\n                       [3., 4.]],\n                      [[5., 6.],\n                       [7., 8.]]]), size=(2, 2), nnz=2, dtype=torch.float64,\n       layout=torch.sparse_bsc)\n\n\n"
    },
    {
        "api_name": "torch.sparse_bsr_tensor",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.sparse_bsr_tensor.html#torch.sparse_bsr_tensor",
        "api_signature": "torch.sparse_bsr_tensor(crow_indices, col_indices, values, size=None, *, dtype=None, device=None, requires_grad=False, check_invariants=None)",
        "api_description": "Constructs a sparse tensor in BSR (Block Compressed Sparse Row)) with specified 2-dimensional blocks at the given\ncrow_indices and col_indices. Sparse matrix\nmultiplication operations in BSR format are typically faster than that\nfor sparse tensors in COO format. Make you have a look at the\nnote on the data type of the indices.",
        "return_value": "",
        "parameters": "crow_indices (array_like) – (B+1)-dimensional array of size\n(*batchsize, nrowblocks + 1).  The last element of each\nbatch is the number of non-zeros. This tensor encodes the\nblock index in values and col_indices depending on where the\ngiven row block starts. Each successive number in the tensor\nsubtracted by the number before it denotes the number of\nblocks in a given row.\ncol_indices (array_like) – Column block co-ordinates of each block\nin values. (B+1)-dimensional tensor with the same length as\nvalues.\nvalues (array_list) – Initial values for the tensor. Can be a list,\ntuple, NumPy ndarray, scalar, and other types that\nrepresents a (1 + 2 + K)-dimensional tensor where K is the\nnumber of dense dimensions.\nsize (list, tuple, torch.Size, optional) – Size of the\nsparse tensor: (*batchsize, nrows * blocksize[0], ncols *\nblocksize[1], *densesize) where blocksize ==\nvalues.shape[1:3]. If not provided, the size will be\ninferred as the minimum size big enough to hold all non-zero\nblocks.\ndtype (torch.dtype, optional) – the desired data type of\nreturned tensor.  Default: if None, infers data type from\nvalues.\ndevice (torch.device, optional) – the desired device of\nreturned tensor.  Default: if None, uses the current device\nfor the default tensor type (see\ntorch.set_default_device()). device will be\nthe CPU for CPU tensor types and the current CUDA device for\nCUDA tensor types.\nrequires_grad (bool, optional) – If autograd should record operations on the\nreturned tensor. Default: False.\ncheck_invariants (bool, optional) – If sparse tensor invariants are checked.\nDefault: as returned by torch.sparse.check_sparse_tensor_invariants.is_enabled(),\ninitially False.",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> crow_indices = [0, 1, 2]\n>>> col_indices = [0, 1]\n>>> values = [[[1, 2], [3, 4]], [[5, 6], [7, 8]]]\n>>> torch.sparse_bsr_tensor(torch.tensor(crow_indices, dtype=torch.int64),\n...                         torch.tensor(col_indices, dtype=torch.int64),\n...                         torch.tensor(values), dtype=torch.double)\ntensor(crow_indices=tensor([0, 1, 2]),\n       col_indices=tensor([0, 1]),\n       values=tensor([[[1., 2.],\n                       [3., 4.]],\n                      [[5., 6.],\n                       [7., 8.]]]), size=(2, 2), nnz=2, dtype=torch.float64,\n       layout=torch.sparse_bsr)\n\n\n"
    },
    {
        "api_name": "torch.sparse_compressed_tensor",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.sparse_compressed_tensor.html#torch.sparse_compressed_tensor",
        "api_signature": "torch.sparse_compressed_tensor(compressed_indices, plain_indices, values, size=None, *, dtype=None, layout=None, device=None, requires_grad=False, check_invariants=None)",
        "api_description": "Constructs a sparse tensor in Compressed Sparse format - CSR,\nCSC, BSR, or BSC - with specified values at\nthe given compressed_indices and plain_indices. Sparse\nmatrix multiplication operations in Compressed Sparse format are\ntypically faster than that for sparse tensors in COO format. Make you\nhave a look at the note on the data type of the indices.",
        "return_value": "",
        "parameters": "compressed_indices (array_like) – (B+1)-dimensional array of size\n(*batchsize, compressed_dim_size + 1).  The last element of\neach batch is the number of non-zero elements or blocks. This\ntensor encodes the index in values and plain_indices\ndepending on where the given compressed dimension (row or\ncolumn) starts. Each successive number in the tensor\nsubtracted by the number before it denotes the number of\nelements or blocks in a given compressed dimension.\nplain_indices (array_like) – Plain dimension (column or row)\nco-ordinates of each element or block in values. (B+1)-dimensional\ntensor with the same length as values.\nvalues (array_list) – Initial values for the tensor. Can be a list,\ntuple, NumPy ndarray, scalar, and other types.  that\nrepresents a (1+K)-dimensional (for CSR and CSC layouts) or\n(1+2+K)-dimensional tensor (for BSR and BSC layouts) where\nK is the number of dense dimensions.\nsize (list, tuple, torch.Size, optional) – Size of the\nsparse tensor: (*batchsize, nrows * blocksize[0], ncols *\nblocksize[1], *densesize) where blocksize[0] ==\nblocksize[1] == 1 for CSR and CSC formats. If not provided,\nthe size will be inferred as the minimum size big enough to\nhold all non-zero elements or blocks.\ndtype (torch.dtype, optional) – the desired data type of\nreturned tensor.  Default: if None, infers data type from\nvalues.\nlayout (torch.layout, required) – the desired layout of\nreturned tensor: torch.sparse_csr,\ntorch.sparse_csc, torch.sparse_bsr, or\ntorch.sparse_bsc.\ndevice (torch.device, optional) – the desired device of\nreturned tensor.  Default: if None, uses the current device\nfor the default tensor type (see\ntorch.set_default_device()). device will be\nthe CPU for CPU tensor types and the current CUDA device for\nCUDA tensor types.\nrequires_grad (bool, optional) – If autograd should record operations on the\nreturned tensor. Default: False.\ncheck_invariants (bool, optional) – If sparse tensor invariants are checked.\nDefault: as returned by torch.sparse.check_sparse_tensor_invariants.is_enabled(),\ninitially False.",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> compressed_indices = [0, 2, 4]\n>>> plain_indices = [0, 1, 0, 1]\n>>> values = [1, 2, 3, 4]\n>>> torch.sparse_compressed_tensor(torch.tensor(compressed_indices, dtype=torch.int64),\n...                                torch.tensor(plain_indices, dtype=torch.int64),\n...                                torch.tensor(values), dtype=torch.double, layout=torch.sparse_csr)\ntensor(crow_indices=tensor([0, 2, 4]),\n       col_indices=tensor([0, 1, 0, 1]),\n       values=tensor([1., 2., 3., 4.]), size=(2, 2), nnz=4,\n       dtype=torch.float64, layout=torch.sparse_csr)\n\n\n"
    },
    {
        "api_name": "torch.sparse_coo_tensor",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.sparse_coo_tensor.html#torch.sparse_coo_tensor",
        "api_signature": "torch.sparse_coo_tensor(indices, values, size=None, *, dtype=None, device=None, requires_grad=False, check_invariants=None, is_coalesced=None)",
        "api_description": "Constructs a sparse tensor in COO(rdinate) format with specified values at the given\nindices.",
        "return_value": "",
        "parameters": "indices (array_like) – Initial data for the tensor. Can be a list, tuple,\nNumPy ndarray, scalar, and other types. Will be cast to a torch.LongTensor\ninternally. The indices are the coordinates of the non-zero values in the matrix, and thus\nshould be two-dimensional where the first dimension is the number of tensor dimensions and\nthe second dimension is the number of non-zero values.\nvalues (array_like) – Initial values for the tensor. Can be a list, tuple,\nNumPy ndarray, scalar, and other types.\nsize (list, tuple, or torch.Size, optional) – Size of the sparse tensor. If not\nprovided the size will be inferred as the minimum size big enough to hold all non-zero\nelements.\ndtype (torch.dtype, optional) – the desired data type of returned tensor.\nDefault: if None, infers data type from values.\ndevice (torch.device, optional) – the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_device()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types.\nrequires_grad (bool, optional) – If autograd should record operations on the\nreturned tensor. Default: False.\ncheck_invariants (bool, optional) – If sparse tensor invariants are checked.\nDefault: as returned by torch.sparse.check_sparse_tensor_invariants.is_enabled(),\ninitially False.\nis_coalesced (bool, optional) – When``True``, the caller is\nresponsible for providing tensor indices that correspond to a\ncoalesced tensor.  If the check_invariants flag is\nFalse, no error will be raised if the prerequisites are not\nmet and this will lead to silently incorrect results. To force\ncoalescion please use coalesce() on the resulting\nTensor.\nDefault: None: except for trivial cases (e.g. nnz < 2) the\nresulting Tensor has is_coalesced set to False`.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.sparse_csc_tensor",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.sparse_csc_tensor.html#torch.sparse_csc_tensor",
        "api_signature": "torch.sparse_csc_tensor(ccol_indices, row_indices, values, size=None, *, dtype=None, device=None, requires_grad=False, check_invariants=None)",
        "api_description": "Constructs a sparse tensor in CSC (Compressed Sparse Column) with specified values at the given\nccol_indices and row_indices. Sparse matrix\nmultiplication operations in CSC format are typically faster than that\nfor sparse tensors in COO format. Make you have a look at the\nnote on the data type of the indices.",
        "return_value": "",
        "parameters": "ccol_indices (array_like) – (B+1)-dimensional array of size\n(*batchsize, ncols + 1).  The last element of each batch\nis the number of non-zeros. This tensor encodes the index in\nvalues and row_indices depending on where the given column\nstarts. Each successive number in the tensor subtracted by the\nnumber before it denotes the number of elements in a given\ncolumn.\nrow_indices (array_like) – Row co-ordinates of each element in\nvalues. (B+1)-dimensional tensor with the same length as\nvalues.\nvalues (array_list) – Initial values for the tensor. Can be a list,\ntuple, NumPy ndarray, scalar, and other types that\nrepresents a (1+K)-dimensional tensor where K is the number\nof dense dimensions.\nsize (list, tuple, torch.Size, optional) – Size of the\nsparse tensor: (*batchsize, nrows, ncols, *densesize). If\nnot provided, the size will be inferred as the minimum size\nbig enough to hold all non-zero elements.\ndtype (torch.dtype, optional) – the desired data type of\nreturned tensor.  Default: if None, infers data type from\nvalues.\ndevice (torch.device, optional) – the desired device of\nreturned tensor.  Default: if None, uses the current device\nfor the default tensor type (see\ntorch.set_default_device()). device will be\nthe CPU for CPU tensor types and the current CUDA device for\nCUDA tensor types.\nrequires_grad (bool, optional) – If autograd should record operations on the\nreturned tensor. Default: False.\ncheck_invariants (bool, optional) – If sparse tensor invariants are checked.\nDefault: as returned by torch.sparse.check_sparse_tensor_invariants.is_enabled(),\ninitially False.",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> ccol_indices = [0, 2, 4]\n>>> row_indices = [0, 1, 0, 1]\n>>> values = [1, 2, 3, 4]\n>>> torch.sparse_csc_tensor(torch.tensor(ccol_indices, dtype=torch.int64),\n...                         torch.tensor(row_indices, dtype=torch.int64),\n...                         torch.tensor(values), dtype=torch.double)\ntensor(ccol_indices=tensor([0, 2, 4]),\n       row_indices=tensor([0, 1, 0, 1]),\n       values=tensor([1., 2., 3., 4.]), size=(2, 2), nnz=4,\n       dtype=torch.float64, layout=torch.sparse_csc)\n\n\n"
    },
    {
        "api_name": "torch.sparse_csr_tensor",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.sparse_csr_tensor.html#torch.sparse_csr_tensor",
        "api_signature": "torch.sparse_csr_tensor(crow_indices, col_indices, values, size=None, *, dtype=None, device=None, requires_grad=False, check_invariants=None)",
        "api_description": "Constructs a sparse tensor in CSR (Compressed Sparse Row) with specified\nvalues at the given crow_indices and col_indices. Sparse matrix multiplication operations\nin CSR format are typically faster than that for sparse tensors in COO format. Make you have a look\nat the note on the data type of the indices.",
        "return_value": "",
        "parameters": "crow_indices (array_like) – (B+1)-dimensional array of size\n(*batchsize, nrows + 1).  The last element of each batch\nis the number of non-zeros. This tensor encodes the index in\nvalues and col_indices depending on where the given row\nstarts. Each successive number in the tensor subtracted by the\nnumber before it denotes the number of elements in a given\nrow.\ncol_indices (array_like) – Column co-ordinates of each element in\nvalues. (B+1)-dimensional tensor with the same length\nas values.\nvalues (array_list) – Initial values for the tensor. Can be a list,\ntuple, NumPy ndarray, scalar, and other types that\nrepresents a (1+K)-dimensional tensor where K is the number\nof dense dimensions.\nsize (list, tuple, torch.Size, optional) – Size of the\nsparse tensor: (*batchsize, nrows, ncols, *densesize). If\nnot provided, the size will be inferred as the minimum size\nbig enough to hold all non-zero elements.\ndtype (torch.dtype, optional) – the desired data type of\nreturned tensor.  Default: if None, infers data type from\nvalues.\ndevice (torch.device, optional) – the desired device of\nreturned tensor.  Default: if None, uses the current device\nfor the default tensor type (see\ntorch.set_default_device()). device will be\nthe CPU for CPU tensor types and the current CUDA device for\nCUDA tensor types.\nrequires_grad (bool, optional) – If autograd should record operations on the\nreturned tensor. Default: False.\ncheck_invariants (bool, optional) – If sparse tensor invariants are checked.\nDefault: as returned by torch.sparse.check_sparse_tensor_invariants.is_enabled(),\ninitially False.",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> crow_indices = [0, 2, 4]\n>>> col_indices = [0, 1, 0, 1]\n>>> values = [1, 2, 3, 4]\n>>> torch.sparse_csr_tensor(torch.tensor(crow_indices, dtype=torch.int64),\n...                         torch.tensor(col_indices, dtype=torch.int64),\n...                         torch.tensor(values), dtype=torch.double)\ntensor(crow_indices=tensor([0, 2, 4]),\n       col_indices=tensor([0, 1, 0, 1]),\n       values=tensor([1., 2., 3., 4.]), size=(2, 2), nnz=4,\n       dtype=torch.float64, layout=torch.sparse_csr)\n\n\n"
    },
    {
        "api_name": "torch.Tensor.sparse_dim",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.sparse_dim.html#torch.Tensor.sparse_dim",
        "api_signature": "Tensor.sparse_dim()",
        "api_description": "Return the number of sparse dimensions in a sparse tensor self.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.sparse_mask",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.sparse_mask.html#torch.Tensor.sparse_mask",
        "api_signature": "Tensor.sparse_mask(mask)",
        "api_description": "Returns a new sparse tensor with values from a\nstrided tensor self filtered by the indices of the sparse\ntensor mask. The values of mask sparse tensor are\nignored. self and mask tensors must have the same\nshape.",
        "return_value": "",
        "parameters": "mask (Tensor) – a sparse tensor whose indices are used as a filter",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.sparse_resize_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.sparse_resize_.html#torch.Tensor.sparse_resize_",
        "api_signature": "Tensor.sparse_resize_(size, sparse_dim, dense_dim)",
        "api_description": "Resizes self sparse tensor to the desired\nsize and the number of sparse and dense dimensions.",
        "return_value": "",
        "parameters": "size (torch.Size) – the desired size. If self is non-empty\nsparse tensor, the desired size cannot be smaller than the\noriginal size.\nsparse_dim (int) – the number of sparse dimensions\ndense_dim (int) – the number of dense dimensions",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.sparse_resize_and_clear_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.sparse_resize_and_clear_.html#torch.Tensor.sparse_resize_and_clear_",
        "api_signature": "Tensor.sparse_resize_and_clear_(size, sparse_dim, dense_dim)",
        "api_description": "Removes all specified elements from a sparse tensor self and resizes self to the desired\nsize and the number of sparse and dense dimensions.",
        "return_value": "",
        "parameters": "size (torch.Size) – the desired size.\nsparse_dim (int) – the number of sparse dimensions\ndense_dim (int) – the number of dense dimensions",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.SparseAdam",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.SparseAdam.html#torch.optim.SparseAdam",
        "api_signature": "torch.optim.SparseAdam(params, lr=0.001, betas=(0.9, 0.999)",
        "api_description": "SparseAdam implements a masked version of the Adam algorithm\nsuitable for sparse gradients. Currently, due to implementation constraints (explained\nbelow), SparseAdam is only intended for a narrow subset of use cases, specifically\nparameters of a dense layout with gradients of a sparse layout. This occurs in a\nspecial case where the module backwards produces grads already in a sparse layout.\nOne example NN module that behaves as such is nn.Embedding(sparse=True).",
        "return_value": "a handle that can be used to remove the added hook by calling\nhandle.remove()\na handle that can be used to remove the added hook by calling\nhandle.remove()\na handle that can be used to remove the added hook by calling\nhandle.remove()\na handle that can be used to remove the added hook by calling\nhandle.remove()\na handle that can be used to remove the added hook by calling\nhandle.remove()\na handle that can be used to remove the added hook by calling\nhandle.remove()\n",
        "parameters": "params (iterable) – iterable of parameters to optimize or dicts defining\nparameter groups\nlr (float, optional) – learning rate (default: 1e-3)\nbetas (Tuple[float, float], optional) – coefficients used for computing\nrunning averages of gradient and its square (default: (0.9, 0.999))\neps (float, optional) – term added to the denominator to improve\nnumerical stability (default: 1e-8)\nmaximize (bool, optional) – maximize the objective with respect to the\nparams, instead of minimizing (default: False)\nparam_group (dict) – Specifies what Tensors should be optimized along with group\nspecific optimization options.\nstate_dict (dict) – optimizer state. Should be an object returned\nfrom a call to state_dict().\nhook (Callable) – The user defined hook to be registered.\nprepend (bool) – If True, the provided post hook will be fired before\nall the already registered post-hooks on load_state_dict. Otherwise,\nthe provided hook will be fired after all the already registered\npost-hooks. (default: False)\nhook (Callable) – The user defined hook to be registered.\nprepend (bool) – If True, the provided pre hook will be fired before\nall the already registered pre-hooks on load_state_dict. Otherwise,\nthe provided hook will be fired after all the already registered\npre-hooks. (default: False)\nhook (Callable) – The user defined hook to be registered.\nprepend (bool) – If True, the provided post hook will be fired before\nall the already registered post-hooks on state_dict. Otherwise,\nthe provided hook will be fired after all the already registered\npost-hooks. (default: False)\nhook (Callable) – The user defined hook to be registered.\nprepend (bool) – If True, the provided pre hook will be fired before\nall the already registered pre-hooks on state_dict. Otherwise,\nthe provided hook will be fired after all the already registered\npre-hooks. (default: False)\nhook (Callable) – The user defined hook to be registered.\nhook (Callable) – The user defined hook to be registered.\nclosure (Callable, optional) – A closure that reevaluates the model\nand returns the loss.\nset_to_none (bool) – instead of setting to zero, set the grads to None.\nThis will in general have lower memory footprint, and can modestly improve performance.\nHowever, it changes certain behaviors. For example:\n1. When the user tries to access a gradient and perform manual ops on it,\na None attribute or a Tensor full of 0s will behave differently.\n2. If the user requests zero_grad(set_to_none=True) followed by a backward pass, .grads\nare guaranteed to be None for params that did not receive a gradient.\n3. torch.optim optimizers have a different behavior if the gradient is 0 or None\n(in one case it does the step with a gradient of 0 and in the other it skips\nthe step altogether).",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.multiprocessing.spawn.spawn",
        "api_url": "https://pytorch.org/docs/stable/multiprocessing.html#torch.multiprocessing.spawn.spawn",
        "api_signature": "torch.multiprocessing.spawn.spawn(fn, args=()",
        "api_description": "Spawns nprocs processes that run fn with args.",
        "return_value": "None if join is True,\nProcessContext if join is False\n",
        "parameters": "fn (function) – Function is called as the entrypoint of the\nspawned process. This function must be defined at the top\nlevel of a module so it can be pickled and spawned. This\nis a requirement imposed by multiprocessing.\nThe function is called as fn(i, *args), where i is\nthe process index and args is the passed through tuple\nof arguments.\nargs (tuple) – Arguments passed to fn.\nnprocs (int) – Number of processes to spawn.\njoin (bool) – Perform a blocking join on all processes.\ndaemon (bool) – The spawned processes’ daemon flag. If set to True,\ndaemonic processes will be created.\nstart_method (str) – (deprecated) this method will always use spawn\nas the start method. To use a different start method\nuse start_processes().",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.multiprocessing.SpawnContext",
        "api_url": "https://pytorch.org/docs/stable/multiprocessing.html#torch.multiprocessing.SpawnContext",
        "api_signature": null,
        "api_description": "Returned by spawn() when called with join=False.",
        "return_value": "",
        "parameters": "timeout (float) – Wait this long before giving up on waiting.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.sparse.spdiags",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.sparse.spdiags.html#torch.sparse.spdiags",
        "api_signature": "torch.sparse.spdiags(diagonals, offsets, shape, layout=None)",
        "api_description": "Creates a sparse 2D tensor by placing the values from rows of\ndiagonals along specified diagonals of the output",
        "return_value": "",
        "parameters": "diagonals (Tensor) – Matrix storing diagonals row-wise\noffsets (Tensor) – The diagonals to be set, stored as a vector\nshape (2-tuple of ints) – The desired shape of the result\nlayout (torch.layout, optional) – The desired layout of the\nreturned tensor. torch.sparse_coo, torch.sparse_csc and torch.sparse_csr\nare supported. Default: torch.sparse_coo",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.utils.spectral_norm",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.utils.spectral_norm.html#torch.nn.utils.spectral_norm",
        "api_signature": "torch.nn.utils.spectral_norm(module, name='weight', n_power_iterations=1, eps=1e-12, dim=None)",
        "api_description": "Apply spectral normalization to a parameter in the given module.",
        "return_value": "The original module with the spectral norm hook\n",
        "parameters": "module (nn.Module) – containing module\nname (str, optional) – name of weight parameter\nn_power_iterations (int, optional) – number of power iterations to\ncalculate spectral norm\neps (float, optional) – epsilon for numerical stability in\ncalculating norms\ndim (int, optional) – dimension corresponding to number of outputs,\nthe default is 0, except for modules that are instances of\nConvTranspose{1,2,3}d, when it is 1",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.utils.parametrizations.spectral_norm",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.utils.parametrizations.spectral_norm.html#torch.nn.utils.parametrizations.spectral_norm",
        "api_signature": "torch.nn.utils.parametrizations.spectral_norm(module, name='weight', n_power_iterations=1, eps=1e-12, dim=None)",
        "api_description": "Apply spectral normalization to a parameter in the given module.",
        "return_value": "The original module with a new parametrization registered to the specified\nweight\n",
        "parameters": "module (nn.Module) – containing module\nname (str, optional) – name of weight parameter. Default: \"weight\".\nn_power_iterations (int, optional) – number of power iterations to\ncalculate spectral norm. Default: 1.\neps (float, optional) – epsilon for numerical stability in\ncalculating norms. Default: 1e-12.\ndim (int, optional) – dimension corresponding to number of outputs.\nDefault: 0, except for modules that are instances of\nConvTranspose{1,2,3}d, when it is 1",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.special.spherical_bessel_j0",
        "api_url": "https://pytorch.org/docs/stable/special.html#torch.special.spherical_bessel_j0",
        "api_signature": "torch.special.spherical_bessel_j0(input, *, out=None)",
        "api_description": "Spherical Bessel function of the first kind of order 000.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.split",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.split.html#torch.split",
        "api_signature": "torch.split(tensor, split_size_or_sections, dim=0)",
        "api_description": "Splits the tensor into chunks. Each chunk is a view of the original tensor.",
        "return_value": "",
        "parameters": "tensor (Tensor) – tensor to split.\nsplit_size_or_sections (int) or (list(int)) – size of a single chunk or\nlist of sizes for each chunk\ndim (int) – dimension along which to split the tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.split",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.split.html#torch.Tensor.split",
        "api_signature": "Tensor.split(split_size, dim=0)",
        "api_description": "See torch.split()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.sqrt",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.sqrt.html#torch.sqrt",
        "api_signature": "torch.sqrt(input, *, out=None)",
        "api_description": "Returns a new tensor with the square-root of the elements of input.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.sqrt",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.sqrt.html#torch.Tensor.sqrt",
        "api_signature": "Tensor.sqrt()",
        "api_description": "See torch.sqrt()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.sqrt_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.sqrt_.html#torch.Tensor.sqrt_",
        "api_signature": "Tensor.sqrt_()",
        "api_description": "In-place version of sqrt()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.square",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.square.html#torch.square",
        "api_signature": "torch.square(input, *, out=None)",
        "api_description": "Returns a new tensor with the square of the elements of input.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.square",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.square.html#torch.Tensor.square",
        "api_signature": "Tensor.square()",
        "api_description": "See torch.square()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.square_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.square_.html#torch.Tensor.square_",
        "api_signature": "Tensor.square_()",
        "api_description": "In-place version of square()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.squeeze",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.squeeze.html#torch.squeeze",
        "api_signature": "torch.squeeze(input, dim=None)",
        "api_description": "Returns a tensor with all specified dimensions of input of size 1 removed.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.\ndim (int or tuple of ints, optional) –\nif given, the input will be squeezedonly in the specified dimensions.\nChanged in version 2.0: dim now accepts tuples of dimensions.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.squeeze",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.squeeze.html#torch.Tensor.squeeze",
        "api_signature": "Tensor.squeeze(dim=None)",
        "api_description": "See torch.squeeze()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.squeeze_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.squeeze_.html#torch.Tensor.squeeze_",
        "api_signature": "Tensor.squeeze_(dim=None)",
        "api_description": "In-place version of squeeze()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.sspaddmm",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.sspaddmm.html#torch.sspaddmm",
        "api_signature": "torch.sspaddmm(input, mat1, mat2, *, beta=1, alpha=1, out=None)",
        "api_description": "Matrix multiplies a sparse tensor mat1 with a dense tensor\nmat2, then adds the sparse tensor input to the result.",
        "return_value": "",
        "parameters": "input (Tensor) – a sparse matrix to be added\nmat1 (Tensor) – a sparse matrix to be matrix multiplied\nmat2 (Tensor) – a dense matrix to be matrix multiplied\nbeta (Number, optional) – multiplier for mat (β\\betaβ)\nalpha (Number, optional) – multiplier for mat1@mat2mat1 @ mat2mat1@mat2 (α\\alphaα)\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.sspaddmm",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.sspaddmm.html#torch.Tensor.sspaddmm",
        "api_signature": "Tensor.sspaddmm(mat1, mat2, *, beta=1, alpha=1)",
        "api_description": "See torch.sspaddmm()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.constraints.stack",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.constraints.stack",
        "api_signature": null,
        "api_description": "alias of _Stack",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.stack",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.stack.html#torch.stack",
        "api_signature": "torch.stack(tensors, dim=0, *, out=None)",
        "api_description": "Concatenates a sequence of tensors along a new dimension.",
        "return_value": "",
        "parameters": "tensors (sequence of Tensors) – sequence of tensors to concatenate\ndim (int, optional) – dimension to insert. Has to be between 0 and the number\nof dimensions of concatenated tensors (inclusive). Default: 0\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.func.stack_module_state",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.func.stack_module_state.html#torch.func.stack_module_state",
        "api_signature": "torch.func.stack_module_state(models)",
        "api_description": "Prepares a list of torch.nn.Modules for ensembling with vmap().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.Node.stack_trace",
        "api_url": "https://pytorch.org/docs/stable/fx.html#torch.fx.Node.stack_trace",
        "api_signature": null,
        "api_description": "Return the Python stack trace that was recorded during tracing, if any.\nWhen traced with fx.Tracer, this property is usually populated by\nTracer.create_proxy. To record stack traces during tracing for debug purposes,\nset record_stack_traces = True on the Tracer instance.\nWhen traced with dynamo, this property will be populated by default by\nOutputGraph.create_proxy.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.data.StackDataset",
        "api_url": "https://pytorch.org/docs/stable/data.html#torch.utils.data.StackDataset",
        "api_signature": "torch.utils.data.StackDataset(*args, **kwargs)",
        "api_description": "Dataset as a stacking of multiple datasets.",
        "return_value": "",
        "parameters": "*args (Dataset) – Datasets for stacking returned as tuple.\n**kwargs (Dataset) – Datasets for stacking returned as dict.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.transforms.StackTransform",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.transforms.StackTransform",
        "api_signature": "torch.distributions.transforms.StackTransform(tseq, dim=0, cache_size=0)",
        "api_description": "Transform functor that applies a sequence of transforms tseq\ncomponent-wise to each submatrix at dim\nin a way compatible with torch.stack().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.fx.custom_config.StandaloneModuleConfigEntry",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.quantization.fx.custom_config.StandaloneModuleConfigEntry.html#torch.ao.quantization.fx.custom_config.StandaloneModuleConfigEntry",
        "api_signature": "torch.ao.quantization.fx.custom_config.StandaloneModuleConfigEntry(qconfig_mapping: 'Optional[QConfigMapping]', example_inputs: 'Tuple[Any, ...]', prepare_custom_config: 'Optional[PrepareCustomConfig]', backend_config: 'Optional[BackendConfig]')",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.mps.profiler.start",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.mps.profiler.start.html#torch.mps.profiler.start",
        "api_signature": "torch.mps.profiler.start(mode='interval', wait_until_completed=False)",
        "api_description": "Start OS Signpost tracing from MPS backend.",
        "return_value": "",
        "parameters": "mode (str) – OS Signpost tracing mode could be “interval”, “event”,\nor both “interval,event”.\nThe interval mode traces the duration of execution of the operations,\nwhereas event mode marks the completion of executions.\nSee document Recording Performance Data for more info.\nwait_until_completed (bool) – Waits until the MPS Stream complete\nexecuting each encoded GPU operation. This helps generating single\ndispatches on the trace’s timeline.\nNote that enabling this option would affect the performance negatively.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.multiprocessing.start_processes",
        "api_url": "https://pytorch.org/docs/stable/elastic/multiprocessing.html#torch.distributed.elastic.multiprocessing.start_processes",
        "api_signature": "torch.distributed.elastic.multiprocessing.start_processes(name, entrypoint, args, envs, logs_specs, log_line_prefixes=None, start_method='spawn')",
        "api_description": "Start n copies of entrypoint processes with the provided options.",
        "return_value": "",
        "parameters": "name (str) – a human readable short name that describes what the processes are\n(used as header when tee’ing stdout/stderr outputs)\nentrypoint (Union[Callable, str]) – either a Callable (function) or cmd (binary)\nargs (Dict[int, Tuple]) – arguments to each replica\nenvs (Dict[int, Dict[str, str]]) – env vars to each replica\nlog_dir – directory used to write log files\nstart_method (str) – multiprocessing start method (spawn, fork, forkserver)\nignored for binaries\nredirects – which std streams to redirect to a log file\ntee – which std streams to redirect + print to console\nlocal_ranks_filter – which ranks’ logs to print to console",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.pipeline.sync.skip.skippable.stash",
        "api_url": "https://pytorch.org/docs/stable/pipeline.html#torch.distributed.pipeline.sync.skip.skippable.stash",
        "api_signature": "torch.distributed.pipeline.sync.skip.skippable.stash(name, tensor)",
        "api_description": "The command to stash a skip tensor.",
        "return_value": "",
        "parameters": "name (str) – name of skip tensor\ninput (torch.Tensor or None) – tensor to pass to the skip connection",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.monitor.Stat",
        "api_url": "https://pytorch.org/docs/stable/monitor.html#torch.monitor.Stat",
        "api_signature": null,
        "api_description": "Stat is used to compute summary statistics in a performant way over\nfixed intervals. Stat logs the statistics as an Event once every\nwindow_size duration. When the window closes the stats are logged\nvia the event handlers as a torch.monitor.Stat event.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.checkpoint.stateful.Stateful.state_dict",
        "api_url": "https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.stateful.Stateful.state_dict",
        "api_signature": "state_dict()",
        "api_description": "Objects should return their state_dict representation as a dictionary.\nThe output of this function will be checkpointed, and later restored in\nload_state_dict().",
        "return_value": "The objects state dict\n",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.optim.PostLocalSGDOptimizer.state_dict",
        "api_url": "https://pytorch.org/docs/stable/distributed.optim.html#torch.distributed.optim.PostLocalSGDOptimizer.state_dict",
        "api_signature": "state_dict()",
        "api_description": "This is the same as torch.optim.Optimizer state_dict(),\nbut adds an extra entry to record model averager’s step to the checkpoint\nto ensure reload does not cause unnecessary warm up again.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.optim.ZeroRedundancyOptimizer.state_dict",
        "api_url": "https://pytorch.org/docs/stable/distributed.optim.html#torch.distributed.optim.ZeroRedundancyOptimizer.state_dict",
        "api_signature": "state_dict()",
        "api_description": "Return the last global optimizer state known to this rank.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.jit.ScriptModule.state_dict",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.jit.ScriptModule.html#torch.jit.ScriptModule.state_dict",
        "api_signature": "state_dict(*args, destination=None, prefix='', keep_vars=False)",
        "api_description": "Return a dictionary containing references to the whole state of the module.",
        "return_value": "a dictionary containing a whole state of the module\n",
        "parameters": "destination (dict, optional) – If provided, the state of module will\nbe updated into the dict and the same object is returned.\nOtherwise, an OrderedDict will be created and returned.\nDefault: None.\nprefix (str, optional) – a prefix added to parameter and buffer\nnames to compose the keys in state_dict. Default: ''.\nkeep_vars (bool, optional) – by default the Tensor s\nreturned in the state dict are detached from autograd. If it’s\nset to True, detaching will not be performed.\nDefault: False.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.Module.state_dict",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.state_dict",
        "api_signature": "state_dict(*, destination: T_destination, prefix: str = '', keep_vars: bool = False)",
        "api_description": "",
        "return_value": "a dictionary containing a whole state of the module\n",
        "parameters": "destination (dict, optional) – If provided, the state of module will\nbe updated into the dict and the same object is returned.\nOtherwise, an OrderedDict will be created and returned.\nDefault: None.\nprefix (str, optional) – a prefix added to parameter and buffer\nnames to compose the keys in state_dict. Default: ''.\nkeep_vars (bool, optional) – by default the Tensor s\nreturned in the state dict are detached from autograd. If it’s\nset to True, detaching will not be performed.\nDefault: False.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.Adadelta.state_dict",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.Adadelta.html#torch.optim.Adadelta.state_dict",
        "api_signature": "state_dict()",
        "api_description": "Returns the state of the optimizer as a dict.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.Adagrad.state_dict",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.Adagrad.html#torch.optim.Adagrad.state_dict",
        "api_signature": "state_dict()",
        "api_description": "Returns the state of the optimizer as a dict.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.Adam.state_dict",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam.state_dict",
        "api_signature": "state_dict()",
        "api_description": "Returns the state of the optimizer as a dict.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.Adamax.state_dict",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.Adamax.html#torch.optim.Adamax.state_dict",
        "api_signature": "state_dict()",
        "api_description": "Returns the state of the optimizer as a dict.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.AdamW.state_dict",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html#torch.optim.AdamW.state_dict",
        "api_signature": "state_dict()",
        "api_description": "Returns the state of the optimizer as a dict.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.ASGD.state_dict",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.ASGD.html#torch.optim.ASGD.state_dict",
        "api_signature": "state_dict()",
        "api_description": "Returns the state of the optimizer as a dict.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.LBFGS.state_dict",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.LBFGS.html#torch.optim.LBFGS.state_dict",
        "api_signature": "state_dict()",
        "api_description": "Returns the state of the optimizer as a dict.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.lr_scheduler.ChainedScheduler.state_dict",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.ChainedScheduler.html#torch.optim.lr_scheduler.ChainedScheduler.state_dict",
        "api_signature": "state_dict()",
        "api_description": "Returns the state of the scheduler as a dict.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.lr_scheduler.ConstantLR.state_dict",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.ConstantLR.html#torch.optim.lr_scheduler.ConstantLR.state_dict",
        "api_signature": "state_dict()",
        "api_description": "Returns the state of the scheduler as a dict.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.lr_scheduler.CosineAnnealingLR.state_dict",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.CosineAnnealingLR.html#torch.optim.lr_scheduler.CosineAnnealingLR.state_dict",
        "api_signature": "state_dict()",
        "api_description": "Returns the state of the scheduler as a dict.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.lr_scheduler.CosineAnnealingWarmRestarts.state_dict",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.CosineAnnealingWarmRestarts.html#torch.optim.lr_scheduler.CosineAnnealingWarmRestarts.state_dict",
        "api_signature": "state_dict()",
        "api_description": "Returns the state of the scheduler as a dict.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.lr_scheduler.ExponentialLR.state_dict",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.ExponentialLR.html#torch.optim.lr_scheduler.ExponentialLR.state_dict",
        "api_signature": "state_dict()",
        "api_description": "Returns the state of the scheduler as a dict.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.lr_scheduler.LambdaLR.state_dict",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.LambdaLR.html#torch.optim.lr_scheduler.LambdaLR.state_dict",
        "api_signature": "state_dict()",
        "api_description": "Returns the state of the scheduler as a dict.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.lr_scheduler.LinearLR.state_dict",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.LinearLR.html#torch.optim.lr_scheduler.LinearLR.state_dict",
        "api_signature": "state_dict()",
        "api_description": "Returns the state of the scheduler as a dict.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.lr_scheduler.MultiplicativeLR.state_dict",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.MultiplicativeLR.html#torch.optim.lr_scheduler.MultiplicativeLR.state_dict",
        "api_signature": "state_dict()",
        "api_description": "Returns the state of the scheduler as a dict.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.lr_scheduler.MultiStepLR.state_dict",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.MultiStepLR.html#torch.optim.lr_scheduler.MultiStepLR.state_dict",
        "api_signature": "state_dict()",
        "api_description": "Returns the state of the scheduler as a dict.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.lr_scheduler.OneCycleLR.state_dict",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.OneCycleLR.html#torch.optim.lr_scheduler.OneCycleLR.state_dict",
        "api_signature": "state_dict()",
        "api_description": "Returns the state of the scheduler as a dict.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.lr_scheduler.PolynomialLR.state_dict",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.PolynomialLR.html#torch.optim.lr_scheduler.PolynomialLR.state_dict",
        "api_signature": "state_dict()",
        "api_description": "Returns the state of the scheduler as a dict.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.lr_scheduler.SequentialLR.state_dict",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.SequentialLR.html#torch.optim.lr_scheduler.SequentialLR.state_dict",
        "api_signature": "state_dict()",
        "api_description": "Returns the state of the scheduler as a dict.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.lr_scheduler.StepLR.state_dict",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.StepLR.html#torch.optim.lr_scheduler.StepLR.state_dict",
        "api_signature": "state_dict()",
        "api_description": "Returns the state of the scheduler as a dict.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.NAdam.state_dict",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.NAdam.html#torch.optim.NAdam.state_dict",
        "api_signature": "state_dict()",
        "api_description": "Returns the state of the optimizer as a dict.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.Optimizer.state_dict",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.Optimizer.state_dict.html#torch.optim.Optimizer.state_dict",
        "api_signature": "Optimizer.state_dict()",
        "api_description": "Returns the state of the optimizer as a dict.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.RAdam.state_dict",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.RAdam.html#torch.optim.RAdam.state_dict",
        "api_signature": "state_dict()",
        "api_description": "Returns the state of the optimizer as a dict.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.RMSprop.state_dict",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.RMSprop.html#torch.optim.RMSprop.state_dict",
        "api_signature": "state_dict()",
        "api_description": "Returns the state of the optimizer as a dict.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.Rprop.state_dict",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.Rprop.html#torch.optim.Rprop.state_dict",
        "api_signature": "state_dict()",
        "api_description": "Returns the state of the optimizer as a dict.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.SGD.state_dict",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.SGD.html#torch.optim.SGD.state_dict",
        "api_signature": "state_dict()",
        "api_description": "Returns the state of the optimizer as a dict.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.SparseAdam.state_dict",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.SparseAdam.html#torch.optim.SparseAdam.state_dict",
        "api_signature": "state_dict()",
        "api_description": "Returns the state of the optimizer as a dict.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.fsdp.FullyShardedDataParallel.state_dict_type",
        "api_url": "https://pytorch.org/docs/stable/fsdp.html#torch.distributed.fsdp.FullyShardedDataParallel.state_dict_type",
        "api_signature": "state_dict_type(module, state_dict_type, state_dict_config=None, optim_state_dict_config=None)",
        "api_description": "Set the state_dict_type of all the descendant FSDP modules of the target module.",
        "return_value": "",
        "parameters": "module (torch.nn.Module) – Root module.\nstate_dict_type (StateDictType) – the desired state_dict_type to set.\nstate_dict_config (Optional[StateDictConfig]) – the model state_dict\nconfiguration for the target state_dict_type.\noptim_state_dict_config (Optional[OptimStateDictConfig]) – the optimizer\nstate_dict configuration for the target state_dict_type.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.fsdp.StateDictConfig",
        "api_url": "https://pytorch.org/docs/stable/fsdp.html#torch.distributed.fsdp.StateDictConfig",
        "api_signature": "torch.distributed.fsdp.StateDictConfig(offload_to_cpu=False)",
        "api_description": "StateDictConfig is the base class for all state_dict configuration\nclasses. Users should instantiate a child class (e.g.\nFullStateDictConfig) in order to configure settings for the\ncorresponding state_dict type supported by FSDP.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.checkpoint.state_dict.StateDictOptions",
        "api_url": "https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.StateDictOptions",
        "api_signature": "torch.distributed.checkpoint.state_dict.StateDictOptions(full_state_dict=False, cpu_offload=False, ignore_frozen_params=False, keep_submodule_prefixes=True, strict=True)",
        "api_description": "This dataclass specifies how get_state_dict/set_state_dict will work.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.fsdp.StateDictSettings",
        "api_url": "https://pytorch.org/docs/stable/fsdp.html#torch.distributed.fsdp.StateDictSettings",
        "api_signature": "torch.distributed.fsdp.StateDictSettings(state_dict_type: torch.distributed.fsdp.api.StateDictType, state_dict_config: torch.distributed.fsdp.api.StateDictConfig, optim_state_dict_config: torch.distributed.fsdp.api.OptimStateDictConfig)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.checkpoint.stateful.Stateful",
        "api_url": "https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.stateful.Stateful",
        "api_signature": "torch.distributed.checkpoint.stateful.Stateful(*args, **kwargs)",
        "api_description": "Stateful protocol for objects that can be checkpointed and restored.",
        "return_value": "The objects state dict\n",
        "parameters": "state_dict (Dict[str, Any]) – The state dict to restore from",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.symbolic_shapes.StatefulSymbolicContext",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.fx.experimental.symbolic_shapes.StatefulSymbolicContext.html#torch.fx.experimental.symbolic_shapes.StatefulSymbolicContext",
        "api_signature": "torch.fx.experimental.symbolic_shapes.StatefulSymbolicContext(dynamic_sizes, constraint_sizes=None, view_base_context=None, tensor_source=None, shape_env_to_source_to_symbol_cache=None)",
        "api_description": "Create symbols in create_symbolic_sizes_strides_storage_offset via\na symbolic_context determination as given by a cache of Source:Symbol. A cache hit\nwill reuse a stored symbol, and a cache miss will write to this cache.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.symbolic_shapes.StatelessSymbolicContext",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.fx.experimental.symbolic_shapes.StatelessSymbolicContext.html#torch.fx.experimental.symbolic_shapes.StatelessSymbolicContext",
        "api_signature": "torch.fx.experimental.symbolic_shapes.StatelessSymbolicContext(dynamic_sizes, constraint_sizes=None, view_base_context=None)",
        "api_description": "Create symbols in create_symbolic_sizes_strides_storage_offset via\na symbolic_context determination as given by DimDynamic and DimConstraint.\nThis will cause fresh symbols to be allocated",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.symbolic_shapes.statically_known_true",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.fx.experimental.symbolic_shapes.statically_known_true.html#torch.fx.experimental.symbolic_shapes.statically_known_true",
        "api_signature": "torch.fx.experimental.symbolic_shapes.statically_known_true(x)",
        "api_description": "Returns True if x can be simplified to a constant and is true.",
        "return_value": "",
        "parameters": "x (bool, SymBool) – The expression to try statically evaluating",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.benchmark.CallgrindStats.stats",
        "api_url": "https://pytorch.org/docs/stable/benchmark_utils.html#torch.utils.benchmark.CallgrindStats.stats",
        "api_signature": "stats(inclusive=False)",
        "api_description": "Returns detailed function counts.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.std",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.std.html#torch.std",
        "api_signature": "torch.std(input, dim=None, *, correction=1, keepdim=False, out=None)",
        "api_description": "Calculates the standard deviation over the dimensions specified by dim.\ndim can be a single dimension, list of dimensions, or None to\nreduce over all dimensions.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.\ndim (int or tuple of ints) – the dimension or dimensions to reduce.\ncorrection (int) – difference between the sample size and sample degrees of freedom.\nDefaults to Bessel’s correction, correction=1.\nChanged in version 2.0: Previously this argument was called unbiased and was a boolean\nwith True corresponding to correction=1 and False being\ncorrection=0.\nkeepdim (bool) – whether the output tensor has dim retained or not.\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.std",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.std.html#torch.Tensor.std",
        "api_signature": "Tensor.std(dim=None, *, correction=1, keepdim=False)",
        "api_description": "See torch.std()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.std_mean",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.std_mean.html#torch.std_mean",
        "api_signature": "torch.std_mean(input, dim=None, *, correction=1, keepdim=False, out=None)",
        "api_description": "Calculates the standard deviation and mean over the dimensions specified by\ndim. dim can be a single dimension, list of dimensions, or\nNone to reduce over all dimensions.",
        "return_value": "A tuple (std, mean) containing the standard deviation and mean.\n",
        "parameters": "input (Tensor) – the input tensor.\ndim (int or tuple of ints, optional) – the dimension or dimensions to reduce.\nIf None, all dimensions are reduced.\ncorrection (int) – difference between the sample size and sample degrees of freedom.\nDefaults to Bessel’s correction, correction=1.\nChanged in version 2.0: Previously this argument was called unbiased and was a boolean\nwith True corresponding to correction=1 and False being\ncorrection=0.\nkeepdim (bool) – whether the output tensor has dim retained or not.\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.continuous_bernoulli.ContinuousBernoulli.stddev",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.continuous_bernoulli.ContinuousBernoulli.stddev",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.distribution.Distribution.stddev",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.distribution.Distribution.stddev",
        "api_signature": null,
        "api_description": "Returns the standard deviation of the distribution.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.exponential.Exponential.stddev",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.exponential.Exponential.stddev",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.gumbel.Gumbel.stddev",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.gumbel.Gumbel.stddev",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.laplace.Laplace.stddev",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.laplace.Laplace.stddev",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.normal.Normal.stddev",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.normal.Normal.stddev",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.uniform.Uniform.stddev",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.uniform.Uniform.stddev",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.optim.DistributedOptimizer.step",
        "api_url": "https://pytorch.org/docs/stable/distributed.optim.html#torch.distributed.optim.DistributedOptimizer.step",
        "api_signature": "step(context_id)",
        "api_description": "Performs a single optimization step.",
        "return_value": "",
        "parameters": "context_id – the autograd context id for which we should run the\noptimizer step.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.optim.PostLocalSGDOptimizer.step",
        "api_url": "https://pytorch.org/docs/stable/distributed.optim.html#torch.distributed.optim.PostLocalSGDOptimizer.step",
        "api_signature": "step()",
        "api_description": "Performs a single optimization step (parameter update).",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.optim.ZeroRedundancyOptimizer.step",
        "api_url": "https://pytorch.org/docs/stable/distributed.optim.html#torch.distributed.optim.ZeroRedundancyOptimizer.step",
        "api_signature": "step(closure=None, **kwargs)",
        "api_description": "Perform a single optimizer step and syncs parameters across all ranks.",
        "return_value": "Optional loss depending on the underlying local optimizer.\n",
        "parameters": "closure (Callable) – a closure that re-evaluates the model and\nreturns the loss; optional for most optimizers.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.Adadelta.step",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.Adadelta.html#torch.optim.Adadelta.step",
        "api_signature": "step(closure=None)",
        "api_description": "Perform a single optimization step.",
        "return_value": "",
        "parameters": "closure (Callable, optional) – A closure that reevaluates the model\nand returns the loss.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.Adagrad.step",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.Adagrad.html#torch.optim.Adagrad.step",
        "api_signature": "step(closure=None)",
        "api_description": "Perform a single optimization step.",
        "return_value": "",
        "parameters": "closure (Callable, optional) – A closure that reevaluates the model\nand returns the loss.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.Adam.step",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam.step",
        "api_signature": "step(closure=None)",
        "api_description": "Perform a single optimization step.",
        "return_value": "",
        "parameters": "closure (Callable, optional) – A closure that reevaluates the model\nand returns the loss.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.Adamax.step",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.Adamax.html#torch.optim.Adamax.step",
        "api_signature": "step(closure=None)",
        "api_description": "Performs a single optimization step.",
        "return_value": "",
        "parameters": "closure (Callable, optional) – A closure that reevaluates the model\nand returns the loss.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.AdamW.step",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html#torch.optim.AdamW.step",
        "api_signature": "step(closure=None)",
        "api_description": "Perform a single optimization step.",
        "return_value": "",
        "parameters": "closure (Callable, optional) – A closure that reevaluates the model\nand returns the loss.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.ASGD.step",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.ASGD.html#torch.optim.ASGD.step",
        "api_signature": "step(closure=None)",
        "api_description": "Perform a single optimization step.",
        "return_value": "",
        "parameters": "closure (Callable, optional) – A closure that reevaluates the model\nand returns the loss.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.LBFGS.step",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.LBFGS.html#torch.optim.LBFGS.step",
        "api_signature": "step(closure)",
        "api_description": "Perform a single optimization step.",
        "return_value": "",
        "parameters": "closure (Callable) – A closure that reevaluates the model\nand returns the loss.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.lr_scheduler.CosineAnnealingWarmRestarts.step",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.CosineAnnealingWarmRestarts.html#torch.optim.lr_scheduler.CosineAnnealingWarmRestarts.step",
        "api_signature": "step(epoch=None)",
        "api_description": "Step could be called after every batch update",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.NAdam.step",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.NAdam.html#torch.optim.NAdam.step",
        "api_signature": "step(closure=None)",
        "api_description": "Performs a single optimization step.",
        "return_value": "",
        "parameters": "closure (Callable, optional) – A closure that reevaluates the model\nand returns the loss.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.Optimizer.step",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.Optimizer.step.html#torch.optim.Optimizer.step",
        "api_signature": "Optimizer.step(closure: None = None)",
        "api_description": "",
        "return_value": "",
        "parameters": "closure (Callable) – A closure that reevaluates the model and\nreturns the loss. Optional for most optimizers.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.RAdam.step",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.RAdam.html#torch.optim.RAdam.step",
        "api_signature": "step(closure=None)",
        "api_description": "Performs a single optimization step.",
        "return_value": "",
        "parameters": "closure (Callable, optional) – A closure that reevaluates the model\nand returns the loss.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.RMSprop.step",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.RMSprop.html#torch.optim.RMSprop.step",
        "api_signature": "step(closure=None)",
        "api_description": "Performs a single optimization step.",
        "return_value": "",
        "parameters": "closure (Callable, optional) – A closure that reevaluates the model\nand returns the loss.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.Rprop.step",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.Rprop.html#torch.optim.Rprop.step",
        "api_signature": "step(closure=None)",
        "api_description": "Performs a single optimization step.",
        "return_value": "",
        "parameters": "closure (Callable, optional) – A closure that reevaluates the model\nand returns the loss.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.SGD.step",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.SGD.html#torch.optim.SGD.step",
        "api_signature": "step(closure=None)",
        "api_description": "Performs a single optimization step.",
        "return_value": "",
        "parameters": "closure (Callable, optional) – A closure that reevaluates the model\nand returns the loss.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.SparseAdam.step",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.SparseAdam.html#torch.optim.SparseAdam.step",
        "api_signature": "step(closure=None)",
        "api_description": "Perform a single optimization step.",
        "return_value": "",
        "parameters": "closure (Callable, optional) – A closure that reevaluates the model\nand returns the loss.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.profiler.profile.step",
        "api_url": "https://pytorch.org/docs/stable/profiler.html#torch.profiler.profile.step",
        "api_signature": "step()",
        "api_description": "Signals the profiler that the next profiling step has started.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.lr_scheduler.StepLR",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.StepLR.html#torch.optim.lr_scheduler.StepLR",
        "api_signature": "torch.optim.lr_scheduler.StepLR(optimizer, step_size, gamma=0.1, last_epoch=-1, verbose='deprecated')",
        "api_description": "Decays the learning rate of each parameter group by gamma every\nstep_size epochs. Notice that such decay can happen simultaneously with\nother changes to the learning rate from outside this scheduler. When\nlast_epoch=-1, sets initial lr as lr.",
        "return_value": "",
        "parameters": "optimizer (Optimizer) – Wrapped optimizer.\nstep_size (int) – Period of learning rate decay.\ngamma (float) – Multiplicative factor of learning rate decay.\nDefault: 0.1.\nlast_epoch (int) – The index of last epoch. Default: -1.\nverbose (bool) – If True, prints a message to stdout for\neach update. Default: False.\nDeprecated since version 2.2: verbose is deprecated. Please use get_last_lr() to access the\nlearning rate.\nstate_dict (dict) – scheduler state. Should be an object returned\nfrom a call to state_dict().",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.stft",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft",
        "api_signature": "torch.stft(input, n_fft, hop_length=None, win_length=None, window=None, center=True, pad_mode='reflect', normalized=False, onesided=None, return_complex=None)",
        "api_description": "Short-time Fourier transform (STFT).",
        "return_value": "\nA tensor containing the STFT result with shape (B?, N, T, C?) where\nB? is an optional batch dimension from the input.\nN is the number of frequency samples, (n_fft // 2) + 1 for\nonesided=True, or otherwise n_fft.\nT is the number of frames, 1 + L // hop_length\nfor center=True, or 1 + (L - n_fft) // hop_length otherwise.\nC? is an optional length-2 dimension of real and imaginary\ncomponents, present when return_complex=False.\n\n\n\n\n",
        "parameters": "input (Tensor) – the input tensor of shape (B?, L) where B? is an optional\nbatch dimension\nn_fft (int) – size of Fourier transform\nhop_length (int, optional) – the distance between neighboring sliding window\nframes. Default: None (treated as equal to floor(n_fft / 4))\nwin_length (int, optional) – the size of window frame and STFT filter.\nDefault: None  (treated as equal to n_fft)\nwindow (Tensor, optional) – the optional window function.\nShape must be 1d and <= n_fft\nDefault: None (treated as window of all 111 s)\ncenter (bool, optional) – whether to pad input on both sides so\nthat the ttt-th frame is centered at time t×hop_lengtht \\times \\text{hop\\_length}t×hop_length.\nDefault: True\npad_mode (str, optional) – controls the padding method used when\ncenter is True. Default: \"reflect\"\nnormalized (bool, optional) – controls whether to return the normalized STFT results\nDefault: False\nonesided (bool, optional) – controls whether to return half of results to\navoid redundancy for real inputs.\nDefault: True for real input and window, False otherwise.\nreturn_complex (bool, optional) – whether to return a complex tensor, or\na real tensor with an extra last dimension for the real and\nimaginary components.\nChanged in version 2.0: return_complex is now a required argument for real inputs,\nas the default is being transitioned to True.\nDeprecated since version 2.0: return_complex=False is deprecated, instead use return_complex=True\nNote that calling torch.view_as_real() on the output will\nrecover the deprecated output format.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.stft",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.stft.html#torch.Tensor.stft",
        "api_signature": "Tensor.stft(n_fft, hop_length=None, win_length=None, window=None, center=True, pad_mode='reflect', normalized=False, onesided=None, return_complex=None)",
        "api_description": "See torch.stft()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.transforms.StickBreakingTransform",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.transforms.StickBreakingTransform",
        "api_signature": "torch.distributions.transforms.StickBreakingTransform(cache_size=0)",
        "api_description": "Transform from unconstrained space to the simplex of one additional\ndimension via a stick-breaking process.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.mps.profiler.stop",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.mps.profiler.stop.html#torch.mps.profiler.stop",
        "api_signature": "torch.mps.profiler.stop()",
        "api_description": "Stops generating OS Signpost tracing from MPS backend.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.storage",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.storage.html#torch.Tensor.storage",
        "api_signature": "Tensor.storage()",
        "api_description": "Returns the underlying TypedStorage.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.storage_offset",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.storage_offset.html#torch.Tensor.storage_offset",
        "api_signature": "Tensor.storage_offset()",
        "api_description": "Returns self tensor’s offset in the underlying storage in terms of\nnumber of storage elements (not bytes).",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.storage_type",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.storage_type.html#torch.Tensor.storage_type",
        "api_signature": "Tensor.storage_type()",
        "api_description": "Returns the type of the underlying storage.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.checkpoint.StorageReader",
        "api_url": "https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.StorageReader",
        "api_signature": null,
        "api_description": "Interface used by load_state_dict to read from storage.",
        "return_value": "A list of transformed LoadPlan after storage global planning\nA transformed LoadPlan after storage local planning\nA future that completes once all reads are finished.\nThe metadata object associated with the checkpoint being loaded.\n",
        "parameters": "plans (List[LoadPlan]) – A list of LoadPlan instances, one for each rank.\nplan (LoadPlan) – The local plan from the LoadPlan in use.\nplan (LoadPlan) – The local plan to execute on\nplanner (LoadPlanner) – The planner object to use to resolve items.\ncheckpoint_id (Union[str, os.PathLike, None]) – The ID of this checkpoint instance. The meaning of the checkpoint_id\ndepends on the storage. It can be a path to a folder or to a file.\nIt can also be a key if the storage is more like a key-value store.\n(Default: None)\nmetadata (Metadata) – The metadata schema to use.\nis_coordinator (bool) – Whether this instance is responsible for coordinating\nthe checkpoint.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.checkpoint.StorageWriter",
        "api_url": "https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.StorageWriter",
        "api_signature": null,
        "api_description": "Interface used by save_state_dict to write to storage.",
        "return_value": "None\nA list of transformed SavePlan after storage global planning\nA transformed SavePlan after storage local planning\nA future that completes to a list of WriteResult\n",
        "parameters": "metadata (Metadata) – metadata for the new checkpoint\nresults (List[List[WriteResult]]) – A list of WriteResults from all ranks.\nplans (List[SavePlan]) – A list of SavePlan instances, one for each rank.\nplan (SavePlan) – The local plan from the SavePlanner in use.\ncheckpoint_id (Union[str, os.PathLike, None]) – The ID of this checkpoint instance. The meaning of the checkpoint_id\ndepends on the storage. It can be a path to a folder or to a file.\nIt can also be a key if the storage is a key-value store.\n(Default: None)\nis_coordinator (bool) – Whether this instance is responsible for coordinating\nthe checkpoint.\nplan (SavePlan) – The save plan to execute.\nplanner (SavePlanner) – Planner object to be used to resolve items to data.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.Store",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#torch.distributed.Store",
        "api_signature": null,
        "api_description": "Base class for all store implementations, such as the 3 provided by PyTorch\ndistributed: (TCPStore, FileStore,\nand HashStore).",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.backends.opt_einsum.strategy",
        "api_url": "https://pytorch.org/docs/stable/backends.html#torch.backends.opt_einsum.strategy",
        "api_signature": null,
        "api_description": "A :class:str that specifies which strategies to try when torch.backends.opt_einsum.enabled\nis True. By default, torch.einsum will try the “auto” strategy, but the “greedy” and “optimal”\nstrategies are also supported. Note that the “optimal” strategy is factorial on the number of\ninputs as it tries all possible paths. See more details in opt_einsum’s docs\n(https://optimized-einsum.readthedocs.io/en/stable/path_finding.html).",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cpu.Stream",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.cpu.Stream.html#torch.cpu.Stream",
        "api_signature": "torch.cpu.Stream(priority=-1)",
        "api_description": "N.B. This class only exists to facilitate device-agnostic code",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda.Stream",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.cuda.Stream.html#torch.cuda.Stream",
        "api_signature": "torch.cuda.Stream(device=None, priority=0, **kwargs)",
        "api_description": "Wrapper around a CUDA stream.",
        "return_value": "A boolean indicating if all kernels in this stream are completed.\nRecorded event.\n",
        "parameters": "device (torch.device or int, optional) – a device on which to allocate\nthe stream. If device is None (default) or a negative\ninteger, this will use the current device.\npriority (int, optional) – priority of the stream, should be 0 or\nnegative, where negative numbers indicate higher priority. By default,\nstreams have priority 0.\nevent (torch.cuda.Event, optional) – event to record. If not given, a new one\nwill be allocated.\nevent (torch.cuda.Event) – an event to wait for.\nstream (Stream) – a stream to synchronize.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.xpu.Stream",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.xpu.Stream.html#torch.xpu.Stream",
        "api_signature": "torch.xpu.Stream(device=None, priority=0, **kwargs)",
        "api_description": "Wrapper around a XPU stream.",
        "return_value": "A boolean indicating if all kernels in this stream are completed.\nRecorded event.\n",
        "parameters": "device (torch.device or int, optional) – a device on which to allocate\nthe stream. If device is None (default) or a negative\ninteger, this will use the current device.\npriority (int, optional) – priority of the stream, should be 0 or\nnegative, where negative numbers indicate higher priority. By default,\nstreams have priority 0.\nevent (torch.xpu.Event, optional) – event to record. If not given, a new one\nwill be allocated.\nevent (torch.xpu.Event) – an event to wait for.\nstream (Stream) – a stream to synchronize.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cpu.stream",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.cpu.stream.html#torch.cpu.stream",
        "api_signature": "torch.cpu.stream(stream)",
        "api_description": "Wrapper around the Context-manager StreamContext that\nselects a given stream.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda.stream",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.cuda.stream.html#torch.cuda.stream",
        "api_signature": "torch.cuda.stream(stream)",
        "api_description": "Wrap around the Context-manager StreamContext that selects a given stream.",
        "return_value": "",
        "parameters": "stream (Stream) – selected stream. This manager is a no-op if it’s\nNone.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.xpu.stream",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.xpu.stream.html#torch.xpu.stream",
        "api_signature": "torch.xpu.stream(stream)",
        "api_description": "Wrap around the Context-manager StreamContext that selects a given stream.",
        "return_value": "",
        "parameters": "stream (Stream) – selected stream. This manager is a no-op if it’s None.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cpu.StreamContext",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.cpu.StreamContext.html#torch.cpu.StreamContext",
        "api_signature": "torch.cpu.StreamContext(stream)",
        "api_description": "Context-manager that selects a given stream.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda.StreamContext",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.cuda.StreamContext.html#torch.cuda.StreamContext",
        "api_signature": "torch.cuda.StreamContext(stream)",
        "api_description": "Context-manager that selects a given stream.",
        "return_value": "",
        "parameters": "Stream (Stream) – selected stream. This manager is a no-op if it’s\nNone.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.xpu.StreamContext",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.xpu.StreamContext.html#torch.xpu.StreamContext",
        "api_signature": "torch.xpu.StreamContext(stream)",
        "api_description": "Context-manager that selects a given stream.",
        "return_value": "",
        "parameters": "Stream (Stream) – selected stream. This manager is a no-op if it’s\nNone.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.jit.strict_fusion",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.jit.strict_fusion.html#torch.jit.strict_fusion",
        "api_signature": null,
        "api_description": "Give errors if not all nodes have been fused in inference, or symbolically differentiated in training.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.symbolic_shapes.StrictMinMaxConstraint",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.fx.experimental.symbolic_shapes.StrictMinMaxConstraint.html#torch.fx.experimental.symbolic_shapes.StrictMinMaxConstraint",
        "api_signature": "torch.fx.experimental.symbolic_shapes.StrictMinMaxConstraint(warn_only, vr)",
        "api_description": "For clients: the size at this dimension must be within ‘vr’ (which\nspecifies a lower and upper bound, inclusive-inclusive) AND it\nmust be non-negative and should not be 0 or 1 (but see NB below).",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.stride",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.stride.html#torch.Tensor.stride",
        "api_signature": "Tensor.stride(dim)",
        "api_description": "Returns the stride of self tensor.",
        "return_value": "",
        "parameters": "dim (int, optional) – the desired dimension in which stride is required",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.autograd.profiler_util.StringTable",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.autograd.profiler_util.StringTable.html#torch.autograd.profiler_util.StringTable",
        "api_signature": null,
        "api_description": "Factory for default value called by __missing__().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.studentT.StudentT",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.studentT.StudentT",
        "api_signature": "torch.distributions.studentT.StudentT(df, loc=0.0, scale=1.0, validate_args=None)",
        "api_description": "Bases: Distribution",
        "return_value": "",
        "parameters": "df (float or Tensor) – degrees of freedom\nloc (float or Tensor) – mean of the distribution\nscale (float or Tensor) – scale of the distribution",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.sub",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.sub.html#torch.sub",
        "api_signature": "torch.sub(input, other, *, alpha=1, out=None)",
        "api_description": "Subtracts other, scaled by alpha, from input.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.\nother (Tensor or Number) – the tensor or number to subtract from input.\nalpha (Number) – the multiplier for other.\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.sub",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.sub.html#torch.Tensor.sub",
        "api_signature": "Tensor.sub(other, *, alpha=1)",
        "api_description": "See torch.sub().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.sub_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.sub_.html#torch.Tensor.sub_",
        "api_signature": "Tensor.sub_(other, *, alpha=1)",
        "api_description": "In-place version of sub()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.symbolic_shapes.SubclassSymbolicContext",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.fx.experimental.symbolic_shapes.SubclassSymbolicContext.html#torch.fx.experimental.symbolic_shapes.SubclassSymbolicContext",
        "api_signature": "torch.fx.experimental.symbolic_shapes.SubclassSymbolicContext(dynamic_sizes, constraint_sizes=None, view_base_context=None, tensor_source=None, shape_env_to_source_to_symbol_cache=None, inner_contexts=None)",
        "api_description": "The correct symbolic context for a given inner tensor of a traceable tensor subclass\nmay differ from that of the outer symbolic context. This structure allows for this\nflexibility, with inner symbolic contexts mapped via attr -> symbolic context.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.multiprocessing.api.SubprocessContext",
        "api_url": "https://pytorch.org/docs/stable/elastic/multiprocessing.html#torch.distributed.elastic.multiprocessing.api.SubprocessContext",
        "api_signature": "torch.distributed.elastic.multiprocessing.api.SubprocessContext(name, entrypoint, args, envs, logs_specs, log_line_prefixes=None)",
        "api_description": "PContext holding worker processes invoked as a binary.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.multiprocessing.subprocess_handler.subprocess_handler.SubprocessHandler",
        "api_url": "https://pytorch.org/docs/stable/elastic/subprocess_handler.html#torch.distributed.elastic.multiprocessing.subprocess_handler.subprocess_handler.SubprocessHandler",
        "api_signature": "torch.distributed.elastic.multiprocessing.subprocess_handler.subprocess_handler.SubprocessHandler(entrypoint, args, env, stdout, stderr, local_rank_id)",
        "api_description": "Convenience wrapper around python’s subprocess.Popen. Keeps track of\nmeta-objects associated to the process (e.g. stdout and stderr redirect fds).",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.data.Subset",
        "api_url": "https://pytorch.org/docs/stable/data.html#torch.utils.data.Subset",
        "api_signature": "torch.utils.data.Subset(dataset, indices)",
        "api_description": "Subset of a dataset at specified indices.",
        "return_value": "",
        "parameters": "dataset (Dataset) – The whole Dataset\nindices (sequence) – Indices in the whole set selected for subset",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.data.SubsetRandomSampler",
        "api_url": "https://pytorch.org/docs/stable/data.html#torch.utils.data.SubsetRandomSampler",
        "api_signature": "torch.utils.data.SubsetRandomSampler(indices, generator=None)",
        "api_description": "Samples elements randomly from a given list of indices, without replacement.",
        "return_value": "",
        "parameters": "indices (sequence) – a sequence of indices\ngenerator (Generator) – Generator used in sampling.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.subtract",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.subtract.html#torch.subtract",
        "api_signature": "torch.subtract(input, other, *, alpha=1, out=None)",
        "api_description": "Alias for torch.sub().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.subtract",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.subtract.html#torch.Tensor.subtract",
        "api_signature": "Tensor.subtract(other, *, alpha=1)",
        "api_description": "See torch.subtract().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.subtract_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.subtract_.html#torch.Tensor.subtract_",
        "api_signature": "Tensor.subtract_(other, *, alpha=1)",
        "api_description": "In-place version of subtract().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.sum",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.sum.html#torch.sum",
        "api_signature": "torch.sum(input, *, dtype=None)",
        "api_description": "Returns the sum of all elements in the input tensor.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.\ndtype (torch.dtype, optional) – the desired data type of returned tensor.\nIf specified, the input tensor is casted to dtype before the operation\nis performed. This is useful for preventing data type overflows. Default: None.\ninput (Tensor) – the input tensor.\ndim (int or tuple of ints, optional) – the dimension or dimensions to reduce.\nIf None, all dimensions are reduced.\nkeepdim (bool) – whether the output tensor has dim retained or not.\ndtype (torch.dtype, optional) – the desired data type of returned tensor.\nIf specified, the input tensor is casted to dtype before the operation\nis performed. This is useful for preventing data type overflows. Default: None.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.sparse.sum",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.sparse.sum.html#torch.sparse.sum",
        "api_signature": "torch.sparse.sum(input, dim=None, dtype=None)",
        "api_description": "Return the sum of each row of the given sparse tensor.",
        "return_value": "",
        "parameters": "input (Tensor) – the input sparse tensor\ndim (int or tuple of ints) – a dimension or a list of dimensions to reduce. Default: reduce\nover all dims.\ndtype (torch.dtype, optional) – the desired data type of returned Tensor.\nDefault: dtype of input.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.sum",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.sum.html#torch.Tensor.sum",
        "api_signature": "Tensor.sum(dim=None, keepdim=False, dtype=None)",
        "api_description": "See torch.sum()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.sum_to_size",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.sum_to_size.html#torch.Tensor.sum_to_size",
        "api_signature": "Tensor.sum_to_size(*size)",
        "api_description": "Sum this tensor to size.\nsize must be broadcastable to this tensor size.",
        "return_value": "",
        "parameters": "size (int...) – a sequence of integers defining the shape of the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.tensorboard.writer.SummaryWriter",
        "api_url": "https://pytorch.org/docs/stable/tensorboard.html#torch.utils.tensorboard.writer.SummaryWriter",
        "api_signature": "torch.utils.tensorboard.writer.SummaryWriter(log_dir=None, comment='', purge_step=None, max_queue=10, flush_secs=120, filename_suffix='')",
        "api_description": "Writes entries directly to event files in the log_dir to be consumed by TensorBoard.",
        "return_value": "",
        "parameters": "log_dir (str) – Save directory location. Default is\nruns/CURRENT_DATETIME_HOSTNAME, which changes after each run.\nUse hierarchical folder structure to compare\nbetween runs easily. e.g. pass in ‘runs/exp1’, ‘runs/exp2’, etc.\nfor each new experiment to compare across them.\ncomment (str) – Comment log_dir suffix appended to the default\nlog_dir. If log_dir is assigned, this argument has no effect.\npurge_step (int) – When logging crashes at step T+XT+XT+X and restarts at step TTT,\nany events whose global_step larger or equal to TTT will be\npurged and hidden from TensorBoard.\nNote that crashed and resumed experiments should have the same log_dir.\nmax_queue (int) – Size of the queue for pending events and\nsummaries before one of the ‘add’ calls forces a flush to disk.\nDefault is ten items.\nflush_secs (int) – How often, in seconds, to flush the\npending events and summaries to disk. Default is every two minutes.\nfilename_suffix (str) – Suffix added to all event filenames in\nthe log_dir directory. More details on filename construction in\ntensorboard.summary.writer.event_file_writer.EventFileWriter.\ntag (str) – Data identifier\nscalar_value (float or string/blobname) – Value to save\nglobal_step (int) – Global step value to record\nwalltime (float) – Optional override default walltime (time.time())\nwith seconds after epoch of event\nnew_style (boolean) – Whether to use new style (tensor field) or old\nstyle (simple_value field). New style could lead to faster data loading.\nmain_tag (str) – The parent name for the tags\ntag_scalar_dict (dict) – Key-value pair storing the tag and corresponding values\nglobal_step (int) – Global step value to record\nwalltime (float) – Optional override default walltime (time.time())\nseconds after epoch of event\ntag (str) – Data identifier\nvalues (torch.Tensor, numpy.ndarray, or string/blobname) – Values to build histogram\nglobal_step (int) – Global step value to record\nbins (str) – One of {‘tensorflow’,’auto’, ‘fd’, …}. This determines how the bins are made. You can find\nother options in: https://docs.scipy.org/doc/numpy/reference/generated/numpy.histogram.html\nwalltime (float) – Optional override default walltime (time.time())\nseconds after epoch of event\ntag (str) – Data identifier\nimg_tensor (torch.Tensor, numpy.ndarray, or string/blobname) – Image data\nglobal_step (int) – Global step value to record\nwalltime (float) – Optional override default walltime (time.time())\nseconds after epoch of event\ndataformats (str) – Image data format specification of the form\nCHW, HWC, HW, WH, etc.\ntag (str) – Data identifier\nimg_tensor (torch.Tensor, numpy.ndarray, or string/blobname) – Image data\nglobal_step (int) – Global step value to record\nwalltime (float) – Optional override default walltime (time.time())\nseconds after epoch of event\ndataformats (str) – Image data format specification of the form\nNCHW, NHWC, CHW, HWC, HW, WH, etc.\ntag (str) – Data identifier\nfigure (Union[Figure, List[Figure]]) – Figure or a list of figures\nglobal_step (Optional[int]) – Global step value to record\nclose (bool) – Flag to automatically close the figure\nwalltime (Optional[float]) – Optional override default walltime (time.time())\nseconds after epoch of event\ntag (str) – Data identifier\nvid_tensor (torch.Tensor) – Video data\nglobal_step (int) – Global step value to record\nfps (float or int) – Frames per second\nwalltime (float) – Optional override default walltime (time.time())\nseconds after epoch of event\ntag (str) – Data identifier\nsnd_tensor (torch.Tensor) – Sound data\nglobal_step (int) – Global step value to record\nsample_rate (int) – sample rate in Hz\nwalltime (float) – Optional override default walltime (time.time())\nseconds after epoch of event\ntag (str) – Data identifier\ntext_string (str) – String to save\nglobal_step (int) – Global step value to record\nwalltime (float) – Optional override default walltime (time.time())\nseconds after epoch of event\nmodel (torch.nn.Module) – Model to draw.\ninput_to_model (torch.Tensor or list of torch.Tensor) – A variable or a tuple of\nvariables to be fed.\nverbose (bool) – Whether to print graph structure in console.\nuse_strict_trace (bool) – Whether to pass keyword argument strict to\ntorch.jit.trace. Pass False when you want the tracer to\nrecord your mutable container types (list, dict)\nmat (torch.Tensor or numpy.ndarray) – A matrix which each row is the feature vector of the data point\nmetadata (list) – A list of labels, each element will be convert to string\nlabel_img (torch.Tensor) – Images correspond to each data point\nglobal_step (int) – Global step value to record\ntag (str) – Name for the embedding\ntag (str) – Data identifier\nlabels (torch.Tensor, numpy.ndarray, or string/blobname) – Ground truth data. Binary label for each element.\npredictions (torch.Tensor, numpy.ndarray, or string/blobname) – The probability that an element be classified as true.\nValue should be in [0, 1]\nglobal_step (int) – Global step value to record\nnum_thresholds (int) – Number of thresholds used to draw the curve.\nwalltime (float) – Optional override default walltime (time.time())\nseconds after epoch of event\nlayout (dict) – {categoryName: charts}, where charts is also a dictionary\n{chartName: ListOfProperties}. The first element in ListOfProperties is the chart’s type\n(one of Multiline or Margin) and the second element should be a list containing the tags\nyou have used in add_scalar function, which will be collected into the new chart.\ntag (str) – Data identifier\nvertices (torch.Tensor) – List of the 3D coordinates of vertices.\ncolors (torch.Tensor) – Colors for each vertex\nfaces (torch.Tensor) – Indices of vertices within each triangle. (Optional)\nconfig_dict – Dictionary with ThreeJS classes names and configuration.\nglobal_step (int) – Global step value to record\nwalltime (float) – Optional override default walltime (time.time())\nseconds after epoch of event\nhparam_dict (dict) – Each key-value pair in the dictionary is the\nname of the hyper parameter and it’s corresponding value.\nThe type of the value can be one of bool, string, float,\nint, or None.\nmetric_dict (dict) – Each key-value pair in the dictionary is the\nname of the metric and it’s corresponding value. Note that the key used\nhere should be unique in the tensorboard record. Otherwise the value\nyou added by add_scalar will be displayed in hparam plugin. In most\ncases, this is unwanted.\nhparam_domain_discrete – (Optional[Dict[str, List[Any]]]) A dictionary that\ncontains names of the hyperparameters and all discrete values they can hold\nrun_name (str) – Name of the run, to be included as part of the logdir.\nIf unspecified, will use current timestamp.\nglobal_step (int) – Global step value to record",
        "input_shape": "img_tensor: Default is (3,H,W)(3, H, W)(3,H,W). You can use torchvision.utils.make_grid() to\nconvert a batch of tensor into 3xHxW format or call add_images and let us do the job.\nTensor with (1,H,W)(1, H, W)(1,H,W), (H,W)(H, W)(H,W), (H,W,3)(H, W, 3)(H,W,3) is also suitable as long as\ncorresponding dataformats argument is passed, e.g. CHW, HWC, HW.\nimg_tensor: Default is (N,3,H,W)(N, 3, H, W)(N,3,H,W). If dataformats is specified, other shape will be\naccepted. e.g. NCHW or NHWC.\nvid_tensor: (N,T,C,H,W)(N, T, C, H, W)(N,T,C,H,W). The values should lie in [0, 255] for type uint8 or [0, 1] for type float.\nsnd_tensor: (1,L)(1, L)(1,L). The values should lie between [-1, 1].\nmat: (N,D)(N, D)(N,D), where N is number of data and D is feature dimension\nlabel_img: (N,C,H,W)(N, C, H, W)(N,C,H,W)\nvertices: (B,N,3)(B, N, 3)(B,N,3). (batch, number_of_vertices, channels)\ncolors: (B,N,3)(B, N, 3)(B,N,3). The values should lie in [0, 255] for type uint8 or [0, 1] for type float.\nfaces: (B,N,3)(B, N, 3)(B,N,3). The values should lie in [0, number_of_vertices] for type uint8.\n",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.fsdp.FullyShardedDataParallel.summon_full_params",
        "api_url": "https://pytorch.org/docs/stable/fsdp.html#torch.distributed.fsdp.FullyShardedDataParallel.summon_full_params",
        "api_signature": "summon_full_params(module, recurse=True, writeback=True, rank0_only=False, offload_to_cpu=False, with_grads=False)",
        "api_description": "Expose full params for FSDP instances with this context manager.",
        "return_value": "",
        "parameters": "recurse (bool, Optional) – recursively summon all params for nested\nFSDP instances (default: True).\nwriteback (bool, Optional) – if False, modifications to params are\ndiscarded after the context manager exits;\ndisabling this can be slightly more efficient (default: True)\nrank0_only (bool, Optional) – if True, full parameters are\nmaterialized on only global rank 0. This means that within the\ncontext, only rank 0 will have full parameters and the other\nranks will have sharded parameters. Note that setting\nrank0_only=True with writeback=True is not supported,\nas model parameter shapes will be different across ranks\nwithin the context, and writing to them can lead to\ninconsistency across ranks when the context is exited.\noffload_to_cpu (bool, Optional) – If True, full parameters are\noffloaded to CPU. Note that this offloading currently only\noccurs if the parameter is sharded (which is only not the case\nfor world_size = 1 or NO_SHARD config). It is recommended\nto use offload_to_cpu with rank0_only=True to avoid\nredundant copies of model parameters being offloaded to the same CPU memory.\nwith_grads (bool, Optional) – If True, gradients are also\nunsharded with the parameters. Currently, this is only\nsupported when passing use_orig_params=True to the FSDP\nconstructor and offload_to_cpu=False to this method.\n(Default: False)",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.bernoulli.Bernoulli.support",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.bernoulli.Bernoulli.support",
        "api_signature": "Boolean()",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.beta.Beta.support",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.beta.Beta.support",
        "api_signature": "Interval(lower_bound=0.0, upper_bound=1.0)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.binomial.Binomial.support",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.binomial.Binomial.support",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.categorical.Categorical.support",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.categorical.Categorical.support",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.cauchy.Cauchy.support",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.cauchy.Cauchy.support",
        "api_signature": "Real()",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.continuous_bernoulli.ContinuousBernoulli.support",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.continuous_bernoulli.ContinuousBernoulli.support",
        "api_signature": "Interval(lower_bound=0.0, upper_bound=1.0)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.dirichlet.Dirichlet.support",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.dirichlet.Dirichlet.support",
        "api_signature": "Simplex()",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.distribution.Distribution.support",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.distribution.Distribution.support",
        "api_signature": null,
        "api_description": "Returns a Constraint object\nrepresenting this distribution’s support.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.exponential.Exponential.support",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.exponential.Exponential.support",
        "api_signature": "GreaterThanEq(lower_bound=0.0)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.fishersnedecor.FisherSnedecor.support",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.fishersnedecor.FisherSnedecor.support",
        "api_signature": "GreaterThan(lower_bound=0.0)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.gamma.Gamma.support",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.gamma.Gamma.support",
        "api_signature": "GreaterThanEq(lower_bound=0.0)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.geometric.Geometric.support",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.geometric.Geometric.support",
        "api_signature": "IntegerGreaterThan(lower_bound=0)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.gumbel.Gumbel.support",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.gumbel.Gumbel.support",
        "api_signature": "Real()",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.half_cauchy.HalfCauchy.support",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.half_cauchy.HalfCauchy.support",
        "api_signature": "GreaterThanEq(lower_bound=0.0)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.half_normal.HalfNormal.support",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.half_normal.HalfNormal.support",
        "api_signature": "GreaterThanEq(lower_bound=0.0)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.independent.Independent.support",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.independent.Independent.support",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.inverse_gamma.InverseGamma.support",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.inverse_gamma.InverseGamma.support",
        "api_signature": "GreaterThan(lower_bound=0.0)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.kumaraswamy.Kumaraswamy.support",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.kumaraswamy.Kumaraswamy.support",
        "api_signature": "Interval(lower_bound=0.0, upper_bound=1.0)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.laplace.Laplace.support",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.laplace.Laplace.support",
        "api_signature": "Real()",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.lkj_cholesky.LKJCholesky.support",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.lkj_cholesky.LKJCholesky.support",
        "api_signature": "CorrCholesky()",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.log_normal.LogNormal.support",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.log_normal.LogNormal.support",
        "api_signature": "GreaterThan(lower_bound=0.0)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.support",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.support",
        "api_signature": "IndependentConstraint(Real()",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.mixture_same_family.MixtureSameFamily.support",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.mixture_same_family.MixtureSameFamily.support",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.multinomial.Multinomial.support",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.multinomial.Multinomial.support",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.multivariate_normal.MultivariateNormal.support",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.multivariate_normal.MultivariateNormal.support",
        "api_signature": "IndependentConstraint(Real()",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.negative_binomial.NegativeBinomial.support",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.negative_binomial.NegativeBinomial.support",
        "api_signature": "IntegerGreaterThan(lower_bound=0)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.normal.Normal.support",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.normal.Normal.support",
        "api_signature": "Real()",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.one_hot_categorical.OneHotCategorical.support",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.one_hot_categorical.OneHotCategorical.support",
        "api_signature": "OneHot()",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.pareto.Pareto.support",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.pareto.Pareto.support",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.poisson.Poisson.support",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.poisson.Poisson.support",
        "api_signature": "IntegerGreaterThan(lower_bound=0)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.support",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.support",
        "api_signature": "Real()",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.relaxed_bernoulli.RelaxedBernoulli.support",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.relaxed_bernoulli.RelaxedBernoulli.support",
        "api_signature": "Interval(lower_bound=0.0, upper_bound=1.0)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.support",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.support",
        "api_signature": "Simplex()",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.studentT.StudentT.support",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.studentT.StudentT.support",
        "api_signature": "Real()",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.transformed_distribution.TransformedDistribution.support",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.transformed_distribution.TransformedDistribution.support",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.uniform.Uniform.support",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.uniform.Uniform.support",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.von_mises.VonMises.support",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.von_mises.VonMises.support",
        "api_signature": "Real()",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.weibull.Weibull.support",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.weibull.Weibull.support",
        "api_signature": "GreaterThan(lower_bound=0.0)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.wishart.Wishart.support",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.wishart.Wishart.support",
        "api_signature": "PositiveDefinite()",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.symbolic_shapes.ShapeEnv.suppress_guards",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.fx.experimental.symbolic_shapes.ShapeEnv.html#torch.fx.experimental.symbolic_shapes.ShapeEnv.suppress_guards",
        "api_signature": "suppress_guards()",
        "api_description": "Context manager to ignore all guards generated inside",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.svd",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd",
        "api_signature": "torch.svd(input, some=True, compute_uv=True, *, out=None)",
        "api_description": "Computes the singular value decomposition of either a matrix or batch of\nmatrices input. The singular value decomposition is represented as a\nnamedtuple (U, S, V), such that input =Udiag(S)VH= U \\text{diag}(S) V^{\\text{H}}=Udiag(S)VH.\nwhere VHV^{\\text{H}}VH is the transpose of V for real inputs,\nand the conjugate transpose of V for complex inputs.\nIf input is a batch of matrices, then U, S, and V are also\nbatched with the same batch dimensions as input.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor of size (*, m, n) where * is zero or more\nbatch dimensions consisting of (m, n) matrices.\nsome (bool, optional) – controls whether to compute the reduced or full decomposition, and\nconsequently, the shape of returned U and V. Default: True.\ncompute_uv (bool, optional) – controls whether to compute U and V. Default: True.\nout (tuple, optional) – the output tuple of tensors",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.linalg.svd",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.linalg.svd.html#torch.linalg.svd",
        "api_signature": "torch.linalg.svd(A, full_matrices=True, *, driver=None, out=None)",
        "api_description": "Computes the singular value decomposition (SVD) of a matrix.",
        "return_value": "A named tuple (U, S, Vh) which corresponds to UUU, SSS, VHV^{\\text{H}}VH above.\nS will always be real-valued, even when A is complex.\nIt will also be ordered in descending order.\nU and Vh will have the same dtype as A. The left / right singular vectors will be given by\nthe columns of U and the rows of Vh respectively.\n\n",
        "parameters": "A (Tensor) – tensor of shape (*, m, n) where * is zero or more batch dimensions.\nfull_matrices (bool, optional) – controls whether to compute the full or reduced\nSVD, and consequently,\nthe shape of the returned tensors\nU and Vh. Default: True.\ndriver (str, optional) – name of the cuSOLVER method to be used. This keyword argument only works on CUDA inputs.\nAvailable options are: None, gesvd, gesvdj, and gesvda.\nDefault: None.\nout (tuple, optional) – output tuple of three tensors. Ignored if None.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.svd",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.svd.html#torch.Tensor.svd",
        "api_signature": "Tensor.svd(some=True, compute_uv=True)",
        "api_description": "See torch.svd()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.svd_lowrank",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.svd_lowrank.html#torch.svd_lowrank",
        "api_signature": "torch.svd_lowrank(A, q=6, niter=2, M=None)",
        "api_description": "Return the singular value decomposition (U, S, V) of a matrix,\nbatches of matrices, or a sparse matrix AAA such that\nA≈Udiag(S)VTA \\approx U diag(S) V^TA≈Udiag(S)VT. In case MMM is given, then\nSVD is computed for the matrix A−MA - MA−M.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.linalg.svdvals",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.linalg.svdvals.html#torch.linalg.svdvals",
        "api_signature": "torch.linalg.svdvals(A, *, driver=None, out=None)",
        "api_description": "Computes the singular values of a matrix.",
        "return_value": "A real-valued tensor, even when A is complex.\n",
        "parameters": "A (Tensor) – tensor of shape (*, m, n) where * is zero or more batch dimensions.\ndriver (str, optional) – name of the cuSOLVER method to be used. This keyword argument only works on CUDA inputs.\nAvailable options are: None, gesvd, gesvdj, and gesvda.\nCheck torch.linalg.svd() for details.\nDefault: None.\nout (Tensor, optional) – output tensor. Ignored if None. Default: None.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.swap_module",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.quantization.swap_module.html#torch.ao.quantization.swap_module",
        "api_signature": "torch.ao.quantization.swap_module(mod, mapping, custom_module_class_mapping)",
        "api_description": "Swaps the module if it has a quantized counterpart and it has an\nobserver attached.",
        "return_value": "The corresponding quantized module of mod\n",
        "parameters": "mod – input module\nmapping – a dictionary that maps from nn module to nnq module",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.swap_tensors",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.utils.swap_tensors.html#torch.utils.swap_tensors",
        "api_signature": "torch.utils.swap_tensors(t1, t2)",
        "api_description": "This function swaps the content of the two Tensor objects.\nAt a high level, this will make t1 have the content of t2 while preserving\nits identity.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.swapaxes",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.swapaxes.html#torch.swapaxes",
        "api_signature": "torch.swapaxes(input, axis0, axis1)",
        "api_description": "Alias for torch.transpose().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.swapaxes",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.swapaxes.html#torch.Tensor.swapaxes",
        "api_signature": "Tensor.swapaxes(axis0, axis1)",
        "api_description": "See torch.swapaxes()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.swapdims",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.swapdims.html#torch.swapdims",
        "api_signature": "torch.swapdims(input, dim0, dim1)",
        "api_description": "Alias for torch.transpose().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.swapdims",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.swapdims.html#torch.Tensor.swapdims",
        "api_signature": "Tensor.swapdims(dim0, dim1)",
        "api_description": "See torch.swapdims()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.symbolic_shapes.sym_eq",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.fx.experimental.symbolic_shapes.sym_eq.html#torch.fx.experimental.symbolic_shapes.sym_eq",
        "api_signature": "torch.fx.experimental.symbolic_shapes.sym_eq(x, y)",
        "api_description": "Like ==, but when run on list/tuple, it will recursively test equality\nand use sym_and to join the results together, without guarding.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.sym_float",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.sym_float.html#torch.sym_float",
        "api_signature": "torch.sym_float(a)",
        "api_description": "SymInt-aware utility for float casting.",
        "return_value": "",
        "parameters": "a (SymInt, SymFloat, or object) – Object to cast",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.sym_int",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.sym_int.html#torch.sym_int",
        "api_signature": "torch.sym_int(a)",
        "api_description": "SymInt-aware utility for int casting.",
        "return_value": "",
        "parameters": "a (SymInt, SymFloat, or object) – Object to cast",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.sym_ite",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.sym_ite.html#torch.sym_ite",
        "api_signature": "torch.sym_ite(b, t, f)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.sym_max",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.sym_max.html#torch.sym_max",
        "api_signature": "torch.sym_max(a, b)",
        "api_description": "SymInt-aware utility for max().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.sym_min",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.sym_min.html#torch.sym_min",
        "api_signature": "torch.sym_min(a, b)",
        "api_description": "SymInt-aware utility for max().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.sym_not",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.sym_not.html#torch.sym_not",
        "api_signature": "torch.sym_not(a)",
        "api_description": "SymInt-aware utility for logical negation.",
        "return_value": "",
        "parameters": "a (SymBool or bool) – Object to negate",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.symbolic_trace",
        "api_url": "https://pytorch.org/docs/stable/fx.html#torch.fx.symbolic_trace",
        "api_signature": "torch.fx.symbolic_trace(root, concrete_args=None)",
        "api_description": "Symbolic tracing API",
        "return_value": "a Module created from the recorded operations from root.\n",
        "parameters": "root (Union[torch.nn.Module, Callable]) – Module or function to be traced and converted\ninto a Graph representation.\nconcrete_args (Optional[Dict[str, any]]) – Inputs to be partially specialized",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.symbolic_shapes.SymbolicContext",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.fx.experimental.symbolic_shapes.SymbolicContext.html#torch.fx.experimental.symbolic_shapes.SymbolicContext",
        "api_signature": null,
        "api_description": "Data structure specifying how we should create symbols in\ncreate_symbolic_sizes_strides_storage_offset; e.g., should\nthey be static or dynamic.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.SymBool",
        "api_url": "https://pytorch.org/docs/stable/torch.html#torch.SymBool",
        "api_signature": "torch.SymBool(node)",
        "api_description": "Like an bool (including magic methods), but redirects all operations on the\nwrapped node. This is used in particular to symbolically record operations\nin the symbolic shape workflow.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.SymFloat",
        "api_url": "https://pytorch.org/docs/stable/torch.html#torch.SymFloat",
        "api_signature": "torch.SymFloat(node)",
        "api_description": "Like an float (including magic methods), but redirects all operations on the\nwrapped node. This is used in particular to symbolically record operations\nin the symbolic shape workflow.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.SymInt",
        "api_url": "https://pytorch.org/docs/stable/torch.html#torch.SymInt",
        "api_signature": "torch.SymInt(node)",
        "api_description": "Like an int (including magic methods), but redirects all operations on the\nwrapped node. This is used in particular to symbolically record operations\nin the symbolic shape workflow.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.SyncBatchNorm",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.SyncBatchNorm.html#torch.nn.SyncBatchNorm",
        "api_signature": "torch.nn.SyncBatchNorm(num_features, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, process_group=None, device=None, dtype=None)",
        "api_description": "Applies Batch Normalization over a N-Dimensional input.",
        "return_value": "The original module with the converted torch.nn.SyncBatchNorm\nlayers. If the original module is a BatchNorm*D layer,\na new torch.nn.SyncBatchNorm layer object will be returned\ninstead.\n",
        "parameters": "num_features (int) – CCC from an expected input of size\n(N,C,+)(N, C, +)(N,C,+)\neps (float) – a value added to the denominator for numerical stability.\nDefault: 1e-5\nmomentum (float) – the value used for the running_mean and running_var\ncomputation. Can be set to None for cumulative moving average\n(i.e. simple average). Default: 0.1\naffine (bool) – a boolean value that when set to True, this module has\nlearnable affine parameters. Default: True\ntrack_running_stats (bool) – a boolean value that when set to True, this\nmodule tracks the running mean and variance, and when set to False,\nthis module does not track such statistics, and initializes statistics\nbuffers running_mean and running_var as None.\nWhen these buffers are None, this module always uses batch statistics.\nin both training and eval modes. Default: True\nprocess_group (Optional[Any]) – synchronization of stats happen within each process group\nindividually. Default behavior is synchronization across the whole\nworld\nmodule (nn.Module) – module containing one or more BatchNorm*D layers\nprocess_group (optional) – process group to scope synchronization,\ndefault is the whole world",
        "input_shape": "\nInput: (N,C,+)(N, C, +)(N,C,+)\nOutput: (N,C,+)(N, C, +)(N,C,+) (same shape as input)\n\n",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cpu.synchronize",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.cpu.synchronize.html#torch.cpu.synchronize",
        "api_signature": "torch.cpu.synchronize(device=None)",
        "api_description": "Waits for all kernels in all streams on the CPU device to complete.",
        "return_value": "",
        "parameters": "device (torch.device or int, optional) – ignored, there’s only one CPU device.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda.synchronize",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.cuda.synchronize.html#torch.cuda.synchronize",
        "api_signature": "torch.cuda.synchronize(device=None)",
        "api_description": "Wait for all kernels in all streams on a CUDA device to complete.",
        "return_value": "",
        "parameters": "device (torch.device or int, optional) – device for which to synchronize.\nIt uses the current device, given by current_device(),\nif device is None (default).",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.mps.synchronize",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.mps.synchronize.html#torch.mps.synchronize",
        "api_signature": "torch.mps.synchronize()",
        "api_description": "Waits for all kernels in all streams on a MPS device to complete.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.xpu.synchronize",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.xpu.synchronize.html#torch.xpu.synchronize",
        "api_signature": "torch.xpu.synchronize(device=None)",
        "api_description": "Wait for all kernels in all streams on a XPU device to complete.",
        "return_value": "",
        "parameters": "device (torch.device or int, optional) – device for which to synchronize.\nIt uses the current device, given by current_device(),\nif device is None (default).",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda.Event.synchronize",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.cuda.Event.html#torch.cuda.Event.synchronize",
        "api_signature": "synchronize()",
        "api_description": "Wait for the event to complete.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda.ExternalStream.synchronize",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.cuda.ExternalStream.html#torch.cuda.ExternalStream.synchronize",
        "api_signature": "synchronize()",
        "api_description": "Wait for all the kernels in this stream to complete.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda.Stream.synchronize",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.cuda.Stream.html#torch.cuda.Stream.synchronize",
        "api_signature": "synchronize()",
        "api_description": "Wait for all the kernels in this stream to complete.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.mps.event.Event.synchronize",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.mps.event.Event.html#torch.mps.event.Event.synchronize",
        "api_signature": "synchronize()",
        "api_description": "Waits until the completion of all work currently captured in this event.\nThis prevents the CPU thread from proceeding until the event completes.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.xpu.Event.synchronize",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.xpu.Event.html#torch.xpu.Event.synchronize",
        "api_signature": "synchronize()",
        "api_description": "Wait for the event to complete.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.xpu.Stream.synchronize",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.xpu.Stream.html#torch.xpu.Stream.synchronize",
        "api_signature": "synchronize()",
        "api_description": "Wait for all the kernels in this stream to complete.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.T",
        "api_url": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor.T",
        "api_signature": null,
        "api_description": "Returns a view of this tensor with its dimensions reversed.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.t",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.t.html#torch.t",
        "api_signature": "torch.t(input)",
        "api_description": "Expects input to be <= 2-D tensor and transposes dimensions 0\nand 1.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.t",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.t.html#torch.Tensor.t",
        "api_signature": "Tensor.t()",
        "api_description": "See torch.t()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.t_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.t_.html#torch.Tensor.t_",
        "api_signature": "Tensor.t_()",
        "api_description": "In-place version of t()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tag",
        "api_url": "https://pytorch.org/docs/stable/torch.html#torch.Tag",
        "api_signature": null,
        "api_description": "Members:",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.take",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.take.html#torch.take",
        "api_signature": "torch.take(input, index)",
        "api_description": "Returns a new tensor with the elements of input at the given indices.\nThe input tensor is treated as if it were viewed as a 1-D tensor. The result\ntakes the same shape as the indices.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.\nindex (LongTensor) – the indices into tensor",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.take",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.take.html#torch.Tensor.take",
        "api_signature": "Tensor.take(indices)",
        "api_description": "See torch.take()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.take_along_dim",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.take_along_dim.html#torch.take_along_dim",
        "api_signature": "torch.take_along_dim(input, indices, dim=None, *, out=None)",
        "api_description": "Selects values from input at the 1-dimensional indices from indices along the given dim.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.\nindices (tensor) – the indices into input. Must have long dtype.\ndim (int, optional) – dimension to select along.\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.take_along_dim",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.take_along_dim.html#torch.Tensor.take_along_dim",
        "api_signature": "Tensor.take_along_dim(indices, dim)",
        "api_description": "See torch.take_along_dim()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.tan",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.tan.html#torch.tan",
        "api_signature": "torch.tan(input, *, out=None)",
        "api_description": "Returns a new tensor with the tangent of the elements of input.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.tan",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.tan.html#torch.Tensor.tan",
        "api_signature": "Tensor.tan()",
        "api_description": "See torch.tan()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.tan_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.tan_.html#torch.Tensor.tan_",
        "api_signature": "Tensor.tan_()",
        "api_description": "In-place version of tan()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.autograd.forward_ad.UnpackedDualTensor.tangent",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.autograd.forward_ad.UnpackedDualTensor.html#torch.autograd.forward_ad.UnpackedDualTensor.tangent",
        "api_signature": null,
        "api_description": "Alias for field number 1",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.Tanh",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.Tanh.html#torch.nn.Tanh",
        "api_signature": "torch.nn.Tanh(*args, **kwargs)",
        "api_description": "Applies the Hyperbolic Tangent (Tanh) function element-wise.",
        "return_value": "",
        "parameters": "",
        "input_shape": "\nInput: (∗)(*)(∗), where ∗*∗ means any number of dimensions.\nOutput: (∗)(*)(∗), same shape as the input.\n\n",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.tanh",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.tanh.html#torch.tanh",
        "api_signature": "torch.tanh(input, *, out=None)",
        "api_description": "Returns a new tensor with the hyperbolic tangent of the elements\nof input.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.functional.tanh",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.functional.tanh.html#torch.nn.functional.tanh",
        "api_signature": "torch.nn.functional.tanh(input)",
        "api_description": "Applies element-wise,\nTanh(x)=tanh⁡(x)=exp⁡(x)−exp⁡(−x)exp⁡(x)+exp⁡(−x)\\text{Tanh}(x) = \\tanh(x) = \\frac{\\exp(x) - \\exp(-x)}{\\exp(x) + \\exp(-x)}Tanh(x)=tanh(x)=exp(x)+exp(−x)exp(x)−exp(−x)​",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.tanh",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.tanh.html#torch.Tensor.tanh",
        "api_signature": "Tensor.tanh()",
        "api_description": "See torch.tanh()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.tanh_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.tanh_.html#torch.Tensor.tanh_",
        "api_signature": "Tensor.tanh_()",
        "api_description": "In-place version of tanh()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.Tanhshrink",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.Tanhshrink.html#torch.nn.Tanhshrink",
        "api_signature": "torch.nn.Tanhshrink(*args, **kwargs)",
        "api_description": "Applies the element-wise Tanhshrink function.",
        "return_value": "",
        "parameters": "",
        "input_shape": "\nInput: (∗)(*)(∗), where ∗*∗ means any number of dimensions.\nOutput: (∗)(*)(∗), same shape as the input.\n\n",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.functional.tanhshrink",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.functional.tanhshrink.html#torch.nn.functional.tanhshrink",
        "api_signature": "torch.nn.functional.tanhshrink(input)",
        "api_description": "Applies element-wise, Tanhshrink(x)=x−Tanh(x)\\text{Tanhshrink}(x) = x - \\text{Tanh}(x)Tanhshrink(x)=x−Tanh(x)",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.transforms.TanhTransform",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.transforms.TanhTransform",
        "api_signature": "torch.distributions.transforms.TanhTransform(cache_size=0)",
        "api_description": "Transform via the mapping y=tanh⁡(x)y = \\tanh(x)y=tanh(x).",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.TCPStore",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#torch.distributed.TCPStore",
        "api_signature": null,
        "api_description": "A TCP-based distributed key-value store implementation. The server store holds\nthe data, while the client stores can connect to the server store over TCP and\nperform actions such as set() to insert a key-value\npair, get() to retrieve a key-value pair, etc. There\nshould always be one server store initialized because the client store(s) will wait for\nthe server to establish a connection.",
        "return_value": "",
        "parameters": "host_name (str) – The hostname or IP Address the server store should run on.\nport (int) – The port on which the server store should listen for incoming requests.\nworld_size (int, optional) – The total number of store users (number of clients + 1 for the server). Default is None (None indicates a non-fixed number of store users).\nis_master (bool, optional) – True when initializing the server store and False for client stores. Default is False.\ntimeout (timedelta, optional) – Timeout used by the store during initialization and for methods such as get() and wait(). Default is timedelta(seconds=300)\nwait_for_workers (bool, optional) – Whether to wait for all the workers to connect with the server store. This is only applicable when world_size is a fixed value. Default is True.\nmulti_tenant (bool, optional) – If True, all TCPStore instances in the current process with the same host/port will use the same underlying TCPServer. Default is False.\nmaster_listen_fd (int, optional) – If specified, the underlying TCPServer will listen on this file descriptor, which must be a socket already bound to port. Useful to avoid port assignment races in some scenarios. Default is None (meaning the server creates a new socket and attempts to bind it to port).",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> # Run on process 1 (server)\n>>> server_store = dist.TCPStore(\"127.0.0.1\", 1234, 2, True, timedelta(seconds=30))\n>>> # Run on process 2 (client)\n>>> client_store = dist.TCPStore(\"127.0.0.1\", 1234, 2, False)\n>>> # Use any of the store methods from either the client or server after initialization\n>>> server_store.set(\"first_key\", \"first_value\")\n>>> client_store.get(\"first_key\")\n\n\n"
    },
    {
        "api_name": "torch.distributions.relaxed_bernoulli.RelaxedBernoulli.temperature",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.relaxed_bernoulli.RelaxedBernoulli.temperature",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.temperature",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.temperature",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda.temperature",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.cuda.temperature.html#torch.cuda.temperature",
        "api_signature": "torch.cuda.temperature(device=None)",
        "api_description": "Return the average temperature of the GPU sensor in Degrees C (Centigrades).",
        "return_value": "",
        "parameters": "device (torch.device or int, optional) – selected device. Returns\nstatistic for the current device, given by current_device(),\nif device is None (default).",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor",
        "api_url": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor",
        "api_signature": null,
        "api_description": "There are a few main ways to create a tensor, depending on your use case.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.tensor",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor",
        "api_signature": "torch.tensor(data, *, dtype=None, device=None, requires_grad=False, pin_memory=False)",
        "api_description": "Constructs a tensor with no autograd history (also known as a “leaf tensor”, see Autograd mechanics) by copying data.",
        "return_value": "",
        "parameters": "data (array_like) – Initial data for the tensor. Can be a list, tuple,\nNumPy ndarray, scalar, and other types.\ndtype (torch.dtype, optional) – the desired data type of returned tensor.\nDefault: if None, infers data type from data.\ndevice (torch.device, optional) – the device of the constructed tensor. If None and data is a tensor\nthen the device of data is used. If None and data is not a tensor then\nthe result tensor is constructed on the current device.\nrequires_grad (bool, optional) – If autograd should record operations on the\nreturned tensor. Default: False.\npin_memory (bool, optional) – If set, returned tensor would be allocated in\nthe pinned memory. Works only for CPU tensors. Default: False.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.tensor_split",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.tensor_split.html#torch.tensor_split",
        "api_signature": "torch.tensor_split(input, indices_or_sections, dim=0)",
        "api_description": "Splits a tensor into multiple sub-tensors, all of which are views of input,\nalong dimension dim according to the indices or number of sections specified\nby indices_or_sections. This function is based on NumPy’s\nnumpy.array_split().",
        "return_value": "",
        "parameters": "input (Tensor) – the tensor to split\nindices_or_sections (Tensor, int or list or tuple of ints) – If indices_or_sections is an integer n or a zero dimensional long tensor\nwith value n, input is split into n sections along dimension dim.\nIf input is divisible by n along dimension dim, each\nsection will be of equal size, input.size(dim) / n. If input\nis not divisible by n, the sizes of the first int(input.size(dim) % n)\nsections will have size int(input.size(dim) / n) + 1, and the rest will\nhave size int(input.size(dim) / n).\nIf indices_or_sections is a list or tuple of ints, or a one-dimensional long\ntensor, then input is split along dimension dim at each of the indices\nin the list, tuple or tensor. For instance, indices_or_sections=[2, 3] and dim=0\nwould result in the tensors input[:2], input[2:3], and input[3:].\nIf indices_or_sections is a tensor, it must be a zero-dimensional or one-dimensional\nlong tensor on the CPU.\ndim (int, optional) – dimension along which to split the tensor. Default: 0",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.tensor_split",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.tensor_split.html#torch.Tensor.tensor_split",
        "api_signature": "Tensor.tensor_split(indices_or_sections, dim=0)",
        "api_description": "See torch.tensor_split()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.checkpoint.planner.WriteItem.tensor_storage_size",
        "api_url": "https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.planner.WriteItem.tensor_storage_size",
        "api_signature": "tensor_storage_size()",
        "api_description": "Calculates the storage size of the underlying tensor, or None if this is not a tensor write.",
        "return_value": "Optional[int] storage size, in bytes of underlying tensor if any.\n",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.profiler.tensorboard_trace_handler",
        "api_url": "https://pytorch.org/docs/stable/profiler.html#torch.profiler.tensorboard_trace_handler",
        "api_signature": "torch.profiler.tensorboard_trace_handler(dir_name, worker_name=None, use_gzip=False)",
        "api_description": "Outputs tracing files to directory of dir_name, then that directory can be\ndirectly delivered to tensorboard as logdir.\nworker_name should be unique for each worker in distributed scenario,\nit will be set to ‘[hostname]_[pid]’ by default.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.monitor.TensorboardEventHandler",
        "api_url": "https://pytorch.org/docs/stable/monitor.html#torch.monitor.TensorboardEventHandler",
        "api_signature": "torch.monitor.TensorboardEventHandler(writer)",
        "api_description": "TensorboardEventHandler is an event handler that will write known events to\nthe provided SummaryWriter.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.data.TensorDataset",
        "api_url": "https://pytorch.org/docs/stable/data.html#torch.utils.data.TensorDataset",
        "api_signature": "torch.utils.data.TensorDataset(*tensors)",
        "api_description": "Dataset wrapping tensors.",
        "return_value": "",
        "parameters": "*tensors (Tensor) – tensors that have the same size of the first dimension.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.tensordot",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.tensordot.html#torch.tensordot",
        "api_signature": "torch.tensordot(a, b, dims=2, out=None)",
        "api_description": "Returns a contraction of a and b over multiple dimensions.",
        "return_value": "",
        "parameters": "a (Tensor) – Left tensor to contract\nb (Tensor) – Right tensor to contract\ndims (int or Tuple[List[int], List[int]] or List[List[int]] containing two lists or Tensor) – number of dimensions to\ncontract or explicit lists of dimensions for a and\nb respectively",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.linalg.tensorinv",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.linalg.tensorinv.html#torch.linalg.tensorinv",
        "api_signature": "torch.linalg.tensorinv(A, ind=2, *, out=None)",
        "api_description": "Computes the multiplicative inverse of torch.tensordot().",
        "return_value": "",
        "parameters": "A (Tensor) – tensor to invert. Its shape must satisfy\nprod(A.shape[:ind]) ==\nprod(A.shape[ind:]).\nind (int) – index at which to compute the inverse of torch.tensordot(). Default: 2.\nout (Tensor, optional) – output tensor. Ignored if None. Default: None.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.rpc.TensorPipeRpcBackendOptions",
        "api_url": "https://pytorch.org/docs/stable/rpc.html#torch.distributed.rpc.TensorPipeRpcBackendOptions",
        "api_signature": "torch.distributed.rpc.TensorPipeRpcBackendOptions(*, num_worker_threads=16, rpc_timeout=60.0, init_method='env://', device_maps=None, devices=None, _transports=None, _channels=None)",
        "api_description": "The backend options for\nTensorPipeAgent, derived from\nRpcBackendOptions.",
        "return_value": "",
        "parameters": "num_worker_threads (int, optional) – The number of threads in the\nthread-pool used by\nTensorPipeAgent to execute\nrequests (default: 16).\nrpc_timeout (float, optional) – The default timeout, in seconds,\nfor RPC requests (default: 60 seconds). If the RPC has not\ncompleted in this timeframe, an exception indicating so will\nbe raised. Callers can override this timeout for individual\nRPCs in rpc_sync() and\nrpc_async() if necessary.\ninit_method (str, optional) – The URL to initialize the distributed\nstore used for rendezvous. It takes any value accepted for the\nsame argument of init_process_group()\n(default: env://).\ndevice_maps (Dict[str, Dict], optional) – Device placement mappings from\nthis worker to the callee. Key is the callee worker name and value\nthe dictionary (Dict of int, str, or torch.device)\nthat maps this worker’s devices to the callee worker’s devices.\n(default: None)\ndevices (List[int, str, or torch.device], optional) – all local\nCUDA devices used by RPC agent. By Default, it will be initialized\nto all local devices from its own device_maps and corresponding\ndevices from its peers’ device_maps. When processing CUDA RPC\nrequests, the agent will properly synchronize CUDA streams for\nall devices in this List.\nto (str) – Callee name.\ndevice_map (Dict of int, str, or torch.device) – Device placement\nmappings from this worker to the callee. This map must be\ninvertible.\ndevices (List of int, str, or torch.device) – local devices used by\nthe TensorPipe RPC agent.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.linalg.tensorsolve",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.linalg.tensorsolve.html#torch.linalg.tensorsolve",
        "api_signature": "torch.linalg.tensorsolve(A, B, dims=None, *, out=None)",
        "api_description": "Computes the solution X to the system torch.tensordot(A, X) = B.",
        "return_value": "",
        "parameters": "A (Tensor) – tensor to solve for. Its shape must satisfy\nprod(A.shape[:B.ndim]) ==\nprod(A.shape[B.ndim:]).\nB (Tensor) – tensor of shape A.shape[:B.ndim].\ndims (Tuple[int], optional) – dimensions of A to be moved.\nIf None, no dimensions are moved. Default: None.\nout (Tensor, optional) – output tensor. Ignored if None. Default: None.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.futures.Future.then",
        "api_url": "https://pytorch.org/docs/stable/futures.html#torch.futures.Future.then",
        "api_signature": "then(callback)",
        "api_description": "Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed (to enforce a certain order consider chaining:\nfut.then(cb1).then(cb2)). The callback must take one argument, which\nis the reference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run immediately inline.",
        "return_value": "A new Future object that holds the return value of the\ncallback and will be marked as completed when the given\ncallback finishes.\n",
        "parameters": "callback (Callable) – a Callable that takes this Future as\nthe only argument.",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> def callback(fut):\n...     print(f\"RPC return value is {fut.wait()}.\")\n>>> fut = torch.futures.Future()\n>>> # The inserted callback will print the return value when\n>>> # receiving the response from \"worker1\"\n>>> cb_fut = fut.then(callback)\n>>> chain_cb_fut = cb_fut.then(\n...     lambda x : print(f\"Chained cb done. {x.wait()}\")\n... )\n>>> fut.set_result(5)\nRPC return value is 5.\nChained cb done. None\n\n\n"
    },
    {
        "api_name": "torch.ao.nn.quantized.functional.threshold",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.nn.quantized.functional.threshold.html#torch.ao.nn.quantized.functional.threshold",
        "api_signature": "torch.ao.nn.quantized.functional.threshold(input, threshold, value)",
        "api_description": "Applies the quantized version of the threshold function element-wise:",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.Threshold",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.Threshold.html#torch.nn.Threshold",
        "api_signature": "torch.nn.Threshold(threshold, value, inplace=False)",
        "api_description": "Thresholds each element of the input Tensor.",
        "return_value": "",
        "parameters": "threshold (float) – The value to threshold at\nvalue (float) – The value to replace with\ninplace (bool) – can optionally do the operation in-place. Default: False",
        "input_shape": "\nInput: (∗)(*)(∗), where ∗*∗ means any number of dimensions.\nOutput: (∗)(*)(∗), same shape as the input.\n\n",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.functional.threshold",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.functional.threshold.html#torch.nn.functional.threshold",
        "api_signature": "torch.nn.functional.threshold(input, threshold, value, inplace=False)",
        "api_description": "Apply a threshold to each element of the input Tensor.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.functional.threshold_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.functional.threshold_.html#torch.nn.functional.threshold_",
        "api_signature": "torch.nn.functional.threshold_(input, threshold, value)",
        "api_description": "In-place version of threshold().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.tile",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.tile.html#torch.tile",
        "api_signature": "torch.tile(input, dims)",
        "api_description": "Constructs a tensor by repeating the elements of input.\nThe dims argument specifies the number of repetitions\nin each dimension.",
        "return_value": "",
        "parameters": "input (Tensor) – the tensor whose elements to repeat.\ndims (tuple) – the number of repetitions per dimension.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.tile",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.tile.html#torch.Tensor.tile",
        "api_signature": "Tensor.tile(dims)",
        "api_description": "See torch.tile()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.benchmark.Timer.timeit",
        "api_url": "https://pytorch.org/docs/stable/benchmark_utils.html#torch.utils.benchmark.Timer.timeit",
        "api_signature": "timeit(number=1000000)",
        "api_description": "Mirrors the semantics of timeit.Timer.timeit().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.benchmark.Timer",
        "api_url": "https://pytorch.org/docs/stable/benchmark_utils.html#torch.utils.benchmark.Timer",
        "api_signature": "torch.utils.benchmark.Timer(stmt='pass', setup='pass', global_setup='', timer=<built-in function perf_counter>, globals=None, label=None, sub_label=None, description=None, env=None, num_threads=1, language=Language.PYTHON)",
        "api_description": "Helper class for measuring execution time of PyTorch statements.",
        "return_value": "A Measurement object that contains measured runtimes and\nrepetition counts, and can be used to compute statistics.\n(mean, median, etc.)\nA Measurement object that contains measured runtimes and\nrepetition counts, and can be used to compute statistics.\n(mean, median, etc.)\nA CallgrindStats object which provides instruction counts and\nsome basic facilities for analyzing and manipulating results.\n",
        "parameters": "stmt (str) – Code snippet to be run in a loop and timed.\nsetup (str) – Optional setup code. Used to define variables used in stmt\nglobal_setup (str) – (C++ only)\nCode which is placed at the top level of the file for things like\n#include statements.\nwithout CUDA or there is no GPU present, this defaults to\ntimeit.default_timer; otherwise it will synchronize CUDA before\nmeasuring the time.\nglobals (Optional[Dict[str, Any]]) – A dict which defines the global variables when stmt is being\nexecuted. This is the other method for providing variables which\nstmt needs.\nlabel (Optional[str]) – String which summarizes stmt. For instance, if stmt is\n“torch.nn.functional.relu(torch.add(x, 1, out=out))”\none might set label to “ReLU(x + 1)” to improve readability.\nsub_label (Optional[str]) – Provide supplemental information to disambiguate measurements\nwith identical stmt or label. For instance, in our example\nabove sub_label might be “float” or “int”, so that it is easy\nto differentiate:\n“ReLU(x + 1): (float)”\n”ReLU(x + 1): (int)”\nwhen printing Measurements or summarizing using Compare.\ndescription (Optional[str]) – String to distinguish measurements with identical label and\nsub_label. The principal use of description is to signal to\nCompare the columns of data. For instance one might set it\nbased on the input size  to create a table of the form:\n| n=1 | n=4 | ...\n------------- ...\nReLU(x + 1): (float)    | ... | ... | ...\nReLU(x + 1): (int)      | ... | ... | ...\nusing Compare. It is also included when printing a Measurement.\nenv (Optional[str]) – This tag indicates that otherwise identical tasks were run in\ndifferent environments, and are therefore not equivalent, for\ninstance when A/B testing a change to a kernel. Compare will\ntreat Measurements with different env specification as distinct\nwhen merging replicate runs.\nthreaded performance is important as both a key inference workload\nand a good indicator of intrinsic algorithmic efficiency, so the\nthreadpool size which tries to utilize all cores.\nthreshold (float) – value of iqr/median threshold for stopping\nmin_run_time (float) – total runtime needed before checking threshold\nmax_run_time (float) – total runtime  for all measurements regardless of threshold",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.timer.TimerClient",
        "api_url": "https://pytorch.org/docs/stable/elastic/timer.html#torch.distributed.elastic.timer.TimerClient",
        "api_signature": null,
        "api_description": "Client library to acquire and release countdown timers by communicating\nwith the TimerServer.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.timer.TimerRequest",
        "api_url": "https://pytorch.org/docs/stable/elastic/timer.html#torch.distributed.elastic.timer.TimerRequest",
        "api_signature": "torch.distributed.elastic.timer.TimerRequest(worker_id, scope_id, expiration_time)",
        "api_description": "Data object representing a countdown timer acquisition and release\nthat is used between the TimerClient and TimerServer.\nA negative expiration_time should be interpreted as a “release”\nrequest.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.timer.TimerServer",
        "api_url": "https://pytorch.org/docs/stable/elastic/timer.html#torch.distributed.elastic.timer.TimerServer",
        "api_signature": "torch.distributed.elastic.timer.TimerServer(request_queue, max_interval, daemon=True)",
        "api_description": "Entity that monitors active timers and expires them\nin a timely fashion. This server is responsible for\nreaping workers that have expired timers.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.monitor.Event.timestamp",
        "api_url": "https://pytorch.org/docs/stable/monitor.html#torch.monitor.Event.timestamp",
        "api_signature": null,
        "api_description": "The timestamp when the Event happened.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.jit.ScriptModule.to",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.jit.ScriptModule.html#torch.jit.ScriptModule.to",
        "api_signature": "to(*args, **kwargs)",
        "api_description": "Move and/or cast the parameters and buffers.",
        "return_value": "self\n",
        "parameters": "device (torch.device) – the desired device of the parameters\nand buffers in this module\ndtype (torch.dtype) – the desired floating point or complex dtype of\nthe parameters and buffers in this module\ntensor (torch.Tensor) – Tensor whose dtype and device are the desired\ndtype and device for all parameters and buffers in this module\nmemory_format (torch.memory_format) – the desired memory\nformat for 4D parameters and buffers in this module (keyword\nonly argument)",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.Module.to",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.to",
        "api_signature": "to(device: Optional[Union[str, device, int]] = ..., dtype: Optional[dtype] = ..., non_blocking: bool = ...)",
        "api_description": "",
        "return_value": "self\n",
        "parameters": "device (torch.device) – the desired device of the parameters\nand buffers in this module\ndtype (torch.dtype) – the desired floating point or complex dtype of\nthe parameters and buffers in this module\ntensor (torch.Tensor) – Tensor whose dtype and device are the desired\ndtype and device for all parameters and buffers in this module\nmemory_format (torch.memory_format) – the desired memory\nformat for 4D parameters and buffers in this module (keyword\nonly argument)",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.utils.rnn.PackedSequence.to",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.PackedSequence.html#torch.nn.utils.rnn.PackedSequence.to",
        "api_signature": "to(*args, **kwargs)",
        "api_description": "Perform dtype and/or device conversion on self.data.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.to",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.to.html#torch.Tensor.to",
        "api_signature": "Tensor.to(*args, **kwargs)",
        "api_description": "Performs Tensor dtype and/or device conversion. A torch.dtype and torch.device are\ninferred from the arguments of self.to(*args, **kwargs).",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.Tracer.to_bool",
        "api_url": "https://pytorch.org/docs/stable/fx.html#torch.fx.Tracer.to_bool",
        "api_signature": "to_bool(obj)",
        "api_description": "when used in control flow.  Normally we don’t know what to do because\nwe don’t know the value of the proxy, but a custom tracer can attach more\ninformation to the graph node using create_node and can choose to return a value.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.to_dense",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.to_dense.html#torch.Tensor.to_dense",
        "api_signature": "Tensor.to_dense(dtype=None, *, masked_grad=True)",
        "api_description": "Creates a strided copy of self if self is not a strided tensor, otherwise returns self.",
        "return_value": "",
        "parameters": "{dtype} –\nmasked_grad (bool, optional) – If set to True (default) and\nself has a sparse layout then the backward of\nto_dense() returns grad.sparse_mask(self).",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.backend_config.BackendConfig.to_dict",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.quantization.backend_config.BackendConfig.html#torch.ao.quantization.backend_config.BackendConfig.to_dict",
        "api_signature": "to_dict()",
        "api_description": "Convert this BackendConfig to a dictionary with the items described in\nfrom_dict().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.backend_config.BackendPatternConfig.to_dict",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.quantization.backend_config.BackendPatternConfig.html#torch.ao.quantization.backend_config.BackendPatternConfig.to_dict",
        "api_signature": "to_dict()",
        "api_description": "Convert this BackendPatternConfig to a dictionary with the items described in\nfrom_dict().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.backend_config.DTypeConfig.to_dict",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.quantization.backend_config.DTypeConfig.html#torch.ao.quantization.backend_config.DTypeConfig.to_dict",
        "api_signature": "to_dict()",
        "api_description": "Convert this DTypeConfig to a dictionary with the items described in\nfrom_dict().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.fx.custom_config.ConvertCustomConfig.to_dict",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.quantization.fx.custom_config.ConvertCustomConfig.html#torch.ao.quantization.fx.custom_config.ConvertCustomConfig.to_dict",
        "api_signature": "to_dict()",
        "api_description": "Convert this ConvertCustomConfig to a dictionary with the items described in\nfrom_dict().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.fx.custom_config.FuseCustomConfig.to_dict",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.quantization.fx.custom_config.FuseCustomConfig.html#torch.ao.quantization.fx.custom_config.FuseCustomConfig.to_dict",
        "api_signature": "to_dict()",
        "api_description": "Convert this FuseCustomConfig to a dictionary with the items described in\nfrom_dict().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.fx.custom_config.PrepareCustomConfig.to_dict",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.quantization.fx.custom_config.PrepareCustomConfig.html#torch.ao.quantization.fx.custom_config.PrepareCustomConfig.to_dict",
        "api_signature": "to_dict()",
        "api_description": "Convert this PrepareCustomConfig to a dictionary with the items described in\nfrom_dict().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.qconfig_mapping.QConfigMapping.to_dict",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.quantization.qconfig_mapping.QConfigMapping.html#torch.ao.quantization.qconfig_mapping.QConfigMapping.to_dict",
        "api_signature": "to_dict()",
        "api_description": "Convert this QConfigMapping to a dictionary with the following keys:",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.dlpack.to_dlpack",
        "api_url": "https://pytorch.org/docs/stable/dlpack.html#torch.utils.dlpack.to_dlpack",
        "api_signature": "torch.utils.dlpack.to_dlpack(tensor)",
        "api_description": "Returns an opaque object (a “DLPack capsule”) representing the tensor.",
        "return_value": "",
        "parameters": "tensor – a tensor to be exported",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.jit.ScriptModule.to_empty",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.jit.ScriptModule.html#torch.jit.ScriptModule.to_empty",
        "api_signature": "to_empty(*, device, recurse=True)",
        "api_description": "Move the parameters and buffers to the specified device without copying storage.",
        "return_value": "self\n",
        "parameters": "device (torch.device) – The desired device of the parameters\nand buffers in this module.\nrecurse (bool) – Whether parameters and buffers of submodules should\nbe recursively moved to the specified device.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.Module.to_empty",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.to_empty",
        "api_signature": "to_empty(*, device, recurse=True)",
        "api_description": "Move the parameters and buffers to the specified device without copying storage.",
        "return_value": "self\n",
        "parameters": "device (torch.device) – The desired device of the parameters\nand buffers in this module.\nrecurse (bool) – Whether parameters and buffers of submodules should\nbe recursively moved to the specified device.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.GraphModule.to_folder",
        "api_url": "https://pytorch.org/docs/stable/fx.html#torch.fx.GraphModule.to_folder",
        "api_signature": "to_folder(folder, module_name='FxModule')",
        "api_description": "imported with from <folder> import <module_name>",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.rpc.PyRRef.to_here",
        "api_url": "https://pytorch.org/docs/stable/rpc.html#torch.distributed.rpc.PyRRef.to_here",
        "api_signature": "to_here(self: torch._C._distributed_rpc.PyRRef, timeout: float = -1.0)",
        "api_description": "Blocking call that copies the value of the RRef from the owner\nto the local node and returns it. If the current node is the\nowner, returns a reference to the local value.",
        "return_value": "",
        "parameters": "timeout (float, optional) – Timeout for to_here. If\nthe call does not complete within this timeframe, an\nexception indicating so will be raised. If this\nargument is not provided, the default RPC timeout\n(60s) will be used.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.to_mkldnn",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.to_mkldnn.html#torch.Tensor.to_mkldnn",
        "api_signature": "Tensor.to_mkldnn()",
        "api_description": "Returns a copy of the tensor in torch.mkldnn layout.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nested.to_padded_tensor",
        "api_url": "https://pytorch.org/docs/stable/nested.html#torch.nested.to_padded_tensor",
        "api_signature": "torch.nested.to_padded_tensor(input, padding, output_size=None, out=None)",
        "api_description": "Returns a new (non-nested) Tensor by padding the input nested tensor.\nThe leading entries will be filled with the nested data,\nwhile the trailing entries will be padded.",
        "return_value": "",
        "parameters": "padding (float) – The padding value for the trailing entries.\noutput_size (Tuple[int]) – The size of the output tensor.\nIf given, it must be large enough to contain all nested data;\nelse, will infer by taking the max size of each nested sub-tensor along each dimension.\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.to_sparse",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.to_sparse.html#torch.Tensor.to_sparse",
        "api_signature": "Tensor.to_sparse(sparseDims)",
        "api_description": "Returns a sparse copy of the tensor.  PyTorch supports sparse tensors in\ncoordinate format.",
        "return_value": "",
        "parameters": "sparseDims (int, optional) – the number of sparse dimensions to include in the new sparse tensor\nlayout (torch.layout, optional) – The desired sparse\nlayout. One of torch.sparse_coo, torch.sparse_csr,\ntorch.sparse_csc, torch.sparse_bsr, or\ntorch.sparse_bsc. Default: if None,\ntorch.sparse_coo.\nblocksize (list, tuple, torch.Size, optional) – Block size\nof the resulting BSR or BSC tensor. For other layouts,\nspecifying the block size that is not None will result in a\nRuntimeError exception.  A block size must be a tuple of length\ntwo such that its items evenly divide the two sparse dimensions.\ndense_dim (int, optional) – Number of dense dimensions of the\nresulting CSR, CSC, BSR or BSC tensor.  This argument should be\nused only if self is a strided tensor, and must be a\nvalue between 0 and dimension of self tensor minus two.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.to_sparse_bsc",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.to_sparse_bsc.html#torch.Tensor.to_sparse_bsc",
        "api_signature": "Tensor.to_sparse_bsc(blocksize, dense_dim)",
        "api_description": "Convert a tensor to a block sparse column (BSC) storage format of\ngiven blocksize.  If the self is strided, then the number of\ndense dimensions could be specified, and a hybrid BSC tensor will be\ncreated, with dense_dim dense dimensions and self.dim() - 2 -\ndense_dim batch dimension.",
        "return_value": "",
        "parameters": "blocksize (list, tuple, torch.Size, optional) – Block size\nof the resulting BSC tensor. A block size must be a tuple of\nlength two such that its items evenly divide the two sparse\ndimensions.\ndense_dim (int, optional) – Number of dense dimensions of the\nresulting BSC tensor.  This argument should be used only if\nself is a strided tensor, and must be a value between 0\nand dimension of self tensor minus two.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.to_sparse_bsr",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.to_sparse_bsr.html#torch.Tensor.to_sparse_bsr",
        "api_signature": "Tensor.to_sparse_bsr(blocksize, dense_dim)",
        "api_description": "Convert a tensor to a block sparse row (BSR) storage format of given\nblocksize.  If the self is strided, then the number of dense\ndimensions could be specified, and a hybrid BSR tensor will be\ncreated, with dense_dim dense dimensions and self.dim() - 2 -\ndense_dim batch dimension.",
        "return_value": "",
        "parameters": "blocksize (list, tuple, torch.Size, optional) – Block size\nof the resulting BSR tensor. A block size must be a tuple of\nlength two such that its items evenly divide the two sparse\ndimensions.\ndense_dim (int, optional) – Number of dense dimensions of the\nresulting BSR tensor.  This argument should be used only if\nself is a strided tensor, and must be a value between 0\nand dimension of self tensor minus two.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.to_sparse_coo",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.to_sparse_coo.html#torch.Tensor.to_sparse_coo",
        "api_signature": "Tensor.to_sparse_coo()",
        "api_description": "Convert a tensor to coordinate format.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.to_sparse_csc",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.to_sparse_csc.html#torch.Tensor.to_sparse_csc",
        "api_signature": "Tensor.to_sparse_csc()",
        "api_description": "Convert a tensor to compressed column storage (CSC) format.  Except\nfor strided tensors, only works with 2D tensors.  If the self\nis strided, then the number of dense dimensions could be specified,\nand a hybrid CSC tensor will be created, with dense_dim dense\ndimensions and self.dim() - 2 - dense_dim batch dimension.",
        "return_value": "",
        "parameters": "dense_dim (int, optional) – Number of dense dimensions of the\nresulting CSC tensor.  This argument should be used only if\nself is a strided tensor, and must be a value between 0\nand dimension of self tensor minus two.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.to_sparse_csr",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.to_sparse_csr.html#torch.Tensor.to_sparse_csr",
        "api_signature": "Tensor.to_sparse_csr(dense_dim=None)",
        "api_description": "Convert a tensor to compressed row storage format (CSR).  Except for\nstrided tensors, only works with 2D tensors.  If the self is\nstrided, then the number of dense dimensions could be specified, and a\nhybrid CSR tensor will be created, with dense_dim dense dimensions\nand self.dim() - 2 - dense_dim batch dimension.",
        "return_value": "",
        "parameters": "dense_dim (int, optional) – Number of dense dimensions of the\nresulting CSR tensor.  This argument should be used only if\nself is a strided tensor, and must be a value between 0\nand dimension of self tensor minus two.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.tolist",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.tolist.html#torch.Tensor.tolist",
        "api_signature": "Tensor.tolist()",
        "api_description": "Returns the tensor as a (nested) list. For scalars, a standard\nPython number is returned, just like with item().\nTensors are automatically moved to the CPU first if necessary.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.TypedStorage.tolist",
        "api_url": "https://pytorch.org/docs/stable/storage.html#torch.TypedStorage.tolist",
        "api_signature": "tolist()",
        "api_description": "Return a list containing the elements of this storage.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.UntypedStorage.tolist",
        "api_url": "https://pytorch.org/docs/stable/storage.html#torch.UntypedStorage.tolist",
        "api_signature": "tolist()",
        "api_description": "Return a list containing the elements of this storage.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.topk",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.topk.html#torch.topk",
        "api_signature": "torch.topk(input, k, dim=None, largest=True, sorted=True, *, out=None)",
        "api_description": "Returns the k largest elements of the given input tensor along\na given dimension.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.\nk (int) – the k in “top-k”\ndim (int, optional) – the dimension to sort along\nlargest (bool, optional) – controls whether to return largest or\nsmallest elements\nsorted (bool, optional) – controls whether to return the elements\nin sorted order\nout (tuple, optional) – the output tuple of (Tensor, LongTensor) that can be\noptionally given to be used as output buffers",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.topk",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.topk.html#torch.Tensor.topk",
        "api_signature": "Tensor.topk(k, dim=None, largest=True, sorted=True)",
        "api_description": "See torch.topk()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch",
        "api_url": "https://pytorch.org/docs/stable/torch.html#module-torch",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch",
        "api_url": "https://pytorch.org/docs/stable/torch.html#module-torch",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.__config__",
        "api_url": "https://pytorch.org/docs/stable/config_mod.html#module-torch.__config__",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.__config__",
        "api_url": "https://pytorch.org/docs/stable/config_mod.html#module-torch.__config__",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.__future__",
        "api_url": "https://pytorch.org/docs/stable/future_mod.html#module-torch.__future__",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.__future__",
        "api_url": "https://pytorch.org/docs/stable/future_mod.html#module-torch.__future__",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch._logging",
        "api_url": "https://pytorch.org/docs/stable/logging.html#module-torch._logging",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch._logging",
        "api_url": "https://pytorch.org/docs/stable/logging.html#module-torch._logging",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.amp",
        "api_url": "https://pytorch.org/docs/stable/amp.html#module-torch.amp",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.amp",
        "api_url": "https://pytorch.org/docs/stable/amp.html#module-torch.amp",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.amp.autocast_mode",
        "api_url": "https://pytorch.org/docs/stable/amp.html#module-torch.amp.autocast_mode",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.amp.autocast_mode",
        "api_url": "https://pytorch.org/docs/stable/amp.html#module-torch.amp.autocast_mode",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.amp.grad_scaler",
        "api_url": "https://pytorch.org/docs/stable/amp.html#module-torch.amp.grad_scaler",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.amp.grad_scaler",
        "api_url": "https://pytorch.org/docs/stable/amp.html#module-torch.amp.grad_scaler",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.nn",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.nn",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.intrinsic",
        "api_url": "https://pytorch.org/docs/stable/quantization-support.html#module-torch.ao.nn.intrinsic",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.intrinsic",
        "api_url": "https://pytorch.org/docs/stable/quantization-support.html#module-torch.ao.nn.intrinsic",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.intrinsic.modules",
        "api_url": "https://pytorch.org/docs/stable/quantization-support.html#module-torch.ao.nn.intrinsic.modules",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.intrinsic.modules",
        "api_url": "https://pytorch.org/docs/stable/quantization-support.html#module-torch.ao.nn.intrinsic.modules",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.intrinsic.modules.fused",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.nn.intrinsic.modules.fused",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.intrinsic.modules.fused",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.nn.intrinsic.modules.fused",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.intrinsic.qat",
        "api_url": "https://pytorch.org/docs/stable/quantization-support.html#module-torch.ao.nn.intrinsic.qat",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.intrinsic.qat",
        "api_url": "https://pytorch.org/docs/stable/quantization-support.html#module-torch.ao.nn.intrinsic.qat",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.intrinsic.qat.modules",
        "api_url": "https://pytorch.org/docs/stable/quantization-support.html#module-torch.ao.nn.intrinsic.qat.modules",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.intrinsic.qat.modules",
        "api_url": "https://pytorch.org/docs/stable/quantization-support.html#module-torch.ao.nn.intrinsic.qat.modules",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.intrinsic.qat.modules.conv_fused",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.nn.intrinsic.qat.modules.conv_fused",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.intrinsic.qat.modules.conv_fused",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.nn.intrinsic.qat.modules.conv_fused",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.intrinsic.qat.modules.linear_fused",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.nn.intrinsic.qat.modules.linear_fused",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.intrinsic.qat.modules.linear_fused",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.nn.intrinsic.qat.modules.linear_fused",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.intrinsic.qat.modules.linear_relu",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.nn.intrinsic.qat.modules.linear_relu",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.intrinsic.qat.modules.linear_relu",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.nn.intrinsic.qat.modules.linear_relu",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.intrinsic.quantized",
        "api_url": "https://pytorch.org/docs/stable/quantization-support.html#module-torch.ao.nn.intrinsic.quantized",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.intrinsic.quantized",
        "api_url": "https://pytorch.org/docs/stable/quantization-support.html#module-torch.ao.nn.intrinsic.quantized",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.intrinsic.quantized.dynamic",
        "api_url": "https://pytorch.org/docs/stable/quantization-support.html#module-torch.ao.nn.intrinsic.quantized.dynamic",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.intrinsic.quantized.dynamic",
        "api_url": "https://pytorch.org/docs/stable/quantization-support.html#module-torch.ao.nn.intrinsic.quantized.dynamic",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.intrinsic.quantized.dynamic.modules",
        "api_url": "https://pytorch.org/docs/stable/quantization-support.html#module-torch.ao.nn.intrinsic.quantized.dynamic.modules",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.intrinsic.quantized.dynamic.modules",
        "api_url": "https://pytorch.org/docs/stable/quantization-support.html#module-torch.ao.nn.intrinsic.quantized.dynamic.modules",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.intrinsic.quantized.dynamic.modules.linear_relu",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.nn.intrinsic.quantized.dynamic.modules.linear_relu",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.intrinsic.quantized.dynamic.modules.linear_relu",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.nn.intrinsic.quantized.dynamic.modules.linear_relu",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.intrinsic.quantized.modules",
        "api_url": "https://pytorch.org/docs/stable/quantization-support.html#module-torch.ao.nn.intrinsic.quantized.modules",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.intrinsic.quantized.modules",
        "api_url": "https://pytorch.org/docs/stable/quantization-support.html#module-torch.ao.nn.intrinsic.quantized.modules",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.intrinsic.quantized.modules.bn_relu",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.nn.intrinsic.quantized.modules.bn_relu",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.intrinsic.quantized.modules.bn_relu",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.nn.intrinsic.quantized.modules.bn_relu",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.intrinsic.quantized.modules.conv_add",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.nn.intrinsic.quantized.modules.conv_add",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.intrinsic.quantized.modules.conv_add",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.nn.intrinsic.quantized.modules.conv_add",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.intrinsic.quantized.modules.conv_relu",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.nn.intrinsic.quantized.modules.conv_relu",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.intrinsic.quantized.modules.conv_relu",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.nn.intrinsic.quantized.modules.conv_relu",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.intrinsic.quantized.modules.linear_relu",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.nn.intrinsic.quantized.modules.linear_relu",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.intrinsic.quantized.modules.linear_relu",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.nn.intrinsic.quantized.modules.linear_relu",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.qat",
        "api_url": "https://pytorch.org/docs/stable/quantization-support.html#module-torch.ao.nn.qat",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.qat",
        "api_url": "https://pytorch.org/docs/stable/quantization-support.html#module-torch.ao.nn.qat",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.qat.dynamic",
        "api_url": "https://pytorch.org/docs/stable/quantization-support.html#module-torch.ao.nn.qat.dynamic",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.qat.dynamic",
        "api_url": "https://pytorch.org/docs/stable/quantization-support.html#module-torch.ao.nn.qat.dynamic",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.qat.dynamic.modules",
        "api_url": "https://pytorch.org/docs/stable/quantization-support.html#module-torch.ao.nn.qat.dynamic.modules",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.qat.dynamic.modules",
        "api_url": "https://pytorch.org/docs/stable/quantization-support.html#module-torch.ao.nn.qat.dynamic.modules",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.qat.dynamic.modules.linear",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.nn.qat.dynamic.modules.linear",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.qat.dynamic.modules.linear",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.nn.qat.dynamic.modules.linear",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.qat.modules",
        "api_url": "https://pytorch.org/docs/stable/quantization-support.html#module-torch.ao.nn.qat.modules",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.qat.modules",
        "api_url": "https://pytorch.org/docs/stable/quantization-support.html#module-torch.ao.nn.qat.modules",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.qat.modules.conv",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.nn.qat.modules.conv",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.qat.modules.conv",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.nn.qat.modules.conv",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.qat.modules.embedding_ops",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.nn.qat.modules.embedding_ops",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.qat.modules.embedding_ops",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.nn.qat.modules.embedding_ops",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.qat.modules.linear",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.nn.qat.modules.linear",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.qat.modules.linear",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.nn.qat.modules.linear",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.quantizable",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.nn.quantizable",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.quantizable",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.nn.quantizable",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.quantizable.modules",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.nn.quantizable.modules",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.quantizable.modules",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.nn.quantizable.modules",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.quantizable.modules.activation",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.nn.quantizable.modules.activation",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.quantizable.modules.activation",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.nn.quantizable.modules.activation",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.quantizable.modules.rnn",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.nn.quantizable.modules.rnn",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.quantizable.modules.rnn",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.nn.quantizable.modules.rnn",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.quantized",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.nn.quantized",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.quantized",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.nn.quantized",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.quantized.dynamic",
        "api_url": "https://pytorch.org/docs/stable/quantization-support.html#module-torch.ao.nn.quantized.dynamic",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.quantized.dynamic",
        "api_url": "https://pytorch.org/docs/stable/quantization-support.html#module-torch.ao.nn.quantized.dynamic",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.quantized.dynamic.modules",
        "api_url": "https://pytorch.org/docs/stable/quantization-support.html#module-torch.ao.nn.quantized.dynamic.modules",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.quantized.dynamic.modules",
        "api_url": "https://pytorch.org/docs/stable/quantization-support.html#module-torch.ao.nn.quantized.dynamic.modules",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.quantized.dynamic.modules.conv",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.nn.quantized.dynamic.modules.conv",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.quantized.dynamic.modules.conv",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.nn.quantized.dynamic.modules.conv",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.quantized.dynamic.modules.linear",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.nn.quantized.dynamic.modules.linear",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.quantized.dynamic.modules.linear",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.nn.quantized.dynamic.modules.linear",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.quantized.dynamic.modules.rnn",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.nn.quantized.dynamic.modules.rnn",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.quantized.dynamic.modules.rnn",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.nn.quantized.dynamic.modules.rnn",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.quantized.functional",
        "api_url": "https://pytorch.org/docs/stable/quantization-support.html#module-torch.ao.nn.quantized.functional",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.quantized.functional",
        "api_url": "https://pytorch.org/docs/stable/quantization-support.html#module-torch.ao.nn.quantized.functional",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.quantized.modules",
        "api_url": "https://pytorch.org/docs/stable/quantization-support.html#module-torch.ao.nn.quantized.modules",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.quantized.modules",
        "api_url": "https://pytorch.org/docs/stable/quantization-support.html#module-torch.ao.nn.quantized.modules",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.quantized.modules.activation",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.nn.quantized.modules.activation",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.quantized.modules.activation",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.nn.quantized.modules.activation",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.quantized.modules.batchnorm",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.nn.quantized.modules.batchnorm",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.quantized.modules.batchnorm",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.nn.quantized.modules.batchnorm",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.quantized.modules.conv",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.nn.quantized.modules.conv",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.quantized.modules.conv",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.nn.quantized.modules.conv",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.quantized.modules.dropout",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.nn.quantized.modules.dropout",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.quantized.modules.dropout",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.nn.quantized.modules.dropout",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.quantized.modules.embedding_ops",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.nn.quantized.modules.embedding_ops",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.quantized.modules.embedding_ops",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.nn.quantized.modules.embedding_ops",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.quantized.modules.functional_modules",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.nn.quantized.modules.functional_modules",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.quantized.modules.functional_modules",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.nn.quantized.modules.functional_modules",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.quantized.modules.linear",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.nn.quantized.modules.linear",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.quantized.modules.linear",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.nn.quantized.modules.linear",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.quantized.modules.normalization",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.nn.quantized.modules.normalization",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.quantized.modules.normalization",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.nn.quantized.modules.normalization",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.quantized.modules.rnn",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.nn.quantized.modules.rnn",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.quantized.modules.rnn",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.nn.quantized.modules.rnn",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.quantized.modules.utils",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.nn.quantized.modules.utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.quantized.modules.utils",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.nn.quantized.modules.utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.quantized.reference",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.nn.quantized.reference",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.quantized.reference",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.nn.quantized.reference",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.quantized.reference.modules",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.nn.quantized.reference.modules",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.quantized.reference.modules",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.nn.quantized.reference.modules",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.quantized.reference.modules.conv",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.nn.quantized.reference.modules.conv",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.quantized.reference.modules.conv",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.nn.quantized.reference.modules.conv",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.quantized.reference.modules.linear",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.nn.quantized.reference.modules.linear",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.quantized.reference.modules.linear",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.nn.quantized.reference.modules.linear",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.quantized.reference.modules.rnn",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.nn.quantized.reference.modules.rnn",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.quantized.reference.modules.rnn",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.nn.quantized.reference.modules.rnn",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.quantized.reference.modules.sparse",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.nn.quantized.reference.modules.sparse",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.quantized.reference.modules.sparse",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.nn.quantized.reference.modules.sparse",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.quantized.reference.modules.utils",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.nn.quantized.reference.modules.utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.quantized.reference.modules.utils",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.nn.quantized.reference.modules.utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.sparse",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.nn.sparse",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.sparse",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.nn.sparse",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.sparse.quantized",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.nn.sparse.quantized",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.sparse.quantized",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.nn.sparse.quantized",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.sparse.quantized.dynamic",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.nn.sparse.quantized.dynamic",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.sparse.quantized.dynamic",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.nn.sparse.quantized.dynamic",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.sparse.quantized.dynamic.linear",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.nn.sparse.quantized.dynamic.linear",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.sparse.quantized.dynamic.linear",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.nn.sparse.quantized.dynamic.linear",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.sparse.quantized.linear",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.nn.sparse.quantized.linear",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.sparse.quantized.linear",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.nn.sparse.quantized.linear",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.sparse.quantized.utils",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.nn.sparse.quantized.utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.sparse.quantized.utils",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.nn.sparse.quantized.utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.ns",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.ns",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.ns",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.ns",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.ns._numeric_suite",
        "api_url": "https://pytorch.org/docs/stable/torch.ao.ns._numeric_suite.html#module-torch.ao.ns._numeric_suite",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.ns._numeric_suite",
        "api_url": "https://pytorch.org/docs/stable/torch.ao.ns._numeric_suite.html#module-torch.ao.ns._numeric_suite",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.ns._numeric_suite_fx",
        "api_url": "https://pytorch.org/docs/stable/torch.ao.ns._numeric_suite_fx.html#module-torch.ao.ns._numeric_suite_fx",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.ns._numeric_suite_fx",
        "api_url": "https://pytorch.org/docs/stable/torch.ao.ns._numeric_suite_fx.html#module-torch.ao.ns._numeric_suite_fx",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.ns.fx",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.ns.fx",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.ns.fx",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.ns.fx",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.ns.fx.graph_matcher",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.ns.fx.graph_matcher",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.ns.fx.graph_matcher",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.ns.fx.graph_matcher",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.ns.fx.graph_passes",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.ns.fx.graph_passes",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.ns.fx.graph_passes",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.ns.fx.graph_passes",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.ns.fx.mappings",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.ns.fx.mappings",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.ns.fx.mappings",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.ns.fx.mappings",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.ns.fx.n_shadows_utils",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.ns.fx.n_shadows_utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.ns.fx.n_shadows_utils",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.ns.fx.n_shadows_utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.ns.fx.ns_types",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.ns.fx.ns_types",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.ns.fx.ns_types",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.ns.fx.ns_types",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.ns.fx.pattern_utils",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.ns.fx.pattern_utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.ns.fx.pattern_utils",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.ns.fx.pattern_utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.ns.fx.qconfig_multi_mapping",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.ns.fx.qconfig_multi_mapping",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.ns.fx.qconfig_multi_mapping",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.ns.fx.qconfig_multi_mapping",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.ns.fx.utils",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.ns.fx.utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.ns.fx.utils",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.ns.fx.utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.ns.fx.weight_utils",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.ns.fx.weight_utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.ns.fx.weight_utils",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.ns.fx.weight_utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.pruning",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.pruning",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.pruning",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.pruning",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.pruning.scheduler",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.pruning.scheduler",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.pruning.scheduler",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.pruning.scheduler",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.pruning.scheduler.base_scheduler",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.pruning.scheduler.base_scheduler",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.pruning.scheduler.base_scheduler",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.pruning.scheduler.base_scheduler",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.pruning.scheduler.cubic_scheduler",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.pruning.scheduler.cubic_scheduler",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.pruning.scheduler.cubic_scheduler",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.pruning.scheduler.cubic_scheduler",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.pruning.scheduler.lambda_scheduler",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.pruning.scheduler.lambda_scheduler",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.pruning.scheduler.lambda_scheduler",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.pruning.scheduler.lambda_scheduler",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.pruning.sparsifier",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.pruning.sparsifier",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.pruning.sparsifier",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.pruning.sparsifier",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.pruning.sparsifier.base_sparsifier",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.pruning.sparsifier.base_sparsifier",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.pruning.sparsifier.base_sparsifier",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.pruning.sparsifier.base_sparsifier",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.pruning.sparsifier.nearly_diagonal_sparsifier",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.pruning.sparsifier.nearly_diagonal_sparsifier",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.pruning.sparsifier.nearly_diagonal_sparsifier",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.pruning.sparsifier.nearly_diagonal_sparsifier",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.pruning.sparsifier.utils",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.pruning.sparsifier.utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.pruning.sparsifier.utils",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.pruning.sparsifier.utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.pruning.sparsifier.weight_norm_sparsifier",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.pruning.sparsifier.weight_norm_sparsifier",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.pruning.sparsifier.weight_norm_sparsifier",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.pruning.sparsifier.weight_norm_sparsifier",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.backend_config",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.backend_config",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.backend_config",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.backend_config",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.backend_config.backend_config",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.backend_config.backend_config",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.backend_config.backend_config",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.backend_config.backend_config",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.backend_config.executorch",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.backend_config.executorch",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.backend_config.executorch",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.backend_config.executorch",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.backend_config.fbgemm",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.backend_config.fbgemm",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.backend_config.fbgemm",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.backend_config.fbgemm",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.backend_config.native",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.backend_config.native",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.backend_config.native",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.backend_config.native",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.backend_config.observation_type",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.backend_config.observation_type",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.backend_config.observation_type",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.backend_config.observation_type",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.backend_config.onednn",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.backend_config.onednn",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.backend_config.onednn",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.backend_config.onednn",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.backend_config.qnnpack",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.backend_config.qnnpack",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.backend_config.qnnpack",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.backend_config.qnnpack",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.backend_config.tensorrt",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.backend_config.tensorrt",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.backend_config.tensorrt",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.backend_config.tensorrt",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.backend_config.utils",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.backend_config.utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.backend_config.utils",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.backend_config.utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.backend_config.x86",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.backend_config.x86",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.backend_config.x86",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.backend_config.x86",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.fake_quantize",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.fake_quantize",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.fake_quantize",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.fake_quantize",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.fuse_modules",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.fuse_modules",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.fuse_modules",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.fuse_modules",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.fuser_method_mappings",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.fuser_method_mappings",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.fuser_method_mappings",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.fuser_method_mappings",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.fx",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.fx",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.fx",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.fx",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.fx.convert",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.fx.convert",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.fx.convert",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.fx.convert",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.fx.custom_config",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.fx.custom_config",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.fx.custom_config",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.fx.custom_config",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.fx.fuse",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.fx.fuse",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.fx.fuse",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.fx.fuse",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.fx.fuse_handler",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.fx.fuse_handler",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.fx.fuse_handler",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.fx.fuse_handler",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.fx.graph_module",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.fx.graph_module",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.fx.graph_module",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.fx.graph_module",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.fx.lower_to_fbgemm",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.fx.lower_to_fbgemm",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.fx.lower_to_fbgemm",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.fx.lower_to_fbgemm",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.fx.lower_to_qnnpack",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.fx.lower_to_qnnpack",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.fx.lower_to_qnnpack",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.fx.lower_to_qnnpack",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.fx.lstm_utils",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.fx.lstm_utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.fx.lstm_utils",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.fx.lstm_utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.fx.match_utils",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.fx.match_utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.fx.match_utils",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.fx.match_utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.fx.pattern_utils",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.fx.pattern_utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.fx.pattern_utils",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.fx.pattern_utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.fx.prepare",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.fx.prepare",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.fx.prepare",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.fx.prepare",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.fx.qconfig_mapping_utils",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.fx.qconfig_mapping_utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.fx.qconfig_mapping_utils",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.fx.qconfig_mapping_utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.fx.quantize_handler",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.fx.quantize_handler",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.fx.quantize_handler",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.fx.quantize_handler",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.fx.tracer",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.fx.tracer",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.fx.tracer",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.fx.tracer",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.fx.utils",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.fx.utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.fx.utils",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.fx.utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.observer",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.observer",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.observer",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.observer",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.pt2e",
        "api_url": "https://pytorch.org/docs/stable/quantization-support.html#module-torch.ao.quantization.pt2e",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.pt2e",
        "api_url": "https://pytorch.org/docs/stable/quantization-support.html#module-torch.ao.quantization.pt2e",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.pt2e.duplicate_dq_pass",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.pt2e.duplicate_dq_pass",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.pt2e.duplicate_dq_pass",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.pt2e.duplicate_dq_pass",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.pt2e.export_utils",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.pt2e.export_utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.pt2e.export_utils",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.pt2e.export_utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.pt2e.generate_numeric_debug_handle",
        "api_url": "https://pytorch.org/docs/stable/quantization-support.html#module-torch.ao.quantization.pt2e.generate_numeric_debug_handle",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.pt2e.generate_numeric_debug_handle",
        "api_url": "https://pytorch.org/docs/stable/quantization-support.html#module-torch.ao.quantization.pt2e.generate_numeric_debug_handle",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.pt2e.graph_utils",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.pt2e.graph_utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.pt2e.graph_utils",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.pt2e.graph_utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.pt2e.port_metadata_pass",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.pt2e.port_metadata_pass",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.pt2e.port_metadata_pass",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.pt2e.port_metadata_pass",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.pt2e.prepare",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.pt2e.prepare",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.pt2e.prepare",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.pt2e.prepare",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.pt2e.qat_utils",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.pt2e.qat_utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.pt2e.qat_utils",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.pt2e.qat_utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.pt2e.representation",
        "api_url": "https://pytorch.org/docs/stable/quantization-support.html#module-torch.ao.quantization.pt2e.representation",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.pt2e.representation",
        "api_url": "https://pytorch.org/docs/stable/quantization-support.html#module-torch.ao.quantization.pt2e.representation",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.pt2e.representation.rewrite",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.pt2e.representation.rewrite",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.pt2e.representation.rewrite",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.pt2e.representation.rewrite",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.pt2e.utils",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.pt2e.utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.pt2e.utils",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.pt2e.utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.qconfig",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.qconfig",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.qconfig",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.qconfig",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.qconfig_mapping",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.qconfig_mapping",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.qconfig_mapping",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.qconfig_mapping",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.quant_type",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.quant_type",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.quant_type",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.quant_type",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.quantization_mappings",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.quantization_mappings",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.quantization_mappings",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.quantization_mappings",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.quantize",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.quantize",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.quantize",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.quantize",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.quantize_fx",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.quantize_fx",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.quantize_fx",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.quantize_fx",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.quantize_jit",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.quantize_jit",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.quantize_jit",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.quantize_jit",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.quantize_pt2e",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.quantize_pt2e",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.quantize_pt2e",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.quantize_pt2e",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.quantizer",
        "api_url": "https://pytorch.org/docs/stable/quantization-support.html#module-torch.ao.quantization.quantizer",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.quantizer",
        "api_url": "https://pytorch.org/docs/stable/quantization-support.html#module-torch.ao.quantization.quantizer",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.quantizer.composable_quantizer",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.quantizer.composable_quantizer",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.quantizer.composable_quantizer",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.quantizer.composable_quantizer",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.quantizer.embedding_quantizer",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.quantizer.embedding_quantizer",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.quantizer.embedding_quantizer",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.quantizer.embedding_quantizer",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.quantizer.quantizer",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.quantizer.quantizer",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.quantizer.quantizer",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.quantizer.quantizer",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.quantizer.utils",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.quantizer.utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.quantizer.utils",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.quantizer.utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.quantizer.x86_inductor_quantizer",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.quantizer.x86_inductor_quantizer",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.quantizer.x86_inductor_quantizer",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.quantizer.x86_inductor_quantizer",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.quantizer.xnnpack_quantizer",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.quantizer.xnnpack_quantizer",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.quantizer.xnnpack_quantizer",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.quantizer.xnnpack_quantizer",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.quantizer.xnnpack_quantizer_utils",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.quantizer.xnnpack_quantizer_utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.quantizer.xnnpack_quantizer_utils",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.quantizer.xnnpack_quantizer_utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.stubs",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.stubs",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.stubs",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.stubs",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.utils",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.utils",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.ao.quantization.utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.autograd",
        "api_url": "https://pytorch.org/docs/stable/autograd.html#module-torch.autograd",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.autograd",
        "api_url": "https://pytorch.org/docs/stable/autograd.html#module-torch.autograd",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.autograd.anomaly_mode",
        "api_url": "https://pytorch.org/docs/stable/autograd.html#module-torch.autograd.anomaly_mode",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.autograd.anomaly_mode",
        "api_url": "https://pytorch.org/docs/stable/autograd.html#module-torch.autograd.anomaly_mode",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.autograd.forward_ad",
        "api_url": "https://pytorch.org/docs/stable/autograd.html#module-torch.autograd.forward_ad",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.autograd.forward_ad",
        "api_url": "https://pytorch.org/docs/stable/autograd.html#module-torch.autograd.forward_ad",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.autograd.function",
        "api_url": "https://pytorch.org/docs/stable/autograd.html#module-torch.autograd.function",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.autograd.function",
        "api_url": "https://pytorch.org/docs/stable/autograd.html#module-torch.autograd.function",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.autograd.functional",
        "api_url": "https://pytorch.org/docs/stable/autograd.html#module-torch.autograd.functional",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.autograd.functional",
        "api_url": "https://pytorch.org/docs/stable/autograd.html#module-torch.autograd.functional",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.autograd.grad_mode",
        "api_url": "https://pytorch.org/docs/stable/autograd.html#module-torch.autograd.grad_mode",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.autograd.grad_mode",
        "api_url": "https://pytorch.org/docs/stable/autograd.html#module-torch.autograd.grad_mode",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.autograd.gradcheck",
        "api_url": "https://pytorch.org/docs/stable/autograd.html#module-torch.autograd.gradcheck",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.autograd.gradcheck",
        "api_url": "https://pytorch.org/docs/stable/autograd.html#module-torch.autograd.gradcheck",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.autograd.graph",
        "api_url": "https://pytorch.org/docs/stable/autograd.html#module-torch.autograd.graph",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.autograd.graph",
        "api_url": "https://pytorch.org/docs/stable/autograd.html#module-torch.autograd.graph",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.autograd.profiler",
        "api_url": "https://pytorch.org/docs/stable/autograd.html#module-torch.autograd.profiler",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.autograd.profiler",
        "api_url": "https://pytorch.org/docs/stable/autograd.html#module-torch.autograd.profiler",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.autograd.profiler_legacy",
        "api_url": "https://pytorch.org/docs/stable/autograd.html#module-torch.autograd.profiler_legacy",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.autograd.profiler_legacy",
        "api_url": "https://pytorch.org/docs/stable/autograd.html#module-torch.autograd.profiler_legacy",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.autograd.profiler_util",
        "api_url": "https://pytorch.org/docs/stable/autograd.html#module-torch.autograd.profiler_util",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.autograd.profiler_util",
        "api_url": "https://pytorch.org/docs/stable/autograd.html#module-torch.autograd.profiler_util",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.autograd.variable",
        "api_url": "https://pytorch.org/docs/stable/autograd.html#module-torch.autograd.variable",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.autograd.variable",
        "api_url": "https://pytorch.org/docs/stable/autograd.html#module-torch.autograd.variable",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.backends",
        "api_url": "https://pytorch.org/docs/stable/backends.html#module-torch.backends",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.backends",
        "api_url": "https://pytorch.org/docs/stable/backends.html#module-torch.backends",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.backends.cpu",
        "api_url": "https://pytorch.org/docs/stable/backends.html#module-torch.backends.cpu",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.backends.cpu",
        "api_url": "https://pytorch.org/docs/stable/backends.html#module-torch.backends.cpu",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.backends.cuda",
        "api_url": "https://pytorch.org/docs/stable/backends.html#module-torch.backends.cuda",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.backends.cuda",
        "api_url": "https://pytorch.org/docs/stable/backends.html#module-torch.backends.cuda",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.backends.cudnn",
        "api_url": "https://pytorch.org/docs/stable/backends.html#module-torch.backends.cudnn",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.backends.cudnn",
        "api_url": "https://pytorch.org/docs/stable/backends.html#module-torch.backends.cudnn",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.backends.cudnn.rnn",
        "api_url": "https://pytorch.org/docs/stable/backends.html#module-torch.backends.cudnn.rnn",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.backends.cudnn.rnn",
        "api_url": "https://pytorch.org/docs/stable/backends.html#module-torch.backends.cudnn.rnn",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.backends.mha",
        "api_url": "https://pytorch.org/docs/stable/backends.html#module-torch.backends.mha",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.backends.mha",
        "api_url": "https://pytorch.org/docs/stable/backends.html#module-torch.backends.mha",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.backends.mkl",
        "api_url": "https://pytorch.org/docs/stable/backends.html#module-torch.backends.mkl",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.backends.mkl",
        "api_url": "https://pytorch.org/docs/stable/backends.html#module-torch.backends.mkl",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.backends.mkldnn",
        "api_url": "https://pytorch.org/docs/stable/backends.html#module-torch.backends.mkldnn",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.backends.mkldnn",
        "api_url": "https://pytorch.org/docs/stable/backends.html#module-torch.backends.mkldnn",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.backends.mps",
        "api_url": "https://pytorch.org/docs/stable/backends.html#module-torch.backends.mps",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.backends.mps",
        "api_url": "https://pytorch.org/docs/stable/backends.html#module-torch.backends.mps",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.backends.nnpack",
        "api_url": "https://pytorch.org/docs/stable/backends.html#module-torch.backends.nnpack",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.backends.nnpack",
        "api_url": "https://pytorch.org/docs/stable/backends.html#module-torch.backends.nnpack",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.backends.openmp",
        "api_url": "https://pytorch.org/docs/stable/backends.html#module-torch.backends.openmp",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.backends.openmp",
        "api_url": "https://pytorch.org/docs/stable/backends.html#module-torch.backends.openmp",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.backends.opt_einsum",
        "api_url": "https://pytorch.org/docs/stable/backends.html#module-torch.backends.opt_einsum",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.backends.opt_einsum",
        "api_url": "https://pytorch.org/docs/stable/backends.html#module-torch.backends.opt_einsum",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.backends.quantized",
        "api_url": "https://pytorch.org/docs/stable/backends.html#module-torch.backends.quantized",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.backends.quantized",
        "api_url": "https://pytorch.org/docs/stable/backends.html#module-torch.backends.quantized",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.backends.xeon",
        "api_url": "https://pytorch.org/docs/stable/backends.html#module-torch.backends.xeon",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.backends.xeon",
        "api_url": "https://pytorch.org/docs/stable/backends.html#module-torch.backends.xeon",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.backends.xeon.run_cpu",
        "api_url": "https://pytorch.org/docs/stable/backends.html#module-torch.backends.xeon.run_cpu",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.backends.xeon.run_cpu",
        "api_url": "https://pytorch.org/docs/stable/backends.html#module-torch.backends.xeon.run_cpu",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.backends.xnnpack",
        "api_url": "https://pytorch.org/docs/stable/backends.html#module-torch.backends.xnnpack",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.backends.xnnpack",
        "api_url": "https://pytorch.org/docs/stable/backends.html#module-torch.backends.xnnpack",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.compiler",
        "api_url": "https://pytorch.org/docs/stable/torch.compiler_api.html#module-torch.compiler",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.compiler",
        "api_url": "https://pytorch.org/docs/stable/torch.compiler_api.html#module-torch.compiler",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.contrib",
        "api_url": "https://pytorch.org/docs/stable/torch.html#module-torch.contrib",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.contrib",
        "api_url": "https://pytorch.org/docs/stable/torch.html#module-torch.contrib",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cpu",
        "api_url": "https://pytorch.org/docs/stable/cpu.html#module-torch.cpu",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cpu",
        "api_url": "https://pytorch.org/docs/stable/cpu.html#module-torch.cpu",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cpu.amp",
        "api_url": "https://pytorch.org/docs/stable/amp.html#module-torch.cpu.amp",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cpu.amp",
        "api_url": "https://pytorch.org/docs/stable/amp.html#module-torch.cpu.amp",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cpu.amp.autocast_mode",
        "api_url": "https://pytorch.org/docs/stable/amp.html#module-torch.cpu.amp.autocast_mode",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cpu.amp.autocast_mode",
        "api_url": "https://pytorch.org/docs/stable/amp.html#module-torch.cpu.amp.autocast_mode",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cpu.amp.grad_scaler",
        "api_url": "https://pytorch.org/docs/stable/amp.html#module-torch.cpu.amp.grad_scaler",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cpu.amp.grad_scaler",
        "api_url": "https://pytorch.org/docs/stable/amp.html#module-torch.cpu.amp.grad_scaler",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda",
        "api_url": "https://pytorch.org/docs/stable/cuda.html#module-torch.cuda",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda",
        "api_url": "https://pytorch.org/docs/stable/cuda.html#module-torch.cuda",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda._sanitizer",
        "api_url": "https://pytorch.org/docs/stable/cuda._sanitizer.html#module-torch.cuda._sanitizer",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda._sanitizer",
        "api_url": "https://pytorch.org/docs/stable/cuda._sanitizer.html#module-torch.cuda._sanitizer",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda.amp",
        "api_url": "https://pytorch.org/docs/stable/amp.html#module-torch.cuda.amp",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda.amp",
        "api_url": "https://pytorch.org/docs/stable/amp.html#module-torch.cuda.amp",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda.amp.autocast_mode",
        "api_url": "https://pytorch.org/docs/stable/amp.html#module-torch.cuda.amp.autocast_mode",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda.amp.autocast_mode",
        "api_url": "https://pytorch.org/docs/stable/amp.html#module-torch.cuda.amp.autocast_mode",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda.amp.common",
        "api_url": "https://pytorch.org/docs/stable/amp.html#module-torch.cuda.amp.common",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda.amp.common",
        "api_url": "https://pytorch.org/docs/stable/amp.html#module-torch.cuda.amp.common",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda.amp.grad_scaler",
        "api_url": "https://pytorch.org/docs/stable/amp.html#module-torch.cuda.amp.grad_scaler",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda.amp.grad_scaler",
        "api_url": "https://pytorch.org/docs/stable/amp.html#module-torch.cuda.amp.grad_scaler",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda.comm",
        "api_url": "https://pytorch.org/docs/stable/cuda.html#module-torch.cuda.comm",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda.comm",
        "api_url": "https://pytorch.org/docs/stable/cuda.html#module-torch.cuda.comm",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda.error",
        "api_url": "https://pytorch.org/docs/stable/cuda.html#module-torch.cuda.error",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda.error",
        "api_url": "https://pytorch.org/docs/stable/cuda.html#module-torch.cuda.error",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda.graphs",
        "api_url": "https://pytorch.org/docs/stable/cuda.html#module-torch.cuda.graphs",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda.graphs",
        "api_url": "https://pytorch.org/docs/stable/cuda.html#module-torch.cuda.graphs",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda.jiterator",
        "api_url": "https://pytorch.org/docs/stable/cuda.html#module-torch.cuda.jiterator",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda.jiterator",
        "api_url": "https://pytorch.org/docs/stable/cuda.html#module-torch.cuda.jiterator",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda.memory",
        "api_url": "https://pytorch.org/docs/stable/cuda.html#module-torch.cuda.memory",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda.memory",
        "api_url": "https://pytorch.org/docs/stable/cuda.html#module-torch.cuda.memory",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda.nccl",
        "api_url": "https://pytorch.org/docs/stable/cuda.html#module-torch.cuda.nccl",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda.nccl",
        "api_url": "https://pytorch.org/docs/stable/cuda.html#module-torch.cuda.nccl",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda.nvtx",
        "api_url": "https://pytorch.org/docs/stable/cuda.html#module-torch.cuda.nvtx",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda.nvtx",
        "api_url": "https://pytorch.org/docs/stable/cuda.html#module-torch.cuda.nvtx",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda.profiler",
        "api_url": "https://pytorch.org/docs/stable/cuda.html#module-torch.cuda.profiler",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda.profiler",
        "api_url": "https://pytorch.org/docs/stable/cuda.html#module-torch.cuda.profiler",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda.random",
        "api_url": "https://pytorch.org/docs/stable/cuda.html#module-torch.cuda.random",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda.random",
        "api_url": "https://pytorch.org/docs/stable/cuda.html#module-torch.cuda.random",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda.sparse",
        "api_url": "https://pytorch.org/docs/stable/cuda.html#module-torch.cuda.sparse",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda.sparse",
        "api_url": "https://pytorch.org/docs/stable/cuda.html#module-torch.cuda.sparse",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda.streams",
        "api_url": "https://pytorch.org/docs/stable/cuda.html#module-torch.cuda.streams",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda.streams",
        "api_url": "https://pytorch.org/docs/stable/cuda.html#module-torch.cuda.streams",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.algorithms",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.algorithms",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.algorithms",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.algorithms",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.algorithms.ddp_comm_hooks",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.algorithms.ddp_comm_hooks",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.algorithms.ddp_comm_hooks",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.algorithms.ddp_comm_hooks",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.algorithms.ddp_comm_hooks.ddp_zero_hook",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.algorithms.ddp_comm_hooks.ddp_zero_hook",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.algorithms.ddp_comm_hooks.ddp_zero_hook",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.algorithms.ddp_comm_hooks.ddp_zero_hook",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.algorithms.ddp_comm_hooks.debugging_hooks",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.algorithms.ddp_comm_hooks.debugging_hooks",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.algorithms.ddp_comm_hooks.debugging_hooks",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.algorithms.ddp_comm_hooks.debugging_hooks",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.algorithms.ddp_comm_hooks.default_hooks",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.algorithms.ddp_comm_hooks.default_hooks",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.algorithms.ddp_comm_hooks.default_hooks",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.algorithms.ddp_comm_hooks.default_hooks",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.algorithms.ddp_comm_hooks.mixed_precision_hooks",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.algorithms.ddp_comm_hooks.mixed_precision_hooks",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.algorithms.ddp_comm_hooks.mixed_precision_hooks",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.algorithms.ddp_comm_hooks.mixed_precision_hooks",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.algorithms.ddp_comm_hooks.optimizer_overlap_hooks",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.algorithms.ddp_comm_hooks.optimizer_overlap_hooks",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.algorithms.ddp_comm_hooks.optimizer_overlap_hooks",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.algorithms.ddp_comm_hooks.optimizer_overlap_hooks",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.algorithms.ddp_comm_hooks.post_localSGD_hook",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.algorithms.ddp_comm_hooks.post_localSGD_hook",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.algorithms.ddp_comm_hooks.post_localSGD_hook",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.algorithms.ddp_comm_hooks.post_localSGD_hook",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.algorithms.ddp_comm_hooks.quantization_hooks",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.algorithms.ddp_comm_hooks.quantization_hooks",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.algorithms.ddp_comm_hooks.quantization_hooks",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.algorithms.ddp_comm_hooks.quantization_hooks",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.algorithms.join",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.algorithms.join",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.algorithms.join",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.algorithms.join",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.algorithms.model_averaging",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.algorithms.model_averaging",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.algorithms.model_averaging",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.algorithms.model_averaging",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.algorithms.model_averaging.averagers",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.algorithms.model_averaging.averagers",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.algorithms.model_averaging.averagers",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.algorithms.model_averaging.averagers",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.algorithms.model_averaging.hierarchical_model_averager",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.algorithms.model_averaging.hierarchical_model_averager",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.algorithms.model_averaging.hierarchical_model_averager",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.algorithms.model_averaging.hierarchical_model_averager",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.algorithms.model_averaging.utils",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.algorithms.model_averaging.utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.algorithms.model_averaging.utils",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.algorithms.model_averaging.utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.argparse_util",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.argparse_util",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.argparse_util",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.argparse_util",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.autograd",
        "api_url": "https://pytorch.org/docs/stable/rpc.html#module-torch.distributed.autograd",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.autograd",
        "api_url": "https://pytorch.org/docs/stable/rpc.html#module-torch.distributed.autograd",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.c10d_logger",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.c10d_logger",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.c10d_logger",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.c10d_logger",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.checkpoint",
        "api_url": "https://pytorch.org/docs/stable/distributed.checkpoint.html#module-torch.distributed.checkpoint",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.checkpoint",
        "api_url": "https://pytorch.org/docs/stable/distributed.checkpoint.html#module-torch.distributed.checkpoint",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.checkpoint.api",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.checkpoint.api",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.checkpoint.api",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.checkpoint.api",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.checkpoint.default_planner",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.checkpoint.default_planner",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.checkpoint.default_planner",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.checkpoint.default_planner",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.checkpoint.filesystem",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.checkpoint.filesystem",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.checkpoint.filesystem",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.checkpoint.filesystem",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.checkpoint.format_utils",
        "api_url": "https://pytorch.org/docs/stable/distributed.checkpoint.html#module-torch.distributed.checkpoint.format_utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.checkpoint.format_utils",
        "api_url": "https://pytorch.org/docs/stable/distributed.checkpoint.html#module-torch.distributed.checkpoint.format_utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.checkpoint.fsspec",
        "api_url": "https://pytorch.org/docs/stable/distributed.checkpoint.html#module-torch.distributed.checkpoint.fsspec",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.checkpoint.fsspec",
        "api_url": "https://pytorch.org/docs/stable/distributed.checkpoint.html#module-torch.distributed.checkpoint.fsspec",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.checkpoint.metadata",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.checkpoint.metadata",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.checkpoint.metadata",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.checkpoint.metadata",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.checkpoint.optimizer",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.checkpoint.optimizer",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.checkpoint.optimizer",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.checkpoint.optimizer",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.checkpoint.planner",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.checkpoint.planner",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.checkpoint.planner",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.checkpoint.planner",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.checkpoint.planner_helpers",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.checkpoint.planner_helpers",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.checkpoint.planner_helpers",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.checkpoint.planner_helpers",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.checkpoint.resharding",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.checkpoint.resharding",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.checkpoint.resharding",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.checkpoint.resharding",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.checkpoint.state_dict",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.checkpoint.state_dict",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.checkpoint.state_dict",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.checkpoint.state_dict",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.checkpoint.state_dict_loader",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.checkpoint.state_dict_loader",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.checkpoint.state_dict_loader",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.checkpoint.state_dict_loader",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.checkpoint.state_dict_saver",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.checkpoint.state_dict_saver",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.checkpoint.state_dict_saver",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.checkpoint.state_dict_saver",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.checkpoint.stateful",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.checkpoint.stateful",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.checkpoint.stateful",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.checkpoint.stateful",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.checkpoint.storage",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.checkpoint.storage",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.checkpoint.storage",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.checkpoint.storage",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.checkpoint.utils",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.checkpoint.utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.checkpoint.utils",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.checkpoint.utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.collective_utils",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.collective_utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.collective_utils",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.collective_utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.constants",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.constants",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.constants",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.constants",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.device_mesh",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.device_mesh",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.device_mesh",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.device_mesh",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.distributed_c10d",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.distributed_c10d",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.distributed_c10d",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.distributed_c10d",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.elastic",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.elastic",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.agent",
        "api_url": "https://pytorch.org/docs/stable/elastic/agent.html#module-torch.distributed.elastic.agent",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.agent",
        "api_url": "https://pytorch.org/docs/stable/elastic/agent.html#module-torch.distributed.elastic.agent",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.agent.server",
        "api_url": "https://pytorch.org/docs/stable/elastic/agent.html#module-torch.distributed.elastic.agent.server",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.agent.server",
        "api_url": "https://pytorch.org/docs/stable/elastic/agent.html#module-torch.distributed.elastic.agent.server",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.agent.server.api",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.elastic.agent.server.api",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.agent.server.api",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.elastic.agent.server.api",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.agent.server.local_elastic_agent",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.elastic.agent.server.local_elastic_agent",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.agent.server.local_elastic_agent",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.elastic.agent.server.local_elastic_agent",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.events",
        "api_url": "https://pytorch.org/docs/stable/elastic/events.html#module-torch.distributed.elastic.events",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.events",
        "api_url": "https://pytorch.org/docs/stable/elastic/events.html#module-torch.distributed.elastic.events",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.events.api",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.elastic.events.api",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.events.api",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.elastic.events.api",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.events.handlers",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.elastic.events.handlers",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.events.handlers",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.elastic.events.handlers",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.metrics",
        "api_url": "https://pytorch.org/docs/stable/elastic/metrics.html#module-torch.distributed.elastic.metrics",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.metrics",
        "api_url": "https://pytorch.org/docs/stable/elastic/metrics.html#module-torch.distributed.elastic.metrics",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.metrics.api",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.elastic.metrics.api",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.metrics.api",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.elastic.metrics.api",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.multiprocessing",
        "api_url": "https://pytorch.org/docs/stable/elastic/multiprocessing.html#module-torch.distributed.elastic.multiprocessing",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.multiprocessing",
        "api_url": "https://pytorch.org/docs/stable/elastic/multiprocessing.html#module-torch.distributed.elastic.multiprocessing",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.multiprocessing.api",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.elastic.multiprocessing.api",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.multiprocessing.api",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.elastic.multiprocessing.api",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.multiprocessing.errors",
        "api_url": "https://pytorch.org/docs/stable/elastic/errors.html#module-torch.distributed.elastic.multiprocessing.errors",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.multiprocessing.errors",
        "api_url": "https://pytorch.org/docs/stable/elastic/errors.html#module-torch.distributed.elastic.multiprocessing.errors",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.multiprocessing.errors.error_handler",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.elastic.multiprocessing.errors.error_handler",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.multiprocessing.errors.error_handler",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.elastic.multiprocessing.errors.error_handler",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.multiprocessing.errors.handlers",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.elastic.multiprocessing.errors.handlers",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.multiprocessing.errors.handlers",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.elastic.multiprocessing.errors.handlers",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.multiprocessing.redirects",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.elastic.multiprocessing.redirects",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.multiprocessing.redirects",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.elastic.multiprocessing.redirects",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.multiprocessing.subprocess_handler",
        "api_url": "https://pytorch.org/docs/stable/elastic/subprocess_handler.html#module-torch.distributed.elastic.multiprocessing.subprocess_handler",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.multiprocessing.subprocess_handler",
        "api_url": "https://pytorch.org/docs/stable/elastic/subprocess_handler.html#module-torch.distributed.elastic.multiprocessing.subprocess_handler",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.multiprocessing.subprocess_handler.handlers",
        "api_url": "https://pytorch.org/docs/stable/elastic/subprocess_handler.html#module-torch.distributed.elastic.multiprocessing.subprocess_handler.handlers",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.multiprocessing.subprocess_handler.handlers",
        "api_url": "https://pytorch.org/docs/stable/elastic/subprocess_handler.html#module-torch.distributed.elastic.multiprocessing.subprocess_handler.handlers",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.multiprocessing.subprocess_handler.subprocess_handler",
        "api_url": "https://pytorch.org/docs/stable/elastic/subprocess_handler.html#module-torch.distributed.elastic.multiprocessing.subprocess_handler.subprocess_handler",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.multiprocessing.subprocess_handler.subprocess_handler",
        "api_url": "https://pytorch.org/docs/stable/elastic/subprocess_handler.html#module-torch.distributed.elastic.multiprocessing.subprocess_handler.subprocess_handler",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.multiprocessing.tail_log",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.elastic.multiprocessing.tail_log",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.multiprocessing.tail_log",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.elastic.multiprocessing.tail_log",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.rendezvous",
        "api_url": "https://pytorch.org/docs/stable/elastic/rendezvous.html#module-torch.distributed.elastic.rendezvous",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.rendezvous",
        "api_url": "https://pytorch.org/docs/stable/elastic/rendezvous.html#module-torch.distributed.elastic.rendezvous",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.rendezvous.api",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.elastic.rendezvous.api",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.rendezvous.api",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.elastic.rendezvous.api",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.rendezvous.c10d_rendezvous_backend",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.elastic.rendezvous.c10d_rendezvous_backend",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.rendezvous.c10d_rendezvous_backend",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.elastic.rendezvous.c10d_rendezvous_backend",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.rendezvous.dynamic_rendezvous",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.elastic.rendezvous.dynamic_rendezvous",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.rendezvous.dynamic_rendezvous",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.elastic.rendezvous.dynamic_rendezvous",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.rendezvous.etcd_rendezvous",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.elastic.rendezvous.etcd_rendezvous",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.rendezvous.etcd_rendezvous",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.elastic.rendezvous.etcd_rendezvous",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.rendezvous.etcd_rendezvous_backend",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.elastic.rendezvous.etcd_rendezvous_backend",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.rendezvous.etcd_rendezvous_backend",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.elastic.rendezvous.etcd_rendezvous_backend",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.rendezvous.etcd_server",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.elastic.rendezvous.etcd_server",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.rendezvous.etcd_server",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.elastic.rendezvous.etcd_server",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.rendezvous.etcd_store",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.elastic.rendezvous.etcd_store",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.rendezvous.etcd_store",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.elastic.rendezvous.etcd_store",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.rendezvous.registry",
        "api_url": "https://pytorch.org/docs/stable/elastic/rendezvous.html#module-torch.distributed.elastic.rendezvous.registry",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.rendezvous.registry",
        "api_url": "https://pytorch.org/docs/stable/elastic/rendezvous.html#module-torch.distributed.elastic.rendezvous.registry",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.rendezvous.static_tcp_rendezvous",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.elastic.rendezvous.static_tcp_rendezvous",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.rendezvous.static_tcp_rendezvous",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.elastic.rendezvous.static_tcp_rendezvous",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.rendezvous.utils",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.elastic.rendezvous.utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.rendezvous.utils",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.elastic.rendezvous.utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.timer",
        "api_url": "https://pytorch.org/docs/stable/elastic/timer.html#module-torch.distributed.elastic.timer",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.timer",
        "api_url": "https://pytorch.org/docs/stable/elastic/timer.html#module-torch.distributed.elastic.timer",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.timer.api",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.elastic.timer.api",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.timer.api",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.elastic.timer.api",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.timer.file_based_local_timer",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.elastic.timer.file_based_local_timer",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.timer.file_based_local_timer",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.elastic.timer.file_based_local_timer",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.timer.local_timer",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.elastic.timer.local_timer",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.timer.local_timer",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.elastic.timer.local_timer",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.utils",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.elastic.utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.utils",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.elastic.utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.utils.api",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.elastic.utils.api",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.utils.api",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.elastic.utils.api",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.utils.data",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.elastic.utils.data",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.utils.data",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.elastic.utils.data",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.utils.data.cycling_iterator",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.elastic.utils.data.cycling_iterator",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.utils.data.cycling_iterator",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.elastic.utils.data.cycling_iterator",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.utils.data.elastic_distributed_sampler",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.elastic.utils.data.elastic_distributed_sampler",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.utils.data.elastic_distributed_sampler",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.elastic.utils.data.elastic_distributed_sampler",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.utils.distributed",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.elastic.utils.distributed",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.utils.distributed",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.elastic.utils.distributed",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.utils.log_level",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.elastic.utils.log_level",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.utils.log_level",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.elastic.utils.log_level",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.utils.logging",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.elastic.utils.logging",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.utils.logging",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.elastic.utils.logging",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.utils.store",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.elastic.utils.store",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.utils.store",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.elastic.utils.store",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.fsdp",
        "api_url": "https://pytorch.org/docs/stable/fsdp.html#module-torch.distributed.fsdp",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.fsdp",
        "api_url": "https://pytorch.org/docs/stable/fsdp.html#module-torch.distributed.fsdp",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.fsdp.api",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.fsdp.api",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.fsdp.api",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.fsdp.api",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.fsdp.fully_sharded_data_parallel",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.fsdp.fully_sharded_data_parallel",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.fsdp.fully_sharded_data_parallel",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.fsdp.fully_sharded_data_parallel",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.fsdp.sharded_grad_scaler",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.fsdp.sharded_grad_scaler",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.fsdp.sharded_grad_scaler",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.fsdp.sharded_grad_scaler",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.fsdp.wrap",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.fsdp.wrap",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.fsdp.wrap",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.fsdp.wrap",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.launch",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.launch",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.launch",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.launch",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.launcher",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.launcher",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.launcher",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.launcher",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.launcher.api",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.launcher.api",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.launcher.api",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.launcher.api",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.logging_handlers",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.logging_handlers",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.logging_handlers",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.logging_handlers",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.nn",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.nn",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.nn",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.nn",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.nn.api",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.nn.api",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.nn.api",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.nn.api",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.nn.api.remote_module",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.nn.api.remote_module",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.nn.api.remote_module",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.nn.api.remote_module",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.nn.functional",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.nn.functional",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.nn.functional",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.nn.functional",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.nn.jit",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.nn.jit",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.nn.jit",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.nn.jit",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.nn.jit.instantiator",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.nn.jit.instantiator",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.nn.jit.instantiator",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.nn.jit.instantiator",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.nn.jit.templates",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.nn.jit.templates",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.nn.jit.templates",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.nn.jit.templates",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.nn.jit.templates.remote_module_template",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.nn.jit.templates.remote_module_template",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.nn.jit.templates.remote_module_template",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.nn.jit.templates.remote_module_template",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.optim",
        "api_url": "https://pytorch.org/docs/stable/distributed.optim.html#module-torch.distributed.optim",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.optim",
        "api_url": "https://pytorch.org/docs/stable/distributed.optim.html#module-torch.distributed.optim",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.optim.apply_optimizer_in_backward",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.optim.apply_optimizer_in_backward",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.optim.apply_optimizer_in_backward",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.optim.apply_optimizer_in_backward",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.optim.functional_adadelta",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.optim.functional_adadelta",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.optim.functional_adadelta",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.optim.functional_adadelta",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.optim.functional_adagrad",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.optim.functional_adagrad",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.optim.functional_adagrad",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.optim.functional_adagrad",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.optim.functional_adam",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.optim.functional_adam",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.optim.functional_adam",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.optim.functional_adam",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.optim.functional_adamax",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.optim.functional_adamax",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.optim.functional_adamax",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.optim.functional_adamax",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.optim.functional_adamw",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.optim.functional_adamw",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.optim.functional_adamw",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.optim.functional_adamw",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.optim.functional_rmsprop",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.optim.functional_rmsprop",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.optim.functional_rmsprop",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.optim.functional_rmsprop",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.optim.functional_rprop",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.optim.functional_rprop",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.optim.functional_rprop",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.optim.functional_rprop",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.optim.functional_sgd",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.optim.functional_sgd",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.optim.functional_sgd",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.optim.functional_sgd",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.optim.named_optimizer",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.optim.named_optimizer",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.optim.named_optimizer",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.optim.named_optimizer",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.optim.optimizer",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.optim.optimizer",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.optim.optimizer",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.optim.optimizer",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.optim.post_localSGD_optimizer",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.optim.post_localSGD_optimizer",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.optim.post_localSGD_optimizer",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.optim.post_localSGD_optimizer",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.optim.utils",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.optim.utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.optim.utils",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.optim.utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.optim.zero_redundancy_optimizer",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.optim.zero_redundancy_optimizer",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.optim.zero_redundancy_optimizer",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.optim.zero_redundancy_optimizer",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.pipeline",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.pipeline",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.pipeline",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.pipeline",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.pipeline.sync",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.pipeline.sync",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.pipeline.sync",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.pipeline.sync",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.pipeline.sync.batchnorm",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.pipeline.sync.batchnorm",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.pipeline.sync.batchnorm",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.pipeline.sync.batchnorm",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.pipeline.sync.checkpoint",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.pipeline.sync.checkpoint",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.pipeline.sync.checkpoint",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.pipeline.sync.checkpoint",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.pipeline.sync.copy",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.pipeline.sync.copy",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.pipeline.sync.copy",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.pipeline.sync.copy",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.pipeline.sync.dependency",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.pipeline.sync.dependency",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.pipeline.sync.dependency",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.pipeline.sync.dependency",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.pipeline.sync.microbatch",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.pipeline.sync.microbatch",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.pipeline.sync.microbatch",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.pipeline.sync.microbatch",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.pipeline.sync.phony",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.pipeline.sync.phony",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.pipeline.sync.phony",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.pipeline.sync.phony",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.pipeline.sync.pipe",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.pipeline.sync.pipe",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.pipeline.sync.pipe",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.pipeline.sync.pipe",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.pipeline.sync.pipeline",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.pipeline.sync.pipeline",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.pipeline.sync.pipeline",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.pipeline.sync.pipeline",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.pipeline.sync.skip",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.pipeline.sync.skip",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.pipeline.sync.skip",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.pipeline.sync.skip",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.pipeline.sync.skip.layout",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.pipeline.sync.skip.layout",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.pipeline.sync.skip.layout",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.pipeline.sync.skip.layout",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.pipeline.sync.skip.namespace",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.pipeline.sync.skip.namespace",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.pipeline.sync.skip.namespace",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.pipeline.sync.skip.namespace",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.pipeline.sync.skip.portal",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.pipeline.sync.skip.portal",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.pipeline.sync.skip.portal",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.pipeline.sync.skip.portal",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.pipeline.sync.skip.skippable",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.pipeline.sync.skip.skippable",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.pipeline.sync.skip.skippable",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.pipeline.sync.skip.skippable",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.pipeline.sync.skip.tracker",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.pipeline.sync.skip.tracker",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.pipeline.sync.skip.tracker",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.pipeline.sync.skip.tracker",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.pipeline.sync.stream",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.pipeline.sync.stream",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.pipeline.sync.stream",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.pipeline.sync.stream",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.pipeline.sync.utils",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.pipeline.sync.utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.pipeline.sync.utils",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.pipeline.sync.utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.pipeline.sync.worker",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.pipeline.sync.worker",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.pipeline.sync.worker",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.pipeline.sync.worker",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.remote_device",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.remote_device",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.remote_device",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.remote_device",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.rendezvous",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.rendezvous",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.rendezvous",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.rendezvous",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.rpc",
        "api_url": "https://pytorch.org/docs/stable/rpc.html#module-torch.distributed.rpc",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.rpc",
        "api_url": "https://pytorch.org/docs/stable/rpc.html#module-torch.distributed.rpc",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.rpc.api",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.rpc.api",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.rpc.api",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.rpc.api",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.rpc.backend_registry",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.rpc.backend_registry",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.rpc.backend_registry",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.rpc.backend_registry",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.rpc.constants",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.rpc.constants",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.rpc.constants",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.rpc.constants",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.rpc.functions",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.rpc.functions",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.rpc.functions",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.rpc.functions",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.rpc.internal",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.rpc.internal",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.rpc.internal",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.rpc.internal",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.rpc.options",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.rpc.options",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.rpc.options",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.rpc.options",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.rpc.rref_proxy",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.rpc.rref_proxy",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.rpc.rref_proxy",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.rpc.rref_proxy",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.rpc.server_process_global_profiler",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.rpc.server_process_global_profiler",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.rpc.server_process_global_profiler",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.rpc.server_process_global_profiler",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.run",
        "api_url": "https://pytorch.org/docs/stable/elastic/run.html#module-torch.distributed.run",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.run",
        "api_url": "https://pytorch.org/docs/stable/elastic/run.html#module-torch.distributed.run",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.tensor",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.tensor",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.tensor",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.tensor",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.tensor.parallel",
        "api_url": "https://pytorch.org/docs/stable/distributed.tensor.parallel.html#module-torch.distributed.tensor.parallel",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.tensor.parallel",
        "api_url": "https://pytorch.org/docs/stable/distributed.tensor.parallel.html#module-torch.distributed.tensor.parallel",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.tensor.parallel.api",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.tensor.parallel.api",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.tensor.parallel.api",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.tensor.parallel.api",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.tensor.parallel.ddp",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.tensor.parallel.ddp",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.tensor.parallel.ddp",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.tensor.parallel.ddp",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.tensor.parallel.fsdp",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.tensor.parallel.fsdp",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.tensor.parallel.fsdp",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.tensor.parallel.fsdp",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.tensor.parallel.input_reshard",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.tensor.parallel.input_reshard",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.tensor.parallel.input_reshard",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.tensor.parallel.input_reshard",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.tensor.parallel.loss",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.tensor.parallel.loss",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.tensor.parallel.loss",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.tensor.parallel.loss",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.tensor.parallel.style",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.tensor.parallel.style",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.tensor.parallel.style",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.tensor.parallel.style",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.utils",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.utils",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#module-torch.distributed.utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#module-torch.distributions",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#module-torch.distributions",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.bernoulli",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#module-torch.distributions.bernoulli",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.bernoulli",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#module-torch.distributions.bernoulli",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.beta",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#module-torch.distributions.beta",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.beta",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#module-torch.distributions.beta",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.binomial",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#module-torch.distributions.binomial",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.binomial",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#module-torch.distributions.binomial",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.categorical",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#module-torch.distributions.categorical",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.categorical",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#module-torch.distributions.categorical",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.cauchy",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#module-torch.distributions.cauchy",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.cauchy",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#module-torch.distributions.cauchy",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.chi2",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#module-torch.distributions.chi2",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.chi2",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#module-torch.distributions.chi2",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.constraint_registry",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#module-torch.distributions.constraint_registry",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.constraint_registry",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#module-torch.distributions.constraint_registry",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.constraints",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#module-torch.distributions.constraints",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.constraints",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#module-torch.distributions.constraints",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.continuous_bernoulli",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#module-torch.distributions.continuous_bernoulli",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.continuous_bernoulli",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#module-torch.distributions.continuous_bernoulli",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.dirichlet",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#module-torch.distributions.dirichlet",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.dirichlet",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#module-torch.distributions.dirichlet",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.distribution",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#module-torch.distributions.distribution",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.distribution",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#module-torch.distributions.distribution",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.exp_family",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#module-torch.distributions.exp_family",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.exp_family",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#module-torch.distributions.exp_family",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.exponential",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#module-torch.distributions.exponential",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.exponential",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#module-torch.distributions.exponential",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.fishersnedecor",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#module-torch.distributions.fishersnedecor",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.fishersnedecor",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#module-torch.distributions.fishersnedecor",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.gamma",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#module-torch.distributions.gamma",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.gamma",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#module-torch.distributions.gamma",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.geometric",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#module-torch.distributions.geometric",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.geometric",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#module-torch.distributions.geometric",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.gumbel",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#module-torch.distributions.gumbel",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.gumbel",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#module-torch.distributions.gumbel",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.half_cauchy",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#module-torch.distributions.half_cauchy",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.half_cauchy",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#module-torch.distributions.half_cauchy",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.half_normal",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#module-torch.distributions.half_normal",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.half_normal",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#module-torch.distributions.half_normal",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.independent",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#module-torch.distributions.independent",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.independent",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#module-torch.distributions.independent",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.inverse_gamma",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#module-torch.distributions.inverse_gamma",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.inverse_gamma",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#module-torch.distributions.inverse_gamma",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.kl",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#module-torch.distributions.kl",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.kl",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#module-torch.distributions.kl",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.kumaraswamy",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#module-torch.distributions.kumaraswamy",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.kumaraswamy",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#module-torch.distributions.kumaraswamy",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.laplace",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#module-torch.distributions.laplace",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.laplace",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#module-torch.distributions.laplace",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.lkj_cholesky",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#module-torch.distributions.lkj_cholesky",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.lkj_cholesky",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#module-torch.distributions.lkj_cholesky",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.log_normal",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#module-torch.distributions.log_normal",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.log_normal",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#module-torch.distributions.log_normal",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.logistic_normal",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#module-torch.distributions.logistic_normal",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.logistic_normal",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#module-torch.distributions.logistic_normal",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.lowrank_multivariate_normal",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#module-torch.distributions.lowrank_multivariate_normal",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.lowrank_multivariate_normal",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#module-torch.distributions.lowrank_multivariate_normal",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.mixture_same_family",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#module-torch.distributions.mixture_same_family",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.mixture_same_family",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#module-torch.distributions.mixture_same_family",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.multinomial",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#module-torch.distributions.multinomial",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.multinomial",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#module-torch.distributions.multinomial",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.multivariate_normal",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#module-torch.distributions.multivariate_normal",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.multivariate_normal",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#module-torch.distributions.multivariate_normal",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.negative_binomial",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#module-torch.distributions.negative_binomial",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.negative_binomial",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#module-torch.distributions.negative_binomial",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.normal",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#module-torch.distributions.normal",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.normal",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#module-torch.distributions.normal",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.one_hot_categorical",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#module-torch.distributions.one_hot_categorical",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.one_hot_categorical",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#module-torch.distributions.one_hot_categorical",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.pareto",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#module-torch.distributions.pareto",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.pareto",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#module-torch.distributions.pareto",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.poisson",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#module-torch.distributions.poisson",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.poisson",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#module-torch.distributions.poisson",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.relaxed_bernoulli",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#module-torch.distributions.relaxed_bernoulli",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.relaxed_bernoulli",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#module-torch.distributions.relaxed_bernoulli",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.relaxed_categorical",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#module-torch.distributions.relaxed_categorical",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.relaxed_categorical",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#module-torch.distributions.relaxed_categorical",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.studentT",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#module-torch.distributions.studentT",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.studentT",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#module-torch.distributions.studentT",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.transformed_distribution",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#module-torch.distributions.transformed_distribution",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.transformed_distribution",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#module-torch.distributions.transformed_distribution",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.transforms",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#module-torch.distributions.transforms",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.transforms",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#module-torch.distributions.transforms",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.uniform",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#module-torch.distributions.uniform",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.uniform",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#module-torch.distributions.uniform",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.utils",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#module-torch.distributions.utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.utils",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#module-torch.distributions.utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.von_mises",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#module-torch.distributions.von_mises",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.von_mises",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#module-torch.distributions.von_mises",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.weibull",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#module-torch.distributions.weibull",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.weibull",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#module-torch.distributions.weibull",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.wishart",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#module-torch.distributions.wishart",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.wishart",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#module-torch.distributions.wishart",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.export",
        "api_url": "https://pytorch.org/docs/stable/export.html#module-torch.export",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.export",
        "api_url": "https://pytorch.org/docs/stable/export.html#module-torch.export",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.export.custom_obj",
        "api_url": "https://pytorch.org/docs/stable/export.html#module-torch.export.custom_obj",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.export.custom_obj",
        "api_url": "https://pytorch.org/docs/stable/export.html#module-torch.export.custom_obj",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.export.dynamic_shapes",
        "api_url": "https://pytorch.org/docs/stable/export.html#module-torch.export.dynamic_shapes",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.export.dynamic_shapes",
        "api_url": "https://pytorch.org/docs/stable/export.html#module-torch.export.dynamic_shapes",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.export.exported_program",
        "api_url": "https://pytorch.org/docs/stable/export.html#module-torch.export.exported_program",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.export.exported_program",
        "api_url": "https://pytorch.org/docs/stable/export.html#module-torch.export.exported_program",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.export.graph_signature",
        "api_url": "https://pytorch.org/docs/stable/export.html#module-torch.export.graph_signature",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.export.graph_signature",
        "api_url": "https://pytorch.org/docs/stable/export.html#module-torch.export.graph_signature",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.export.unflatten",
        "api_url": "https://pytorch.org/docs/stable/export.html#module-torch.export.unflatten",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.export.unflatten",
        "api_url": "https://pytorch.org/docs/stable/export.html#module-torch.export.unflatten",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fft",
        "api_url": "https://pytorch.org/docs/stable/fft.html#module-torch.fft",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fft",
        "api_url": "https://pytorch.org/docs/stable/fft.html#module-torch.fft",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.torch.finfo",
        "api_url": "https://pytorch.org/docs/stable/type_info.html#torch.torch.finfo",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.func",
        "api_url": "https://pytorch.org/docs/stable/func.api.html#module-torch.func",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.func",
        "api_url": "https://pytorch.org/docs/stable/func.api.html#module-torch.func",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.functional",
        "api_url": "https://pytorch.org/docs/stable/torch.html#module-torch.functional",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.functional",
        "api_url": "https://pytorch.org/docs/stable/torch.html#module-torch.functional",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.futures",
        "api_url": "https://pytorch.org/docs/stable/futures.html#module-torch.futures",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.futures",
        "api_url": "https://pytorch.org/docs/stable/futures.html#module-torch.futures",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.annotate",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.annotate",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.annotate",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.annotate",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.config",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.config",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.config",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.config",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.experimental",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.experimental",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.accelerator_partitioner",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.experimental.accelerator_partitioner",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.accelerator_partitioner",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.experimental.accelerator_partitioner",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.const_fold",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.experimental.const_fold",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.const_fold",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.experimental.const_fold",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.debug",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.experimental.debug",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.debug",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.experimental.debug",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.graph_gradual_typechecker",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.experimental.graph_gradual_typechecker",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.graph_gradual_typechecker",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.experimental.graph_gradual_typechecker",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.merge_matmul",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.experimental.merge_matmul",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.merge_matmul",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.experimental.merge_matmul",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.meta_tracer",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.experimental.meta_tracer",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.meta_tracer",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.experimental.meta_tracer",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.migrate_gradual_types",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.experimental.migrate_gradual_types",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.migrate_gradual_types",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.experimental.migrate_gradual_types",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.migrate_gradual_types.constraint",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.experimental.migrate_gradual_types.constraint",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.migrate_gradual_types.constraint",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.experimental.migrate_gradual_types.constraint",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.migrate_gradual_types.constraint_generator",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.experimental.migrate_gradual_types.constraint_generator",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.migrate_gradual_types.constraint_generator",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.experimental.migrate_gradual_types.constraint_generator",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.migrate_gradual_types.constraint_transformation",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.experimental.migrate_gradual_types.constraint_transformation",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.migrate_gradual_types.constraint_transformation",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.experimental.migrate_gradual_types.constraint_transformation",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.migrate_gradual_types.operation",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.experimental.migrate_gradual_types.operation",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.migrate_gradual_types.operation",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.experimental.migrate_gradual_types.operation",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.migrate_gradual_types.transform_to_z3",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.experimental.migrate_gradual_types.transform_to_z3",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.migrate_gradual_types.transform_to_z3",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.experimental.migrate_gradual_types.transform_to_z3",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.migrate_gradual_types.util",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.experimental.migrate_gradual_types.util",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.migrate_gradual_types.util",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.experimental.migrate_gradual_types.util",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.migrate_gradual_types.z3_types",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.experimental.migrate_gradual_types.z3_types",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.migrate_gradual_types.z3_types",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.experimental.migrate_gradual_types.z3_types",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.normalize",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.experimental.normalize",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.normalize",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.experimental.normalize",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.optimization",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.experimental.optimization",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.optimization",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.experimental.optimization",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.partitioner_utils",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.experimental.partitioner_utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.partitioner_utils",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.experimental.partitioner_utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.proxy_tensor",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.experimental.proxy_tensor",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.proxy_tensor",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.experimental.proxy_tensor",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.recording",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.experimental.recording",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.recording",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.experimental.recording",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.refinement_types",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.experimental.refinement_types",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.refinement_types",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.experimental.refinement_types",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.rewriter",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.experimental.rewriter",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.rewriter",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.experimental.rewriter",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.schema_type_annotation",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.experimental.schema_type_annotation",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.schema_type_annotation",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.experimental.schema_type_annotation",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.sym_node",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.experimental.sym_node",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.sym_node",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.experimental.sym_node",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.symbolic_shapes",
        "api_url": "https://pytorch.org/docs/stable/fx.experimental.html#module-torch.fx.experimental.symbolic_shapes",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.symbolic_shapes",
        "api_url": "https://pytorch.org/docs/stable/fx.experimental.html#module-torch.fx.experimental.symbolic_shapes",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.unification",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.experimental.unification",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.unification",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.experimental.unification",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.unification.core",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.experimental.unification.core",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.unification.core",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.experimental.unification.core",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.unification.dispatch",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.experimental.unification.dispatch",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.unification.dispatch",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.experimental.unification.dispatch",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.unification.match",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.experimental.unification.match",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.unification.match",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.experimental.unification.match",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.unification.more",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.experimental.unification.more",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.unification.more",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.experimental.unification.more",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.unification.multipledispatch",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.experimental.unification.multipledispatch",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.unification.multipledispatch",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.experimental.unification.multipledispatch",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.unification.multipledispatch.conflict",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.experimental.unification.multipledispatch.conflict",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.unification.multipledispatch.conflict",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.experimental.unification.multipledispatch.conflict",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.unification.multipledispatch.core",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.experimental.unification.multipledispatch.core",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.unification.multipledispatch.core",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.experimental.unification.multipledispatch.core",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.unification.multipledispatch.dispatcher",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.experimental.unification.multipledispatch.dispatcher",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.unification.multipledispatch.dispatcher",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.experimental.unification.multipledispatch.dispatcher",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.unification.multipledispatch.utils",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.experimental.unification.multipledispatch.utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.unification.multipledispatch.utils",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.experimental.unification.multipledispatch.utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.unification.multipledispatch.variadic",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.experimental.unification.multipledispatch.variadic",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.unification.multipledispatch.variadic",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.experimental.unification.multipledispatch.variadic",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.unification.unification_tools",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.experimental.unification.unification_tools",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.unification.unification_tools",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.experimental.unification.unification_tools",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.unification.utils",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.experimental.unification.utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.unification.utils",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.experimental.unification.utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.unification.variable",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.experimental.unification.variable",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.unification.variable",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.experimental.unification.variable",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.unify_refinements",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.experimental.unify_refinements",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.unify_refinements",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.experimental.unify_refinements",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.validator",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.experimental.validator",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.experimental.validator",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.experimental.validator",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.graph",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.graph",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.graph",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.graph",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.graph_module",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.graph_module",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.graph_module",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.graph_module",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.immutable_collections",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.immutable_collections",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.immutable_collections",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.immutable_collections",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.interpreter",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.interpreter",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.interpreter",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.interpreter",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.node",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.node",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.node",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.node",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.operator_schemas",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.operator_schemas",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.operator_schemas",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.operator_schemas",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.passes",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.passes",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.passes",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.passes",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.passes.annotate_getitem_nodes",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.passes.annotate_getitem_nodes",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.passes.annotate_getitem_nodes",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.passes.annotate_getitem_nodes",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.passes.backends",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.passes.backends",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.passes.backends",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.passes.backends",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.passes.backends.cudagraphs",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.passes.backends.cudagraphs",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.passes.backends.cudagraphs",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.passes.backends.cudagraphs",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.passes.dialect",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.passes.dialect",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.passes.dialect",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.passes.dialect",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.passes.dialect.common",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.passes.dialect.common",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.passes.dialect.common",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.passes.dialect.common",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.passes.dialect.common.cse_pass",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.passes.dialect.common.cse_pass",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.passes.dialect.common.cse_pass",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.passes.dialect.common.cse_pass",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.passes.fake_tensor_prop",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.passes.fake_tensor_prop",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.passes.fake_tensor_prop",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.passes.fake_tensor_prop",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.passes.graph_drawer",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.passes.graph_drawer",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.passes.graph_drawer",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.passes.graph_drawer",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.passes.graph_manipulation",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.passes.graph_manipulation",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.passes.graph_manipulation",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.passes.graph_manipulation",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.passes.infra",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.passes.infra",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.passes.infra",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.passes.infra",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.passes.infra.partitioner",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.passes.infra.partitioner",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.passes.infra.partitioner",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.passes.infra.partitioner",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.passes.infra.pass_base",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.passes.infra.pass_base",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.passes.infra.pass_base",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.passes.infra.pass_base",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.passes.infra.pass_manager",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.passes.infra.pass_manager",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.passes.infra.pass_manager",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.passes.infra.pass_manager",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.passes.net_min_base",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.passes.net_min_base",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.passes.net_min_base",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.passes.net_min_base",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.passes.operator_support",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.passes.operator_support",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.passes.operator_support",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.passes.operator_support",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.passes.param_fetch",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.passes.param_fetch",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.passes.param_fetch",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.passes.param_fetch",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.passes.pass_manager",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.passes.pass_manager",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.passes.pass_manager",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.passes.pass_manager",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.passes.reinplace",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.passes.reinplace",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.passes.reinplace",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.passes.reinplace",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.passes.shape_prop",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.passes.shape_prop",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.passes.shape_prop",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.passes.shape_prop",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.passes.split_module",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.passes.split_module",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.passes.split_module",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.passes.split_module",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.passes.split_utils",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.passes.split_utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.passes.split_utils",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.passes.split_utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.passes.splitter_base",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.passes.splitter_base",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.passes.splitter_base",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.passes.splitter_base",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.passes.tests",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.passes.tests",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.passes.tests",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.passes.tests",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.passes.tests.test_pass_manager",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.passes.tests.test_pass_manager",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.passes.tests.test_pass_manager",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.passes.tests.test_pass_manager",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.passes.tools_common",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.passes.tools_common",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.passes.tools_common",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.passes.tools_common",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.passes.utils",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.passes.utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.passes.utils",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.passes.utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.passes.utils.common",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.passes.utils.common",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.passes.utils.common",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.passes.utils.common",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.passes.utils.fuser_utils",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.passes.utils.fuser_utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.passes.utils.fuser_utils",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.passes.utils.fuser_utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.passes.utils.matcher_utils",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.passes.utils.matcher_utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.passes.utils.matcher_utils",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.passes.utils.matcher_utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.passes.utils.matcher_with_name_node_map_utils",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.passes.utils.matcher_with_name_node_map_utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.passes.utils.matcher_with_name_node_map_utils",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.passes.utils.matcher_with_name_node_map_utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.passes.utils.source_matcher_utils",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.passes.utils.source_matcher_utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.passes.utils.source_matcher_utils",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.passes.utils.source_matcher_utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.proxy",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.proxy",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.proxy",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.proxy",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.subgraph_rewriter",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.subgraph_rewriter",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.subgraph_rewriter",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.subgraph_rewriter",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.tensor_type",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.tensor_type",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.tensor_type",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.tensor_type",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.traceback",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.traceback",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.traceback",
        "api_url": "https://pytorch.org/docs/stable/fx.html#module-torch.fx.traceback",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.hub",
        "api_url": "https://pytorch.org/docs/stable/hub.html#module-torch.hub",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.hub",
        "api_url": "https://pytorch.org/docs/stable/hub.html#module-torch.hub",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.torch.iinfo",
        "api_url": "https://pytorch.org/docs/stable/type_info.html#torch.torch.iinfo",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.jit",
        "api_url": "https://pytorch.org/docs/stable/jit.html#module-torch.jit",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.jit",
        "api_url": "https://pytorch.org/docs/stable/jit.html#module-torch.jit",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.jit.annotations",
        "api_url": "https://pytorch.org/docs/stable/jit.html#module-torch.jit.annotations",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.jit.annotations",
        "api_url": "https://pytorch.org/docs/stable/jit.html#module-torch.jit.annotations",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.jit.frontend",
        "api_url": "https://pytorch.org/docs/stable/jit.html#module-torch.jit.frontend",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.jit.frontend",
        "api_url": "https://pytorch.org/docs/stable/jit.html#module-torch.jit.frontend",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.jit.generate_bytecode",
        "api_url": "https://pytorch.org/docs/stable/jit.html#module-torch.jit.generate_bytecode",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.jit.generate_bytecode",
        "api_url": "https://pytorch.org/docs/stable/jit.html#module-torch.jit.generate_bytecode",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.jit.mobile",
        "api_url": "https://pytorch.org/docs/stable/jit.html#module-torch.jit.mobile",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.jit.mobile",
        "api_url": "https://pytorch.org/docs/stable/jit.html#module-torch.jit.mobile",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.jit.quantized",
        "api_url": "https://pytorch.org/docs/stable/jit.html#module-torch.jit.quantized",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.jit.quantized",
        "api_url": "https://pytorch.org/docs/stable/jit.html#module-torch.jit.quantized",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.jit.supported_ops",
        "api_url": "https://pytorch.org/docs/stable/jit_builtin_functions.html#module-torch.jit.supported_ops",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.jit.supported_ops",
        "api_url": "https://pytorch.org/docs/stable/jit_builtin_functions.html#module-torch.jit.supported_ops",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.jit.unsupported_tensor_ops",
        "api_url": "https://pytorch.org/docs/stable/jit_unsupported.html#module-torch.jit.unsupported_tensor_ops",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.jit.unsupported_tensor_ops",
        "api_url": "https://pytorch.org/docs/stable/jit_unsupported.html#module-torch.jit.unsupported_tensor_ops",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.library",
        "api_url": "https://pytorch.org/docs/stable/library.html#module-torch.library",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.library",
        "api_url": "https://pytorch.org/docs/stable/library.html#module-torch.library",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.linalg",
        "api_url": "https://pytorch.org/docs/stable/linalg.html#module-torch.linalg",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.linalg",
        "api_url": "https://pytorch.org/docs/stable/linalg.html#module-torch.linalg",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.masked",
        "api_url": "https://pytorch.org/docs/stable/masked.html#module-torch.masked",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.masked",
        "api_url": "https://pytorch.org/docs/stable/masked.html#module-torch.masked",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.masked.maskedtensor",
        "api_url": "https://pytorch.org/docs/stable/masked.html#module-torch.masked.maskedtensor",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.masked.maskedtensor",
        "api_url": "https://pytorch.org/docs/stable/masked.html#module-torch.masked.maskedtensor",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.masked.maskedtensor.binary",
        "api_url": "https://pytorch.org/docs/stable/masked.html#module-torch.masked.maskedtensor.binary",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.masked.maskedtensor.binary",
        "api_url": "https://pytorch.org/docs/stable/masked.html#module-torch.masked.maskedtensor.binary",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.masked.maskedtensor.core",
        "api_url": "https://pytorch.org/docs/stable/masked.html#module-torch.masked.maskedtensor.core",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.masked.maskedtensor.core",
        "api_url": "https://pytorch.org/docs/stable/masked.html#module-torch.masked.maskedtensor.core",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.masked.maskedtensor.creation",
        "api_url": "https://pytorch.org/docs/stable/masked.html#module-torch.masked.maskedtensor.creation",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.masked.maskedtensor.creation",
        "api_url": "https://pytorch.org/docs/stable/masked.html#module-torch.masked.maskedtensor.creation",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.masked.maskedtensor.passthrough",
        "api_url": "https://pytorch.org/docs/stable/masked.html#module-torch.masked.maskedtensor.passthrough",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.masked.maskedtensor.passthrough",
        "api_url": "https://pytorch.org/docs/stable/masked.html#module-torch.masked.maskedtensor.passthrough",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.masked.maskedtensor.reductions",
        "api_url": "https://pytorch.org/docs/stable/masked.html#module-torch.masked.maskedtensor.reductions",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.masked.maskedtensor.reductions",
        "api_url": "https://pytorch.org/docs/stable/masked.html#module-torch.masked.maskedtensor.reductions",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.masked.maskedtensor.unary",
        "api_url": "https://pytorch.org/docs/stable/masked.html#module-torch.masked.maskedtensor.unary",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.masked.maskedtensor.unary",
        "api_url": "https://pytorch.org/docs/stable/masked.html#module-torch.masked.maskedtensor.unary",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.monitor",
        "api_url": "https://pytorch.org/docs/stable/monitor.html#module-torch.monitor",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.monitor",
        "api_url": "https://pytorch.org/docs/stable/monitor.html#module-torch.monitor",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.mps",
        "api_url": "https://pytorch.org/docs/stable/mps.html#module-torch.mps",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.mps",
        "api_url": "https://pytorch.org/docs/stable/mps.html#module-torch.mps",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.mps.event",
        "api_url": "https://pytorch.org/docs/stable/mps.html#module-torch.mps.event",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.mps.event",
        "api_url": "https://pytorch.org/docs/stable/mps.html#module-torch.mps.event",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.mps.profiler",
        "api_url": "https://pytorch.org/docs/stable/mps.html#module-torch.mps.profiler",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.mps.profiler",
        "api_url": "https://pytorch.org/docs/stable/mps.html#module-torch.mps.profiler",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.multiprocessing",
        "api_url": "https://pytorch.org/docs/stable/multiprocessing.html#module-torch.multiprocessing",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.multiprocessing",
        "api_url": "https://pytorch.org/docs/stable/multiprocessing.html#module-torch.multiprocessing",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.multiprocessing.pool",
        "api_url": "https://pytorch.org/docs/stable/multiprocessing.html#module-torch.multiprocessing.pool",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.multiprocessing.pool",
        "api_url": "https://pytorch.org/docs/stable/multiprocessing.html#module-torch.multiprocessing.pool",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.multiprocessing.queue",
        "api_url": "https://pytorch.org/docs/stable/multiprocessing.html#module-torch.multiprocessing.queue",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.multiprocessing.queue",
        "api_url": "https://pytorch.org/docs/stable/multiprocessing.html#module-torch.multiprocessing.queue",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.multiprocessing.reductions",
        "api_url": "https://pytorch.org/docs/stable/multiprocessing.html#module-torch.multiprocessing.reductions",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.multiprocessing.reductions",
        "api_url": "https://pytorch.org/docs/stable/multiprocessing.html#module-torch.multiprocessing.reductions",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.multiprocessing.spawn",
        "api_url": "https://pytorch.org/docs/stable/multiprocessing.html#module-torch.multiprocessing.spawn",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.multiprocessing.spawn",
        "api_url": "https://pytorch.org/docs/stable/multiprocessing.html#module-torch.multiprocessing.spawn",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nested",
        "api_url": "https://pytorch.org/docs/stable/nested.html#module-torch.nested",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nested",
        "api_url": "https://pytorch.org/docs/stable/nested.html#module-torch.nested",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.attention",
        "api_url": "https://pytorch.org/docs/stable/nn.attention.html#module-torch.nn.attention",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.attention",
        "api_url": "https://pytorch.org/docs/stable/nn.attention.html#module-torch.nn.attention",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.attention.bias",
        "api_url": "https://pytorch.org/docs/stable/nn.attention.bias.html#module-torch.nn.attention.bias",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.attention.bias",
        "api_url": "https://pytorch.org/docs/stable/nn.attention.bias.html#module-torch.nn.attention.bias",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.backends",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn.backends",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.backends",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn.backends",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.backends.thnn",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn.backends.thnn",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.backends.thnn",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn.backends.thnn",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.common_types",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn.common_types",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.common_types",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn.common_types",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.cpp",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn.cpp",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.cpp",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn.cpp",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.functional",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn.functional",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.functional",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn.functional",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.grad",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn.grad",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.grad",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn.grad",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.init",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn.init",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.init",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn.init",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.intrinsic",
        "api_url": "https://pytorch.org/docs/stable/quantization-support.html#module-torch.nn.intrinsic",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.intrinsic",
        "api_url": "https://pytorch.org/docs/stable/quantization-support.html#module-torch.nn.intrinsic",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.intrinsic.modules",
        "api_url": "https://pytorch.org/docs/stable/quantization-support.html#module-torch.nn.intrinsic.modules",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.intrinsic.modules",
        "api_url": "https://pytorch.org/docs/stable/quantization-support.html#module-torch.nn.intrinsic.modules",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.intrinsic.modules.fused",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.nn.intrinsic.modules.fused",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.intrinsic.modules.fused",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.nn.intrinsic.modules.fused",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.intrinsic.qat",
        "api_url": "https://pytorch.org/docs/stable/quantization-support.html#module-torch.nn.intrinsic.qat",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.intrinsic.qat",
        "api_url": "https://pytorch.org/docs/stable/quantization-support.html#module-torch.nn.intrinsic.qat",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.intrinsic.qat.modules",
        "api_url": "https://pytorch.org/docs/stable/quantization-support.html#module-torch.nn.intrinsic.qat.modules",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.intrinsic.qat.modules",
        "api_url": "https://pytorch.org/docs/stable/quantization-support.html#module-torch.nn.intrinsic.qat.modules",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.intrinsic.qat.modules.conv_fused",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.nn.intrinsic.qat.modules.conv_fused",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.intrinsic.qat.modules.conv_fused",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.nn.intrinsic.qat.modules.conv_fused",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.intrinsic.qat.modules.linear_fused",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.nn.intrinsic.qat.modules.linear_fused",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.intrinsic.qat.modules.linear_fused",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.nn.intrinsic.qat.modules.linear_fused",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.intrinsic.qat.modules.linear_relu",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.nn.intrinsic.qat.modules.linear_relu",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.intrinsic.qat.modules.linear_relu",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.nn.intrinsic.qat.modules.linear_relu",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.intrinsic.quantized",
        "api_url": "https://pytorch.org/docs/stable/quantization-support.html#module-torch.nn.intrinsic.quantized",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.intrinsic.quantized",
        "api_url": "https://pytorch.org/docs/stable/quantization-support.html#module-torch.nn.intrinsic.quantized",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.intrinsic.quantized.dynamic",
        "api_url": "https://pytorch.org/docs/stable/quantization-support.html#module-torch.nn.intrinsic.quantized.dynamic",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.intrinsic.quantized.dynamic",
        "api_url": "https://pytorch.org/docs/stable/quantization-support.html#module-torch.nn.intrinsic.quantized.dynamic",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.intrinsic.quantized.dynamic.modules",
        "api_url": "https://pytorch.org/docs/stable/quantization-support.html#module-torch.nn.intrinsic.quantized.dynamic.modules",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.intrinsic.quantized.dynamic.modules",
        "api_url": "https://pytorch.org/docs/stable/quantization-support.html#module-torch.nn.intrinsic.quantized.dynamic.modules",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.intrinsic.quantized.dynamic.modules.linear_relu",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.nn.intrinsic.quantized.dynamic.modules.linear_relu",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.intrinsic.quantized.dynamic.modules.linear_relu",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.nn.intrinsic.quantized.dynamic.modules.linear_relu",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.intrinsic.quantized.modules",
        "api_url": "https://pytorch.org/docs/stable/quantization-support.html#module-torch.nn.intrinsic.quantized.modules",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.intrinsic.quantized.modules",
        "api_url": "https://pytorch.org/docs/stable/quantization-support.html#module-torch.nn.intrinsic.quantized.modules",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.intrinsic.quantized.modules.bn_relu",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.nn.intrinsic.quantized.modules.bn_relu",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.intrinsic.quantized.modules.bn_relu",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.nn.intrinsic.quantized.modules.bn_relu",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.intrinsic.quantized.modules.conv_relu",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.nn.intrinsic.quantized.modules.conv_relu",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.intrinsic.quantized.modules.conv_relu",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.nn.intrinsic.quantized.modules.conv_relu",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.intrinsic.quantized.modules.linear_relu",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.nn.intrinsic.quantized.modules.linear_relu",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.intrinsic.quantized.modules.linear_relu",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.nn.intrinsic.quantized.modules.linear_relu",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.modules",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn.modules",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.modules",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn.modules",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.modules.activation",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn.modules.activation",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.modules.activation",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn.modules.activation",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.modules.adaptive",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn.modules.adaptive",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.modules.adaptive",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn.modules.adaptive",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.modules.batchnorm",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn.modules.batchnorm",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.modules.batchnorm",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn.modules.batchnorm",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.modules.channelshuffle",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn.modules.channelshuffle",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.modules.channelshuffle",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn.modules.channelshuffle",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.modules.container",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn.modules.container",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.modules.container",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn.modules.container",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.modules.conv",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn.modules.conv",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.modules.conv",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn.modules.conv",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.modules.distance",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn.modules.distance",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.modules.distance",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn.modules.distance",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.modules.dropout",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn.modules.dropout",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.modules.dropout",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn.modules.dropout",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.modules.flatten",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn.modules.flatten",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.modules.flatten",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn.modules.flatten",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.modules.fold",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn.modules.fold",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.modules.fold",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn.modules.fold",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.modules.instancenorm",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn.modules.instancenorm",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.modules.instancenorm",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn.modules.instancenorm",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.modules.lazy",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn.modules.lazy",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.modules.lazy",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn.modules.lazy",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.modules.linear",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn.modules.linear",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.modules.linear",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn.modules.linear",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.modules.loss",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn.modules.loss",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.modules.loss",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn.modules.loss",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.modules.module",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn.modules.module",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.modules.module",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn.modules.module",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.modules.normalization",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn.modules.normalization",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.modules.normalization",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn.modules.normalization",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.modules.padding",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn.modules.padding",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.modules.padding",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn.modules.padding",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.modules.pixelshuffle",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn.modules.pixelshuffle",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.modules.pixelshuffle",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn.modules.pixelshuffle",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.modules.pooling",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn.modules.pooling",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.modules.pooling",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn.modules.pooling",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.modules.rnn",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn.modules.rnn",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.modules.rnn",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn.modules.rnn",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.modules.sparse",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn.modules.sparse",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.modules.sparse",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn.modules.sparse",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.modules.transformer",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn.modules.transformer",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.modules.transformer",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn.modules.transformer",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.modules.upsampling",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn.modules.upsampling",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.modules.upsampling",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn.modules.upsampling",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.modules.utils",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn.modules.utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.modules.utils",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn.modules.utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.parallel",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn.parallel",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.parallel",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn.parallel",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.parallel.comm",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn.parallel.comm",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.parallel.comm",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn.parallel.comm",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.parallel.data_parallel",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn.parallel.data_parallel",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.parallel.data_parallel",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn.parallel.data_parallel",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.parallel.distributed",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn.parallel.distributed",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.parallel.distributed",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn.parallel.distributed",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.parallel.parallel_apply",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn.parallel.parallel_apply",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.parallel.parallel_apply",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn.parallel.parallel_apply",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.parallel.replicate",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn.parallel.replicate",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.parallel.replicate",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn.parallel.replicate",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.parallel.scatter_gather",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn.parallel.scatter_gather",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.parallel.scatter_gather",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn.parallel.scatter_gather",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.parameter",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn.parameter",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.parameter",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn.parameter",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.qat",
        "api_url": "https://pytorch.org/docs/stable/quantization-support.html#module-torch.nn.qat",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.qat",
        "api_url": "https://pytorch.org/docs/stable/quantization-support.html#module-torch.nn.qat",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.qat.dynamic",
        "api_url": "https://pytorch.org/docs/stable/quantization-support.html#module-torch.nn.qat.dynamic",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.qat.dynamic",
        "api_url": "https://pytorch.org/docs/stable/quantization-support.html#module-torch.nn.qat.dynamic",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.qat.dynamic.modules",
        "api_url": "https://pytorch.org/docs/stable/quantization-support.html#module-torch.nn.qat.dynamic.modules",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.qat.dynamic.modules",
        "api_url": "https://pytorch.org/docs/stable/quantization-support.html#module-torch.nn.qat.dynamic.modules",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.qat.dynamic.modules.linear",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.nn.qat.dynamic.modules.linear",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.qat.dynamic.modules.linear",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.nn.qat.dynamic.modules.linear",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.qat.modules",
        "api_url": "https://pytorch.org/docs/stable/quantization-support.html#module-torch.nn.qat.modules",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.qat.modules",
        "api_url": "https://pytorch.org/docs/stable/quantization-support.html#module-torch.nn.qat.modules",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.qat.modules.conv",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.nn.qat.modules.conv",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.qat.modules.conv",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.nn.qat.modules.conv",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.qat.modules.embedding_ops",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.nn.qat.modules.embedding_ops",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.qat.modules.embedding_ops",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.nn.qat.modules.embedding_ops",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.qat.modules.linear",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.nn.qat.modules.linear",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.qat.modules.linear",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.nn.qat.modules.linear",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.quantizable",
        "api_url": "https://pytorch.org/docs/stable/quantization-support.html#module-torch.nn.quantizable",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.quantizable",
        "api_url": "https://pytorch.org/docs/stable/quantization-support.html#module-torch.nn.quantizable",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.quantizable.modules",
        "api_url": "https://pytorch.org/docs/stable/quantization-support.html#module-torch.nn.quantizable.modules",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.quantizable.modules",
        "api_url": "https://pytorch.org/docs/stable/quantization-support.html#module-torch.nn.quantizable.modules",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.quantizable.modules.activation",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.nn.quantizable.modules.activation",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.quantizable.modules.activation",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.nn.quantizable.modules.activation",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.quantizable.modules.rnn",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.nn.quantizable.modules.rnn",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.quantizable.modules.rnn",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.nn.quantizable.modules.rnn",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.quantized",
        "api_url": "https://pytorch.org/docs/stable/quantization-support.html#module-torch.nn.quantized",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.quantized",
        "api_url": "https://pytorch.org/docs/stable/quantization-support.html#module-torch.nn.quantized",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.quantized.dynamic",
        "api_url": "https://pytorch.org/docs/stable/quantization-support.html#module-torch.nn.quantized.dynamic",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.quantized.dynamic",
        "api_url": "https://pytorch.org/docs/stable/quantization-support.html#module-torch.nn.quantized.dynamic",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.quantized.dynamic.modules",
        "api_url": "https://pytorch.org/docs/stable/quantization-support.html#module-torch.nn.quantized.dynamic.modules",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.quantized.dynamic.modules",
        "api_url": "https://pytorch.org/docs/stable/quantization-support.html#module-torch.nn.quantized.dynamic.modules",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.quantized.dynamic.modules.conv",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.nn.quantized.dynamic.modules.conv",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.quantized.dynamic.modules.conv",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.nn.quantized.dynamic.modules.conv",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.quantized.dynamic.modules.linear",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.nn.quantized.dynamic.modules.linear",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.quantized.dynamic.modules.linear",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.nn.quantized.dynamic.modules.linear",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.quantized.dynamic.modules.rnn",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.nn.quantized.dynamic.modules.rnn",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.quantized.dynamic.modules.rnn",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.nn.quantized.dynamic.modules.rnn",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.quantized.functional",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.nn.quantized.functional",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.quantized.functional",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.nn.quantized.functional",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.quantized.modules",
        "api_url": "https://pytorch.org/docs/stable/quantization-support.html#module-torch.nn.quantized.modules",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.quantized.modules",
        "api_url": "https://pytorch.org/docs/stable/quantization-support.html#module-torch.nn.quantized.modules",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.quantized.modules.activation",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.nn.quantized.modules.activation",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.quantized.modules.activation",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.nn.quantized.modules.activation",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.quantized.modules.batchnorm",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.nn.quantized.modules.batchnorm",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.quantized.modules.batchnorm",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.nn.quantized.modules.batchnorm",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.quantized.modules.conv",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.nn.quantized.modules.conv",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.quantized.modules.conv",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.nn.quantized.modules.conv",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.quantized.modules.dropout",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.nn.quantized.modules.dropout",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.quantized.modules.dropout",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.nn.quantized.modules.dropout",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.quantized.modules.embedding_ops",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.nn.quantized.modules.embedding_ops",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.quantized.modules.embedding_ops",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.nn.quantized.modules.embedding_ops",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.quantized.modules.functional_modules",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.nn.quantized.modules.functional_modules",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.quantized.modules.functional_modules",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.nn.quantized.modules.functional_modules",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.quantized.modules.linear",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.nn.quantized.modules.linear",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.quantized.modules.linear",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.nn.quantized.modules.linear",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.quantized.modules.normalization",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.nn.quantized.modules.normalization",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.quantized.modules.normalization",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.nn.quantized.modules.normalization",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.quantized.modules.rnn",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.nn.quantized.modules.rnn",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.quantized.modules.rnn",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.nn.quantized.modules.rnn",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.quantized.modules.utils",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.nn.quantized.modules.utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.quantized.modules.utils",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.nn.quantized.modules.utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.utils",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn.utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.utils",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn.utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.utils.clip_grad",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn.utils.clip_grad",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.utils.clip_grad",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn.utils.clip_grad",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.utils.convert_parameters",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn.utils.convert_parameters",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.utils.convert_parameters",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn.utils.convert_parameters",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.utils.fusion",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn.utils.fusion",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.utils.fusion",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn.utils.fusion",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.utils.init",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn.utils.init",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.utils.init",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn.utils.init",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.utils.memory_format",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn.utils.memory_format",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.utils.memory_format",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn.utils.memory_format",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.utils.parametrizations",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn.utils.parametrizations",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.utils.parametrizations",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn.utils.parametrizations",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.utils.parametrize",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn.utils.parametrize",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.utils.parametrize",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn.utils.parametrize",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.utils.prune",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn.utils.prune",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.utils.prune",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn.utils.prune",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.utils.rnn",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn.utils.rnn",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.utils.rnn",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn.utils.rnn",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.utils.spectral_norm",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn.utils.spectral_norm",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.utils.spectral_norm",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn.utils.spectral_norm",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.utils.stateless",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn.utils.stateless",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.utils.stateless",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn.utils.stateless",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.utils.weight_norm",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn.utils.weight_norm",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.utils.weight_norm",
        "api_url": "https://pytorch.org/docs/stable/nn.html#module-torch.nn.utils.weight_norm",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.onnx",
        "api_url": "https://pytorch.org/docs/stable/onnx_torchscript.html#module-torch.onnx",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.onnx",
        "api_url": "https://pytorch.org/docs/stable/onnx_torchscript.html#module-torch.onnx",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.onnx.errors",
        "api_url": "https://pytorch.org/docs/stable/onnx.html#module-torch.onnx.errors",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.onnx.errors",
        "api_url": "https://pytorch.org/docs/stable/onnx.html#module-torch.onnx.errors",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.onnx.operators",
        "api_url": "https://pytorch.org/docs/stable/onnx.html#module-torch.onnx.operators",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.onnx.operators",
        "api_url": "https://pytorch.org/docs/stable/onnx.html#module-torch.onnx.operators",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.onnx.symbolic_caffe2",
        "api_url": "https://pytorch.org/docs/stable/onnx.html#module-torch.onnx.symbolic_caffe2",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.onnx.symbolic_caffe2",
        "api_url": "https://pytorch.org/docs/stable/onnx.html#module-torch.onnx.symbolic_caffe2",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.onnx.symbolic_helper",
        "api_url": "https://pytorch.org/docs/stable/onnx.html#module-torch.onnx.symbolic_helper",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.onnx.symbolic_helper",
        "api_url": "https://pytorch.org/docs/stable/onnx.html#module-torch.onnx.symbolic_helper",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.onnx.symbolic_opset10",
        "api_url": "https://pytorch.org/docs/stable/onnx.html#module-torch.onnx.symbolic_opset10",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.onnx.symbolic_opset10",
        "api_url": "https://pytorch.org/docs/stable/onnx.html#module-torch.onnx.symbolic_opset10",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.onnx.symbolic_opset11",
        "api_url": "https://pytorch.org/docs/stable/onnx.html#module-torch.onnx.symbolic_opset11",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.onnx.symbolic_opset11",
        "api_url": "https://pytorch.org/docs/stable/onnx.html#module-torch.onnx.symbolic_opset11",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.onnx.symbolic_opset12",
        "api_url": "https://pytorch.org/docs/stable/onnx.html#module-torch.onnx.symbolic_opset12",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.onnx.symbolic_opset12",
        "api_url": "https://pytorch.org/docs/stable/onnx.html#module-torch.onnx.symbolic_opset12",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.onnx.symbolic_opset13",
        "api_url": "https://pytorch.org/docs/stable/onnx.html#module-torch.onnx.symbolic_opset13",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.onnx.symbolic_opset13",
        "api_url": "https://pytorch.org/docs/stable/onnx.html#module-torch.onnx.symbolic_opset13",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.onnx.symbolic_opset14",
        "api_url": "https://pytorch.org/docs/stable/onnx.html#module-torch.onnx.symbolic_opset14",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.onnx.symbolic_opset14",
        "api_url": "https://pytorch.org/docs/stable/onnx.html#module-torch.onnx.symbolic_opset14",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.onnx.symbolic_opset15",
        "api_url": "https://pytorch.org/docs/stable/onnx.html#module-torch.onnx.symbolic_opset15",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.onnx.symbolic_opset15",
        "api_url": "https://pytorch.org/docs/stable/onnx.html#module-torch.onnx.symbolic_opset15",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.onnx.symbolic_opset16",
        "api_url": "https://pytorch.org/docs/stable/onnx.html#module-torch.onnx.symbolic_opset16",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.onnx.symbolic_opset16",
        "api_url": "https://pytorch.org/docs/stable/onnx.html#module-torch.onnx.symbolic_opset16",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.onnx.symbolic_opset17",
        "api_url": "https://pytorch.org/docs/stable/onnx.html#module-torch.onnx.symbolic_opset17",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.onnx.symbolic_opset17",
        "api_url": "https://pytorch.org/docs/stable/onnx.html#module-torch.onnx.symbolic_opset17",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.onnx.symbolic_opset18",
        "api_url": "https://pytorch.org/docs/stable/onnx.html#module-torch.onnx.symbolic_opset18",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.onnx.symbolic_opset18",
        "api_url": "https://pytorch.org/docs/stable/onnx.html#module-torch.onnx.symbolic_opset18",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.onnx.symbolic_opset7",
        "api_url": "https://pytorch.org/docs/stable/onnx.html#module-torch.onnx.symbolic_opset7",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.onnx.symbolic_opset7",
        "api_url": "https://pytorch.org/docs/stable/onnx.html#module-torch.onnx.symbolic_opset7",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.onnx.symbolic_opset8",
        "api_url": "https://pytorch.org/docs/stable/onnx.html#module-torch.onnx.symbolic_opset8",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.onnx.symbolic_opset8",
        "api_url": "https://pytorch.org/docs/stable/onnx.html#module-torch.onnx.symbolic_opset8",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.onnx.symbolic_opset9",
        "api_url": "https://pytorch.org/docs/stable/onnx.html#module-torch.onnx.symbolic_opset9",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.onnx.symbolic_opset9",
        "api_url": "https://pytorch.org/docs/stable/onnx.html#module-torch.onnx.symbolic_opset9",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.onnx.utils",
        "api_url": "https://pytorch.org/docs/stable/onnx.html#module-torch.onnx.utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.onnx.utils",
        "api_url": "https://pytorch.org/docs/stable/onnx.html#module-torch.onnx.utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.onnx.verification",
        "api_url": "https://pytorch.org/docs/stable/onnx.html#module-torch.onnx.verification",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.onnx.verification",
        "api_url": "https://pytorch.org/docs/stable/onnx.html#module-torch.onnx.verification",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim",
        "api_url": "https://pytorch.org/docs/stable/optim.html#module-torch.optim",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim",
        "api_url": "https://pytorch.org/docs/stable/optim.html#module-torch.optim",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.adadelta",
        "api_url": "https://pytorch.org/docs/stable/optim.html#module-torch.optim.adadelta",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.adadelta",
        "api_url": "https://pytorch.org/docs/stable/optim.html#module-torch.optim.adadelta",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.adagrad",
        "api_url": "https://pytorch.org/docs/stable/optim.html#module-torch.optim.adagrad",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.adagrad",
        "api_url": "https://pytorch.org/docs/stable/optim.html#module-torch.optim.adagrad",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.adam",
        "api_url": "https://pytorch.org/docs/stable/optim.html#module-torch.optim.adam",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.adam",
        "api_url": "https://pytorch.org/docs/stable/optim.html#module-torch.optim.adam",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.adamax",
        "api_url": "https://pytorch.org/docs/stable/optim.html#module-torch.optim.adamax",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.adamax",
        "api_url": "https://pytorch.org/docs/stable/optim.html#module-torch.optim.adamax",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.adamw",
        "api_url": "https://pytorch.org/docs/stable/optim.html#module-torch.optim.adamw",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.adamw",
        "api_url": "https://pytorch.org/docs/stable/optim.html#module-torch.optim.adamw",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.asgd",
        "api_url": "https://pytorch.org/docs/stable/optim.html#module-torch.optim.asgd",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.asgd",
        "api_url": "https://pytorch.org/docs/stable/optim.html#module-torch.optim.asgd",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.lbfgs",
        "api_url": "https://pytorch.org/docs/stable/optim.html#module-torch.optim.lbfgs",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.lbfgs",
        "api_url": "https://pytorch.org/docs/stable/optim.html#module-torch.optim.lbfgs",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.lr_scheduler",
        "api_url": "https://pytorch.org/docs/stable/optim.html#module-torch.optim.lr_scheduler",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.lr_scheduler",
        "api_url": "https://pytorch.org/docs/stable/optim.html#module-torch.optim.lr_scheduler",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.nadam",
        "api_url": "https://pytorch.org/docs/stable/optim.html#module-torch.optim.nadam",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.nadam",
        "api_url": "https://pytorch.org/docs/stable/optim.html#module-torch.optim.nadam",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.optimizer",
        "api_url": "https://pytorch.org/docs/stable/optim.html#module-torch.optim.optimizer",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.optimizer",
        "api_url": "https://pytorch.org/docs/stable/optim.html#module-torch.optim.optimizer",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.radam",
        "api_url": "https://pytorch.org/docs/stable/optim.html#module-torch.optim.radam",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.radam",
        "api_url": "https://pytorch.org/docs/stable/optim.html#module-torch.optim.radam",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.rmsprop",
        "api_url": "https://pytorch.org/docs/stable/optim.html#module-torch.optim.rmsprop",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.rmsprop",
        "api_url": "https://pytorch.org/docs/stable/optim.html#module-torch.optim.rmsprop",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.rprop",
        "api_url": "https://pytorch.org/docs/stable/optim.html#module-torch.optim.rprop",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.rprop",
        "api_url": "https://pytorch.org/docs/stable/optim.html#module-torch.optim.rprop",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.sgd",
        "api_url": "https://pytorch.org/docs/stable/optim.html#module-torch.optim.sgd",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.sgd",
        "api_url": "https://pytorch.org/docs/stable/optim.html#module-torch.optim.sgd",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.sparse_adam",
        "api_url": "https://pytorch.org/docs/stable/optim.html#module-torch.optim.sparse_adam",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.sparse_adam",
        "api_url": "https://pytorch.org/docs/stable/optim.html#module-torch.optim.sparse_adam",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.swa_utils",
        "api_url": "https://pytorch.org/docs/stable/optim.html#module-torch.optim.swa_utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.swa_utils",
        "api_url": "https://pytorch.org/docs/stable/optim.html#module-torch.optim.swa_utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.overrides",
        "api_url": "https://pytorch.org/docs/stable/torch.overrides.html#module-torch.overrides",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.overrides",
        "api_url": "https://pytorch.org/docs/stable/torch.overrides.html#module-torch.overrides",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.package",
        "api_url": "https://pytorch.org/docs/stable/package.html#module-torch.package",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.package",
        "api_url": "https://pytorch.org/docs/stable/package.html#module-torch.package",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.package.analyze",
        "api_url": "https://pytorch.org/docs/stable/package.html#module-torch.package.analyze",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.package.analyze",
        "api_url": "https://pytorch.org/docs/stable/package.html#module-torch.package.analyze",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.package.analyze.find_first_use_of_broken_modules",
        "api_url": "https://pytorch.org/docs/stable/package.html#module-torch.package.analyze.find_first_use_of_broken_modules",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.package.analyze.find_first_use_of_broken_modules",
        "api_url": "https://pytorch.org/docs/stable/package.html#module-torch.package.analyze.find_first_use_of_broken_modules",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.package.analyze.is_from_package",
        "api_url": "https://pytorch.org/docs/stable/package.html#module-torch.package.analyze.is_from_package",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.package.analyze.is_from_package",
        "api_url": "https://pytorch.org/docs/stable/package.html#module-torch.package.analyze.is_from_package",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.package.analyze.trace_dependencies",
        "api_url": "https://pytorch.org/docs/stable/package.html#module-torch.package.analyze.trace_dependencies",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.package.analyze.trace_dependencies",
        "api_url": "https://pytorch.org/docs/stable/package.html#module-torch.package.analyze.trace_dependencies",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.package.file_structure_representation",
        "api_url": "https://pytorch.org/docs/stable/package.html#module-torch.package.file_structure_representation",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.package.file_structure_representation",
        "api_url": "https://pytorch.org/docs/stable/package.html#module-torch.package.file_structure_representation",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.package.find_file_dependencies",
        "api_url": "https://pytorch.org/docs/stable/package.html#module-torch.package.find_file_dependencies",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.package.find_file_dependencies",
        "api_url": "https://pytorch.org/docs/stable/package.html#module-torch.package.find_file_dependencies",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.package.glob_group",
        "api_url": "https://pytorch.org/docs/stable/package.html#module-torch.package.glob_group",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.package.glob_group",
        "api_url": "https://pytorch.org/docs/stable/package.html#module-torch.package.glob_group",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.package.importer",
        "api_url": "https://pytorch.org/docs/stable/package.html#module-torch.package.importer",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.package.importer",
        "api_url": "https://pytorch.org/docs/stable/package.html#module-torch.package.importer",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.package.package_exporter",
        "api_url": "https://pytorch.org/docs/stable/package.html#module-torch.package.package_exporter",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.package.package_exporter",
        "api_url": "https://pytorch.org/docs/stable/package.html#module-torch.package.package_exporter",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.package.package_importer",
        "api_url": "https://pytorch.org/docs/stable/package.html#module-torch.package.package_importer",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.package.package_importer",
        "api_url": "https://pytorch.org/docs/stable/package.html#module-torch.package.package_importer",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.profiler",
        "api_url": "https://pytorch.org/docs/stable/profiler.html#module-torch.profiler",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.profiler",
        "api_url": "https://pytorch.org/docs/stable/profiler.html#module-torch.profiler",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.profiler.itt",
        "api_url": "https://pytorch.org/docs/stable/profiler.html#module-torch.profiler.itt",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.profiler.itt",
        "api_url": "https://pytorch.org/docs/stable/profiler.html#module-torch.profiler.itt",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.profiler.profiler",
        "api_url": "https://pytorch.org/docs/stable/profiler.html#module-torch.profiler.profiler",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.profiler.profiler",
        "api_url": "https://pytorch.org/docs/stable/profiler.html#module-torch.profiler.profiler",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.profiler.python_tracer",
        "api_url": "https://pytorch.org/docs/stable/profiler.html#module-torch.profiler.python_tracer",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.profiler.python_tracer",
        "api_url": "https://pytorch.org/docs/stable/profiler.html#module-torch.profiler.python_tracer",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.quantization",
        "api_url": "https://pytorch.org/docs/stable/quantization-support.html#module-torch.quantization",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.quantization",
        "api_url": "https://pytorch.org/docs/stable/quantization-support.html#module-torch.quantization",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.quantization.fake_quantize",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.quantization.fake_quantize",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.quantization.fake_quantize",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.quantization.fake_quantize",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.quantization.fuse_modules",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.quantization.fuse_modules",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.quantization.fuse_modules",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.quantization.fuse_modules",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.quantization.fuser_method_mappings",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.quantization.fuser_method_mappings",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.quantization.fuser_method_mappings",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.quantization.fuser_method_mappings",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.quantization.fx",
        "api_url": "https://pytorch.org/docs/stable/quantization-support.html#module-torch.quantization.fx",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.quantization.fx",
        "api_url": "https://pytorch.org/docs/stable/quantization-support.html#module-torch.quantization.fx",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.quantization.fx.convert",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.quantization.fx.convert",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.quantization.fx.convert",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.quantization.fx.convert",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.quantization.fx.fuse",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.quantization.fx.fuse",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.quantization.fx.fuse",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.quantization.fx.fuse",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.quantization.fx.fusion_patterns",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.quantization.fx.fusion_patterns",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.quantization.fx.fusion_patterns",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.quantization.fx.fusion_patterns",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.quantization.fx.graph_module",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.quantization.fx.graph_module",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.quantization.fx.graph_module",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.quantization.fx.graph_module",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.quantization.fx.match_utils",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.quantization.fx.match_utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.quantization.fx.match_utils",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.quantization.fx.match_utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.quantization.fx.pattern_utils",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.quantization.fx.pattern_utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.quantization.fx.pattern_utils",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.quantization.fx.pattern_utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.quantization.fx.prepare",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.quantization.fx.prepare",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.quantization.fx.prepare",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.quantization.fx.prepare",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.quantization.fx.quantization_patterns",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.quantization.fx.quantization_patterns",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.quantization.fx.quantization_patterns",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.quantization.fx.quantization_patterns",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.quantization.fx.quantization_types",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.quantization.fx.quantization_types",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.quantization.fx.quantization_types",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.quantization.fx.quantization_types",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.quantization.fx.utils",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.quantization.fx.utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.quantization.fx.utils",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.quantization.fx.utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.quantization.observer",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.quantization.observer",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.quantization.observer",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.quantization.observer",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.quantization.qconfig",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.quantization.qconfig",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.quantization.qconfig",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.quantization.qconfig",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.quantization.quant_type",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.quantization.quant_type",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.quantization.quant_type",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.quantization.quant_type",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.quantization.quantization_mappings",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.quantization.quantization_mappings",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.quantization.quantization_mappings",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.quantization.quantization_mappings",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.quantization.quantize",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.quantization.quantize",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.quantization.quantize",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.quantization.quantize",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.quantization.quantize_fx",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.quantization.quantize_fx",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.quantization.quantize_fx",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.quantization.quantize_fx",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.quantization.quantize_jit",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.quantization.quantize_jit",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.quantization.quantize_jit",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.quantization.quantize_jit",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.quantization.stubs",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.quantization.stubs",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.quantization.stubs",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.quantization.stubs",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.quantization.utils",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.quantization.utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.quantization.utils",
        "api_url": "https://pytorch.org/docs/stable/quantization.html#module-torch.quantization.utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.quasirandom",
        "api_url": "https://pytorch.org/docs/stable/torch.html#module-torch.quasirandom",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.quasirandom",
        "api_url": "https://pytorch.org/docs/stable/torch.html#module-torch.quasirandom",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.random",
        "api_url": "https://pytorch.org/docs/stable/random.html#module-torch.random",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.random",
        "api_url": "https://pytorch.org/docs/stable/random.html#module-torch.random",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.return_types",
        "api_url": "https://pytorch.org/docs/stable/torch.html#module-torch.return_types",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.return_types",
        "api_url": "https://pytorch.org/docs/stable/torch.html#module-torch.return_types",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.serialization",
        "api_url": "https://pytorch.org/docs/stable/torch.html#module-torch.serialization",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.serialization",
        "api_url": "https://pytorch.org/docs/stable/torch.html#module-torch.serialization",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.signal",
        "api_url": "https://pytorch.org/docs/stable/signal.html#module-torch.signal",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.signal",
        "api_url": "https://pytorch.org/docs/stable/signal.html#module-torch.signal",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.signal.windows",
        "api_url": "https://pytorch.org/docs/stable/signal.html#module-torch.signal.windows",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.signal.windows",
        "api_url": "https://pytorch.org/docs/stable/signal.html#module-torch.signal.windows",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.signal.windows.windows",
        "api_url": "https://pytorch.org/docs/stable/torch.html#module-torch.signal.windows.windows",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.signal.windows.windows",
        "api_url": "https://pytorch.org/docs/stable/torch.html#module-torch.signal.windows.windows",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.sparse",
        "api_url": "https://pytorch.org/docs/stable/sparse.html#module-torch.sparse",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.sparse",
        "api_url": "https://pytorch.org/docs/stable/sparse.html#module-torch.sparse",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.sparse.semi_structured",
        "api_url": "https://pytorch.org/docs/stable/torch.html#module-torch.sparse.semi_structured",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.sparse.semi_structured",
        "api_url": "https://pytorch.org/docs/stable/torch.html#module-torch.sparse.semi_structured",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.special",
        "api_url": "https://pytorch.org/docs/stable/special.html#module-torch.special",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.special",
        "api_url": "https://pytorch.org/docs/stable/special.html#module-torch.special",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.storage",
        "api_url": "https://pytorch.org/docs/stable/torch.html#module-torch.storage",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.storage",
        "api_url": "https://pytorch.org/docs/stable/torch.html#module-torch.storage",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.testing",
        "api_url": "https://pytorch.org/docs/stable/testing.html#module-torch.testing",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.testing",
        "api_url": "https://pytorch.org/docs/stable/testing.html#module-torch.testing",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.torch_version",
        "api_url": "https://pytorch.org/docs/stable/torch.html#module-torch.torch_version",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.torch_version",
        "api_url": "https://pytorch.org/docs/stable/torch.html#module-torch.torch_version",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.types",
        "api_url": "https://pytorch.org/docs/stable/torch.html#module-torch.types",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.types",
        "api_url": "https://pytorch.org/docs/stable/torch.html#module-torch.types",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.backcompat",
        "api_url": "https://pytorch.org/docs/stable/torch.html#module-torch.utils.backcompat",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.backcompat",
        "api_url": "https://pytorch.org/docs/stable/torch.html#module-torch.utils.backcompat",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.backend_registration",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.backend_registration",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.backend_registration",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.backend_registration",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.benchmark",
        "api_url": "https://pytorch.org/docs/stable/benchmark_utils.html#module-torch.utils.benchmark",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.benchmark",
        "api_url": "https://pytorch.org/docs/stable/benchmark_utils.html#module-torch.utils.benchmark",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.benchmark.examples",
        "api_url": "https://pytorch.org/docs/stable/benchmark_utils.html#module-torch.utils.benchmark.examples",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.benchmark.examples",
        "api_url": "https://pytorch.org/docs/stable/benchmark_utils.html#module-torch.utils.benchmark.examples",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.benchmark.examples.blas_compare_setup",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.benchmark.examples.blas_compare_setup",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.benchmark.examples.blas_compare_setup",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.benchmark.examples.blas_compare_setup",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.benchmark.examples.compare",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.benchmark.examples.compare",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.benchmark.examples.compare",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.benchmark.examples.compare",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.benchmark.examples.fuzzer",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.benchmark.examples.fuzzer",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.benchmark.examples.fuzzer",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.benchmark.examples.fuzzer",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.benchmark.examples.op_benchmark",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.benchmark.examples.op_benchmark",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.benchmark.examples.op_benchmark",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.benchmark.examples.op_benchmark",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.benchmark.examples.simple_timeit",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.benchmark.examples.simple_timeit",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.benchmark.examples.simple_timeit",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.benchmark.examples.simple_timeit",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.benchmark.examples.spectral_ops_fuzz_test",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.benchmark.examples.spectral_ops_fuzz_test",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.benchmark.examples.spectral_ops_fuzz_test",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.benchmark.examples.spectral_ops_fuzz_test",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.benchmark.op_fuzzers",
        "api_url": "https://pytorch.org/docs/stable/benchmark_utils.html#module-torch.utils.benchmark.op_fuzzers",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.benchmark.op_fuzzers",
        "api_url": "https://pytorch.org/docs/stable/benchmark_utils.html#module-torch.utils.benchmark.op_fuzzers",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.benchmark.op_fuzzers.binary",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.benchmark.op_fuzzers.binary",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.benchmark.op_fuzzers.binary",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.benchmark.op_fuzzers.binary",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.benchmark.op_fuzzers.sparse_binary",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.benchmark.op_fuzzers.sparse_binary",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.benchmark.op_fuzzers.sparse_binary",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.benchmark.op_fuzzers.sparse_binary",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.benchmark.op_fuzzers.sparse_unary",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.benchmark.op_fuzzers.sparse_unary",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.benchmark.op_fuzzers.sparse_unary",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.benchmark.op_fuzzers.sparse_unary",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.benchmark.op_fuzzers.spectral",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.benchmark.op_fuzzers.spectral",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.benchmark.op_fuzzers.spectral",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.benchmark.op_fuzzers.spectral",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.benchmark.op_fuzzers.unary",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.benchmark.op_fuzzers.unary",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.benchmark.op_fuzzers.unary",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.benchmark.op_fuzzers.unary",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.benchmark.utils",
        "api_url": "https://pytorch.org/docs/stable/benchmark_utils.html#module-torch.utils.benchmark.utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.benchmark.utils",
        "api_url": "https://pytorch.org/docs/stable/benchmark_utils.html#module-torch.utils.benchmark.utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.benchmark.utils.common",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.benchmark.utils.common",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.benchmark.utils.common",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.benchmark.utils.common",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.benchmark.utils.compare",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.benchmark.utils.compare",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.benchmark.utils.compare",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.benchmark.utils.compare",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.benchmark.utils.compile",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.benchmark.utils.compile",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.benchmark.utils.compile",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.benchmark.utils.compile",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.benchmark.utils.cpp_jit",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.benchmark.utils.cpp_jit",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.benchmark.utils.cpp_jit",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.benchmark.utils.cpp_jit",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.benchmark.utils.fuzzer",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.benchmark.utils.fuzzer",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.benchmark.utils.fuzzer",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.benchmark.utils.fuzzer",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.benchmark.utils.sparse_fuzzer",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.benchmark.utils.sparse_fuzzer",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.benchmark.utils.sparse_fuzzer",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.benchmark.utils.sparse_fuzzer",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.benchmark.utils.timer",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.benchmark.utils.timer",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.benchmark.utils.timer",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.benchmark.utils.timer",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.benchmark.utils.valgrind_wrapper",
        "api_url": "https://pytorch.org/docs/stable/benchmark_utils.html#module-torch.utils.benchmark.utils.valgrind_wrapper",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.benchmark.utils.valgrind_wrapper",
        "api_url": "https://pytorch.org/docs/stable/benchmark_utils.html#module-torch.utils.benchmark.utils.valgrind_wrapper",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.benchmark.utils.valgrind_wrapper.timer_interface",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.benchmark.utils.valgrind_wrapper.timer_interface",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.benchmark.utils.valgrind_wrapper.timer_interface",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.benchmark.utils.valgrind_wrapper.timer_interface",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.bottleneck",
        "api_url": "https://pytorch.org/docs/stable/bottleneck.html#module-torch.utils.bottleneck",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.bottleneck",
        "api_url": "https://pytorch.org/docs/stable/bottleneck.html#module-torch.utils.bottleneck",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.bundled_inputs",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.bundled_inputs",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.bundled_inputs",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.bundled_inputs",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.checkpoint",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.checkpoint",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.checkpoint",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.checkpoint",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.collect_env",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.collect_env",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.collect_env",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.collect_env",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.cpp_backtrace",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.cpp_backtrace",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.cpp_backtrace",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.cpp_backtrace",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.cpp_extension",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.cpp_extension",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.cpp_extension",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.cpp_extension",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.data",
        "api_url": "https://pytorch.org/docs/stable/data.html#module-torch.utils.data",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.data",
        "api_url": "https://pytorch.org/docs/stable/data.html#module-torch.utils.data",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.data.backward_compatibility",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.data.backward_compatibility",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.data.backward_compatibility",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.data.backward_compatibility",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.data.dataloader",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.data.dataloader",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.data.dataloader",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.data.dataloader",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.data.datapipes",
        "api_url": "https://pytorch.org/docs/stable/data.html#module-torch.utils.data.datapipes",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.data.datapipes",
        "api_url": "https://pytorch.org/docs/stable/data.html#module-torch.utils.data.datapipes",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.data.datapipes.dataframe",
        "api_url": "https://pytorch.org/docs/stable/data.html#module-torch.utils.data.datapipes.dataframe",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.data.datapipes.dataframe",
        "api_url": "https://pytorch.org/docs/stable/data.html#module-torch.utils.data.datapipes.dataframe",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.data.datapipes.dataframe.dataframe_wrapper",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.data.datapipes.dataframe.dataframe_wrapper",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.data.datapipes.dataframe.dataframe_wrapper",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.data.datapipes.dataframe.dataframe_wrapper",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.data.datapipes.dataframe.dataframes",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.data.datapipes.dataframe.dataframes",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.data.datapipes.dataframe.dataframes",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.data.datapipes.dataframe.dataframes",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.data.datapipes.dataframe.datapipes",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.data.datapipes.dataframe.datapipes",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.data.datapipes.dataframe.datapipes",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.data.datapipes.dataframe.datapipes",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.data.datapipes.dataframe.structures",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.data.datapipes.dataframe.structures",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.data.datapipes.dataframe.structures",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.data.datapipes.dataframe.structures",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.data.datapipes.datapipe",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.data.datapipes.datapipe",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.data.datapipes.datapipe",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.data.datapipes.datapipe",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.data.datapipes.gen_pyi",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.data.datapipes.gen_pyi",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.data.datapipes.gen_pyi",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.data.datapipes.gen_pyi",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.data.datapipes.iter",
        "api_url": "https://pytorch.org/docs/stable/data.html#module-torch.utils.data.datapipes.iter",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.data.datapipes.iter",
        "api_url": "https://pytorch.org/docs/stable/data.html#module-torch.utils.data.datapipes.iter",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.data.datapipes.iter.callable",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.data.datapipes.iter.callable",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.data.datapipes.iter.callable",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.data.datapipes.iter.callable",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.data.datapipes.iter.combinatorics",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.data.datapipes.iter.combinatorics",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.data.datapipes.iter.combinatorics",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.data.datapipes.iter.combinatorics",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.data.datapipes.iter.combining",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.data.datapipes.iter.combining",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.data.datapipes.iter.combining",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.data.datapipes.iter.combining",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.data.datapipes.iter.filelister",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.data.datapipes.iter.filelister",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.data.datapipes.iter.filelister",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.data.datapipes.iter.filelister",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.data.datapipes.iter.fileopener",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.data.datapipes.iter.fileopener",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.data.datapipes.iter.fileopener",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.data.datapipes.iter.fileopener",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.data.datapipes.iter.grouping",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.data.datapipes.iter.grouping",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.data.datapipes.iter.grouping",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.data.datapipes.iter.grouping",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.data.datapipes.iter.routeddecoder",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.data.datapipes.iter.routeddecoder",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.data.datapipes.iter.routeddecoder",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.data.datapipes.iter.routeddecoder",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.data.datapipes.iter.selecting",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.data.datapipes.iter.selecting",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.data.datapipes.iter.selecting",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.data.datapipes.iter.selecting",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.data.datapipes.iter.sharding",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.data.datapipes.iter.sharding",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.data.datapipes.iter.sharding",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.data.datapipes.iter.sharding",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.data.datapipes.iter.streamreader",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.data.datapipes.iter.streamreader",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.data.datapipes.iter.streamreader",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.data.datapipes.iter.streamreader",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.data.datapipes.iter.utils",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.data.datapipes.iter.utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.data.datapipes.iter.utils",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.data.datapipes.iter.utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.data.datapipes.map",
        "api_url": "https://pytorch.org/docs/stable/data.html#module-torch.utils.data.datapipes.map",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.data.datapipes.map",
        "api_url": "https://pytorch.org/docs/stable/data.html#module-torch.utils.data.datapipes.map",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.data.datapipes.map.callable",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.data.datapipes.map.callable",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.data.datapipes.map.callable",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.data.datapipes.map.callable",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.data.datapipes.map.combinatorics",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.data.datapipes.map.combinatorics",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.data.datapipes.map.combinatorics",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.data.datapipes.map.combinatorics",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.data.datapipes.map.combining",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.data.datapipes.map.combining",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.data.datapipes.map.combining",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.data.datapipes.map.combining",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.data.datapipes.map.grouping",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.data.datapipes.map.grouping",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.data.datapipes.map.grouping",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.data.datapipes.map.grouping",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.data.datapipes.map.utils",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.data.datapipes.map.utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.data.datapipes.map.utils",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.data.datapipes.map.utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.data.datapipes.utils",
        "api_url": "https://pytorch.org/docs/stable/data.html#module-torch.utils.data.datapipes.utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.data.datapipes.utils",
        "api_url": "https://pytorch.org/docs/stable/data.html#module-torch.utils.data.datapipes.utils",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.data.datapipes.utils.common",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.data.datapipes.utils.common",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.data.datapipes.utils.common",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.data.datapipes.utils.common",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.data.datapipes.utils.decoder",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.data.datapipes.utils.decoder",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.data.datapipes.utils.decoder",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.data.datapipes.utils.decoder",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.data.datapipes.utils.snapshot",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.data.datapipes.utils.snapshot",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.data.datapipes.utils.snapshot",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.data.datapipes.utils.snapshot",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.data.dataset",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.data.dataset",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.data.dataset",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.data.dataset",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.data.distributed",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.data.distributed",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.data.distributed",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.data.distributed",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.data.graph",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.data.graph",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.data.graph",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.data.graph",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.data.graph_settings",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.data.graph_settings",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.data.graph_settings",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.data.graph_settings",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.data.sampler",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.data.sampler",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.data.sampler",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.data.sampler",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.deterministic",
        "api_url": "https://pytorch.org/docs/stable/deterministic.html#module-torch.utils.deterministic",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.deterministic",
        "api_url": "https://pytorch.org/docs/stable/deterministic.html#module-torch.utils.deterministic",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.dlpack",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.dlpack",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.dlpack",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.dlpack",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.file_baton",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.file_baton",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.file_baton",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.file_baton",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.flop_counter",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.flop_counter",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.flop_counter",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.flop_counter",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.hipify",
        "api_url": "https://pytorch.org/docs/stable/torch.html#module-torch.utils.hipify",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.hipify",
        "api_url": "https://pytorch.org/docs/stable/torch.html#module-torch.utils.hipify",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.hipify.constants",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.hipify.constants",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.hipify.constants",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.hipify.constants",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.hipify.cuda_to_hip_mappings",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.hipify.cuda_to_hip_mappings",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.hipify.cuda_to_hip_mappings",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.hipify.cuda_to_hip_mappings",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.hipify.hipify_python",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.hipify.hipify_python",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.hipify.hipify_python",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.hipify.hipify_python",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.hipify.version",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.hipify.version",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.hipify.version",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.hipify.version",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.hooks",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.hooks",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.hooks",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.hooks",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.jit",
        "api_url": "https://pytorch.org/docs/stable/jit_utils.html#module-torch.utils.jit",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.jit",
        "api_url": "https://pytorch.org/docs/stable/jit_utils.html#module-torch.utils.jit",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.jit.log_extract",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.jit.log_extract",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.jit.log_extract",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.jit.log_extract",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.mkldnn",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.mkldnn",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.mkldnn",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.mkldnn",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.mobile_optimizer",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.mobile_optimizer",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.mobile_optimizer",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.mobile_optimizer",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.model_dump",
        "api_url": "https://pytorch.org/docs/stable/torch.html#module-torch.utils.model_dump",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.model_dump",
        "api_url": "https://pytorch.org/docs/stable/torch.html#module-torch.utils.model_dump",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.model_zoo",
        "api_url": "https://pytorch.org/docs/stable/model_zoo.html#module-torch.utils.model_zoo",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.model_zoo",
        "api_url": "https://pytorch.org/docs/stable/model_zoo.html#module-torch.utils.model_zoo",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.show_pickle",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.show_pickle",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.show_pickle",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.show_pickle",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.tensorboard",
        "api_url": "https://pytorch.org/docs/stable/tensorboard.html#module-torch.utils.tensorboard",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.tensorboard",
        "api_url": "https://pytorch.org/docs/stable/tensorboard.html#module-torch.utils.tensorboard",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.tensorboard.summary",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.tensorboard.summary",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.tensorboard.summary",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.tensorboard.summary",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.tensorboard.writer",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.tensorboard.writer",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.tensorboard.writer",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.tensorboard.writer",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.throughput_benchmark",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.throughput_benchmark",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.throughput_benchmark",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.throughput_benchmark",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.viz",
        "api_url": "https://pytorch.org/docs/stable/torch.html#module-torch.utils.viz",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.viz",
        "api_url": "https://pytorch.org/docs/stable/torch.html#module-torch.utils.viz",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.weak",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.weak",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.weak",
        "api_url": "https://pytorch.org/docs/stable/utils.html#module-torch.utils.weak",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.version",
        "api_url": "https://pytorch.org/docs/stable/torch.html#module-torch.version",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.version",
        "api_url": "https://pytorch.org/docs/stable/torch.html#module-torch.version",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.xpu",
        "api_url": "https://pytorch.org/docs/stable/xpu.html#module-torch.xpu",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.xpu",
        "api_url": "https://pytorch.org/docs/stable/xpu.html#module-torch.xpu",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.xpu.random",
        "api_url": "https://pytorch.org/docs/stable/xpu.html#module-torch.xpu.random",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.xpu.random",
        "api_url": "https://pytorch.org/docs/stable/xpu.html#module-torch.xpu.random",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.xpu.streams",
        "api_url": "https://pytorch.org/docs/stable/xpu.html#module-torch.xpu.streams",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.xpu.streams",
        "api_url": "https://pytorch.org/docs/stable/xpu.html#module-torch.xpu.streams",
        "api_signature": "",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.onnx.JitScalarType.torch_name",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.onnx.JitScalarType.html#torch.onnx.JitScalarType.torch_name",
        "api_signature": "torch_name()",
        "api_description": "Convert a JitScalarType to a torch type name.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.checkpoint.format_utils.torch_save_to_dcp",
        "api_url": "https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.format_utils.torch_save_to_dcp",
        "api_signature": "torch.distributed.checkpoint.format_utils.torch_save_to_dcp(torch_save_path, dcp_checkpoint_dir)",
        "api_description": "Given the location of a torch save file, converts it into a DCP checkpoint.",
        "return_value": "",
        "parameters": "torch_save_path (Union[str, PathLike]) – Filename to store the converted Torch save file.\ndcp_checkpoint_dir (Union[str, PathLike]) – Directory containing the DCP checkpoint.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.autograd.profiler.profile.total_average",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.autograd.profiler.profile.total_average.html#torch.autograd.profiler.profile.total_average",
        "api_signature": "profile.total_average()",
        "api_description": "Averages all events.",
        "return_value": "A FunctionEventAvg object.\n",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.multinomial.Multinomial.total_count",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.multinomial.Multinomial.total_count",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.trace",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.trace.html#torch.trace",
        "api_signature": "torch.trace(input)",
        "api_description": "Returns the sum of the elements of the diagonal of the input 2-D matrix.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.jit.trace",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.jit.trace.html#torch.jit.trace",
        "api_signature": "torch.jit.trace(func, example_inputs=None, optimize=None, check_trace=True, check_inputs=None, check_tolerance=1e-05, strict=True, _force_outplace=False, _module_class=None, _compilation_unit=<torch.jit.CompilationUnit object>, example_kwarg_inputs=None, _store_inputs=True)",
        "api_description": "Trace a function and return an executable  or ScriptFunction that will be optimized using just-in-time compilation.",
        "return_value": "If func is nn.Module or forward of nn.Module, trace returns\na ScriptModule object with a single forward method\ncontaining the traced code.  The returned ScriptModule will\nhave the same set of sub-modules and parameters as the original\nnn.Module.  If func is a standalone function, trace\nreturns ScriptFunction.\n",
        "parameters": "func (callable or torch.nn.Module) – A Python function or torch.nn.Module\nthat will be run with example_inputs. func arguments and return\nvalues  must be tensors or (possibly nested) tuples that contain\ntensors. When a module is passed torch.jit.trace, only the\nforward method is run and traced (see torch.jit.trace for details).\nexample_inputs (tuple or torch.Tensor or None, optional) – A tuple of example\ninputs that will be passed to the function while tracing.\nDefault: None. Either this argument or example_kwarg_inputs\nshould be specified. The resulting trace can be run with inputs of\ndifferent types and shapes assuming the traced operations support those\ntypes and shapes. example_inputs may also be a single Tensor in which\ncase it is automatically wrapped in a tuple. When the value is None,\nexample_kwarg_inputs should be specified.\ncheck_trace (bool, optional) – Check if the same inputs run through\ntraced code produce the same outputs. Default: True. You might want\nto disable this if, for example, your network contains non-\ndeterministic ops or if you are sure that the network is correct despite\na checker failure.\ncheck_inputs (list of tuples, optional) – A list of tuples of input\narguments that should be used to check the trace against what is\nexpected. Each tuple is equivalent to a set of input arguments that\nwould be specified in example_inputs. For best results, pass in\na set of checking inputs representative of the space of shapes and\ntypes of inputs you expect the network to see.  If not specified,\nthe original example_inputs are used for checking\ncheck_tolerance (float, optional) – Floating-point comparison tolerance\nto use in the checker procedure.  This can be used to relax the\nchecker strictness in the event that results diverge numerically\nfor a known reason, such as operator fusion.\nstrict (bool, optional) – run the tracer in a strict mode or not\n(default: True). Only turn this off when you want the tracer to\nrecord your mutable container types (currently list/dict)\nand you are sure that the container you are using in your\nproblem is a constant structure and does not get used as\ncontrol flow (if, for) conditions.\nexample_kwarg_inputs (dict, optional) – This parameter is a pack of keyword\narguments of example inputs that will be passed to the function while\ntracing. Default: None. Either this argument or example_inputs\nshould be specified. The dict will be unpacking by the arguments name\nof the traced function. If the keys of the dict don’t not match with\nthe traced function’s arguments name, a runtime exception will be raised.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.Tracer.trace",
        "api_url": "https://pytorch.org/docs/stable/fx.html#torch.fx.Tracer.trace",
        "api_signature": "trace(root, concrete_args=None)",
        "api_description": "Trace root and return the corresponding FX Graph representation. root\ncan either be an nn.Module instance or a Python callable.",
        "return_value": "A Graph representing the semantics of the passed-in root.\n",
        "parameters": "root (Union[Module, Callable]) – Either a Module or a function to be\ntraced through. Backwards-compatibility for this parameter is\nguaranteed.\nconcrete_args (Optional[Dict[str, any]]) – Concrete arguments that should\nnot be treated as Proxies. This parameter is experimental and\nits backwards-compatibility is NOT guaranteed.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.trace",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.trace.html#torch.Tensor.trace",
        "api_signature": "Tensor.trace()",
        "api_description": "See torch.trace()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.jit.trace_module",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.jit.trace_module.html#torch.jit.trace_module",
        "api_signature": "torch.jit.trace_module(mod, inputs, optimize=None, check_trace=True, check_inputs=None, check_tolerance=1e-05, strict=True, _force_outplace=False, _module_class=None, _compilation_unit=<torch.jit.CompilationUnit object>, example_inputs_is_kwarg=False, _store_inputs=True)",
        "api_description": "Trace a module and return an executable ScriptModule that will be optimized using just-in-time compilation.",
        "return_value": "A ScriptModule object with a single forward method containing the traced code.\nWhen func is a torch.nn.Module, the returned ScriptModule will have the same set of\nsub-modules and parameters as func.\n",
        "parameters": "mod (torch.nn.Module) – A torch.nn.Module containing methods whose names are\nspecified in inputs. The given methods will be compiled\nas a part of a single ScriptModule.\ninputs (dict) – A dict containing sample inputs indexed by method names in mod.\nThe inputs will be passed to methods whose names correspond to inputs’\nkeys while tracing.\n{ 'forward' : example_forward_input, 'method2': example_method2_input}\ncheck_trace (bool, optional) – Check if the same inputs run through\ntraced code produce the same outputs. Default: True. You might want\nto disable this if, for example, your network contains non-\ndeterministic ops or if you are sure that the network is correct despite\na checker failure.\ncheck_inputs (list of dicts, optional) – A list of dicts of input arguments that should be used\nto check the trace against what is expected. Each tuple\nis equivalent to a set of input arguments that would\nbe specified in inputs. For best results, pass in a\nset of checking inputs representative of the space of\nshapes and types of inputs you expect the network to see.\nIf not specified, the original inputs are used for checking\ncheck_tolerance (float, optional) – Floating-point comparison tolerance to use in the checker procedure.\nThis can be used to relax the checker strictness in the event that\nresults diverge numerically for a known reason, such as operator fusion.\nexample_inputs_is_kwarg (bool, optional) – This parameter indicate whether the example inputs is a pack\npack of keyword arguments. Default: False.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.Tracer",
        "api_url": "https://pytorch.org/docs/stable/fx.html#torch.fx.Tracer",
        "api_signature": "torch.fx.Tracer(autowrap_modules=(math,)",
        "api_description": "Tracer is the class that implements the symbolic tracing functionality\nof torch.fx.symbolic_trace. A call to symbolic_trace(m) is equivalent\nto Tracer().trace(m).",
        "return_value": "The return value from the Module call. In the case that a call_module\nnode was emitted, this is a Proxy value. Otherwise, it is whatever\nvalue was returned from the Module invocation.\nThe value a converted into the appropriate Argument\nThe return value from the getattr call.\nA Graph representing the semantics of the passed-in root.\n",
        "parameters": "m (Module) – The module for which a call is being emitted\nforward (Callable) – The forward() method of the Module to be invoked\nargs (Tuple) – args of the module callsite\nkwargs (Dict) – kwargs of the module callsite\na (Any) – The value to be emitted as an Argument in the Graph.\nattr (str) – The name of the attribute being queried\nattr_val (Any) – The value of the attribute\nparameter_proxy_cache (Dict[str, Any]) – A cache of attr names to proxies\nm (Module) – The module being queried about\nmodule_qualified_name (str) – The path to root of this module. For example,\nif you have a module hierarchy where submodule foo contains\nsubmodule bar, which contains submodule baz, that module will\nappear with the qualified name foo.bar.baz here.\nmod (str) – The Module to retrieve the qualified name for.\nroot (Union[Module, Callable]) – Either a Module or a function to be\ntraced through. Backwards-compatibility for this parameter is\nguaranteed.\nconcrete_args (Optional[Dict[str, any]]) – Concrete arguments that should\nnot be treated as Proxies. This parameter is experimental and\nits backwards-compatibility is NOT guaranteed.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.jit.ScriptModule.train",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.jit.ScriptModule.html#torch.jit.ScriptModule.train",
        "api_signature": "train(mode=True)",
        "api_description": "Set the module in training mode.",
        "return_value": "self\n",
        "parameters": "mode (bool) – whether to set training mode (True) or evaluation\nmode (False). Default: True.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.Module.train",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.train",
        "api_signature": "train(mode=True)",
        "api_description": "Set the module in training mode.",
        "return_value": "self\n",
        "parameters": "mode (bool) – whether to set training mode (True) or evaluation\nmode (False). Default: True.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.transforms.Transform",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.transforms.Transform",
        "api_signature": "torch.distributions.transforms.Transform(cache_size=0)",
        "api_description": "Abstract class for invertable transformations with computable log\ndet jacobians. They are primarily used in\ntorch.distributions.TransformedDistribution.",
        "return_value": "",
        "parameters": "cache_size (int) – Size of cache. If zero, no caching is done. If one,\nthe latest single value is cached. Only 0 and 1 are supported.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.Transformer.transform",
        "api_url": "https://pytorch.org/docs/stable/fx.html#torch.fx.Transformer.transform",
        "api_signature": "transform()",
        "api_description": "Transform self.module and return the transformed\nGraphModule.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.benchmark.FunctionCounts.transform",
        "api_url": "https://pytorch.org/docs/stable/benchmark_utils.html#torch.utils.benchmark.FunctionCounts.transform",
        "api_signature": "transform(map_fn)",
        "api_description": "Apply map_fn to all of the function names.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.checkpoint.DefaultSavePlanner.transform_object",
        "api_url": "https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.DefaultSavePlanner.transform_object",
        "api_signature": "transform_object(write_item, object)",
        "api_description": "Extension from the planner interface to make it easy to extend the default planner.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.checkpoint.DefaultLoadPlanner.transform_tensor",
        "api_url": "https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.DefaultLoadPlanner.transform_tensor",
        "api_signature": "transform_tensor(read_item, tensor)",
        "api_description": "Extension from the planner interface to make it easy to extend the default planner.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.transformed_distribution.TransformedDistribution",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.transformed_distribution.TransformedDistribution",
        "api_signature": "torch.distributions.transformed_distribution.TransformedDistribution(base_distribution, transforms, validate_args=None)",
        "api_description": "Bases: Distribution",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.Transformer",
        "api_url": "https://pytorch.org/docs/stable/fx.html#torch.fx.Transformer",
        "api_signature": "torch.fx.Transformer(module)",
        "api_description": "Transformer is a special type of interpreter that produces a\nnew Module. It exposes a transform() method that returns\nthe transformed Module. Transformer does not require\narguments to run, as Interpreter does. Transformer works\nentirely symbolically.",
        "return_value": "",
        "parameters": "module (GraphModule) – The Module to be transformed.\ntarget (Target) – The call target for this node. See\nNode for\ndetails on semantics\nargs (Tuple) – Tuple of positional args for this invocation\nkwargs (Dict) – Dict of keyword arguments for this invocation\ntarget (Target) – The call target for this node. See\nNode for\ndetails on semantics\nargs (Tuple) – Tuple of positional args for this invocation\nkwargs (Dict) – Dict of keyword arguments for this invocation",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.Transformer",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html#torch.nn.Transformer",
        "api_signature": "torch.nn.Transformer(d_model=512, nhead=8, num_encoder_layers=6, num_decoder_layers=6, dim_feedforward=2048, dropout=0.1, activation=<function relu>, custom_encoder=None, custom_decoder=None, layer_norm_eps=1e-05, batch_first=False, norm_first=False, bias=True, device=None, dtype=None)",
        "api_description": "A transformer model.",
        "return_value": "",
        "parameters": "d_model (int) – the number of expected features in the encoder/decoder inputs (default=512).\nnhead (int) – the number of heads in the multiheadattention models (default=8).\nnum_encoder_layers (int) – the number of sub-encoder-layers in the encoder (default=6).\nnum_decoder_layers (int) – the number of sub-decoder-layers in the decoder (default=6).\ndim_feedforward (int) – the dimension of the feedforward network model (default=2048).\ndropout (float) – the dropout value (default=0.1).\nactivation (Union[str, Callable[[Tensor], Tensor]]) – the activation function of encoder/decoder intermediate layer, can be a string\n(“relu” or “gelu”) or a unary callable. Default: relu\ncustom_encoder (Optional[Any]) – custom encoder (default=None).\ncustom_decoder (Optional[Any]) – custom decoder (default=None).\nlayer_norm_eps (float) – the eps value in layer normalization components (default=1e-5).\nbatch_first (bool) – If True, then the input and output tensors are provided\nas (batch, seq, feature). Default: False (seq, batch, feature).\nnorm_first (bool) – if True, encoder and decoder layers will perform LayerNorms before\nother attention and feedforward operations, otherwise after. Default: False (after).\nbias (bool) – If set to False, Linear and LayerNorm layers will not learn an additive\nbias. Default: True.\nsrc (Tensor) – the sequence to the encoder (required).\ntgt (Tensor) – the sequence to the decoder (required).\nsrc_mask (Optional[Tensor]) – the additive mask for the src sequence (optional).\ntgt_mask (Optional[Tensor]) – the additive mask for the tgt sequence (optional).\nmemory_mask (Optional[Tensor]) – the additive mask for the encoder output (optional).\nsrc_key_padding_mask (Optional[Tensor]) – the Tensor mask for src keys per batch (optional).\ntgt_key_padding_mask (Optional[Tensor]) – the Tensor mask for tgt keys per batch (optional).\nmemory_key_padding_mask (Optional[Tensor]) – the Tensor mask for memory keys per batch (optional).\nsrc_is_causal (Optional[bool]) – If specified, applies a causal mask as src_mask.\nDefault: None; try to detect a causal mask.\nWarning:\nsrc_is_causal provides a hint that src_mask is\nthe causal mask. Providing incorrect hints can result in\nincorrect execution, including forward and backward\ncompatibility.\ntgt_is_causal (Optional[bool]) – If specified, applies a causal mask as tgt_mask.\nDefault: None; try to detect a causal mask.\nWarning:\ntgt_is_causal provides a hint that tgt_mask is\nthe causal mask. Providing incorrect hints can result in\nincorrect execution, including forward and backward\ncompatibility.\nmemory_is_causal (bool) – If specified, applies a causal mask as\nmemory_mask.\nDefault: False.\nWarning:\nmemory_is_causal provides a hint that\nmemory_mask is the causal mask. Providing incorrect\nhints can result in incorrect execution, including\nforward and backward compatibility.",
        "input_shape": "\nsrc: (S,E)(S, E)(S,E) for unbatched input, (S,N,E)(S, N, E)(S,N,E) if batch_first=False or\n(N, S, E) if batch_first=True.\ntgt: (T,E)(T, E)(T,E) for unbatched input, (T,N,E)(T, N, E)(T,N,E) if batch_first=False or\n(N, T, E) if batch_first=True.\nsrc_mask: (S,S)(S, S)(S,S) or (N⋅num_heads,S,S)(N\\cdot\\text{num\\_heads}, S, S)(N⋅num_heads,S,S).\ntgt_mask: (T,T)(T, T)(T,T) or (N⋅num_heads,T,T)(N\\cdot\\text{num\\_heads}, T, T)(N⋅num_heads,T,T).\nmemory_mask: (T,S)(T, S)(T,S).\nsrc_key_padding_mask: (S)(S)(S) for unbatched input otherwise (N,S)(N, S)(N,S).\ntgt_key_padding_mask: (T)(T)(T) for unbatched input otherwise (N,T)(N, T)(N,T).\nmemory_key_padding_mask: (S)(S)(S) for unbatched input otherwise (N,S)(N, S)(N,S).\n\nNote: [src/tgt/memory]_mask ensures that position iii is allowed to attend the unmasked\npositions. If a BoolTensor is provided, positions with True\nare not allowed to attend while False values will be unchanged. If a FloatTensor\nis provided, it will be added to the attention weight.\n[src/tgt/memory]_key_padding_mask provides specified elements in the key to be ignored by\nthe attention. If a BoolTensor is provided, the positions with the\nvalue of True will be ignored while the position with the value of False will be unchanged.\n\noutput: (T,E)(T, E)(T,E) for unbatched input, (T,N,E)(T, N, E)(T,N,E) if batch_first=False or\n(N, T, E) if batch_first=True.\n\nNote: Due to the multi-head attention architecture in the transformer model,\nthe output sequence length of a transformer is same as the input sequence\n(i.e. target) length of the decoder.\nwhere SSS is the source sequence length, TTT is the target sequence length, NNN is the\nbatch size, EEE is the feature number\n",
        "notes": "",
        "code_example": ">>> transformer_model = nn.Transformer(nhead=16, num_encoder_layers=12)\n>>> src = torch.rand((10, 32, 512))\n>>> tgt = torch.rand((20, 32, 512))\n>>> out = transformer_model(src, tgt)\n\n\n"
    },
    {
        "api_name": "torch.nn.TransformerDecoder",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.TransformerDecoder.html#torch.nn.TransformerDecoder",
        "api_signature": "torch.nn.TransformerDecoder(decoder_layer, num_layers, norm=None)",
        "api_description": "TransformerDecoder is a stack of N decoder layers.",
        "return_value": "",
        "parameters": "decoder_layer (TransformerDecoderLayer) – an instance of the TransformerDecoderLayer() class (required).\nnum_layers (int) – the number of sub-decoder-layers in the decoder (required).\nnorm (Optional[Module]) – the layer normalization component (optional).\ntgt (Tensor) – the sequence to the decoder (required).\nmemory (Tensor) – the sequence from the last layer of the encoder (required).\ntgt_mask (Optional[Tensor]) – the mask for the tgt sequence (optional).\nmemory_mask (Optional[Tensor]) – the mask for the memory sequence (optional).\ntgt_key_padding_mask (Optional[Tensor]) – the mask for the tgt keys per batch (optional).\nmemory_key_padding_mask (Optional[Tensor]) – the mask for the memory keys per batch (optional).\ntgt_is_causal (Optional[bool]) – If specified, applies a causal mask as tgt mask.\nDefault: None; try to detect a causal mask.\nWarning:\ntgt_is_causal provides a hint that tgt_mask is\nthe causal mask. Providing incorrect hints can result in\nincorrect execution, including forward and backward\ncompatibility.\nmemory_is_causal (bool) – If specified, applies a causal mask as\nmemory mask.\nDefault: False.\nWarning:\nmemory_is_causal provides a hint that\nmemory_mask is the causal mask. Providing incorrect\nhints can result in incorrect execution, including\nforward and backward compatibility.",
        "input_shape": "see the docs in Transformer.\n",
        "notes": "",
        "code_example": ">>> decoder_layer = nn.TransformerDecoderLayer(d_model=512, nhead=8)\n>>> transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=6)\n>>> memory = torch.rand(10, 32, 512)\n>>> tgt = torch.rand(20, 32, 512)\n>>> out = transformer_decoder(tgt, memory)\n\n\n"
    },
    {
        "api_name": "torch.nn.TransformerDecoderLayer",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.TransformerDecoderLayer.html#torch.nn.TransformerDecoderLayer",
        "api_signature": "torch.nn.TransformerDecoderLayer(d_model, nhead, dim_feedforward=2048, dropout=0.1, activation=<function relu>, layer_norm_eps=1e-05, batch_first=False, norm_first=False, bias=True, device=None, dtype=None)",
        "api_description": "TransformerDecoderLayer is made up of self-attn, multi-head-attn and feedforward network.",
        "return_value": "",
        "parameters": "d_model (int) – the number of expected features in the input (required).\nnhead (int) – the number of heads in the multiheadattention models (required).\ndim_feedforward (int) – the dimension of the feedforward network model (default=2048).\ndropout (float) – the dropout value (default=0.1).\nactivation (Union[str, Callable[[Tensor], Tensor]]) – the activation function of the intermediate layer, can be a string\n(“relu” or “gelu”) or a unary callable. Default: relu\nlayer_norm_eps (float) – the eps value in layer normalization components (default=1e-5).\nbatch_first (bool) – If True, then the input and output tensors are provided\nas (batch, seq, feature). Default: False (seq, batch, feature).\nnorm_first (bool) – if True, layer norm is done prior to self attention, multihead\nattention and feedforward operations, respectively. Otherwise it’s done after.\nDefault: False (after).\nbias (bool) – If set to False, Linear and LayerNorm layers will not learn an additive\nbias. Default: True.\ntgt (Tensor) – the sequence to the decoder layer (required).\nmemory (Tensor) – the sequence from the last layer of the encoder (required).\ntgt_mask (Optional[Tensor]) – the mask for the tgt sequence (optional).\nmemory_mask (Optional[Tensor]) – the mask for the memory sequence (optional).\ntgt_key_padding_mask (Optional[Tensor]) – the mask for the tgt keys per batch (optional).\nmemory_key_padding_mask (Optional[Tensor]) – the mask for the memory keys per batch (optional).\ntgt_is_causal (bool) – If specified, applies a causal mask as tgt mask.\nDefault: False.\nWarning:\ntgt_is_causal provides a hint that tgt_mask is\nthe causal mask. Providing incorrect hints can result in\nincorrect execution, including forward and backward\ncompatibility.\nmemory_is_causal (bool) – If specified, applies a causal mask as\nmemory mask.\nDefault: False.\nWarning:\nmemory_is_causal provides a hint that\nmemory_mask is the causal mask. Providing incorrect\nhints can result in incorrect execution, including\nforward and backward compatibility.",
        "input_shape": "see the docs in Transformer.\n",
        "notes": "",
        "code_example": ">>> decoder_layer = nn.TransformerDecoderLayer(d_model=512, nhead=8)\n>>> memory = torch.rand(10, 32, 512)\n>>> tgt = torch.rand(20, 32, 512)\n>>> out = decoder_layer(tgt, memory)\n\n\n"
    },
    {
        "api_name": "torch.nn.TransformerEncoder",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoder.html#torch.nn.TransformerEncoder",
        "api_signature": "torch.nn.TransformerEncoder(encoder_layer, num_layers, norm=None, enable_nested_tensor=True, mask_check=True)",
        "api_description": "TransformerEncoder is a stack of N encoder layers.",
        "return_value": "",
        "parameters": "encoder_layer (TransformerEncoderLayer) – an instance of the TransformerEncoderLayer() class (required).\nnum_layers (int) – the number of sub-encoder-layers in the encoder (required).\nnorm (Optional[Module]) – the layer normalization component (optional).\nenable_nested_tensor (bool) – if True, input will automatically convert to nested tensor\n(and convert back on output). This will improve the overall performance of\nTransformerEncoder when padding rate is high. Default: True (enabled).\nsrc (Tensor) – the sequence to the encoder (required).\nmask (Optional[Tensor]) – the mask for the src sequence (optional).\nsrc_key_padding_mask (Optional[Tensor]) – the mask for the src keys per batch (optional).\nis_causal (Optional[bool]) – If specified, applies a causal mask as mask.\nDefault: None; try to detect a causal mask.\nWarning:\nis_causal provides a hint that mask is the\ncausal mask. Providing incorrect hints can result in\nincorrect execution, including forward and backward\ncompatibility.",
        "input_shape": "see the docs in Transformer.\n",
        "notes": "",
        "code_example": ">>> encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8)\n>>> transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=6)\n>>> src = torch.rand(10, 32, 512)\n>>> out = transformer_encoder(src)\n\n\n"
    },
    {
        "api_name": "torch.nn.TransformerEncoderLayer",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoderLayer.html#torch.nn.TransformerEncoderLayer",
        "api_signature": "torch.nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward=2048, dropout=0.1, activation=<function relu>, layer_norm_eps=1e-05, batch_first=False, norm_first=False, bias=True, device=None, dtype=None)",
        "api_description": "TransformerEncoderLayer is made up of self-attn and feedforward network.",
        "return_value": "",
        "parameters": "d_model (int) – the number of expected features in the input (required).\nnhead (int) – the number of heads in the multiheadattention models (required).\ndim_feedforward (int) – the dimension of the feedforward network model (default=2048).\ndropout (float) – the dropout value (default=0.1).\nactivation (Union[str, Callable[[Tensor], Tensor]]) – the activation function of the intermediate layer, can be a string\n(“relu” or “gelu”) or a unary callable. Default: relu\nlayer_norm_eps (float) – the eps value in layer normalization components (default=1e-5).\nbatch_first (bool) – If True, then the input and output tensors are provided\nas (batch, seq, feature). Default: False (seq, batch, feature).\nnorm_first (bool) – if True, layer norm is done prior to attention and feedforward\noperations, respectively. Otherwise it’s done after. Default: False (after).\nbias (bool) – If set to False, Linear and LayerNorm layers will not learn an additive\nbias. Default: True.\nsrc (Tensor) – the sequence to the encoder layer (required).\nsrc_mask (Optional[Tensor]) – the mask for the src sequence (optional).\nsrc_key_padding_mask (Optional[Tensor]) – the mask for the src keys per batch (optional).\nis_causal (bool) – If specified, applies a causal mask as src mask.\nDefault: False.\nWarning:\nis_causal provides a hint that src_mask is the\ncausal mask. Providing incorrect hints can result in\nincorrect execution, including forward and backward\ncompatibility.",
        "input_shape": "see the docs in Transformer.\n",
        "notes": "",
        "code_example": ">>> encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8)\n>>> src = torch.rand(10, 32, 512)\n>>> out = encoder_layer(src)\n\n\n"
    },
    {
        "api_name": "torch.transpose",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.transpose.html#torch.transpose",
        "api_signature": "torch.transpose(input, dim0, dim1)",
        "api_description": "Returns a tensor that is a transposed version of input.\nThe given dimensions dim0 and dim1 are swapped.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.\ndim0 (int) – the first dimension to be transposed\ndim1 (int) – the second dimension to be transposed",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.transpose",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.transpose.html#torch.Tensor.transpose",
        "api_signature": "Tensor.transpose(dim0, dim1)",
        "api_description": "See torch.transpose()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.transpose_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.transpose_.html#torch.Tensor.transpose_",
        "api_signature": "Tensor.transpose_(dim0, dim1)",
        "api_description": "In-place version of transpose()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.trapezoid",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.trapezoid.html#torch.trapezoid",
        "api_signature": "torch.trapezoid(y, x=None, *, dx=None, dim=-1)",
        "api_description": "Computes the trapezoidal rule along\ndim. By default the spacing between elements is assumed to be 1, but\ndx can be used to specify a different constant spacing, and x can be\nused to specify arbitrary spacing along dim.",
        "return_value": "",
        "parameters": "y (Tensor) – Values to use when computing the trapezoidal rule.\nx (Tensor) – If specified, defines spacing between values as specified above.\ndx (float) – constant spacing between values. If neither x or dx\nare specified then this defaults to 1. Effectively multiplies the result by its value.\ndim (int) – The dimension along which to compute the trapezoidal rule.\nThe last (inner-most) dimension by default.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.trapz",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.trapz.html#torch.trapz",
        "api_signature": "torch.trapz(y, x, *, dim=-1)",
        "api_description": "Alias for torch.trapezoid().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.triangular_solve",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.triangular_solve.html#torch.triangular_solve",
        "api_signature": "torch.triangular_solve(b, A, upper=True, transpose=False, unitriangular=False, *, out=None)",
        "api_description": "Solves a system of equations with a square upper or lower triangular invertible matrix AAA\nand multiple right-hand sides bbb.",
        "return_value": "A namedtuple (solution, cloned_coefficient) where cloned_coefficient\nis a clone of AAA and solution is the solution XXX to AX=bAX = bAX=b\n(or whatever variant of the system of equations, depending on the keyword arguments.)\n",
        "parameters": "b (Tensor) – multiple right-hand sides of size (∗,m,k)(*, m, k)(∗,m,k) where\n∗*∗ is zero of more batch dimensions\nA (Tensor) – the input triangular coefficient matrix of size (∗,m,m)(*, m, m)(∗,m,m)\nwhere ∗*∗ is zero or more batch dimensions\nupper (bool, optional) – whether AAA is upper or lower triangular. Default: True.\ntranspose (bool, optional) – solves op(A)X = b where op(A) = A^T if this flag is True,\nand op(A) = A if it is False. Default: False.\nunitriangular (bool, optional) – whether AAA is unit triangular.\nIf True, the diagonal elements of AAA are assumed to be\n1 and not referenced from AAA. Default: False.\nout ((Tensor, Tensor), optional) – tuple of two tensors to write\nthe output to. Ignored if None. Default: None.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.triangular_solve",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.triangular_solve.html#torch.Tensor.triangular_solve",
        "api_signature": "Tensor.triangular_solve(A, upper=True, transpose=False, unitriangular=False)",
        "api_description": "See torch.triangular_solve()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.tril",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.tril.html#torch.tril",
        "api_signature": "torch.tril(input, diagonal=0, *, out=None)",
        "api_description": "Returns the lower triangular part of the matrix (2-D tensor) or batch of matrices\ninput, the other elements of the result tensor out are set to 0.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.\ndiagonal (int, optional) – the diagonal to consider\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.tril",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.tril.html#torch.Tensor.tril",
        "api_signature": "Tensor.tril(diagonal=0)",
        "api_description": "See torch.tril()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.tril_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.tril_.html#torch.Tensor.tril_",
        "api_signature": "Tensor.tril_(diagonal=0)",
        "api_description": "In-place version of tril()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.tril_indices",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.tril_indices.html#torch.tril_indices",
        "api_signature": "torch.tril_indices(row, col, offset=0, *, dtype=torch.long, device='cpu', layout=torch.strided)",
        "api_description": "Returns the indices of the lower triangular part of a row-by-\ncol matrix in a 2-by-N Tensor, where the first row contains row\ncoordinates of all indices and the second row contains column coordinates.\nIndices are ordered based on rows and then columns.",
        "return_value": "",
        "parameters": "row (int) – number of rows in the 2-D matrix.\ncol (int) – number of columns in the 2-D matrix.\noffset (int) – diagonal offset from the main diagonal.\nDefault: if not provided, 0.\ndtype (torch.dtype, optional) – the desired data type of returned tensor.\nDefault: if None, torch.long.\ndevice (torch.device, optional) – the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_device()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types.\nlayout (torch.layout, optional) – currently only support torch.strided.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.functional.triplet_margin_loss",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.functional.triplet_margin_loss.html#torch.nn.functional.triplet_margin_loss",
        "api_signature": "torch.nn.functional.triplet_margin_loss(anchor, positive, negative, margin=1.0, p=2, eps=1e-06, swap=False, size_average=None, reduce=None, reduction='mean')",
        "api_description": "Compute the triplet loss between given input tensors and a margin greater than 0.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.functional.triplet_margin_with_distance_loss",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.functional.triplet_margin_with_distance_loss.html#torch.nn.functional.triplet_margin_with_distance_loss",
        "api_signature": "torch.nn.functional.triplet_margin_with_distance_loss(anchor, positive, negative, *, distance_function=None, margin=1.0, swap=False, reduction='mean')",
        "api_description": "Compute the triplet margin loss for input tensors using a custom distance function.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.TripletMarginLoss",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.TripletMarginLoss.html#torch.nn.TripletMarginLoss",
        "api_signature": "torch.nn.TripletMarginLoss(margin=1.0, p=2.0, eps=1e-06, swap=False, size_average=None, reduce=None, reduction='mean')",
        "api_description": "Creates a criterion that measures the triplet loss given an input\ntensors x1x1x1, x2x2x2, x3x3x3 and a margin with a value greater than 000.\nThis is used for measuring a relative similarity between samples. A triplet\nis composed by a, p and n (i.e., anchor, positive examples and negative\nexamples respectively). The shapes of all input tensors should be\n(N,D)(N, D)(N,D).",
        "return_value": "",
        "parameters": "margin (float, optional) – Default: 111.\np (int, optional) – The norm degree for pairwise distance. Default: 222.\neps (float, optional) – Small constant for numerical stability. Default: 1e−61e-61e−6.\nswap (bool, optional) – The distance swap is described in detail in the paper\nLearning shallow convolutional feature descriptors with triplet losses by\nV. Balntas, E. Riba et al. Default: False.\nsize_average (bool, optional) – Deprecated (see reduction). By default,\nthe losses are averaged over each loss element in the batch. Note that for\nsome losses, there are multiple elements per sample. If the field size_average\nis set to False, the losses are instead summed for each minibatch. Ignored\nwhen reduce is False. Default: True\nreduce (bool, optional) – Deprecated (see reduction). By default, the\nlosses are averaged or summed over observations for each minibatch depending\non size_average. When reduce is False, returns a loss per\nbatch element instead and ignores size_average. Default: True\nreduction (str, optional) – Specifies the reduction to apply to the output:\n'none' | 'mean' | 'sum'. 'none': no reduction will be applied,\n'mean': the sum of the output will be divided by the number of\nelements in the output, 'sum': the output will be summed. Note: size_average\nand reduce are in the process of being deprecated, and in the meantime,\nspecifying either of those two args will override reduction. Default: 'mean'",
        "input_shape": "\nInput: (N,D)(N, D)(N,D) or (D)(D)(D) where DDD is the vector dimension.\nOutput: A Tensor of shape (N)(N)(N) if reduction is 'none' and\ninput shape is (N,D)(N, D)(N,D); a scalar otherwise.\n\n",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.TripletMarginWithDistanceLoss",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.TripletMarginWithDistanceLoss.html#torch.nn.TripletMarginWithDistanceLoss",
        "api_signature": "torch.nn.TripletMarginWithDistanceLoss(*, distance_function=None, margin=1.0, swap=False, reduction='mean')",
        "api_description": "Creates a criterion that measures the triplet loss given input\ntensors aaa, ppp, and nnn (representing anchor,\npositive, and negative examples, respectively), and a nonnegative,\nreal-valued function (“distance function”) used to compute the relationship\nbetween the anchor and positive example (“positive distance”) and the\nanchor and negative example (“negative distance”).",
        "return_value": "",
        "parameters": "distance_function (Callable, optional) – A nonnegative, real-valued function that\nquantifies the closeness of two tensors. If not specified,\nnn.PairwiseDistance will be used.  Default: None\nmargin (float, optional) – A nonnegative margin representing the minimum difference\nbetween the positive and negative distances required for the loss to be 0. Larger\nmargins penalize cases where the negative examples are not distant enough from the\nanchors, relative to the positives. Default: 111.\nswap (bool, optional) – Whether to use the distance swap described in the paper\nLearning shallow convolutional feature descriptors with triplet losses by\nV. Balntas, E. Riba et al. If True, and if the positive example is closer to the\nnegative example than the anchor is, swaps the positive example and the anchor in\nthe loss computation. Default: False.\nreduction (str, optional) – Specifies the (optional) reduction to apply to the output:\n'none' | 'mean' | 'sum'. 'none': no reduction will be applied,\n'mean': the sum of the output will be divided by the number of\nelements in the output, 'sum': the output will be summed. Default: 'mean'",
        "input_shape": "\nInput: (N,∗)(N, *)(N,∗) where ∗*∗ represents any number of additional dimensions\nas supported by the distance function.\nOutput: A Tensor of shape (N)(N)(N) if reduction is 'none', or a scalar\notherwise.\n\n",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.triu",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.triu.html#torch.triu",
        "api_signature": "torch.triu(input, diagonal=0, *, out=None)",
        "api_description": "Returns the upper triangular part of a matrix (2-D tensor) or batch of matrices\ninput, the other elements of the result tensor out are set to 0.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.\ndiagonal (int, optional) – the diagonal to consider\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.triu",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.triu.html#torch.Tensor.triu",
        "api_signature": "Tensor.triu(diagonal=0)",
        "api_description": "See torch.triu()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.triu_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.triu_.html#torch.Tensor.triu_",
        "api_signature": "Tensor.triu_(diagonal=0)",
        "api_description": "In-place version of triu()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.triu_indices",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.triu_indices.html#torch.triu_indices",
        "api_signature": "torch.triu_indices(row, col, offset=0, *, dtype=torch.long, device='cpu', layout=torch.strided)",
        "api_description": "Returns the indices of the upper triangular part of a row by\ncol matrix in a 2-by-N Tensor, where the first row contains row\ncoordinates of all indices and the second row contains column coordinates.\nIndices are ordered based on rows and then columns.",
        "return_value": "",
        "parameters": "row (int) – number of rows in the 2-D matrix.\ncol (int) – number of columns in the 2-D matrix.\noffset (int) – diagonal offset from the main diagonal.\nDefault: if not provided, 0.\ndtype (torch.dtype, optional) – the desired data type of returned tensor.\nDefault: if None, torch.long.\ndevice (torch.device, optional) – the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_device()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types.\nlayout (torch.layout, optional) – currently only support torch.strided.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.true_divide",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.true_divide.html#torch.true_divide",
        "api_signature": "torch.true_divide(dividend, divisor, *, out)",
        "api_description": "Alias for torch.div() with rounding_mode=None.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.true_divide",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.true_divide.html#torch.Tensor.true_divide",
        "api_signature": "Tensor.true_divide(value)",
        "api_description": "See torch.true_divide()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.true_divide_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.true_divide_.html#torch.Tensor.true_divide_",
        "api_signature": "Tensor.true_divide_(value)",
        "api_description": "In-place version of true_divide_()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.trunc",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.trunc.html#torch.trunc",
        "api_signature": "torch.trunc(input, *, out=None)",
        "api_description": "Returns a new tensor with the truncated integer values of\nthe elements of input.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.trunc",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.trunc.html#torch.Tensor.trunc",
        "api_signature": "Tensor.trunc()",
        "api_description": "See torch.trunc()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.trunc_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.trunc_.html#torch.Tensor.trunc_",
        "api_signature": "Tensor.trunc_()",
        "api_description": "In-place version of trunc()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.init.trunc_normal_",
        "api_url": "https://pytorch.org/docs/stable/nn.init.html#torch.nn.init.trunc_normal_",
        "api_signature": "torch.nn.init.trunc_normal_(tensor, mean=0.0, std=1.0, a=-2.0, b=2.0, generator=None)",
        "api_description": "Fill the input Tensor with values drawn from a truncated normal distribution.",
        "return_value": "",
        "parameters": "tensor (Tensor) – an n-dimensional torch.Tensor\nmean (float) – the mean of the normal distribution\nstd (float) – the standard deviation of the normal distribution\na (float) – the minimum cutoff value\nb (float) – the maximum cutoff value\ngenerator (Optional[Generator]) – the torch Generator to sample from (default: None)",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.jit.Attribute.type",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.jit.Attribute.html#torch.jit.Attribute.type",
        "api_signature": null,
        "api_description": "Alias for field number 1",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.jit.ScriptModule.type",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.jit.ScriptModule.html#torch.jit.ScriptModule.type",
        "api_signature": "type(dst_type)",
        "api_description": "Casts all parameters and buffers to dst_type.",
        "return_value": "self\n",
        "parameters": "dst_type (type or string) – the desired type",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.Module.type",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.type",
        "api_signature": "type(dst_type)",
        "api_description": "Casts all parameters and buffers to dst_type.",
        "return_value": "self\n",
        "parameters": "dst_type (type or string) – the desired type",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.type",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.type.html#torch.Tensor.type",
        "api_signature": "Tensor.type(dtype=None, non_blocking=False, **kwargs)",
        "api_description": "Returns the type if dtype is not provided, else casts this object to\nthe specified type.",
        "return_value": "",
        "parameters": "dtype (dtype or string) – The desired type\nnon_blocking (bool) – If True, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect.\n**kwargs – For compatibility, may contain the key async in place of\nthe non_blocking argument. The async arg is deprecated.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.TypedStorage.type",
        "api_url": "https://pytorch.org/docs/stable/storage.html#torch.TypedStorage.type",
        "api_signature": "type(dtype=None, non_blocking=False)",
        "api_description": "Returns the type if dtype is not provided, else casts this object to\nthe specified type.",
        "return_value": "",
        "parameters": "dtype (type or string) – The desired type\nnon_blocking (bool) – If True, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect.\n**kwargs – For compatibility, may contain the key async in place of\nthe non_blocking argument. The async arg is deprecated.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.UntypedStorage.type",
        "api_url": "https://pytorch.org/docs/stable/storage.html#torch.UntypedStorage.type",
        "api_signature": "type(dtype=None, non_blocking=False, **kwargs)",
        "api_description": "Returns the type if dtype is not provided, else casts this object to\nthe specified type.",
        "return_value": "",
        "parameters": "dtype (type or string) – The desired type\nnon_blocking (bool) – If True, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect.\n**kwargs – For compatibility, may contain the key async in place of\nthe non_blocking argument. The async arg is deprecated.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.type_as",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.type_as.html#torch.Tensor.type_as",
        "api_signature": "Tensor.type_as(tensor)",
        "api_description": "Returns this tensor cast to the type of the given tensor.",
        "return_value": "",
        "parameters": "tensor (Tensor) – the tensor which has the desired type",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.TypedStorage",
        "api_url": "https://pytorch.org/docs/stable/storage.html#torch.TypedStorage",
        "api_signature": "torch.TypedStorage(*args, wrap_storage=None, dtype=None, device=None, _internal=False)",
        "api_description": "Casts this storage to bfloat16 type.",
        "return_value": "A boolean variable.\nA pinned CPU storage.\n",
        "parameters": "device (int) – The destination GPU id. Defaults to the current device.\nnon_blocking (bool) – If True and the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect.\n**kwargs – For compatibility, may contain the key async in place of\nthe non_blocking argument.\nfilename (str) – file name to map\nshared (bool) – whether to share memory (whether MAP_SHARED or MAP_PRIVATE is passed to the\nunderlying mmap(2) call)\nsize (int) – number of elements in the storage\ndevice (int) – The destination HPU id. Defaults to the current device.\nnon_blocking (bool) – If True and the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect.\n**kwargs – For compatibility, may contain the key async in place of\nthe non_blocking argument.\ndevice (str or torch.device) – The device to pin memory on. Default: 'cuda'\ndevice (str or torch.device) – The device to pin memory on. Default: 'cuda'.\ndtype (type or string) – The desired type\nnon_blocking (bool) – If True, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect.\n**kwargs – For compatibility, may contain the key async in place of\nthe non_blocking argument. The async arg is deprecated.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.unbind",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.unbind.html#torch.unbind",
        "api_signature": "torch.unbind(input, dim=0)",
        "api_description": "Removes a tensor dimension.",
        "return_value": "",
        "parameters": "input (Tensor) – the tensor to unbind\ndim (int) – dimension to remove",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.unbind",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.unbind.html#torch.Tensor.unbind",
        "api_signature": "Tensor.unbind(dim=0)",
        "api_description": "See torch.unbind()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.Unflatten",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.Unflatten.html#torch.nn.Unflatten",
        "api_signature": "torch.nn.Unflatten(dim, unflattened_size)",
        "api_description": "Unflattens a tensor dim expanding it to a desired shape. For use with Sequential.",
        "return_value": "",
        "parameters": "dim (Union[int, str]) – Dimension to be unflattened\nunflattened_size (Union[torch.Size, Tuple, List, NamedShape]) – New shape of the unflattened dimension",
        "input_shape": "\nInput: (∗,Sdim,∗)(*, S_{\\text{dim}}, *)(∗,Sdim​,∗), where SdimS_{\\text{dim}}Sdim​ is the size at\ndimension dim and ∗*∗ means any number of dimensions including none.\nOutput: (∗,U1,...,Un,∗)(*, U_1, ..., U_n, *)(∗,U1​,...,Un​,∗), where UUU = unflattened_size and\n∏i=1nUi=Sdim\\prod_{i=1}^n U_i = S_{\\text{dim}}∏i=1n​Ui​=Sdim​.\n\n",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.unflatten",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.unflatten.html#torch.unflatten",
        "api_signature": "torch.unflatten(input, dim, sizes)",
        "api_description": "Expands a dimension of the input tensor over multiple dimensions.",
        "return_value": "A View of input with the specified dimension unflattened.\n",
        "parameters": "input (Tensor) – the input tensor.\ndim (int) – Dimension to be unflattened, specified as an index into\ninput.shape.\nsizes (Tuple[int]) – New shape of the unflattened dimension.\nOne of its elements can be -1 in which case the corresponding output\ndimension is inferred. Otherwise, the product of sizes must\nequal input.shape[dim].",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> torch.unflatten(torch.randn(3, 4, 1), 1, (2, 2)).shape\ntorch.Size([3, 2, 2, 1])\n>>> torch.unflatten(torch.randn(3, 4, 1), 1, (-1, 2)).shape\ntorch.Size([3, 2, 2, 1])\n>>> torch.unflatten(torch.randn(5, 12, 3), -2, (2, 2, 3, 1, 1)).shape\ntorch.Size([5, 2, 2, 3, 1, 1, 3])\n\n\n"
    },
    {
        "api_name": "torch.export.unflatten.unflatten",
        "api_url": "https://pytorch.org/docs/stable/export.html#torch.export.unflatten.unflatten",
        "api_signature": "torch.export.unflatten.unflatten(module, flat_args_adapter=None)",
        "api_description": "Unflatten an ExportedProgram, producing a module with the same module\nhierarchy as the original eager module. This can be useful if you are trying\nto use torch.export with another system that expects a module\nhierachy instead of the flat graph that torch.export usually produces.",
        "return_value": "An instance of UnflattenedModule, which has the same module\nhierarchy as the original eager module pre-export.\n",
        "parameters": "module (ExportedProgram) – The ExportedProgram to unflatten.\nflat_args_adapter (Optional[FlatArgsAdapter]) – Adapt flat args if input TreeSpec does not match with exported module’s.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.unflatten",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.unflatten.html#torch.Tensor.unflatten",
        "api_signature": "Tensor.unflatten(dim, sizes)",
        "api_description": "See torch.unflatten().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.Unfold",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.Unfold.html#torch.nn.Unfold",
        "api_signature": "torch.nn.Unfold(kernel_size, dilation=1, padding=0, stride=1)",
        "api_description": "Extracts sliding local blocks from a batched input tensor.",
        "return_value": "",
        "parameters": "kernel_size (int or tuple) – the size of the sliding blocks\ndilation (int or tuple, optional) – a parameter that controls the\nstride of elements within the\nneighborhood. Default: 1\npadding (int or tuple, optional) – implicit zero padding to be added on\nboth sides of input. Default: 0\nstride (int or tuple, optional) – the stride of the sliding blocks in the input\nspatial dimensions. Default: 1",
        "input_shape": "\nInput: (N,C,∗)(N, C, *)(N,C,∗)\nOutput: (N,C×∏(kernel_size),L)(N, C \\times \\prod(\\text{kernel\\_size}), L)(N,C×∏(kernel_size),L) as described above\n\n",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.functional.unfold",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.functional.unfold.html#torch.nn.functional.unfold",
        "api_signature": "torch.nn.functional.unfold(input, kernel_size, dilation=1, padding=0, stride=1)",
        "api_description": "Extract sliding local blocks from a batched input tensor.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.unfold",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.unfold.html#torch.Tensor.unfold",
        "api_signature": "Tensor.unfold(dimension, size, step)",
        "api_description": "Returns a view of the original tensor which contains all slices of size size from\nself tensor in the dimension dimension.",
        "return_value": "",
        "parameters": "dimension (int) – dimension in which unfolding happens\nsize (int) – the size of each slice that is unfolded\nstep (int) – the step between each slice",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.uniform.Uniform",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.uniform.Uniform",
        "api_signature": "torch.distributions.uniform.Uniform(low, high, validate_args=None)",
        "api_description": "Bases: Distribution",
        "return_value": "",
        "parameters": "low (float or Tensor) – lower range (inclusive).\nhigh (float or Tensor) – upper range (exclusive).",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.init.uniform_",
        "api_url": "https://pytorch.org/docs/stable/nn.init.html#torch.nn.init.uniform_",
        "api_signature": "torch.nn.init.uniform_(tensor, a=0.0, b=1.0, generator=None)",
        "api_description": "Fill the input Tensor with values drawn from the uniform distribution.",
        "return_value": "",
        "parameters": "tensor (Tensor) – an n-dimensional torch.Tensor\na (float) – the lower bound of the uniform distribution\nb (float) – the upper bound of the uniform distribution\ngenerator (Optional[Generator]) – the torch Generator to sample from (default: None)",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.uniform_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.uniform_.html#torch.Tensor.uniform_",
        "api_signature": "Tensor.uniform_(from=0, to=1, *, generator=None)",
        "api_description": "Fills self tensor with numbers sampled from the continuous uniform\ndistribution:",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.parameter.UninitializedBuffer",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.parameter.UninitializedBuffer.html#torch.nn.parameter.UninitializedBuffer",
        "api_signature": "torch.nn.parameter.UninitializedBuffer(requires_grad=False, device=None, dtype=None)",
        "api_description": "A buffer that is not initialized.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.parameter.UninitializedParameter",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.parameter.UninitializedParameter.html#torch.nn.parameter.UninitializedParameter",
        "api_signature": "torch.nn.parameter.UninitializedParameter(requires_grad=True, device=None, dtype=None)",
        "api_description": "A parameter that is not initialized.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.unique",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.unique.html#torch.unique",
        "api_signature": "torch.unique(input, sorted=True, return_inverse=False, return_counts=False, dim=None)",
        "api_description": "Returns the unique elements of the input tensor.",
        "return_value": "A tensor or a tuple of tensors containing\n\n\noutput (Tensor): the output list of unique scalar elements.\ninverse_indices (Tensor): (optional) if\nreturn_inverse is True, there will be an additional\nreturned tensor (same shape as input) representing the indices\nfor where elements in the original input map to in the output;\notherwise, this function will only return a single tensor.\ncounts (Tensor): (optional) if\nreturn_counts is True, there will be an additional\nreturned tensor (same shape as output or output.size(dim),\nif dim was specified) representing the number of occurrences\nfor each unique value or tensor.\n\n\n\n",
        "parameters": "input (Tensor) – the input tensor\nsorted (bool) – Whether to sort the unique elements in ascending order\nbefore returning as output.\nreturn_inverse (bool) – Whether to also return the indices for where\nelements in the original input ended up in the returned unique list.\nreturn_counts (bool) – Whether to also return the counts for each unique\nelement.\ndim (int, optional) – the dimension to operate upon. If None, the\nunique of the flattened input is returned. Otherwise, each of the\ntensors indexed by the given dimension is treated as one of the\nelements to apply the unique operation upon. See examples for more\ndetails. Default: None",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.unique",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.unique.html#torch.Tensor.unique",
        "api_signature": "Tensor.unique(sorted=True, return_inverse=False, return_counts=False, dim=None)",
        "api_description": "Returns the unique elements of the input tensor.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.unique_consecutive",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.unique_consecutive.html#torch.unique_consecutive",
        "api_signature": "torch.unique_consecutive(*args, **kwargs)",
        "api_description": "Eliminates all but the first element from every consecutive group of equivalent elements.",
        "return_value": "A tensor or a tuple of tensors containing\n\n\noutput (Tensor): the output list of unique scalar elements.\ninverse_indices (Tensor): (optional) if\nreturn_inverse is True, there will be an additional\nreturned tensor (same shape as input) representing the indices\nfor where elements in the original input map to in the output;\notherwise, this function will only return a single tensor.\ncounts (Tensor): (optional) if\nreturn_counts is True, there will be an additional\nreturned tensor (same shape as output or output.size(dim),\nif dim was specified) representing the number of occurrences\nfor each unique value or tensor.\n\n\n\n",
        "parameters": "input (Tensor) – the input tensor\nreturn_inverse (bool) – Whether to also return the indices for where\nelements in the original input ended up in the returned unique list.\nreturn_counts (bool) – Whether to also return the counts for each unique\nelement.\ndim (int) – the dimension to apply unique. If None, the unique of the\nflattened input is returned. default: None",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.unique_consecutive",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.unique_consecutive.html#torch.Tensor.unique_consecutive",
        "api_signature": "Tensor.unique_consecutive(return_inverse=False, return_counts=False, dim=None)",
        "api_description": "Eliminates all but the first element from every consecutive group of equivalent elements.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.autograd.forward_ad.unpack_dual",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.autograd.forward_ad.unpack_dual.html#torch.autograd.forward_ad.unpack_dual",
        "api_signature": "torch.autograd.forward_ad.unpack_dual(tensor, *, level=None)",
        "api_description": "Unpack a “dual tensor” to get both its Tensor value and its forward AD gradient.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.utils.rnn.unpack_sequence",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.unpack_sequence.html#torch.nn.utils.rnn.unpack_sequence",
        "api_signature": "torch.nn.utils.rnn.unpack_sequence(packed_sequences)",
        "api_description": "Unpack PackedSequence into a list of variable length Tensors.",
        "return_value": "a list of Tensor objects\n",
        "parameters": "packed_sequences (PackedSequence) – A PackedSequence object.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.autograd.forward_ad.UnpackedDualTensor",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.autograd.forward_ad.UnpackedDualTensor.html#torch.autograd.forward_ad.UnpackedDualTensor",
        "api_signature": "torch.autograd.forward_ad.UnpackedDualTensor(primal, tangent)",
        "api_description": "Namedtuple returned by unpack_dual() containing the primal and tangent components of the dual tensor.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.utils.rnn.unpad_sequence",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.unpad_sequence.html#torch.nn.utils.rnn.unpad_sequence",
        "api_signature": "torch.nn.utils.rnn.unpad_sequence(padded_sequences, lengths, batch_first=False)",
        "api_description": "Unpad padded Tensor into a list of variable length Tensors.",
        "return_value": "a list of Tensor objects\n",
        "parameters": "padded_sequences (Tensor) – padded sequences.\nlengths (Tensor) – length of original (unpadded) sequences.\nbatch_first (bool, optional) – whether batch dimension first or not. Default: False.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.unravel_index",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.unravel_index.html#torch.unravel_index",
        "api_signature": "torch.unravel_index(indices, shape)",
        "api_description": "Converts a tensor of flat indices into a tuple of coordinate tensors that\nindex into an arbitrary tensor of the specified shape.",
        "return_value": "Each i-th tensor in the output corresponds with\ndimension i of shape. Each tensor has the same shape as\nindices and contains one index into dimension i for each of the\nflat indices given by indices.\n",
        "parameters": "indices (Tensor) – An integer tensor containing indices into the\nflattened version of an arbitrary tensor of shape shape.\nAll elements must be in the range [0, prod(shape) - 1].\nshape (int, sequence of ints, or torch.Size) – The shape of the arbitrary\ntensor. All elements must be non-negative.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.onnx.unregister_custom_op_symbolic",
        "api_url": "https://pytorch.org/docs/stable/onnx_torchscript.html#torch.onnx.unregister_custom_op_symbolic",
        "api_signature": "torch.onnx.unregister_custom_op_symbolic(symbolic_name, opset_version)",
        "api_description": "Unregisters symbolic_name.",
        "return_value": "",
        "parameters": "symbolic_name (str) – The name of the custom operator in “<domain>::<op>”\nformat.\nopset_version (int) – The ONNX opset version in which to unregister.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.monitor.unregister_event_handler",
        "api_url": "https://pytorch.org/docs/stable/monitor.html#torch.monitor.unregister_event_handler",
        "api_signature": "torch.monitor.unregister_event_handler(handler: torch._C._monitor.EventHandlerHandle)",
        "api_description": "unregister_event_handler unregisters the EventHandlerHandle returned\nafter calling register_event_handler. After this returns the event\nhandler will no longer receive events.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.utils.rnn.PackedSequence.unsorted_indices",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.PackedSequence.html#torch.nn.utils.rnn.PackedSequence.unsorted_indices",
        "api_signature": null,
        "api_description": "Alias for field number 3",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.unsqueeze",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.unsqueeze.html#torch.unsqueeze",
        "api_signature": "torch.unsqueeze(input, dim)",
        "api_description": "Returns a new tensor with a dimension of size one inserted at the\nspecified position.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.\ndim (int) – the index at which to insert the singleton dimension",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.unsqueeze",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.unsqueeze.html#torch.Tensor.unsqueeze",
        "api_signature": "Tensor.unsqueeze(dim)",
        "api_description": "See torch.unsqueeze()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.unsqueeze_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.unsqueeze_.html#torch.Tensor.unsqueeze_",
        "api_signature": "Tensor.unsqueeze_(dim)",
        "api_description": "In-place version of unsqueeze()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.TypedStorage.untyped",
        "api_url": "https://pytorch.org/docs/stable/storage.html#torch.TypedStorage.untyped",
        "api_signature": "untyped()",
        "api_description": "Return the internal torch.UntypedStorage.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.UntypedStorage.untyped",
        "api_url": "https://pytorch.org/docs/stable/storage.html#torch.UntypedStorage.untyped",
        "api_signature": "untyped()",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.untyped_storage",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.untyped_storage.html#torch.Tensor.untyped_storage",
        "api_signature": "Tensor.untyped_storage()",
        "api_description": "Returns the underlying UntypedStorage.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.UntypedStorage",
        "api_url": "https://pytorch.org/docs/stable/storage.html#torch.UntypedStorage",
        "api_signature": "torch.UntypedStorage(*args, **kwargs)",
        "api_description": "Casts this storage to bfloat16 type.",
        "return_value": "A boolean variable.\nA pinned CPU storage.\nself\n",
        "parameters": "device (int) – The destination GPU id. Defaults to the current device.\nnon_blocking (bool) – If True and the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect.\n**kwargs – For compatibility, may contain the key async in place of\nthe non_blocking argument.\nfilename (str) – file name to map\nshared (bool) – whether to share memory (whether MAP_SHARED or MAP_PRIVATE is passed to the\nunderlying mmap(2) call)\nsize (int) – number of elements in the storage\ndevice (int) – The destination HPU id. Defaults to the current device.\nnon_blocking (bool) – If True and the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect.\n**kwargs – For compatibility, may contain the key async in place of\nthe non_blocking argument.\ndevice (str or torch.device) – The device to pin memory on. Default: 'cuda'.\ndevice (str or torch.device) – The device to pin memory on. Default: 'cuda'.\ndtype (type or string) – The desired type\nnon_blocking (bool) – If True, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect.\n**kwargs – For compatibility, may contain the key async in place of\nthe non_blocking argument. The async arg is deprecated.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.jit.unused",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.jit.unused.html#torch.jit.unused",
        "api_signature": "torch.jit.unused(fn)",
        "api_description": "This decorator indicates to the compiler that a function or method should\nbe ignored and replaced with the raising of an exception. This allows you\nto leave code in your model that is not yet TorchScript compatible and still\nexport your model.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.autograd.profiler_util.StringTable.update",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.autograd.profiler_util.StringTable.html#torch.autograd.profiler_util.StringTable.update",
        "api_signature": "update([E, ]**F)",
        "api_description": "If E is present and has a .keys() method, then does:  for k in E: D[k] = E[k]\nIf E is present and lacks a .keys() method, then does:  for k, v in E: D[k] = v\nIn either case, this is followed by: for k in F:  D[k] = F[k]",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.ModuleDict.update",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.ModuleDict.html#torch.nn.ModuleDict.update",
        "api_signature": "update(modules)",
        "api_description": "Update the ModuleDict with key-value pairs from a mapping, overwriting existing keys.",
        "return_value": "",
        "parameters": "modules (iterable) – a mapping (dictionary) from string to Module,\nor an iterable of key-value pairs of type (string, Module)",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.ParameterDict.update",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.ParameterDict.html#torch.nn.ParameterDict.update",
        "api_signature": "update(parameters)",
        "api_description": "Update the ParameterDict with key-value pairs from parameters, overwriting existing keys.",
        "return_value": "",
        "parameters": "parameters (iterable) – a mapping (dictionary) from string to\nParameter, or an iterable of\nkey-value pairs of type (string, Parameter)",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.Node.update_arg",
        "api_url": "https://pytorch.org/docs/stable/fx.html#torch.fx.Node.update_arg",
        "api_signature": "update_arg(idx, arg)",
        "api_description": "Update an existing positional argument to contain the new value\narg. After calling, self.args[idx] == arg.",
        "return_value": "",
        "parameters": "idx (int) – The index into self.args of the element to update\narg (Argument) – The new argument value to write into args",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.intrinsic.qat.update_bn_stats",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.nn.intrinsic.qat.update_bn_stats.html#torch.ao.nn.intrinsic.qat.update_bn_stats",
        "api_signature": "torch.ao.nn.intrinsic.qat.update_bn_stats(mod)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.Node.update_kwarg",
        "api_url": "https://pytorch.org/docs/stable/fx.html#torch.fx.Node.update_kwarg",
        "api_signature": "update_kwarg(key, arg)",
        "api_description": "Update an existing keyword argument to contain the new value\narg. After calling, self.kwargs[key] == arg.",
        "return_value": "",
        "parameters": "key (str) – The key in self.kwargs of the element to update\narg (Argument) – The new argument value to write into kwargs",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.quantized.functional.upsample",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.nn.quantized.functional.upsample.html#torch.ao.nn.quantized.functional.upsample",
        "api_signature": "torch.ao.nn.quantized.functional.upsample(input, size=None, scale_factor=None, mode='nearest', align_corners=None)",
        "api_description": "Upsamples the input to either the given size or the given\nscale_factor",
        "return_value": "",
        "parameters": "input (Tensor) – quantized input tensor\nsize (int or Tuple[int] or Tuple[int, int] or Tuple[int, int, int]) – output spatial size.\nscale_factor (float or Tuple[float]) – multiplier for spatial size. Has to be an integer.\nmode (str) – algorithm used for upsampling:\n'nearest' | 'bilinear'\nalign_corners (bool, optional) – Geometrically, we consider the pixels of the\ninput and output as squares rather than points.\nIf set to True, the input and output tensors are aligned by the\ncenter points of their corner pixels, preserving the values at the corner pixels.\nIf set to False, the input and output tensors are aligned by the corner\npoints of their corner pixels, and the interpolation uses edge value padding\nfor out-of-boundary values, making this operation independent of input size\nwhen scale_factor is kept the same. This only has an effect when mode\nis 'bilinear'.\nDefault: False",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.Upsample",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.Upsample.html#torch.nn.Upsample",
        "api_signature": "torch.nn.Upsample(size=None, scale_factor=None, mode='nearest', align_corners=None, recompute_scale_factor=None)",
        "api_description": "Upsamples a given multi-channel 1D (temporal), 2D (spatial) or 3D (volumetric) data.",
        "return_value": "",
        "parameters": "size (int or Tuple[int] or Tuple[int, int] or Tuple[int, int, int], optional) – output spatial sizes\nscale_factor (float or Tuple[float] or Tuple[float, float] or Tuple[float, float, float], optional) – multiplier for spatial size. Has to match input size if it is a tuple.\nmode (str, optional) – the upsampling algorithm: one of 'nearest',\n'linear', 'bilinear', 'bicubic' and 'trilinear'.\nDefault: 'nearest'\nalign_corners (bool, optional) – if True, the corner pixels of the input\nand output tensors are aligned, and thus preserving the values at\nthose pixels. This only has effect when mode is\n'linear', 'bilinear', 'bicubic', or 'trilinear'.\nDefault: False\nrecompute_scale_factor (bool, optional) – recompute the scale_factor for use in the\ninterpolation calculation. If recompute_scale_factor is True, then\nscale_factor must be passed in and scale_factor is used to compute the\noutput size. The computed output size will be used to infer new scales for\nthe interpolation. Note that when scale_factor is floating-point, it may differ\nfrom the recomputed scale_factor due to rounding and precision issues.\nIf recompute_scale_factor is False, then size or scale_factor will\nbe used directly for interpolation.",
        "input_shape": "\nInput: (N,C,Win)(N, C, W_{in})(N,C,Win​), (N,C,Hin,Win)(N, C, H_{in}, W_{in})(N,C,Hin​,Win​) or (N,C,Din,Hin,Win)(N, C, D_{in}, H_{in}, W_{in})(N,C,Din​,Hin​,Win​)\nOutput: (N,C,Wout)(N, C, W_{out})(N,C,Wout​), (N,C,Hout,Wout)(N, C, H_{out}, W_{out})(N,C,Hout​,Wout​)\nor (N,C,Dout,Hout,Wout)(N, C, D_{out}, H_{out}, W_{out})(N,C,Dout​,Hout​,Wout​), where\n\n",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.functional.upsample",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.functional.upsample.html#torch.nn.functional.upsample",
        "api_signature": "torch.nn.functional.upsample(input, size=None, scale_factor=None, mode='nearest', align_corners=None)",
        "api_description": "Upsample input.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor\nsize (int or Tuple[int] or Tuple[int, int] or Tuple[int, int, int]) – output spatial size.\nscale_factor (float or Tuple[float]) – multiplier for spatial size. Has to match input size if it is a tuple.\nmode (str) – algorithm used for upsampling:\n'nearest' | 'linear' | 'bilinear' | 'bicubic' |\n'trilinear'. Default: 'nearest'\nalign_corners (bool, optional) – Geometrically, we consider the pixels of the\ninput and output as squares rather than points.\nIf set to True, the input and output tensors are aligned by the\ncenter points of their corner pixels, preserving the values at the corner pixels.\nIf set to False, the input and output tensors are aligned by the corner\npoints of their corner pixels, and the interpolation uses edge value padding\nfor out-of-boundary values, making this operation independent of input size\nwhen scale_factor is kept the same. This only has an effect when mode\nis 'linear', 'bilinear', 'bicubic' or 'trilinear'.\nDefault: False",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.quantized.functional.upsample_bilinear",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.nn.quantized.functional.upsample_bilinear.html#torch.ao.nn.quantized.functional.upsample_bilinear",
        "api_signature": "torch.ao.nn.quantized.functional.upsample_bilinear(input, size=None, scale_factor=None)",
        "api_description": "Upsamples the input, using bilinear upsampling.",
        "return_value": "",
        "parameters": "input (Tensor) – quantized input\nsize (int or Tuple[int, int]) – output spatial size.\nscale_factor (int or Tuple[int, int]) – multiplier for spatial size",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.functional.upsample_bilinear",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.functional.upsample_bilinear.html#torch.nn.functional.upsample_bilinear",
        "api_signature": "torch.nn.functional.upsample_bilinear(input, size=None, scale_factor=None)",
        "api_description": "Upsamples the input, using bilinear upsampling.",
        "return_value": "",
        "parameters": "input (Tensor) – input\nsize (int or Tuple[int, int]) – output spatial size.\nscale_factor (int or Tuple[int, int]) – multiplier for spatial size",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.nn.quantized.functional.upsample_nearest",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.nn.quantized.functional.upsample_nearest.html#torch.ao.nn.quantized.functional.upsample_nearest",
        "api_signature": "torch.ao.nn.quantized.functional.upsample_nearest(input, size=None, scale_factor=None)",
        "api_description": "Upsamples the input, using nearest neighbours’ pixel values.",
        "return_value": "",
        "parameters": "input (Tensor) – quantized input\nsize (int or Tuple[int, int] or Tuple[int, int, int]) – output spatial\nsize.\nscale_factor (int) – multiplier for spatial size. Has to be an integer.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.functional.upsample_nearest",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.functional.upsample_nearest.html#torch.nn.functional.upsample_nearest",
        "api_signature": "torch.nn.functional.upsample_nearest(input, size=None, scale_factor=None)",
        "api_description": "Upsamples the input, using nearest neighbours’ pixel values.",
        "return_value": "",
        "parameters": "input (Tensor) – input\nsize (int or Tuple[int, int] or Tuple[int, int, int]) – output spatia\nsize.\nscale_factor (int) – multiplier for spatial size. Has to be an integer.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.UpsamplingBilinear2d",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.UpsamplingBilinear2d.html#torch.nn.UpsamplingBilinear2d",
        "api_signature": "torch.nn.UpsamplingBilinear2d(size=None, scale_factor=None)",
        "api_description": "Applies a 2D bilinear upsampling to an input signal composed of several input channels.",
        "return_value": "",
        "parameters": "size (int or Tuple[int, int], optional) – output spatial sizes\nscale_factor (float or Tuple[float, float], optional) – multiplier for\nspatial size.",
        "input_shape": "\nInput: (N,C,Hin,Win)(N, C, H_{in}, W_{in})(N,C,Hin​,Win​)\nOutput: (N,C,Hout,Wout)(N, C, H_{out}, W_{out})(N,C,Hout​,Wout​) where\n\n",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.UpsamplingNearest2d",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.UpsamplingNearest2d.html#torch.nn.UpsamplingNearest2d",
        "api_signature": "torch.nn.UpsamplingNearest2d(size=None, scale_factor=None)",
        "api_description": "Applies a 2D nearest neighbor upsampling to an input signal composed of several input channels.",
        "return_value": "",
        "parameters": "size (int or Tuple[int, int], optional) – output spatial sizes\nscale_factor (float or Tuple[float, float], optional) – multiplier for\nspatial size.",
        "input_shape": "\nInput: (N,C,Hin,Win)(N, C, H_{in}, W_{in})(N,C,Hin​,Win​)\nOutput: (N,C,Hout,Wout)(N, C, H_{out}, W_{out})(N,C,Hout​,Wout​) where\n\n",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.use_deterministic_algorithms",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html#torch.use_deterministic_algorithms",
        "api_signature": "torch.use_deterministic_algorithms(mode, *, warn_only=False)",
        "api_description": "Sets whether PyTorch operations must use “deterministic”\nalgorithms. That is, algorithms which, given the same input, and when\nrun on the same software and hardware, always produce the same output.\nWhen enabled, operations will use deterministic algorithms when available,\nand if only nondeterministic algorithms are available they will throw a\nRuntimeError when called.",
        "return_value": "",
        "parameters": "mode (bool) – If True, makes potentially nondeterministic\noperations switch to a deterministic algorithm or throw a runtime\nerror. If False, allows nondeterministic operations.\nwarn_only (bool, optional) – If True, operations that do not\nhave a deterministic implementation will throw a warning instead of\nan error. Default: False",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda.utilization",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.cuda.utilization.html#torch.cuda.utilization",
        "api_signature": "torch.cuda.utilization(device=None)",
        "api_description": "Return the percent of time over the past sample period during which one or\nmore kernels was executing on the GPU as given by nvidia-smi.",
        "return_value": "",
        "parameters": "device (torch.device or int, optional) – selected device. Returns\nstatistic for the current device, given by current_device(),\nif device is None (default).",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.checkpoint.format_utils.BroadcastingTorchSaveReader.validate_checkpoint_id",
        "api_url": "https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.format_utils.BroadcastingTorchSaveReader.validate_checkpoint_id",
        "api_signature": "validate_checkpoint_id(checkpoint_id)",
        "api_description": "Implementation of the StorageReader method",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.checkpoint.StorageReader.validate_checkpoint_id",
        "api_url": "https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.StorageReader.validate_checkpoint_id",
        "api_signature": "validate_checkpoint_id(checkpoint_id)",
        "api_description": "Check if the given checkpoint_id is supported by the stroage. This allow\nus to enable automatic storage selection.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.checkpoint.StorageWriter.validate_checkpoint_id",
        "api_url": "https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.StorageWriter.validate_checkpoint_id",
        "api_signature": "validate_checkpoint_id(checkpoint_id)",
        "api_description": "Check if the given checkpoint_id is supported by the stroage. This allow\nus to enable automatic storage selection.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.jit.Attribute.value",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.jit.Attribute.html#torch.jit.Attribute.value",
        "api_signature": null,
        "api_description": "Alias for field number 0",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.futures.Future.value",
        "api_url": "https://pytorch.org/docs/stable/futures.html#torch.futures.Future.value",
        "api_signature": "value()",
        "api_description": "Obtain the value of an already-completed future.",
        "return_value": "The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this value() method will\nalso throw an error.\n",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.autograd.profiler_util.StringTable.values",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.autograd.profiler_util.StringTable.html#torch.autograd.profiler_util.StringTable.values",
        "api_signature": "values()",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.ModuleDict.values",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.ModuleDict.html#torch.nn.ModuleDict.values",
        "api_signature": "values()",
        "api_description": "Return an iterable of the ModuleDict values.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.ParameterDict.values",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.ParameterDict.html#torch.nn.ParameterDict.values",
        "api_signature": "values()",
        "api_description": "Return an iterable of the ParameterDict values.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.values",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.values.html#torch.Tensor.values",
        "api_signature": "Tensor.values()",
        "api_description": "Return the values tensor of a sparse COO tensor.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.vander",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.vander.html#torch.vander",
        "api_signature": "torch.vander(x, N=None, increasing=False)",
        "api_description": "Generates a Vandermonde matrix.",
        "return_value": "Vandermonde matrix. If increasing is False, the first column is x(N−1)x^{(N-1)}x(N−1),\nthe second x(N−2)x^{(N-2)}x(N−2) and so forth. If increasing is True, the columns\nare x0,x1,...,x(N−1)x^0, x^1, ..., x^{(N-1)}x0,x1,...,x(N−1).\n",
        "parameters": "x (Tensor) – 1-D input tensor.\nN (int, optional) – Number of columns in the output. If N is not specified,\na square array is returned (N=len(x))(N = len(x))(N=len(x)).\nincreasing (bool, optional) – Order of the powers of the columns. If True,\nthe powers increase from left to right, if False (the default) they are reversed.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.linalg.vander",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.linalg.vander.html#torch.linalg.vander",
        "api_signature": "torch.linalg.vander(x, N=None)",
        "api_description": "Generates a Vandermonde matrix.",
        "return_value": "",
        "parameters": "x (Tensor) – tensor of shape (*, n) where * is zero or more batch dimensions\nconsisting of vectors.\nN (int, optional) – Number of columns in the output. Default: x.size(-1)",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.var",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.var.html#torch.var",
        "api_signature": "torch.var(input, dim=None, *, correction=1, keepdim=False, out=None)",
        "api_description": "Calculates the variance over the dimensions specified by dim. dim\ncan be a single dimension, list of dimensions, or None to reduce over all\ndimensions.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.\ndim (int or tuple of ints, optional) – the dimension or dimensions to reduce.\nIf None, all dimensions are reduced.\ncorrection (int) – difference between the sample size and sample degrees of freedom.\nDefaults to Bessel’s correction, correction=1.\nChanged in version 2.0: Previously this argument was called unbiased and was a boolean\nwith True corresponding to correction=1 and False being\ncorrection=0.\nkeepdim (bool) – whether the output tensor has dim retained or not.\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.var",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.var.html#torch.Tensor.var",
        "api_signature": "Tensor.var(dim=None, *, correction=1, keepdim=False)",
        "api_description": "See torch.var()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.var_mean",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.var_mean.html#torch.var_mean",
        "api_signature": "torch.var_mean(input, dim=None, *, correction=1, keepdim=False, out=None)",
        "api_description": "Calculates the variance and mean over the dimensions specified by dim.\ndim can be a single dimension, list of dimensions, or None to\nreduce over all dimensions.",
        "return_value": "A tuple (var, mean) containing the variance and mean.\n",
        "parameters": "input (Tensor) – the input tensor.\ndim (int or tuple of ints, optional) – the dimension or dimensions to reduce.\nIf None, all dimensions are reduced.\ncorrection (int) – difference between the sample size and sample degrees of freedom.\nDefaults to Bessel’s correction, correction=1.\nChanged in version 2.0: Previously this argument was called unbiased and was a boolean\nwith True corresponding to correction=1 and False being\ncorrection=0.\nkeepdim (bool) – whether the output tensor has dim retained or not.\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.bernoulli.Bernoulli.variance",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.bernoulli.Bernoulli.variance",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.beta.Beta.variance",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.beta.Beta.variance",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.binomial.Binomial.variance",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.binomial.Binomial.variance",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.categorical.Categorical.variance",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.categorical.Categorical.variance",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.cauchy.Cauchy.variance",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.cauchy.Cauchy.variance",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.continuous_bernoulli.ContinuousBernoulli.variance",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.continuous_bernoulli.ContinuousBernoulli.variance",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.dirichlet.Dirichlet.variance",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.dirichlet.Dirichlet.variance",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.distribution.Distribution.variance",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.distribution.Distribution.variance",
        "api_signature": null,
        "api_description": "Returns the variance of the distribution.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.exponential.Exponential.variance",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.exponential.Exponential.variance",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.fishersnedecor.FisherSnedecor.variance",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.fishersnedecor.FisherSnedecor.variance",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.gamma.Gamma.variance",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.gamma.Gamma.variance",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.geometric.Geometric.variance",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.geometric.Geometric.variance",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.gumbel.Gumbel.variance",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.gumbel.Gumbel.variance",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.half_cauchy.HalfCauchy.variance",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.half_cauchy.HalfCauchy.variance",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.half_normal.HalfNormal.variance",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.half_normal.HalfNormal.variance",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.independent.Independent.variance",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.independent.Independent.variance",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.inverse_gamma.InverseGamma.variance",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.inverse_gamma.InverseGamma.variance",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.kumaraswamy.Kumaraswamy.variance",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.kumaraswamy.Kumaraswamy.variance",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.laplace.Laplace.variance",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.laplace.Laplace.variance",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.log_normal.LogNormal.variance",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.log_normal.LogNormal.variance",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.variance",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.variance",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.mixture_same_family.MixtureSameFamily.variance",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.mixture_same_family.MixtureSameFamily.variance",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.multinomial.Multinomial.variance",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.multinomial.Multinomial.variance",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.multivariate_normal.MultivariateNormal.variance",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.multivariate_normal.MultivariateNormal.variance",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.negative_binomial.NegativeBinomial.variance",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.negative_binomial.NegativeBinomial.variance",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.normal.Normal.variance",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.normal.Normal.variance",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.one_hot_categorical.OneHotCategorical.variance",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.one_hot_categorical.OneHotCategorical.variance",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.pareto.Pareto.variance",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.pareto.Pareto.variance",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.poisson.Poisson.variance",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.poisson.Poisson.variance",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.studentT.StudentT.variance",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.studentT.StudentT.variance",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.uniform.Uniform.variance",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.uniform.Uniform.variance",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.von_mises.VonMises.variance",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.von_mises.VonMises.variance",
        "api_signature": null,
        "api_description": "The provided variance is the circular one.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.weibull.Weibull.variance",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.weibull.Weibull.variance",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.wishart.Wishart.variance",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.wishart.Wishart.variance",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.vdot",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.vdot.html#torch.vdot",
        "api_signature": "torch.vdot(input, other, *, out=None)",
        "api_description": "Computes the dot product of two 1D vectors along a dimension.",
        "return_value": "",
        "parameters": "input (Tensor) – first tensor in the dot product, must be 1D. Its conjugate is used if it’s complex.\nother (Tensor) – second tensor in the dot product, must be 1D.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.vdot",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.vdot.html#torch.Tensor.vdot",
        "api_signature": "Tensor.vdot(other)",
        "api_description": "See torch.vdot()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.linalg.vecdot",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.linalg.vecdot.html#torch.linalg.vecdot",
        "api_signature": "torch.linalg.vecdot(x, y, *, dim=-1, out=None)",
        "api_description": "Computes the dot product of two batches of vectors along a dimension.",
        "return_value": "",
        "parameters": "x (Tensor) – first batch of vectors of shape (*, n).\ny (Tensor) – second batch of vectors of shape (*, n).\ndim (int) – Dimension along which to compute the dot product. Default: -1.\nout (Tensor, optional) – output tensor. Ignored if None. Default: None.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.linalg.vector_norm",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.linalg.vector_norm.html#torch.linalg.vector_norm",
        "api_signature": "torch.linalg.vector_norm(x, ord=2, dim=None, keepdim=False, *, dtype=None, out=None)",
        "api_description": "Computes a vector norm.",
        "return_value": "A real-valued tensor, even when x is complex.\n",
        "parameters": "x (Tensor) – tensor, flattened by default, but this behavior can be\ncontrolled using dim.\nord (int, float, inf, -inf, 'fro', 'nuc', optional) – order of norm. Default: 2\ndim (int, Tuple[int], optional) – dimensions over which to compute\nthe norm. See above for the behavior when dim= None.\nDefault: None\nkeepdim (bool, optional) – If set to True, the reduced dimensions are retained\nin the result as dimensions with size one. Default: False\nout (Tensor, optional) – output tensor. Ignored if None. Default: None.\ndtype (torch.dtype, optional) – type used to perform the accumulation and the return.\nIf specified, x is cast to dtype before performing the operation,\nand the returned tensor’s type will be dtype if real and of its real counterpart if complex.\ndtype may be complex if x is complex, otherwise it must be real.\nx should be convertible without narrowing to dtype. Default: None",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.utils.vector_to_parameters",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.utils.vector_to_parameters.html#torch.nn.utils.vector_to_parameters",
        "api_signature": "torch.nn.utils.vector_to_parameters(vec, parameters)",
        "api_description": "Copy slices of a vector into an iterable of parameters.",
        "return_value": "",
        "parameters": "vec (Tensor) – a single vector representing the parameters of a model.\nparameters (Iterable[Tensor]) – an iterable of Tensors that are the\nparameters of a model.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.backends.mkl.verbose",
        "api_url": "https://pytorch.org/docs/stable/backends.html#torch.backends.mkl.verbose",
        "api_signature": "torch.backends.mkl.verbose(enable)",
        "api_description": "On-demand oneMKL verbosing functionality.",
        "return_value": "",
        "parameters": "level – Verbose level\n- VERBOSE_OFF: Disable verbosing\n- VERBOSE_ON:  Enable verbosing",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.backends.mkldnn.verbose",
        "api_url": "https://pytorch.org/docs/stable/backends.html#torch.backends.mkldnn.verbose",
        "api_signature": "torch.backends.mkldnn.verbose(level)",
        "api_description": "On-demand oneDNN (former MKL-DNN) verbosing functionality.",
        "return_value": "",
        "parameters": "level – Verbose level\n- VERBOSE_OFF: Disable verbosing\n- VERBOSE_ON:  Enable verbosing\n- VERBOSE_ON_CREATION: Enable verbosing, including oneDNN kernel creation",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.onnx.verification.VerificationOptions",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.onnx.verification.VerificationOptions.html#torch.onnx.verification.VerificationOptions",
        "api_signature": "torch.onnx.verification.VerificationOptions(flatten=True, ignore_none=True, check_shape=True, check_dtype=True, backend=OnnxBackend.ONNX_RUNTIME_CPU, rtol=0.001, atol=1e-07, remained_onnx_input_idx=None, acceptable_error_percentage=None)",
        "api_description": "Options for ONNX export verification.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.onnx.verification.GraphInfo.verify_export",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.onnx.verification.GraphInfo.html#torch.onnx.verification.GraphInfo.verify_export",
        "api_signature": "verify_export(options)",
        "api_description": "Verify the export from TorchScript IR graph to ONNX.",
        "return_value": "The AssertionError raised during the verification. Returns None if no\nerror is raised.\nonnx_graph: The exported ONNX graph in TorchScript IR format.\nonnx_outs: The outputs from running exported ONNX model under the onnx\nbackend in options.\npt_outs: The outputs from running the TorchScript IR graph.\n",
        "parameters": "options (VerificationOptions) – The verification options.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.cpp_extension.verify_ninja_availability",
        "api_url": "https://pytorch.org/docs/stable/cpp_extension.html#torch.utils.cpp_extension.verify_ninja_availability",
        "api_signature": "torch.utils.cpp_extension.verify_ninja_availability()",
        "api_description": "Raise RuntimeError if ninja build system is not available on the system, does nothing otherwise.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.pipeline.sync.skip.skippable.verify_skippables",
        "api_url": "https://pytorch.org/docs/stable/pipeline.html#torch.distributed.pipeline.sync.skip.skippable.verify_skippables",
        "api_signature": "torch.distributed.pipeline.sync.skip.skippable.verify_skippables(module)",
        "api_description": "Verify if the underlying skippable modules satisfy integrity.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.backends.cudnn.version",
        "api_url": "https://pytorch.org/docs/stable/backends.html#torch.backends.cudnn.version",
        "api_signature": "torch.backends.cudnn.version()",
        "api_description": "Return the version of cuDNN.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.autograd.functional.vhp",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.autograd.functional.vhp.html#torch.autograd.functional.vhp",
        "api_signature": "torch.autograd.functional.vhp(func, inputs, v=None, create_graph=False, strict=False)",
        "api_description": "Compute the dot product between vector v and Hessian of a  given scalar function at a specified point.",
        "return_value": "\ntuple with:func_output (tuple of Tensors or Tensor): output of func(inputs)\nvhp (tuple of Tensors or Tensor): result of the dot product with the\nsame shape as the inputs.\n\n\n\n",
        "parameters": "func (function) – a Python function that takes Tensor inputs and returns\na Tensor with a single element.\ninputs (tuple of Tensors or Tensor) – inputs to the function func.\nv (tuple of Tensors or Tensor) – The vector for which the vector Hessian\nproduct is computed. Must be the same size as the input of\nfunc. This argument is optional when func’s input contains\na single element and (if it is not provided) will be set as a\nTensor containing a single 1.\ncreate_graph (bool, optional) – If True, both the output and result\nwill be computed in a differentiable way. Note that when strict\nis False, the result can not require gradients or be\ndisconnected from the inputs.\nDefaults to False.\nstrict (bool, optional) – If True, an error will be raised when we\ndetect that there exists an input such that all the outputs are\nindependent of it. If False, we return a Tensor of zeros as the\nvhp for said inputs, which is the expected mathematical value.\nDefaults to False.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.view",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.view.html#torch.Tensor.view",
        "api_signature": "Tensor.view(*shape)",
        "api_description": "Returns a new tensor with the same data as the self tensor but of a\ndifferent shape.",
        "return_value": "",
        "parameters": "shape (torch.Size or int...) – the desired size\ndtype (torch.dtype) – the desired dtype",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.view_as",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.view_as.html#torch.Tensor.view_as",
        "api_signature": "Tensor.view_as(other)",
        "api_description": "View this tensor as the same size as other.\nself.view_as(other) is equivalent to self.view(other.size()).",
        "return_value": "",
        "parameters": "other (torch.Tensor) – The result tensor has the same size\nas other.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.view_as_complex",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.view_as_complex.html#torch.view_as_complex",
        "api_signature": "torch.view_as_complex(input)",
        "api_description": "Returns a view of input as a complex tensor. For an input complex\ntensor of size m1,m2,…,mi,2m1, m2, \\dots, mi, 2m1,m2,…,mi,2, this function returns a\nnew complex tensor of size m1,m2,…,mim1, m2, \\dots, mim1,m2,…,mi where the last\ndimension of the input tensor is expected to represent the real and imaginary\ncomponents of complex numbers.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.view_as_real",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.view_as_real.html#torch.view_as_real",
        "api_signature": "torch.view_as_real(input)",
        "api_description": "Returns a view of input as a real tensor. For an input complex tensor of\nsize m1,m2,…,mim1, m2, \\dots, mim1,m2,…,mi, this function returns a new\nreal tensor of size m1,m2,…,mi,2m1, m2, \\dots, mi, 2m1,m2,…,mi,2, where the last dimension of size 2\nrepresents the real and imaginary components of complex numbers.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.autograd.functional.vjp",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.autograd.functional.vjp.html#torch.autograd.functional.vjp",
        "api_signature": "torch.autograd.functional.vjp(func, inputs, v=None, create_graph=False, strict=False)",
        "api_description": "Compute the dot product between a vector v and the Jacobian of the given function at the point given by the inputs.",
        "return_value": "\ntuple with:func_output (tuple of Tensors or Tensor): output of func(inputs)\nvjp (tuple of Tensors or Tensor): result of the dot product with\nthe same shape as the inputs.\n\n\n\n",
        "parameters": "func (function) – a Python function that takes Tensor inputs and returns\na tuple of Tensors or a Tensor.\ninputs (tuple of Tensors or Tensor) – inputs to the function func.\nv (tuple of Tensors or Tensor) – The vector for which the vector\nJacobian product is computed.  Must be the same size as the output\nof func. This argument is optional when the output of func\ncontains a single element and (if it is not provided) will be set\nas a Tensor containing a single 1.\ncreate_graph (bool, optional) – If True, both the output and result\nwill be computed in a differentiable way. Note that when strict\nis False, the result can not require gradients or be\ndisconnected from the inputs.  Defaults to False.\nstrict (bool, optional) – If True, an error will be raised when we\ndetect that there exists an input such that all the outputs are\nindependent of it. If False, we return a Tensor of zeros as the\nvjp for said inputs, which is the expected mathematical value.\nDefaults to False.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.func.vjp",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.func.vjp.html#torch.func.vjp",
        "api_signature": "torch.func.vjp(func, *primals, has_aux=False)",
        "api_description": "Standing for the vector-Jacobian product, returns a tuple containing the\nresults of func applied to primals and a function that, when\ngiven cotangents, computes the reverse-mode Jacobian of func with\nrespect to primals times cotangents.",
        "return_value": "Returns a (output, vjp_fn) tuple containing the output of func\napplied to primals and a function that computes the vjp of\nfunc with respect to all primals using the cotangents passed\nto the returned function. If has_aux is True, then instead returns a\n(output, vjp_fn, aux) tuple.\nThe returned vjp_fn function will return a tuple of each VJP.\n",
        "parameters": "func (Callable) – A Python function that takes one or more arguments. Must\nreturn one or more Tensors.\nprimals (Tensors) – Positional arguments to func that must all be\nTensors. The returned function will also be computing the\nderivative with respect to these arguments\nhas_aux (bool) – Flag indicating that func returns a\n(output, aux) tuple where the first element is the output of\nthe function to be differentiated and the second element is\nother auxiliary objects that will not be differentiated.\nDefault: False.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.autograd.function.InplaceFunction.vjp",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.autograd.function.InplaceFunction.html#torch.autograd.function.InplaceFunction.vjp",
        "api_signature": "vjp(ctx, *grad_outputs)",
        "api_description": "Define a formula for differentiating the operation with backward mode automatic differentiation.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.autograd.function.NestedIOFunction.vjp",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.autograd.function.NestedIOFunction.html#torch.autograd.function.NestedIOFunction.vjp",
        "api_signature": "vjp(ctx, *grad_outputs)",
        "api_description": "Define a formula for differentiating the operation with backward mode automatic differentiation.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.vmap",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.vmap.html#torch.vmap",
        "api_signature": "torch.vmap(func, in_dims=0, out_dims=0, randomness='error', *, chunk_size=None)",
        "api_description": "vmap is the vectorizing map; vmap(func) returns a new function that\nmaps func over some dimension of the inputs. Semantically, vmap\npushes the map into PyTorch operations called by func, effectively\nvectorizing those operations.",
        "return_value": "Returns a new “batched” function. It takes the same inputs as\nfunc, except each input has an extra dimension at the index\nspecified by in_dims. It takes returns the same outputs as\nfunc, except each output has an extra dimension at the index\nspecified by out_dims.\n",
        "parameters": "func (function) – A Python function that takes one or more arguments.\nMust return one or more Tensors.\nin_dims (int or nested structure) – Specifies which dimension of the\ninputs should be mapped over. in_dims should have a\nstructure like the inputs. If the in_dim for a particular\ninput is None, then that indicates there is no map dimension.\nDefault: 0.\nout_dims (int or Tuple[int]) – Specifies where the mapped dimension\nshould appear in the outputs. If out_dims is a Tuple, then\nit should have one element per output. Default: 0.\nrandomness (str) – Specifies whether the randomness in this\nvmap should be the same or different across batches. If ‘different’,\nthe randomness for each batch will be different. If ‘same’, the\nrandomness will be the same across batches. If ‘error’, any calls to\nrandom functions will error. Default: ‘error’. WARNING: this flag\nPython’s random module or numpy randomness.\nchunk_size (None or int) – If None (default), apply a single vmap over inputs.\nIf not None, then compute the vmap chunk_size samples at a time.\nNote that chunk_size=1 is equivalent to computing the vmap with a for-loop.\nIf you run into memory issues computing the vmap, please try a non-None chunk_size.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.func.vmap",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.func.vmap.html#torch.func.vmap",
        "api_signature": "torch.func.vmap(func, in_dims=0, out_dims=0, randomness='error', *, chunk_size=None)",
        "api_description": "vmap is the vectorizing map; vmap(func) returns a new function that\nmaps func over some dimension of the inputs. Semantically, vmap\npushes the map into PyTorch operations called by func, effectively\nvectorizing those operations.",
        "return_value": "Returns a new “batched” function. It takes the same inputs as\nfunc, except each input has an extra dimension at the index\nspecified by in_dims. It takes returns the same outputs as\nfunc, except each output has an extra dimension at the index\nspecified by out_dims.\n",
        "parameters": "func (function) – A Python function that takes one or more arguments.\nMust return one or more Tensors.\nin_dims (int or nested structure) – Specifies which dimension of the\ninputs should be mapped over. in_dims should have a\nstructure like the inputs. If the in_dim for a particular\ninput is None, then that indicates there is no map dimension.\nDefault: 0.\nout_dims (int or Tuple[int]) – Specifies where the mapped dimension\nshould appear in the outputs. If out_dims is a Tuple, then\nit should have one element per output. Default: 0.\nrandomness (str) – Specifies whether the randomness in this\nvmap should be the same or different across batches. If ‘different’,\nthe randomness for each batch will be different. If ‘same’, the\nrandomness will be the same across batches. If ‘error’, any calls to\nrandom functions will error. Default: ‘error’. WARNING: this flag\nPython’s random module or numpy randomness.\nchunk_size (None or int) – If None (default), apply a single vmap over inputs.\nIf not None, then compute the vmap chunk_size samples at a time.\nNote that chunk_size=1 is equivalent to computing the vmap with a for-loop.\nIf you run into memory issues computing the vmap, please try a non-None chunk_size.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.autograd.Function.vmap",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.autograd.Function.vmap.html#torch.autograd.Function.vmap",
        "api_signature": "Function.vmap(info, in_dims, *args)",
        "api_description": "Define the behavior for this autograd.Function underneath torch.vmap().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.autograd.function.InplaceFunction.vmap",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.autograd.function.InplaceFunction.html#torch.autograd.function.InplaceFunction.vmap",
        "api_signature": "vmap(info, in_dims, *args)",
        "api_description": "Define the behavior for this autograd.Function underneath torch.vmap().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.autograd.function.NestedIOFunction.vmap",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.autograd.function.NestedIOFunction.html#torch.autograd.function.NestedIOFunction.vmap",
        "api_signature": "vmap(info, in_dims, *args)",
        "api_description": "Define the behavior for this autograd.Function underneath torch.vmap().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.von_mises.VonMises",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.von_mises.VonMises",
        "api_signature": "torch.distributions.von_mises.VonMises(loc, concentration, validate_args=None)",
        "api_description": "Bases: Distribution",
        "return_value": "",
        "parameters": "loc (torch.Tensor) – an angle in radians.\nconcentration (torch.Tensor) – concentration parameter",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> m = VonMises(torch.tensor([1.0]), torch.tensor([1.0]))\n>>> m.sample()  # von Mises distributed with loc=1 and concentration=1\ntensor([1.9777])\n\n\n"
    },
    {
        "api_name": "torch.vsplit",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.vsplit.html#torch.vsplit",
        "api_signature": "torch.vsplit(input, indices_or_sections)",
        "api_description": "Splits input, a tensor with two or more dimensions, into multiple tensors\nvertically according to indices_or_sections. Each split is a view of\ninput.",
        "return_value": "",
        "parameters": "input (Tensor) – tensor to split.\nindices_or_sections (int or list or tuple of ints) – See argument in torch.tensor_split().",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> t = torch.arange(16.0).reshape(4,4)\n>>> t\ntensor([[ 0.,  1.,  2.,  3.],\n        [ 4.,  5.,  6.,  7.],\n        [ 8.,  9., 10., 11.],\n        [12., 13., 14., 15.]])\n>>> torch.vsplit(t, 2)\n(tensor([[0., 1., 2., 3.],\n         [4., 5., 6., 7.]]),\n tensor([[ 8.,  9., 10., 11.],\n         [12., 13., 14., 15.]]))\n>>> torch.vsplit(t, [3, 6])\n(tensor([[ 0.,  1.,  2.,  3.],\n         [ 4.,  5.,  6.,  7.],\n         [ 8.,  9., 10., 11.]]),\n tensor([[12., 13., 14., 15.]]),\n tensor([], size=(0, 4)))\n\n\n"
    },
    {
        "api_name": "torch.Tensor.vsplit",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.vsplit.html#torch.Tensor.vsplit",
        "api_signature": "Tensor.vsplit(split_size_or_sections)",
        "api_description": "See torch.vsplit()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.vstack",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.vstack.html#torch.vstack",
        "api_signature": "torch.vstack(tensors, *, out=None)",
        "api_description": "Stack tensors in sequence vertically (row wise).",
        "return_value": "",
        "parameters": "tensors (sequence of Tensors) – sequence of tensors to concatenate\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.Store.wait",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#torch.distributed.Store.wait",
        "api_signature": "torch.distributed.Store.wait(*args, **kwargs)",
        "api_description": "Overloaded function.",
        "return_value": "",
        "parameters": "keys (list) – List of keys on which to wait until they are set in the store.\nkeys (list) – List of keys on which to wait until they are set in the store.\ntimeout (timedelta) – Time to wait for the keys to be added before throwing an exception.",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> # Using TCPStore as an example, other store types can also be used\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n>>> # This will throw an exception after 30 seconds\n>>> store.wait([\"bad_key\"])\n\n\n>>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> # Using TCPStore as an example, other store types can also be used\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n>>> # This will throw an exception after 10 seconds\n>>> store.wait([\"bad_key\"], timedelta(seconds=10))\n\n\n"
    },
    {
        "api_name": "torch.jit.wait",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.jit.wait.html#torch.jit.wait",
        "api_signature": "torch.jit.wait(future)",
        "api_description": "Force completion of a torch.jit.Future[T] asynchronous task, returning the result of the task.",
        "return_value": "the return value of the completed task\n",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda.Event.wait",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.cuda.Event.html#torch.cuda.Event.wait",
        "api_signature": "wait(stream=None)",
        "api_description": "Make all future work submitted to the given stream wait for this event.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.rendezvous.etcd_store.EtcdStore.wait",
        "api_url": "https://pytorch.org/docs/stable/elastic/rendezvous.html#torch.distributed.elastic.rendezvous.etcd_store.EtcdStore.wait",
        "api_signature": "wait(keys, override_timeout=None)",
        "api_description": "Wait until all of the keys are published, or until timeout.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.futures.Future.wait",
        "api_url": "https://pytorch.org/docs/stable/futures.html#torch.futures.Future.wait",
        "api_signature": "wait()",
        "api_description": "Block until the value of this Future is ready.",
        "return_value": "The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this wait method will\nalso throw an error.\n",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.mps.event.Event.wait",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.mps.event.Event.html#torch.mps.event.Event.wait",
        "api_signature": "wait()",
        "api_description": "Makes all future work submitted to the default stream wait for this event.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.xpu.Event.wait",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.xpu.Event.html#torch.xpu.Event.wait",
        "api_signature": "wait(stream=None)",
        "api_description": "Make all future work submitted to the given stream wait for this event.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.futures.wait_all",
        "api_url": "https://pytorch.org/docs/stable/futures.html#torch.futures.wait_all",
        "api_signature": "torch.futures.wait_all(futures)",
        "api_description": "Waits for all provided futures to be complete, and returns\nthe list of completed values. If any of the futures encounters an error,\nthe method will exit early and report the error not waiting for other\nfutures to complete.",
        "return_value": "A list of the completed Future results. This\nmethod will throw an error if wait on any\nFuture throws.\n",
        "parameters": "futures (list) – a list of Future object.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda.ExternalStream.wait_event",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.cuda.ExternalStream.html#torch.cuda.ExternalStream.wait_event",
        "api_signature": "wait_event(event)",
        "api_description": "Make all future work submitted to the stream wait for an event.",
        "return_value": "",
        "parameters": "event (torch.cuda.Event) – an event to wait for.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda.Stream.wait_event",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.cuda.Stream.html#torch.cuda.Stream.wait_event",
        "api_signature": "wait_event(event)",
        "api_description": "Make all future work submitted to the stream wait for an event.",
        "return_value": "",
        "parameters": "event (torch.cuda.Event) – an event to wait for.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.xpu.Stream.wait_event",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.xpu.Stream.html#torch.xpu.Stream.wait_event",
        "api_signature": "wait_event(event)",
        "api_description": "Make all future work submitted to the stream wait for an event.",
        "return_value": "",
        "parameters": "event (torch.xpu.Event) – an event to wait for.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda.ExternalStream.wait_stream",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.cuda.ExternalStream.html#torch.cuda.ExternalStream.wait_stream",
        "api_signature": "wait_stream(stream)",
        "api_description": "Synchronize with another stream.",
        "return_value": "",
        "parameters": "stream (Stream) – a stream to synchronize.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.cuda.Stream.wait_stream",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.cuda.Stream.html#torch.cuda.Stream.wait_stream",
        "api_signature": "wait_stream(stream)",
        "api_description": "Synchronize with another stream.",
        "return_value": "",
        "parameters": "stream (Stream) – a stream to synchronize.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.xpu.Stream.wait_stream",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.xpu.Stream.html#torch.xpu.Stream.wait_stream",
        "api_signature": "wait_stream(stream)",
        "api_description": "Synchronize with another stream.",
        "return_value": "",
        "parameters": "stream (Stream) – a stream to synchronize.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.weibull.Weibull",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.weibull.Weibull",
        "api_signature": "torch.distributions.weibull.Weibull(scale, concentration, validate_args=None)",
        "api_description": "Bases: TransformedDistribution",
        "return_value": "",
        "parameters": "scale (float or Tensor) – Scale parameter of distribution (lambda).\nconcentration (float or Tensor) – Concentration parameter of distribution (k/shape).",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.utils.weight_norm",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.utils.weight_norm.html#torch.nn.utils.weight_norm",
        "api_signature": "torch.nn.utils.weight_norm(module, name='weight', dim=0)",
        "api_description": "Apply weight normalization to a parameter in the given module.",
        "return_value": "The original module with the weight norm hook\n",
        "parameters": "module (Module) – containing module\nname (str, optional) – name of weight parameter\ndim (int, optional) – dimension over which to compute the norm",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.utils.parametrizations.weight_norm",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.utils.parametrizations.weight_norm.html#torch.nn.utils.parametrizations.weight_norm",
        "api_signature": "torch.nn.utils.parametrizations.weight_norm(module, name='weight', dim=0)",
        "api_description": "Apply weight normalization to a parameter in the given module.",
        "return_value": "The original module with the weight norm hook\n",
        "parameters": "module (Module) – containing module\nname (str, optional) – name of weight parameter\ndim (int, optional) – dimension over which to compute the norm",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.utils.data.WeightedRandomSampler",
        "api_url": "https://pytorch.org/docs/stable/data.html#torch.utils.data.WeightedRandomSampler",
        "api_signature": "torch.utils.data.WeightedRandomSampler(weights, num_samples, replacement=True, generator=None)",
        "api_description": "Samples elements from [0,..,len(weights)-1] with given probabilities (weights).",
        "return_value": "",
        "parameters": "weights (sequence) – a sequence of weights, not necessary summing up to one\nnum_samples (int) – number of samples to draw\nreplacement (bool) – if True, samples are drawn with replacement.\nIf not, they are drawn without replacement, which means that when a\nsample index is drawn for a row, it cannot be drawn again for that row.\ngenerator (Generator) – Generator used in sampling.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.where",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.where.html#torch.where",
        "api_signature": "torch.where(condition, input, other, *, out=None)",
        "api_description": "Return a tensor of elements selected from either input or other, depending on condition.",
        "return_value": "A tensor of shape equal to the broadcasted shape of condition, input, other\n",
        "parameters": "condition (BoolTensor) – When True (nonzero), yield input, otherwise yield other\ninput (Tensor or Scalar) – value (if input is a scalar) or values selected at indices\nwhere condition is True\nother (Tensor or Scalar) – value (if other is a scalar) or values selected at indices\nwhere condition is False\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.where",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.where.html#torch.Tensor.where",
        "api_signature": "Tensor.where(condition, y)",
        "api_description": "self.where(condition, y) is equivalent to torch.where(condition, self, y).\nSee torch.where()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributions.wishart.Wishart",
        "api_url": "https://pytorch.org/docs/stable/distributions.html#torch.distributions.wishart.Wishart",
        "api_signature": "torch.distributions.wishart.Wishart(df, covariance_matrix=None, precision_matrix=None, scale_tril=None, validate_args=None)",
        "api_description": "Bases: ExponentialFamily",
        "return_value": "",
        "parameters": "df (float or Tensor) – real-valued parameter larger than the (dimension of Square matrix) - 1\ncovariance_matrix (Tensor) – positive-definite covariance matrix\nprecision_matrix (Tensor) – positive-definite precision matrix\nscale_tril (Tensor) – lower-triangular factor of covariance, with positive-valued diagonal",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.observer.ObserverBase.with_args",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.quantization.observer.ObserverBase.html#torch.ao.quantization.observer.ObserverBase.with_args",
        "api_signature": "with_args(**kwargs)",
        "api_description": "Wrapper that allows creation of class factories.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.ao.quantization.observer.ObserverBase.with_callable_args",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.ao.quantization.observer.ObserverBase.html#torch.ao.quantization.observer.ObserverBase.with_callable_args",
        "api_signature": "with_callable_args(**kwargs)",
        "api_description": "Wrapper that allows creation of class factories args that need to be\ncalled at construction time.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.Work",
        "api_url": "https://pytorch.org/docs/stable/distributed.html#torch.distributed.Work",
        "api_signature": null,
        "api_description": "A Work object represents the handle to a pending asynchronous operation in\nPyTorch’s distributed package. It is returned by non-blocking collective operations,\nsuch as dist.all_reduce(tensor, async_op=True).",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.agent.server.Worker",
        "api_url": "https://pytorch.org/docs/stable/elastic/agent.html#torch.distributed.elastic.agent.server.Worker",
        "api_signature": "torch.distributed.elastic.agent.server.Worker(local_rank, global_rank=-1, role_rank=-1, world_size=-1, role_world_size=-1)",
        "api_description": "A worker instance.",
        "return_value": "",
        "parameters": "id (Any) – uniquely identifies a worker (interpreted by the agent)\nlocal_rank (int) – local rank of the worker\nglobal_rank (int) – global rank of the worker\nrole_rank (int) – rank of the worker across all workers that have the same role\nworld_size (int) – number of workers (globally)\nrole_world_size (int) – number of workers that have the same role",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.agent.server.WorkerGroup",
        "api_url": "https://pytorch.org/docs/stable/elastic/agent.html#torch.distributed.elastic.agent.server.WorkerGroup",
        "api_signature": "torch.distributed.elastic.agent.server.WorkerGroup(spec)",
        "api_description": "A set of Worker instances.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.rpc.WorkerInfo",
        "api_url": "https://pytorch.org/docs/stable/rpc.html#torch.distributed.rpc.WorkerInfo",
        "api_signature": null,
        "api_description": "A structure that encapsulates information of a worker in the system.\nContains the name and ID of the worker. This class is not meant to\nbe constructed directly, rather, an instance can be retrieved\nthrough get_worker_info() and the\nresult can be passed in to functions such as\nrpc_sync(), rpc_async(),\nremote() to avoid copying a string on\nevery invocation.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.agent.server.WorkerSpec",
        "api_url": "https://pytorch.org/docs/stable/elastic/agent.html#torch.distributed.elastic.agent.server.WorkerSpec",
        "api_signature": "torch.distributed.elastic.agent.server.WorkerSpec(role, local_world_size, rdzv_handler, fn=None, entrypoint=None, args=()",
        "api_description": "Blueprint information about a particular type of worker.",
        "return_value": "",
        "parameters": "role (str) – user-defined role for the workers with this spec\nlocal_world_size (int) – number local workers to run\nfn (Optional[Callable]) – (deprecated use entrypoint instead)\nentrypoint (Optional[Union[Callable, str]]) – worker function or command\nargs (Tuple) – arguments to pass to entrypoint\nrdzv_handler (RendezvousHandler) – handles rdzv for this set of workers\nmax_restarts (int) – number of max retries for the workers\nmonitor_interval (float) – monitor status of workers every n seconds\nmaster_port (Optional[int]) – fixed port to run the c10d store on rank 0\nif not specified then will chose a random free port\nmaster_addr (Optional[str]) – fixed master_addr to run the c10d store on rank 0\nif not specified then will chose hostname on agent rank 0\nredirects – redirect std streams to a file,\nselectively redirect for a particular\nlocal rank by passing a map\ntee – tees the specified std stream(s) to console + file,\nselectively tee for a particular local rank by passing a map,\ntakes precedence over redirects settings.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.elastic.agent.server.WorkerState",
        "api_url": "https://pytorch.org/docs/stable/elastic/agent.html#torch.distributed.elastic.agent.server.WorkerState",
        "api_signature": "torch.distributed.elastic.agent.server.WorkerState(value)",
        "api_description": "A state of the WorkerGroup.",
        "return_value": "True if the worker state represents workers still running\n(e.g. that the process exists but not necessarily healthy).\n",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.fx.wrap",
        "api_url": "https://pytorch.org/docs/stable/fx.html#torch.fx.wrap",
        "api_signature": "torch.fx.wrap(fn_or_name)",
        "api_description": "This function can be called at module-level scope to register fn_or_name as a “leaf function”.\nA “leaf function” will be preserved as a CallFunction node in the FX trace instead of being\ntraced through:",
        "return_value": "",
        "parameters": "fn_or_name (Union[str, Callable]) – The function or name of the global function to insert into the\ngraph when it’s called",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.overrides.wrap_torch_function",
        "api_url": "https://pytorch.org/docs/stable/torch.overrides.html#torch.overrides.wrap_torch_function",
        "api_signature": "torch.overrides.wrap_torch_function(dispatcher)",
        "api_description": "Wraps a given function with __torch_function__ -related functionality.",
        "return_value": "",
        "parameters": "dispatcher (Callable) – A callable that returns an iterable of Tensor-likes passed into the function.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.checkpoint.StorageWriter.write_data",
        "api_url": "https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.StorageWriter.write_data",
        "api_signature": "write_data(plan, planner)",
        "api_description": "Write all items from plan using planner to resolve the data.",
        "return_value": "A future that completes to a list of WriteResult\n",
        "parameters": "plan (SavePlan) – The save plan to execute.\nplanner (SavePlanner) – Planner object to be used to resolve items to data.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.checkpoint.planner.WriteItem",
        "api_url": "https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.planner.WriteItem",
        "api_signature": "torch.distributed.checkpoint.planner.WriteItem(index, type, tensor_data=None)",
        "api_description": "Dataclass which holds information about what needs to be written to storage.",
        "return_value": "Optional[int] storage size, in bytes of underlying tensor if any.\n",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.init.xavier_normal_",
        "api_url": "https://pytorch.org/docs/stable/nn.init.html#torch.nn.init.xavier_normal_",
        "api_signature": "torch.nn.init.xavier_normal_(tensor, gain=1.0, generator=None)",
        "api_description": "Fill the input Tensor with values using a Xavier normal distribution.",
        "return_value": "",
        "parameters": "tensor (Tensor) – an n-dimensional torch.Tensor\ngain (float) – an optional scaling factor\ngenerator (Optional[Generator]) – the torch Generator to sample from (default: None)",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.init.xavier_uniform_",
        "api_url": "https://pytorch.org/docs/stable/nn.init.html#torch.nn.init.xavier_uniform_",
        "api_signature": "torch.nn.init.xavier_uniform_(tensor, gain=1.0, generator=None)",
        "api_description": "Fill the input Tensor with values using a Xavier uniform distribution.",
        "return_value": "",
        "parameters": "tensor (Tensor) – an n-dimensional torch.Tensor\ngain (float) – an optional scaling factor\ngenerator (Optional[Generator]) – the torch Generator to sample from (default: None)",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.special.xlog1py",
        "api_url": "https://pytorch.org/docs/stable/special.html#torch.special.xlog1py",
        "api_signature": "torch.special.xlog1py(input, other, *, out=None)",
        "api_description": "Computes input * log1p(other) with the following cases.",
        "return_value": "",
        "parameters": "input (Number or Tensor) – Multiplier\nother (Number or Tensor) – Argument\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.xlogy",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.xlogy.html#torch.xlogy",
        "api_signature": "torch.xlogy(input, other, *, out=None)",
        "api_description": "Alias for torch.special.xlogy().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.special.xlogy",
        "api_url": "https://pytorch.org/docs/stable/special.html#torch.special.xlogy",
        "api_signature": "torch.special.xlogy(input, other, *, out=None)",
        "api_description": "Computes input * log(other) with the following cases.",
        "return_value": "",
        "parameters": "input (Number or Tensor) – Multiplier\nother (Number or Tensor) – Argument\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.xlogy",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.xlogy.html#torch.Tensor.xlogy",
        "api_signature": "Tensor.xlogy(other)",
        "api_description": "See torch.xlogy()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.xlogy_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.xlogy_.html#torch.Tensor.xlogy_",
        "api_signature": "Tensor.xlogy_(other)",
        "api_description": "In-place version of xlogy()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.jit.ScriptModule.xpu",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.jit.ScriptModule.html#torch.jit.ScriptModule.xpu",
        "api_signature": "xpu(device=None)",
        "api_description": "Move all model parameters and buffers to the XPU.",
        "return_value": "self\n",
        "parameters": "device (int, optional) – if specified, all parameters will be\ncopied to that device",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.Module.xpu",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.xpu",
        "api_signature": "xpu(device=None)",
        "api_description": "Move all model parameters and buffers to the XPU.",
        "return_value": "self\n",
        "parameters": "device (int, optional) – if specified, all parameters will be\ncopied to that device",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.Tensor.zero_",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.Tensor.zero_.html#torch.Tensor.zero_",
        "api_signature": "Tensor.zero_()",
        "api_description": "Fills self tensor with zeros.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.jit.ScriptModule.zero_grad",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.jit.ScriptModule.html#torch.jit.ScriptModule.zero_grad",
        "api_signature": "zero_grad(set_to_none=True)",
        "api_description": "Reset gradients of all model parameters.",
        "return_value": "",
        "parameters": "set_to_none (bool) – instead of setting to zero, set the grads to None.\nSee torch.optim.Optimizer.zero_grad() for details.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.Module.zero_grad",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.zero_grad",
        "api_signature": "zero_grad(set_to_none=True)",
        "api_description": "Reset gradients of all model parameters.",
        "return_value": "",
        "parameters": "set_to_none (bool) – instead of setting to zero, set the grads to None.\nSee torch.optim.Optimizer.zero_grad() for details.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.Adadelta.zero_grad",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.Adadelta.html#torch.optim.Adadelta.zero_grad",
        "api_signature": "zero_grad(set_to_none=True)",
        "api_description": "Resets the gradients of all optimized torch.Tensor s.",
        "return_value": "",
        "parameters": "set_to_none (bool) – instead of setting to zero, set the grads to None.\nThis will in general have lower memory footprint, and can modestly improve performance.\nHowever, it changes certain behaviors. For example:\n1. When the user tries to access a gradient and perform manual ops on it,\na None attribute or a Tensor full of 0s will behave differently.\n2. If the user requests zero_grad(set_to_none=True) followed by a backward pass, .grads\nare guaranteed to be None for params that did not receive a gradient.\n3. torch.optim optimizers have a different behavior if the gradient is 0 or None\n(in one case it does the step with a gradient of 0 and in the other it skips\nthe step altogether).",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.Adagrad.zero_grad",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.Adagrad.html#torch.optim.Adagrad.zero_grad",
        "api_signature": "zero_grad(set_to_none=True)",
        "api_description": "Resets the gradients of all optimized torch.Tensor s.",
        "return_value": "",
        "parameters": "set_to_none (bool) – instead of setting to zero, set the grads to None.\nThis will in general have lower memory footprint, and can modestly improve performance.\nHowever, it changes certain behaviors. For example:\n1. When the user tries to access a gradient and perform manual ops on it,\na None attribute or a Tensor full of 0s will behave differently.\n2. If the user requests zero_grad(set_to_none=True) followed by a backward pass, .grads\nare guaranteed to be None for params that did not receive a gradient.\n3. torch.optim optimizers have a different behavior if the gradient is 0 or None\n(in one case it does the step with a gradient of 0 and in the other it skips\nthe step altogether).",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.Adam.zero_grad",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam.zero_grad",
        "api_signature": "zero_grad(set_to_none=True)",
        "api_description": "Resets the gradients of all optimized torch.Tensor s.",
        "return_value": "",
        "parameters": "set_to_none (bool) – instead of setting to zero, set the grads to None.\nThis will in general have lower memory footprint, and can modestly improve performance.\nHowever, it changes certain behaviors. For example:\n1. When the user tries to access a gradient and perform manual ops on it,\na None attribute or a Tensor full of 0s will behave differently.\n2. If the user requests zero_grad(set_to_none=True) followed by a backward pass, .grads\nare guaranteed to be None for params that did not receive a gradient.\n3. torch.optim optimizers have a different behavior if the gradient is 0 or None\n(in one case it does the step with a gradient of 0 and in the other it skips\nthe step altogether).",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.Adamax.zero_grad",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.Adamax.html#torch.optim.Adamax.zero_grad",
        "api_signature": "zero_grad(set_to_none=True)",
        "api_description": "Resets the gradients of all optimized torch.Tensor s.",
        "return_value": "",
        "parameters": "set_to_none (bool) – instead of setting to zero, set the grads to None.\nThis will in general have lower memory footprint, and can modestly improve performance.\nHowever, it changes certain behaviors. For example:\n1. When the user tries to access a gradient and perform manual ops on it,\na None attribute or a Tensor full of 0s will behave differently.\n2. If the user requests zero_grad(set_to_none=True) followed by a backward pass, .grads\nare guaranteed to be None for params that did not receive a gradient.\n3. torch.optim optimizers have a different behavior if the gradient is 0 or None\n(in one case it does the step with a gradient of 0 and in the other it skips\nthe step altogether).",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.AdamW.zero_grad",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html#torch.optim.AdamW.zero_grad",
        "api_signature": "zero_grad(set_to_none=True)",
        "api_description": "Resets the gradients of all optimized torch.Tensor s.",
        "return_value": "",
        "parameters": "set_to_none (bool) – instead of setting to zero, set the grads to None.\nThis will in general have lower memory footprint, and can modestly improve performance.\nHowever, it changes certain behaviors. For example:\n1. When the user tries to access a gradient and perform manual ops on it,\na None attribute or a Tensor full of 0s will behave differently.\n2. If the user requests zero_grad(set_to_none=True) followed by a backward pass, .grads\nare guaranteed to be None for params that did not receive a gradient.\n3. torch.optim optimizers have a different behavior if the gradient is 0 or None\n(in one case it does the step with a gradient of 0 and in the other it skips\nthe step altogether).",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.ASGD.zero_grad",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.ASGD.html#torch.optim.ASGD.zero_grad",
        "api_signature": "zero_grad(set_to_none=True)",
        "api_description": "Resets the gradients of all optimized torch.Tensor s.",
        "return_value": "",
        "parameters": "set_to_none (bool) – instead of setting to zero, set the grads to None.\nThis will in general have lower memory footprint, and can modestly improve performance.\nHowever, it changes certain behaviors. For example:\n1. When the user tries to access a gradient and perform manual ops on it,\na None attribute or a Tensor full of 0s will behave differently.\n2. If the user requests zero_grad(set_to_none=True) followed by a backward pass, .grads\nare guaranteed to be None for params that did not receive a gradient.\n3. torch.optim optimizers have a different behavior if the gradient is 0 or None\n(in one case it does the step with a gradient of 0 and in the other it skips\nthe step altogether).",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.LBFGS.zero_grad",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.LBFGS.html#torch.optim.LBFGS.zero_grad",
        "api_signature": "zero_grad(set_to_none=True)",
        "api_description": "Resets the gradients of all optimized torch.Tensor s.",
        "return_value": "",
        "parameters": "set_to_none (bool) – instead of setting to zero, set the grads to None.\nThis will in general have lower memory footprint, and can modestly improve performance.\nHowever, it changes certain behaviors. For example:\n1. When the user tries to access a gradient and perform manual ops on it,\na None attribute or a Tensor full of 0s will behave differently.\n2. If the user requests zero_grad(set_to_none=True) followed by a backward pass, .grads\nare guaranteed to be None for params that did not receive a gradient.\n3. torch.optim optimizers have a different behavior if the gradient is 0 or None\n(in one case it does the step with a gradient of 0 and in the other it skips\nthe step altogether).",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.NAdam.zero_grad",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.NAdam.html#torch.optim.NAdam.zero_grad",
        "api_signature": "zero_grad(set_to_none=True)",
        "api_description": "Resets the gradients of all optimized torch.Tensor s.",
        "return_value": "",
        "parameters": "set_to_none (bool) – instead of setting to zero, set the grads to None.\nThis will in general have lower memory footprint, and can modestly improve performance.\nHowever, it changes certain behaviors. For example:\n1. When the user tries to access a gradient and perform manual ops on it,\na None attribute or a Tensor full of 0s will behave differently.\n2. If the user requests zero_grad(set_to_none=True) followed by a backward pass, .grads\nare guaranteed to be None for params that did not receive a gradient.\n3. torch.optim optimizers have a different behavior if the gradient is 0 or None\n(in one case it does the step with a gradient of 0 and in the other it skips\nthe step altogether).",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.Optimizer.zero_grad",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.Optimizer.zero_grad.html#torch.optim.Optimizer.zero_grad",
        "api_signature": "Optimizer.zero_grad(set_to_none=True)",
        "api_description": "Resets the gradients of all optimized torch.Tensor s.",
        "return_value": "",
        "parameters": "set_to_none (bool) – instead of setting to zero, set the grads to None.\nThis will in general have lower memory footprint, and can modestly improve performance.\nHowever, it changes certain behaviors. For example:\n1. When the user tries to access a gradient and perform manual ops on it,\na None attribute or a Tensor full of 0s will behave differently.\n2. If the user requests zero_grad(set_to_none=True) followed by a backward pass, .grads\nare guaranteed to be None for params that did not receive a gradient.\n3. torch.optim optimizers have a different behavior if the gradient is 0 or None\n(in one case it does the step with a gradient of 0 and in the other it skips\nthe step altogether).",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.RAdam.zero_grad",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.RAdam.html#torch.optim.RAdam.zero_grad",
        "api_signature": "zero_grad(set_to_none=True)",
        "api_description": "Resets the gradients of all optimized torch.Tensor s.",
        "return_value": "",
        "parameters": "set_to_none (bool) – instead of setting to zero, set the grads to None.\nThis will in general have lower memory footprint, and can modestly improve performance.\nHowever, it changes certain behaviors. For example:\n1. When the user tries to access a gradient and perform manual ops on it,\na None attribute or a Tensor full of 0s will behave differently.\n2. If the user requests zero_grad(set_to_none=True) followed by a backward pass, .grads\nare guaranteed to be None for params that did not receive a gradient.\n3. torch.optim optimizers have a different behavior if the gradient is 0 or None\n(in one case it does the step with a gradient of 0 and in the other it skips\nthe step altogether).",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.RMSprop.zero_grad",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.RMSprop.html#torch.optim.RMSprop.zero_grad",
        "api_signature": "zero_grad(set_to_none=True)",
        "api_description": "Resets the gradients of all optimized torch.Tensor s.",
        "return_value": "",
        "parameters": "set_to_none (bool) – instead of setting to zero, set the grads to None.\nThis will in general have lower memory footprint, and can modestly improve performance.\nHowever, it changes certain behaviors. For example:\n1. When the user tries to access a gradient and perform manual ops on it,\na None attribute or a Tensor full of 0s will behave differently.\n2. If the user requests zero_grad(set_to_none=True) followed by a backward pass, .grads\nare guaranteed to be None for params that did not receive a gradient.\n3. torch.optim optimizers have a different behavior if the gradient is 0 or None\n(in one case it does the step with a gradient of 0 and in the other it skips\nthe step altogether).",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.Rprop.zero_grad",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.Rprop.html#torch.optim.Rprop.zero_grad",
        "api_signature": "zero_grad(set_to_none=True)",
        "api_description": "Resets the gradients of all optimized torch.Tensor s.",
        "return_value": "",
        "parameters": "set_to_none (bool) – instead of setting to zero, set the grads to None.\nThis will in general have lower memory footprint, and can modestly improve performance.\nHowever, it changes certain behaviors. For example:\n1. When the user tries to access a gradient and perform manual ops on it,\na None attribute or a Tensor full of 0s will behave differently.\n2. If the user requests zero_grad(set_to_none=True) followed by a backward pass, .grads\nare guaranteed to be None for params that did not receive a gradient.\n3. torch.optim optimizers have a different behavior if the gradient is 0 or None\n(in one case it does the step with a gradient of 0 and in the other it skips\nthe step altogether).",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.SGD.zero_grad",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.SGD.html#torch.optim.SGD.zero_grad",
        "api_signature": "zero_grad(set_to_none=True)",
        "api_description": "Resets the gradients of all optimized torch.Tensor s.",
        "return_value": "",
        "parameters": "set_to_none (bool) – instead of setting to zero, set the grads to None.\nThis will in general have lower memory footprint, and can modestly improve performance.\nHowever, it changes certain behaviors. For example:\n1. When the user tries to access a gradient and perform manual ops on it,\na None attribute or a Tensor full of 0s will behave differently.\n2. If the user requests zero_grad(set_to_none=True) followed by a backward pass, .grads\nare guaranteed to be None for params that did not receive a gradient.\n3. torch.optim optimizers have a different behavior if the gradient is 0 or None\n(in one case it does the step with a gradient of 0 and in the other it skips\nthe step altogether).",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.optim.SparseAdam.zero_grad",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.optim.SparseAdam.html#torch.optim.SparseAdam.zero_grad",
        "api_signature": "zero_grad(set_to_none=True)",
        "api_description": "Resets the gradients of all optimized torch.Tensor s.",
        "return_value": "",
        "parameters": "set_to_none (bool) – instead of setting to zero, set the grads to None.\nThis will in general have lower memory footprint, and can modestly improve performance.\nHowever, it changes certain behaviors. For example:\n1. When the user tries to access a gradient and perform manual ops on it,\na None attribute or a Tensor full of 0s will behave differently.\n2. If the user requests zero_grad(set_to_none=True) followed by a backward pass, .grads\nare guaranteed to be None for params that did not receive a gradient.\n3. torch.optim optimizers have a different behavior if the gradient is 0 or None\n(in one case it does the step with a gradient of 0 and in the other it skips\nthe step altogether).",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.ZeroPad1d",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.ZeroPad1d.html#torch.nn.ZeroPad1d",
        "api_signature": "torch.nn.ZeroPad1d(padding)",
        "api_description": "Pads the input tensor boundaries with zero.",
        "return_value": "",
        "parameters": "padding (int, tuple) – the size of the padding. If is int, uses the same\npadding in both boundaries. If a 2-tuple, uses\n(padding_left\\text{padding\\_left}padding_left, padding_right\\text{padding\\_right}padding_right)",
        "input_shape": "\nInput: (C,Win)(C, W_{in})(C,Win​) or (N,C,Win)(N, C, W_{in})(N,C,Win​).\nOutput: (C,Wout)(C, W_{out})(C,Wout​) or (N,C,Wout)(N, C, W_{out})(N,C,Wout​), where\nWout=Win+padding_left+padding_rightW_{out} = W_{in} + \\text{padding\\_left} + \\text{padding\\_right}Wout​=Win​+padding_left+padding_right\n\n\n",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.ZeroPad2d",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.ZeroPad2d.html#torch.nn.ZeroPad2d",
        "api_signature": "torch.nn.ZeroPad2d(padding)",
        "api_description": "Pads the input tensor boundaries with zero.",
        "return_value": "",
        "parameters": "padding (int, tuple) – the size of the padding. If is int, uses the same\npadding in all boundaries. If a 4-tuple, uses (padding_left\\text{padding\\_left}padding_left,\npadding_right\\text{padding\\_right}padding_right, padding_top\\text{padding\\_top}padding_top, padding_bottom\\text{padding\\_bottom}padding_bottom)",
        "input_shape": "\nInput: (N,C,Hin,Win)(N, C, H_{in}, W_{in})(N,C,Hin​,Win​) or (C,Hin,Win)(C, H_{in}, W_{in})(C,Hin​,Win​).\nOutput: (N,C,Hout,Wout)(N, C, H_{out}, W_{out})(N,C,Hout​,Wout​) or (C,Hout,Wout)(C, H_{out}, W_{out})(C,Hout​,Wout​), where\nHout=Hin+padding_top+padding_bottomH_{out} = H_{in} + \\text{padding\\_top} + \\text{padding\\_bottom}Hout​=Hin​+padding_top+padding_bottom\nWout=Win+padding_left+padding_rightW_{out} = W_{in} + \\text{padding\\_left} + \\text{padding\\_right}Wout​=Win​+padding_left+padding_right\n\n\n",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.ZeroPad3d",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.nn.ZeroPad3d.html#torch.nn.ZeroPad3d",
        "api_signature": "torch.nn.ZeroPad3d(padding)",
        "api_description": "Pads the input tensor boundaries with zero.",
        "return_value": "",
        "parameters": "padding (int, tuple) – the size of the padding. If is int, uses the same\npadding in all boundaries. If a 6-tuple, uses\n(padding_left\\text{padding\\_left}padding_left, padding_right\\text{padding\\_right}padding_right,\npadding_top\\text{padding\\_top}padding_top, padding_bottom\\text{padding\\_bottom}padding_bottom,\npadding_front\\text{padding\\_front}padding_front, padding_back\\text{padding\\_back}padding_back)",
        "input_shape": "\nInput: (N,C,Din,Hin,Win)(N, C, D_{in}, H_{in}, W_{in})(N,C,Din​,Hin​,Win​) or (C,Din,Hin,Win)(C, D_{in}, H_{in}, W_{in})(C,Din​,Hin​,Win​).\nOutput: (N,C,Dout,Hout,Wout)(N, C, D_{out}, H_{out}, W_{out})(N,C,Dout​,Hout​,Wout​) or\n(C,Dout,Hout,Wout)(C, D_{out}, H_{out}, W_{out})(C,Dout​,Hout​,Wout​), where\nDout=Din+padding_front+padding_backD_{out} = D_{in} + \\text{padding\\_front} + \\text{padding\\_back}Dout​=Din​+padding_front+padding_back\nHout=Hin+padding_top+padding_bottomH_{out} = H_{in} + \\text{padding\\_top} + \\text{padding\\_bottom}Hout​=Hin​+padding_top+padding_bottom\nWout=Win+padding_left+padding_rightW_{out} = W_{in} + \\text{padding\\_left} + \\text{padding\\_right}Wout​=Win​+padding_left+padding_right\n\n\n",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.distributed.optim.ZeroRedundancyOptimizer",
        "api_url": "https://pytorch.org/docs/stable/distributed.optim.html#torch.distributed.optim.ZeroRedundancyOptimizer",
        "api_signature": "torch.distributed.optim.ZeroRedundancyOptimizer(params, optimizer_class, process_group=None, parameters_as_bucket_view=False, overlap_with_ddp=False, **defaults)",
        "api_description": "Wrap an arbitrary optim.Optimizer and shards its states across ranks in the group.",
        "return_value": "Optional loss depending on the underlying local optimizer.\n",
        "parameters": "params (Iterable) – an Iterable of torch.Tensor s\nor dict s giving all parameters, which will be sharded\nacross ranks.\noptimizer_class (torch.nn.Optimizer) – the class of the local\noptimizer.\nprocess_group (ProcessGroup, optional) – torch.distributed\nProcessGroup (default: dist.group.WORLD initialized by\ntorch.distributed.init_process_group()).\nparameters_as_bucket_view (bool, optional) – if True, parameters are\npacked into buckets to speed up communication, and param.data\nfields point to bucket views at different offsets; if False,\neach individual parameter is communicated separately, and each\nparams.data stays intact (default: False).\noverlap_with_ddp (bool, optional) – if True, step() is\noverlapped with DistributedDataParallel ‘s gradient\nsynchronization; this requires (1) either a functional optimizer\nfor the optimizer_class argument or one with a functional\nequivalent and (2) registering a DDP communication hook\nconstructed from one of the functions in ddp_zero_hook.py;\nparameters are packed into buckets matching those in\nDistributedDataParallel, meaning that the\nparameters_as_bucket_view argument is ignored.\nIf False, step() runs disjointly after the backward pass\n(per normal).\n(default: False)\n**defaults – any trailing arguments, which are forwarded to the local\noptimizer.\nparam_group (dict) – specifies the parameters to be optimized and\ngroup-specific optimization options.\nto (int) – the rank that receives the optimizer states (default: 0).\nkwargs (dict) – a dict containing any keyword arguments\nto modify the behavior of the join hook at run time; all\nJoinable instances sharing the same join context\nmanager are forwarded the same value for kwargs.\nstate_dict (dict) – optimizer state; should be an object returned\nfrom a call to state_dict().\nclosure (Callable) – a closure that re-evaluates the model and\nreturns the loss; optional for most optimizers.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.zeros",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.zeros.html#torch.zeros",
        "api_signature": "torch.zeros(*size, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False)",
        "api_description": "Returns a tensor filled with the scalar value 0, with the shape defined\nby the variable argument size.",
        "return_value": "",
        "parameters": "size (int...) – a sequence of integers defining the shape of the output tensor.\nCan be a variable number of arguments or a collection like a list or tuple.\nout (Tensor, optional) – the output tensor.\ndtype (torch.dtype, optional) – the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_dtype()).\nlayout (torch.layout, optional) – the desired layout of returned Tensor.\nDefault: torch.strided.\ndevice (torch.device, optional) – the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_device()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types.\nrequires_grad (bool, optional) – If autograd should record operations on the\nreturned tensor. Default: False.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.nn.init.zeros_",
        "api_url": "https://pytorch.org/docs/stable/nn.init.html#torch.nn.init.zeros_",
        "api_signature": "torch.nn.init.zeros_(tensor)",
        "api_description": "Fill the input Tensor with the scalar value 0.",
        "return_value": "",
        "parameters": "tensor (Tensor) – an n-dimensional torch.Tensor",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.zeros_like",
        "api_url": "https://pytorch.org/docs/stable/generated/torch.zeros_like.html#torch.zeros_like",
        "api_signature": "torch.zeros_like(input, *, dtype=None, layout=None, device=None, requires_grad=False, memory_format=torch.preserve_format)",
        "api_description": "Returns a tensor filled with the scalar value 0, with the same size as\ninput. torch.zeros_like(input) is equivalent to\ntorch.zeros(input.size(), dtype=input.dtype, layout=input.layout, device=input.device).",
        "return_value": "",
        "parameters": "input (Tensor) – the size of input will determine size of the output tensor.\ndtype (torch.dtype, optional) – the desired data type of returned Tensor.\nDefault: if None, defaults to the dtype of input.\nlayout (torch.layout, optional) – the desired layout of returned tensor.\nDefault: if None, defaults to the layout of input.\ndevice (torch.device, optional) – the desired device of returned tensor.\nDefault: if None, defaults to the device of input.\nrequires_grad (bool, optional) – If autograd should record operations on the\nreturned tensor. Default: False.\nmemory_format (torch.memory_format, optional) – the desired memory format of\nreturned Tensor. Default: torch.preserve_format.",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "torch.special.zeta",
        "api_url": "https://pytorch.org/docs/stable/special.html#torch.special.zeta",
        "api_signature": "torch.special.zeta(input, other, *, out=None)",
        "api_description": "Computes the Hurwitz zeta function, elementwise.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor corresponding to x.\nother (Tensor) – the input tensor corresponding to q.\nout (Tensor, optional) – the output tensor.",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> x = torch.tensor([2., 4.])\n>>> torch.special.zeta(x, 1)\ntensor([1.6449, 1.0823])\n>>> torch.special.zeta(x, torch.tensor([1., 2.]))\ntensor([1.6449, 0.0823])\n>>> torch.special.zeta(2, torch.tensor([1., 2.]))\ntensor([1.6449, 0.6449])\n\n\n"
    }
]