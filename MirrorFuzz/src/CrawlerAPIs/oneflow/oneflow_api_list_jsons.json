[
    {
        "api_name": "oneflow.BoolTensor",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.BoolTensor.html",
        "api_signature": null,
        "api_description": "Creates a Tensor with the dtype of oneflow.bool and the device on cpu, it has the same parameters as oneflow.Tensor()\n__init__()¶\nInitialize self.  See help(type(self)) for accurate signature.\nMethods\n__delattr__(name, /)\nImplement delattr(self, name).\n__dir__()\nDefault dir() implementation.\n__eq__(value, /)\nReturn self==value.\n__format__(format_spec, /)\nDefault object formatter.\n__ge__(value, /)\nReturn self>=value.\n__getattribute__(name, /)\nReturn getattr(self, name).\n__gt__(value, /)\nReturn self>value.\n__hash__()\nReturn hash(self).\n__init__()\nInitialize self.\n__init_subclass__\nThis method is called when a class is subclassed.\n__le__(value, /)\nReturn self<=value.\n__lt__(value, /)\nReturn self<value.\n__ne__(value, /)\nReturn self!=value.\n__new__(**kwargs)\nCreate and return a new object.\n__reduce__()\nHelper for pickle.\n__reduce_ex__(protocol, /)\nHelper for pickle.\n__repr__()\nReturn repr(self).\n__setattr__(name, value, /)\nImplement setattr(self, name, value).\n__sizeof__()\nSize of object in memory, in bytes.\n__str__()\nReturn str(self).\n__subclasshook__\nAbstract classes can override this to customize issubclass().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.ByteTensor",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.ByteTensor.html",
        "api_signature": null,
        "api_description": "Creates a Tensor with the dtype of oneflow.uint8 and the device on cpu, it has the same parameters as oneflow.Tensor()\n__init__()¶\nInitialize self.  See help(type(self)) for accurate signature.\nMethods\n__delattr__(name, /)\nImplement delattr(self, name).\n__dir__()\nDefault dir() implementation.\n__eq__(value, /)\nReturn self==value.\n__format__(format_spec, /)\nDefault object formatter.\n__ge__(value, /)\nReturn self>=value.\n__getattribute__(name, /)\nReturn getattr(self, name).\n__gt__(value, /)\nReturn self>value.\n__hash__()\nReturn hash(self).\n__init__()\nInitialize self.\n__init_subclass__\nThis method is called when a class is subclassed.\n__le__(value, /)\nReturn self<=value.\n__lt__(value, /)\nReturn self<value.\n__ne__(value, /)\nReturn self!=value.\n__new__(**kwargs)\nCreate and return a new object.\n__reduce__()\nHelper for pickle.\n__reduce_ex__(protocol, /)\nHelper for pickle.\n__repr__()\nReturn repr(self).\n__setattr__(name, value, /)\nImplement setattr(self, name, value).\n__sizeof__()\nSize of object in memory, in bytes.\n__str__()\nReturn str(self).\n__subclasshook__\nAbstract classes can override this to customize issubclass().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.CharTensor",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.CharTensor.html",
        "api_signature": null,
        "api_description": "Creates a Tensor with the dtype of oneflow.int8 and the device on cpu, it has the same parameters as oneflow.Tensor()\n__init__()¶\nInitialize self.  See help(type(self)) for accurate signature.\nMethods\n__delattr__(name, /)\nImplement delattr(self, name).\n__dir__()\nDefault dir() implementation.\n__eq__(value, /)\nReturn self==value.\n__format__(format_spec, /)\nDefault object formatter.\n__ge__(value, /)\nReturn self>=value.\n__getattribute__(name, /)\nReturn getattr(self, name).\n__gt__(value, /)\nReturn self>value.\n__hash__()\nReturn hash(self).\n__init__()\nInitialize self.\n__init_subclass__\nThis method is called when a class is subclassed.\n__le__(value, /)\nReturn self<=value.\n__lt__(value, /)\nReturn self<value.\n__ne__(value, /)\nReturn self!=value.\n__new__(**kwargs)\nCreate and return a new object.\n__reduce__()\nHelper for pickle.\n__reduce_ex__(protocol, /)\nHelper for pickle.\n__repr__()\nReturn repr(self).\n__setattr__(name, value, /)\nImplement setattr(self, name, value).\n__sizeof__()\nSize of object in memory, in bytes.\n__str__()\nReturn str(self).\n__subclasshook__\nAbstract classes can override this to customize issubclass().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.DoubleTensor",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.DoubleTensor.html",
        "api_signature": null,
        "api_description": "Creates a Tensor with the dtype of oneflow.float64 and the device on cpu, it has the same parameters as oneflow.Tensor()\n__init__()¶\nInitialize self.  See help(type(self)) for accurate signature.\nMethods\n__delattr__(name, /)\nImplement delattr(self, name).\n__dir__()\nDefault dir() implementation.\n__eq__(value, /)\nReturn self==value.\n__format__(format_spec, /)\nDefault object formatter.\n__ge__(value, /)\nReturn self>=value.\n__getattribute__(name, /)\nReturn getattr(self, name).\n__gt__(value, /)\nReturn self>value.\n__hash__()\nReturn hash(self).\n__init__()\nInitialize self.\n__init_subclass__\nThis method is called when a class is subclassed.\n__le__(value, /)\nReturn self<=value.\n__lt__(value, /)\nReturn self<value.\n__ne__(value, /)\nReturn self!=value.\n__new__(**kwargs)\nCreate and return a new object.\n__reduce__()\nHelper for pickle.\n__reduce_ex__(protocol, /)\nHelper for pickle.\n__repr__()\nReturn repr(self).\n__setattr__(name, value, /)\nImplement setattr(self, name, value).\n__sizeof__()\nSize of object in memory, in bytes.\n__str__()\nReturn str(self).\n__subclasshook__\nAbstract classes can override this to customize issubclass().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.FloatTensor",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.FloatTensor.html",
        "api_signature": null,
        "api_description": "Creates a Tensor with the dtype of oneflow.float32 and the device on cpu, it has the same parameters as oneflow.Tensor()\n__init__()¶\nInitialize self.  See help(type(self)) for accurate signature.\nMethods\n__delattr__(name, /)\nImplement delattr(self, name).\n__dir__()\nDefault dir() implementation.\n__eq__(value, /)\nReturn self==value.\n__format__(format_spec, /)\nDefault object formatter.\n__ge__(value, /)\nReturn self>=value.\n__getattribute__(name, /)\nReturn getattr(self, name).\n__gt__(value, /)\nReturn self>value.\n__hash__()\nReturn hash(self).\n__init__()\nInitialize self.\n__init_subclass__\nThis method is called when a class is subclassed.\n__le__(value, /)\nReturn self<=value.\n__lt__(value, /)\nReturn self<value.\n__ne__(value, /)\nReturn self!=value.\n__new__(**kwargs)\nCreate and return a new object.\n__reduce__()\nHelper for pickle.\n__reduce_ex__(protocol, /)\nHelper for pickle.\n__repr__()\nReturn repr(self).\n__setattr__(name, value, /)\nImplement setattr(self, name, value).\n__sizeof__()\nSize of object in memory, in bytes.\n__str__()\nReturn str(self).\n__subclasshook__\nAbstract classes can override this to customize issubclass().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.HalfTensor",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.HalfTensor.html",
        "api_signature": null,
        "api_description": "Creates a Tensor with the dtype of oneflow.float16 and the device on cpu, it has the same parameters as oneflow.Tensor()\n__init__()¶\nInitialize self.  See help(type(self)) for accurate signature.\nMethods\n__delattr__(name, /)\nImplement delattr(self, name).\n__dir__()\nDefault dir() implementation.\n__eq__(value, /)\nReturn self==value.\n__format__(format_spec, /)\nDefault object formatter.\n__ge__(value, /)\nReturn self>=value.\n__getattribute__(name, /)\nReturn getattr(self, name).\n__gt__(value, /)\nReturn self>value.\n__hash__()\nReturn hash(self).\n__init__()\nInitialize self.\n__init_subclass__\nThis method is called when a class is subclassed.\n__le__(value, /)\nReturn self<=value.\n__lt__(value, /)\nReturn self<value.\n__ne__(value, /)\nReturn self!=value.\n__new__(**kwargs)\nCreate and return a new object.\n__reduce__()\nHelper for pickle.\n__reduce_ex__(protocol, /)\nHelper for pickle.\n__repr__()\nReturn repr(self).\n__setattr__(name, value, /)\nImplement setattr(self, name, value).\n__sizeof__()\nSize of object in memory, in bytes.\n__str__()\nReturn str(self).\n__subclasshook__\nAbstract classes can override this to customize issubclass().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.IntTensor",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.IntTensor.html",
        "api_signature": null,
        "api_description": "Creates a Tensor with the dtype of oneflow.int32 and the device on cpu, it has the same parameters as oneflow.Tensor()\n__init__()¶\nInitialize self.  See help(type(self)) for accurate signature.\nMethods\n__delattr__(name, /)\nImplement delattr(self, name).\n__dir__()\nDefault dir() implementation.\n__eq__(value, /)\nReturn self==value.\n__format__(format_spec, /)\nDefault object formatter.\n__ge__(value, /)\nReturn self>=value.\n__getattribute__(name, /)\nReturn getattr(self, name).\n__gt__(value, /)\nReturn self>value.\n__hash__()\nReturn hash(self).\n__init__()\nInitialize self.\n__init_subclass__\nThis method is called when a class is subclassed.\n__le__(value, /)\nReturn self<=value.\n__lt__(value, /)\nReturn self<value.\n__ne__(value, /)\nReturn self!=value.\n__new__(**kwargs)\nCreate and return a new object.\n__reduce__()\nHelper for pickle.\n__reduce_ex__(protocol, /)\nHelper for pickle.\n__repr__()\nReturn repr(self).\n__setattr__(name, value, /)\nImplement setattr(self, name, value).\n__sizeof__()\nSize of object in memory, in bytes.\n__str__()\nReturn str(self).\n__subclasshook__\nAbstract classes can override this to customize issubclass().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.LongTensor",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.LongTensor.html",
        "api_signature": null,
        "api_description": "Creates a Tensor with the dtype of oneflow.int64 and the device on cpu, it has the same parameters as oneflow.Tensor()\n__init__()¶\nInitialize self.  See help(type(self)) for accurate signature.\nMethods\n__delattr__(name, /)\nImplement delattr(self, name).\n__dir__()\nDefault dir() implementation.\n__eq__(value, /)\nReturn self==value.\n__format__(format_spec, /)\nDefault object formatter.\n__ge__(value, /)\nReturn self>=value.\n__getattribute__(name, /)\nReturn getattr(self, name).\n__gt__(value, /)\nReturn self>value.\n__hash__()\nReturn hash(self).\n__init__()\nInitialize self.\n__init_subclass__\nThis method is called when a class is subclassed.\n__le__(value, /)\nReturn self<=value.\n__lt__(value, /)\nReturn self<value.\n__ne__(value, /)\nReturn self!=value.\n__new__(**kwargs)\nCreate and return a new object.\n__reduce__()\nHelper for pickle.\n__reduce_ex__(protocol, /)\nHelper for pickle.\n__repr__()\nReturn repr(self).\n__setattr__(name, value, /)\nImplement setattr(self, name, value).\n__sizeof__()\nSize of object in memory, in bytes.\n__str__()\nReturn str(self).\n__subclasshook__\nAbstract classes can override this to customize issubclass().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.is_tensor",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.is_tensor.html",
        "api_signature": "oneflow.is_tensor(input)",
        "api_description": "",
        "return_value": "",
        "parameters": "obj (Object) – Object to test\n\n\n",
        "input_shape": "",
        "notes": "Using that isinstance check is better for typechecking with mypy,\nand more explicit - so it’s recommended to use that instead of\nis_tensor.",
        "code_example": ">>> import oneflow as flow\n\n>>> x=flow.tensor([1,2,3])\n>>> flow.is_tensor(x)\nTrue\n\n\n\n"
    },
    {
        "api_name": "oneflow.is_floating_point",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.is_floating_point.html",
        "api_signature": "oneflow.is_floating_point(input)",
        "api_description": "",
        "return_value": "\n",
        "parameters": "input (Tensor) – the input tensor.\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n\n>>> input = flow.tensor([1, 2, 3, 4, 5], dtype=flow.int)\n>>> output = flow.is_floating_point(input)\n>>> output\nFalse\n\n\n\n"
    },
    {
        "api_name": "oneflow.is_nonzero",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.is_nonzero.html",
        "api_signature": "oneflow.is_nonzero(input)",
        "api_description": "",
        "return_value": "after type conversions. i.e. not equal to flow.tensor([0.]) or flow.tensor([0]).\nThrows a RuntimeError if input.shape.numel() != 1\nFor Example:\n>>> import oneflow as flow\n>>> flow.is_nonzero(flow.tensor([0.]))\nFalse\n>>> flow.is_nonzero(flow.tensor([1.5]))\nTrue\n>>> flow.is_nonzero(flow.tensor([3]))\nTrue\n\n\n\n",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.numel",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.numel.html",
        "api_signature": "oneflow.numel(input)",
        "api_description": "",
        "return_value": "\n",
        "parameters": "input (oneflow.Tensor) – Input Tensor\n\n\n>>> import oneflow as flow\n\n>>> a = flow.randn(1, 2, 3, 4, 5)\n>>> flow.numel(a)\n120\n>>> a = flow.zeros(4,4)\n>>> flow.numel(a)\n16\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.set_printoptions",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.set_printoptions.html",
        "api_signature": "oneflow.set_printoptions(precision=None, threshold=None, edgeitems=None, linewidth=None, profile=None, sci_mode=None)",
        "api_description": "Set options for printing. Items shamelessly taken from NumPy",
        "return_value": "",
        "parameters": "\nprecision – Number of digits of precision for floating point output\n(default = 4).\nthreshold – Total number of array elements which trigger summarization\nrather than full repr (default = 1000).\nedgeitems – Number of array items in summary at beginning and end of\neach dimension (default = 3).\nlinewidth – The number of characters per line for the purpose of\ninserting line breaks (default = terminal_columns).\nprofile – Sane defaults for pretty printing. Can override with any of\nthe above options. (any one of default, short, full)\nsci_mode – Enable (True) or disable (False) scientific notation. If\nNone (default) is specified, the value is defined by\noneflow._tensor_str._Formatter. This value is automatically chosen\nby the framework.\n\n\n\n\n",
        "input_shape": "",
        "notes": "linewidth equals to terminal columns, manual setting will invalidate the default automatic setting.",
        "code_example": ""
    },
    {
        "api_name": "oneflow.get_default_dtype",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.get_default_dtype.html",
        "api_signature": "oneflow.get_default_dtype()",
        "api_description": "",
        "return_value": "\nThe default floating point dtype.\n\n",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> flow.set_default_dtype(flow.float32)\n>>> flow.get_default_dtype()\noneflow.float32\n>>> flow.set_default_dtype(flow.float64)\n>>> flow.get_default_dtype()\noneflow.float64\n>>> flow.set_default_tensor_type(flow.FloatTensor)\n>>> flow.get_default_dtype()\noneflow.float32\n\n\n\n"
    },
    {
        "api_name": "oneflow.set_default_dtype",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.set_default_dtype.html",
        "api_signature": "oneflow.set_default_dtype()",
        "api_description": "Sets the default floating point type for those source operators which create Tensor.\nThe default floating point type is oneflow.float32.",
        "return_value": "",
        "parameters": "dtype (oneflow.dtype) – The floating point dtype.\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow\n>>> oneflow.set_default_dtype(oneflow.float64)\n>>> x = oneflow.randn(2, 3)\n>>> x.dtype\noneflow.float64\n>>> oneflow.set_default_dtype(oneflow.float32)\n>>> x = oneflow.randn(2, 3)\n>>> x.dtype\noneflow.float32\n\n\n\n"
    },
    {
        "api_name": "oneflow.set_default_tensor_type",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.set_default_tensor_type.html",
        "api_signature": "oneflow.set_default_tensor_type(tensor_type)",
        "api_description": "Sets the default floating point type for those source operators which create Tensor.\nThe default floating point type is oneflow.FloatTensor.",
        "return_value": "",
        "parameters": "tensor_type (type or string) – The floating point tensor type or its name.\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow\n>>> oneflow.set_default_tensor_type(oneflow.FloatTensor)\n>>> x = oneflow.ones(2, 3)\n>>> x.dtype\noneflow.float32\n>>> oneflow.set_default_tensor_type(\"oneflow.DoubleTensor\")\n>>> x = oneflow.ones(2, 3)\n>>> x.dtype\noneflow.float64\n>>> oneflow.set_default_tensor_type(oneflow.FloatTensor)\n>>> x = oneflow.tensor([1.0, 2])\n>>> x.dtype\noneflow.float32\n\n\n\n"
    },
    {
        "api_name": "oneflow.tensor",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.tensor.html",
        "api_signature": "oneflow.tensor()",
        "api_description": "Constructs a tensor with data, return a global tensor if placement and sbp are in kwargs,otherwise return a local tensor.",
        "return_value": "",
        "parameters": "data – Initial data for the tensor. Can be a list, tuple, NumPy ndarray, scalar or tensor.\n\nKeyword Arguments\n\ndtype (oneflow.dtype, optional) – Default: if None, infers data type from data.\ndevice (oneflow.device, optional) – the desired device of returned tensor. If placement\nand sbp is None, uses the current cpu for the default tensor type.\nplacement (oneflow.placement, optional) – the desired placement of returned tensor.\nsbp (oneflow.sbp or tuple of oneflow.sbp, optional) – the desired sbp of returned tensor.\nrequires_grad (bool, optional) – If autograd should record operations on the returned tensor. Default: False\npin_memory (bool, optional) – If set, returned tensor would be allocated in the pinned memory. Works only for CPU tensors. Default: False.\n\n\n\n\n",
        "input_shape": "",
        "notes": "The Keyword Argument device is mutually exclusive with placement and sbp.",
        "code_example": ">>> import oneflow as flow\n\n>>> x = flow.tensor([1,2,3])\n>>> x\ntensor([1, 2, 3], dtype=oneflow.int64)\n\n\n\n"
    },
    {
        "api_name": "oneflow.as_tensor",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.as_tensor.html",
        "api_signature": "oneflow.as_tensor(data, dtype=None, device=None)",
        "api_description": "Converts data into a tensor, sharing data and preserving autograd history if possible.\nIf data is already a tensor with the requeseted dtype and device then data itself is returned, but if data is a tensor with a different dtype or device then it’s copied as if using data.to(dtype=dtype, device=device).\nIf data is a NumPy array (an ndarray) with the same dtype and device then a tensor is constructed using oneflow.from_numpy.",
        "return_value": "",
        "parameters": "\ndata (array_like) – Initial data for the tensor. Can be a list, tuple, NumPy ndarray, scalar, and other types.\ndtype (oneflow.dtype, optional) – the desired data type of returned tensor. Default: if None, infers data type from data.\ndevice (oneflow.device, optional) – the device of the constructed tensor. If None and data is a tensor then the device of data is used. If None and data is not a tensor then the result tensor is constructed on the CPU.\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> import numpy as np\n\n>>> a = np.array([1, 2, 3])\n>>> t = flow.as_tensor(a, device=flow.device('cuda'))\n>>> t\ntensor([1, 2, 3], device='cuda:0', dtype=oneflow.int64)\n>>> t[0] = -1\n>>> a\narray([1, 2, 3])\n\n\n\n"
    },
    {
        "api_name": "oneflow.as_strided",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.as_strided.html",
        "api_signature": "oneflow.as_strided(input, size, stride, storage_offset=None)",
        "api_description": "Create a view of an existing oneflow.Tensor input with specified size, stride and storage_offset.\nThe documentation is referenced from:",
        "return_value": "the output Tensor.\n\n",
        "parameters": "\ninput (Tensor) – the input tensor.\nsize (tuple or ints) – the shape of the output tensor.\nstride (tuple or ints) – the stride of the output tensor.\nstorage_offset (int) – the offset in the underlying storage of the output tensor\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n\n>>> input = flow.rand(2,3,5)\n>>> output = flow.as_strided(input, (2,3,3), (1,2,3), 1)\n>>> output.size()\noneflow.Size([2, 3, 3])\n\n\n\n"
    },
    {
        "api_name": "oneflow.from_numpy",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.from_numpy.html",
        "api_signature": "oneflow.from_numpy()",
        "api_description": "Creates a Tensor from a numpy.ndarray.\nThe returned tensor and ndarray share the same memory. Modifications to the tensor\nwill be reflected in the ndarray and vice versa.\nIt currently accepts ndarray with dtypes of numpy.float64, numpy.float32, numpy.float16,\nnumpy.int64, numpy.int32, numpy.int8, numpy.uint8.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> import numpy as np\n>>> np_arr = np.arange(6).reshape(2, 3)\n>>> t = flow.from_numpy(np_arr)\n>>> t\ntensor([[0, 1, 2],\n        [3, 4, 5]], dtype=oneflow.int64)\n>>> np_arr[0, 0] = -1\n>>> t\ntensor([[-1,  1,  2],\n        [ 3,  4,  5]], dtype=oneflow.int64)\n\n\n\n"
    },
    {
        "api_name": "oneflow.zeros",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.zeros.html",
        "api_signature": "oneflow.zeros(*size: Union[int, Tuple[int, …], oneflow.Size, List[int]], dtype: Optional[oneflow._oneflow_internal.dtype] = None, device: Optional[Union[oneflow._oneflow_internal.device, str]] = None, placement: Optional[oneflow._oneflow_internal.placement] = None, sbp: Optional[Union[oneflow._oneflow_internal.sbp.sbp, List[oneflow._oneflow_internal.sbp.sbp]]] = None, requires_grad: bool = False)",
        "api_description": "",
        "return_value": "with the shape defined by the variable argument size.\n\n",
        "parameters": "\nsize (an integer or tuple of integer values) – a variable number of arguments or a collection like a list or tuple.\ndtype (flow.dtype, optional) – the desired data type of returned tensor.\ndevice (flow.device, optional) – the desired device of returned tensor. Default: if None, uses the current device for the default tensor type\nplacement (flow.placement, optional) – the desired placement of returned global tensor. Default: if None, the returned tensor is local one using the argument device.\nsbp (flow.sbp.sbp or tuple of flow.sbp.sbp, optional) – the desired sbp descriptor of returned global tensor. Default: if None, the returned tensor is local one using the argument device.\nrequires_grad (bool, optional) – If autograd should record operations on the returned tensor. Default: False.\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> y = flow.zeros(5)\n>>> y\ntensor([0., 0., 0., 0., 0.], dtype=oneflow.float32)\n>>> y = flow.zeros(2,3)\n>>> y\ntensor([[0., 0., 0.],\n        [0., 0., 0.]], dtype=oneflow.float32)\n\n\n\n"
    },
    {
        "api_name": "oneflow.zeros_like",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.zeros_like.html",
        "api_signature": "oneflow.zeros_like(input, *, dtype=None, device=None, placement=None, sbp=None, requires_grad=False)",
        "api_description": "",
        "return_value": "flow.zeros_like(input) is equivalent to flow.zeros(input.shape, dtype=input.dtype)\n\n",
        "parameters": "\ninput (Tensor) – The size of input will determine size of the output tensor.\ndtype (flow.dtype, optional) – the desired type of returned tensor. Default: if None, same flow.dtype as this tensor.\ndevice (flow.device, optional) – the desired device of returned tensor. Default: if None, same flow.device as this tensor.\nplacement (flow.placement, optional) – the desired placement of returned global tensor. Default: if None, the returned tensor is local one using the argument device.\nsbp (flow.sbp.sbp or tuple of flow.sbp.sbp, optional) – the desired sbp descriptor of returned global tensor. Default: if None, the returned tensor is local one using the argument device.\nrequires_grad (bool, optional) – If autograd should record operations on the returned tensor. Default: False.\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> import numpy as np\n>>> x = flow.tensor(np.random.rand(5), dtype=flow.float32)\n>>> y = flow.zeros_like(x)\n>>> y\ntensor([0., 0., 0., 0., 0.], dtype=oneflow.float32)\n\n\n\n"
    },
    {
        "api_name": "oneflow.ones",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.ones.html",
        "api_signature": "oneflow.ones(*size: Union[int, Tuple[int, …], oneflow.Size, List[int]], dtype: Optional[oneflow._oneflow_internal.dtype] = None, device: Optional[Union[oneflow._oneflow_internal.device, str]] = None, placement: Optional[oneflow._oneflow_internal.placement] = None, sbp: Optional[Union[oneflow._oneflow_internal.sbp.sbp, List[oneflow._oneflow_internal.sbp.sbp]]] = None, requires_grad: bool = False)",
        "api_description": "",
        "return_value": "with the shape defined by the variable argument size.\n\n",
        "parameters": "\nsize (an integer or tuple of integer values) – defining the shape of the output tensor. Can be a variable number of arguments or a collection like a list or tuple.\ndtype (flow.dtype, optional) – the desired data type of returned tensor.\ndevice (flow.device, optional) – the desired device of returned tensor. Default: if None, uses the current device for the default tensor type\nplacement (flow.placement, optional) – the desired placement of returned global tensor. Default: if None, the returned tensor is local one using the argument device.\nsbp (flow.sbp.sbp or tuple of flow.sbp.sbp, optional) – the desired sbp descriptor of returned global tensor. Default: if None, the returned tensor is local one using the argument device.\nrequires_grad (bool, optional) – If autograd should record operations on the returned tensor. Default: False.\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> y = flow.ones(5)\n>>> y\ntensor([1., 1., 1., 1., 1.], dtype=oneflow.float32)\n>>> y = flow.ones(2,3) # construct local tensor\n>>> y\ntensor([[1., 1., 1.],\n        [1., 1., 1.]], dtype=oneflow.float32)\n>>> placement = flow.placement(\"cpu\", ranks=[0])\n>>> y = flow.ones(4, 5, placement=placement, sbp=flow.sbp.broadcast) # construct global tensor\n>>> y.is_global\nTrue\n\n\n\n"
    },
    {
        "api_name": "oneflow.ones_like",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.ones_like.html",
        "api_signature": "oneflow.ones_like(input, *, dtype=None, device=None, placement=None, sbp=None, requires_grad=False)",
        "api_description": "",
        "return_value": "flow.ones_like(input) is equivalent to flow.ones(input.shape, dtype=input.dtype)\n\n",
        "parameters": "\ninput (Tensor) – The size of input will determine size of the output tensor.\ndtype (flow.dtype, optional) – the desired type of returned tensor. Default: if None, same flow.dtype as this tensor.\ndevice (flow.device, optional) – the desired device of returned tensor. Default: if None, same flow.device as this tensor.\nplacement (flow.placement, optional) – the desired placement of returned global tensor. Default: if None, the returned tensor is local one using the argument device.\nsbp (flow.sbp.sbp or tuple of flow.sbp.sbp, optional) – the desired sbp descriptor of returned global tensor. Default: if None, the returned tensor is local one using the argument device.\nrequires_grad (bool, optional) – If autograd should record operations on the returned tensor. Default: False.\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> import numpy as np\n>>> x = flow.tensor(np.random.rand(5), dtype=flow.float32)\n>>> y = flow.ones_like(x)\n>>> y\ntensor([1., 1., 1., 1., 1.], dtype=oneflow.float32)\n\n\n\n"
    },
    {
        "api_name": "oneflow.randn_like",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.randn_like.html",
        "api_signature": "oneflow.randn_like(input, *, dtype=None, generator=None, device=None, placement=None, sbp=None, requires_grad=False)",
        "api_description": "",
        "return_value": "flow.randn_like(input) is equivalent to flow.randn(input.size(), dtype=input.dtype, device=input.device).\n\n",
        "parameters": "\ninput (oneflow.Tensor) – the size of input will determine size of the output tensor.\ndtype (flow.dtype, optional) – The desired data type of returned tensor. defaults to the dtype of input.\ngenerator (flow.Generator, optional) – a pseudorandom number generator for sampling\ndevice (flow.device, optional) – The desired device of returned local tensor. If None, defaults to the device of input.\nplacement (flow.placement, optional) – The desired device of returned global tensor. If None, will\nconstruct local tensor.\nsbp (flow.sbp, optional) – The desired sbp of returned global tensor. It must be equal with the\nnumbers of placement, If None, will construct local tensor.\nrequires_grad (bool, optional) – If autograd should record operations on the returned tensor. Default: False.\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> x = flow.randn(3,3) # construct local tensor\n>>> y = flow.randn_like(x)\n>>> y.shape\noneflow.Size([3, 3])\n>>> y.is_global\nFalse\n>>> placement = flow.placement(\"cpu\", ranks=[0])\n>>> sbp = flow.sbp.broadcast\n>>> z = flow.randn_like(y, placement=placement, sbp=sbp) # construct global tensor\n>>> z.is_global\nTrue\n\n\n\n"
    },
    {
        "api_name": "oneflow.randint_like",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.randint_like.html",
        "api_signature": "oneflow.randint_like(input, low=0, high, size, *, dtype=None, generator=None, device=None, placement=None, sbp=None, requires_grad=False)",
        "api_description": "",
        "return_value": "The interface is consistent with PyTorch.\nThe documentation is referenced from: https://pytorch.org/docs/1.10/generated/torch.randint_like.html.\n\n",
        "parameters": "\ninput (oneflow.Tensor) – the size of input will determine size of the output tensor.\nlow (int, optional) – Lowest integer to be drawn from the distribution. Default: 0.\nhigh (int) – One above the highest integer to be drawn from the distribution.\n\n\nKeyword Arguments\n\ndtype (oneflow.dtype, optional) – The desired data type of returned tensor. Default: flow.int64.\ngenerator (oneflow.Generator, optional) – \ndevice (oneflow.device, optional) – The desired device of returned local tensor. If None, uses the\ncurrent device.\nplacement (oneflow.placement, optional) – The desired device of returned global tensor. If None, will\nconstruct local tensor.\nsbp (oneflow.sbp, optional) – The desired sbp of returned global tensor. It must be equal with the\nnumbers of placement.\nrequires_grad (bool, optional) – If autograd should record operations on the returned tensor. Default: False.\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> generator = flow.Generator()\n>>> generator.manual_seed(0) \n<oneflow._oneflow_internal.Generator object at ...>\n>>> x = flow.randn(2, 2, generator=generator)\n>>> y = flow.randint_like(x, 0, 5, generator=generator) # construct local tensor\n>>> y\ntensor([[3, 4],\n        [2, 4]], dtype=oneflow.int64)\n>>> y.is_global\nFalse\n>>> placement = flow.placement(\"cpu\", ranks=[0])\n>>> y = flow.randint_like(x, 0, 5, generator=generator, placement=placement, sbp=flow.sbp.broadcast) # construct global tensor\n>>> y.is_global\nTrue\n\n\n\n"
    },
    {
        "api_name": "oneflow.masked_fill",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.masked_fill.html",
        "api_signature": "oneflow.masked_fill()",
        "api_description": "Fills elements of self tensor with value where mask is True.\nThe shape of mask must be broadcastable with the shape of the underlying tensor.",
        "return_value": "",
        "parameters": "\nmask (BoolTensor) – the boolean mask\nvalue (float) – the value to fill in with\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> import numpy as np\n>>> in_arr = np.array(\n...     [[[-0.13169311,  0.97277078,  1.23305363,  1.56752789],\n...     [-1.51954275,  1.87629473, -0.53301206,  0.53006478],\n...     [-1.38244183, -2.63448052,  1.30845795, -0.67144869]],\n...     [[ 0.41502161,  0.14452418,  0.38968   , -1.76905653],\n...     [ 0.34675095, -0.7050969 , -0.7647731 , -0.73233418],\n...     [-1.90089858,  0.01262963,  0.74693893,  0.57132389]]]\n... )\n>>> fill_value = 8.7654321 # random value e.g. -1e9 3.1415\n>>> input = flow.tensor(in_arr, dtype=flow.float32)\n>>> mask = flow.tensor((in_arr > 0).astype(np.int8), dtype=flow.int)\n>>> output = flow.masked_fill(input, mask, fill_value)\n\n# tensor([[[-0.1317,  8.7654,  8.7654,  8.7654],\n#  [-1.5195,  8.7654, -0.533 ,  8.7654],\n#  [-1.3824, -2.6345,  8.7654, -0.6714]],\n\n# [[ 8.7654,  8.7654,  8.7654, -1.7691],\n#  [ 8.7654, -0.7051, -0.7648, -0.7323],\n#  [-1.9009,  8.7654,  8.7654,  8.7654]]], dtype=oneflow.float32)\n\n\n\n"
    },
    {
        "api_name": "oneflow.new_ones",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.new_ones.html",
        "api_signature": "oneflow.new_ones(x, size=None, dtype=None, device=None, placement=None, sbp=None, requires_grad=False)",
        "api_description": "",
        "return_value": "\n",
        "parameters": "\nsize (int...) – a list, tuple, or flow.Size of integers defining the shape of the output tensor.\ndtype (flow.dtype, optional) – the desired type of returned tensor. Default: if None, same flow.dtype as this tensor.\ndevice (flow.device, optional) – the desired device of returned tensor. Default: if None, same flow.device as this tensor.\nplacement (flow.placement, optional) – the desired placement of returned global tensor. Default: if None, the returned tensor is local one using the argument device.\nsbp (flow.sbp.sbp or tuple of flow.sbp.sbp, optional) – the desired sbp descriptor of returned global tensor. Default: if None, the returned tensor is local one using the argument device.\nrequires_grad (bool, optional) – If autograd should record operations on the returned tensor. Default: False.\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import numpy as np\n>>> import oneflow as flow\n\n>>> x = flow.Tensor(np.ones((1, 2, 3)))\n>>> y = x.new_ones((2, 2))\n>>> y\ntensor([[1., 1.],\n        [1., 1.]], dtype=oneflow.float32)\n\n\n\n"
    },
    {
        "api_name": "oneflow.arange",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.arange.html",
        "api_signature": "oneflow.arange(start: int = 0, end, step: int = 1, dtype: Optional[oneflow._oneflow_internal.dtype] = None, device: Optional[Union[oneflow._oneflow_internal.device, str]] = None, placement: Optional[oneflow._oneflow_internal.placement] = None, sbp: Optional[Union[oneflow._oneflow_internal.sbp.sbp, List[oneflow._oneflow_internal.sbp.sbp]]] = None, requires_grad: bool = False)",
        "api_description": "",
        "return_value": "with values from start to end with step step. Step is\nthe gap between two values in the tensor.\n\n\\[\\text{out}_{i+1} = \\text{out}_i + \\text{step}.\\]\n\n",
        "parameters": "\nstart (int) – the starting value for the set of points. Default: 0.\nend (int) – the ending value for the set of points\nstep (int) – the gap between each pair of adjacent points. Default: 1.\n\n\nKeyword Arguments\n\ndtype (flow.dtype, optional) – If dtype is not given, infer the dtype from the other input arguments. If any of start, end, or step are floating-point, the dtype is inferred to be the floating-point data type. Otherwise, the dtype is inferred to be flow.int64.\ndevice (flow.device, optional) – the desired device of returned tensor. Default: if None, uses the current device for the default tensor.\nrequires_grad (bool, optional) – If autograd should record operations on the returned tensor. Default: False.\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n\n>>> y = flow.arange(0, 5)\n>>> y\ntensor([0, 1, 2, 3, 4], dtype=oneflow.int64)\n\n\n\n"
    },
    {
        "api_name": "oneflow.linspace",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.linspace.html",
        "api_signature": "oneflow.linspace(start: Union[float, oneflow.Tensor], end: Union[float, oneflow.Tensor], steps: Union[int, oneflow.Tensor], dtype: oneflow._oneflow_internal.dtype = oneflow.float32, device: Optional[Union[str, oneflow._oneflow_internal.device]] = None, placement: Optional[oneflow._oneflow_internal.placement] = None, sbp: Optional[Union[oneflow._oneflow_internal.sbp.sbp, List[oneflow._oneflow_internal.sbp.sbp]]] = None, requires_grad: bool = False)",
        "api_description": "Creates a one-dimensional tensor of size steps whose values are evenly\nspaced from start to end, inclusive. That is, the value are:\n\\[(\\text{start},\n\\text{start} + \\frac{\\text{end} - \\text{start}}{\\text{steps} - 1},\n\\ldots,\n\\text{start} + (\\text{steps} - 2) * \\frac{\\text{end} - \\text{start}}{\\text{steps} - 1},\n\\text{end})\\]",
        "return_value": "",
        "parameters": "\nstart (float) – the starting value for the set of points\nend (float) – the ending value for the set of points\nsteps (int) – size of the constructed tensor\n\n\nKeyword Arguments\n\ndtype (flow.dtype, optional) – If dtype is not given, the dtype is inferred to be flow.float32.\ndevice (flow.device, optional) – the desired device of returned tensor. Default: if None, uses the current device for the default tensor.\nrequires_grad (bool, optional) – If autograd should record operations on the returned tensor. Default: False.\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n\n>>> y = flow.linspace(3, 10, steps=5)\n>>> y\ntensor([ 3.0000,  4.7500,  6.5000,  8.2500, 10.0000], dtype=oneflow.float32)\n\n\n\n"
    },
    {
        "api_name": "oneflow.eye",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.eye.html",
        "api_signature": "oneflow.eye(n, m, *, device=None, requires_grad=False, placement=None, sbp)",
        "api_description": "This operator creates a 2-D Tensor with ones on the diagonal and zeros elsewhere.",
        "return_value": "The result tensor with ones on the diagonal and zeros elsewhere.\n\n",
        "parameters": "\nn (int) – the number of rows.\nm (int, optional) – the number of colums with default being n. Defaults to None.\n\n\nKeyword Arguments\n\ndevice (Union[flow.device, str], optional) – the desired device of returned tensor. Default: if None, uses the current device for the default tensor.\nrequires_grad (bool, optional) – If autograd should record operations on the returned tensor. Default: False.\nplacement (oneflow._oneflow_internal.placement, optional) – The placement attribute allows you to specify which physical device the tensor is stored on.\nsbp (Union[oneflow._oneflow_internal.sbp.sbp, List[oneflow._oneflow_internal.sbp.sbp]], optional) – When creating a global tensor, specify the SBP of the tensor.\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> out = flow.eye(3, 3)\n>>> out\ntensor([[1., 0., 0.],\n        [0., 1., 0.],\n        [0., 0., 1.]], dtype=oneflow.float32)\n>>> out = flow.eye(3, 3, device=\"cuda\")\n>>> out\ntensor([[1., 0., 0.],\n        [0., 1., 0.],\n        [0., 0., 1.]], device='cuda:0', dtype=oneflow.float32)\n\n\n\n"
    },
    {
        "api_name": "oneflow.empty",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.empty.html",
        "api_signature": "oneflow.empty(*size, *, dtype=None, device=None, placement=None, sbp=None, requires_grad=False, pin_memory=False)",
        "api_description": "",
        "return_value": "The shape of the tensor is defined by the variable argument size.\n\n",
        "parameters": "\nsize (int... or oneflow.Size) – Defining the shape of the output tensor.\nCan be a variable number of arguments or a collection like a list or tuple or oneflow.Size.\ndtype (flow.dtype, optional) – The desired data type of returned tensor. Default: flow.float32.\ndevice (oneflow.device, optional) – The desired device of returned local tensor. If None, uses the\ncurrent device.\nplacement (flow.placement, optional) – The desired device of returned global tensor. If None, will\nconstruct local tensor.\nsbp (flow.sbp or List[flow.sbp], optional) – The desired sbp of returned global tensor.\nrequires_grad (bool, optional) – If autograd should record operations on the returned tensor. Default: False.\npin_memory (bool, optional) – False.\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> y = flow.empty(4, 5)  # construct local empty tensor\n>>> y.shape\noneflow.Size([4, 5])\n>>> y.is_global\nFalse\n>>> placement = flow.placement(\"cpu\", ranks=[0])\n>>> y = flow.empty(4, 5, placement=placement, sbp=flow.sbp.broadcast)  # construct consistent empty tensor\n>>> y.is_global\nTrue\n\n\n\n"
    },
    {
        "api_name": "oneflow.empty_like",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.empty_like.html",
        "api_signature": "oneflow.empty_like(input, *, dtype=None, device=None, placement=None, sbp=None, requires_grad=False)",
        "api_description": "",
        "return_value": "oneflow.empty_like(input) is equivalent to\noneflow.empty(input.size(), dtype=input.dtype, device=input.device).\n\n",
        "parameters": "\ninput (Tensor) – The size of input will determine size of the output tensor.\ndtype (flow.dtype, optional) – The desired data type of returned tensor. Default: flow.float32.\ndevice (oneflow.device, optional) – The desired device of returned local tensor. If None, uses the\ncurrent device.\nplacement (flow.placement, optional) – The desired device of returned global tensor. If None, will\nconstruct local tensor.\nsbp (flow.sbp or List[flow.sbp], optional) – The desired sbp of returned global tensor.\nrequires_grad (bool, optional) – If autograd should record operations on the returned tensor. Default: False.\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> x = flow.randn(2, 3)\n>>> y = flow.empty_like(x)\n>>> y.shape\noneflow.Size([2, 3])\n\n\n\n"
    },
    {
        "api_name": "oneflow.full",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.full.html",
        "api_signature": "oneflow.full(size: Union[int, Tuple[int, …], oneflow.Size], fill_value: Union[float, int, complex], dtype: Optional[oneflow._oneflow_internal.dtype] = None, device: Optional[Union[oneflow._oneflow_internal.device, str]] = None, placement: Optional[oneflow._oneflow_internal.placement] = None, sbp: Optional[Union[oneflow._oneflow_internal.sbp.sbp, List[oneflow._oneflow_internal.sbp.sbp]]] = None, requires_grad: bool = False)",
        "api_description": "Creates a tensor of size size filled with fill_value.\nThe tensor’s dtype is inferred from value.",
        "return_value": "",
        "parameters": "\nsize (int...) – a list, tuple, or oneflow.Size of integers defining the shape of the output tensor.\nfill_value (Scalar) – the value to fill the output tensor with.\ndtype (oneflow.dtype, optional) – the desired data type of returned tensor.\ndevice (oneflow.device, optional) – the desired device of returned tensor. Default: if None, uses the current device for the default tensor type\nplacement (oneflow.placement, optional) – the desired placement of returned global tensor. Default: if None, the returned tensor is local one using the argument device.\nsbp (oneflow.sbp.sbp or tuple of oneflow.sbp.sbp, optional) – the desired sbp descriptor of returned global tensor. Default: if None, the returned tensor is local one using the argument device.\nrequires_grad (bool, optional) – If autograd should record operations on the returned tensor. Default: False.\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> y = flow.full((5,),5)\n>>> y\ntensor([5, 5, 5, 5, 5], dtype=oneflow.int64)\n>>> y = flow.full((2,3),5.0) # construct local tensor\n>>> y\ntensor([[5., 5., 5.],\n        [5., 5., 5.]], dtype=oneflow.float32)\n>>> placement = flow.placement(\"cpu\", ranks=[0])\n>>> y = flow.full((2,3), 5.0, placement=placement, sbp=flow.sbp.broadcast)  # construct global tensor\n>>> y.is_global\nTrue\n\n\n\n"
    },
    {
        "api_name": "oneflow.full_like",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.full_like.html",
        "api_signature": "oneflow.full_like(input, fill_value, \\*, dtype=None, device=None, placement=None, sbp=None, requires_grad=False)",
        "api_description": "",
        "return_value": "oneflow.full_like(input, fill_value) is equivalent to\noneflow.full(input.size(), fill_value, dtype=input.dtype, device=input.device).\nThe interface is consistent with PyTorch.\nThe documentation is referenced from: https://pytorch.org/docs/1.10/generated/torch.full_like.html.\n\n",
        "parameters": "\ninput (oneflow.Tensor) – \nfill_value (Scalar) – the value to fill the output tensor with.\ndtype (oneflow.dtype, optional) – the desired data type of returned tensor.\ndevice (oneflow.device, optional) – the desired device of returned tensor. Default: if None, uses the current device for the default tensor type\nplacement (oneflow.placement, optional) – the desired placement of returned global tensor. Default: if None, the returned tensor is local one using the argument device.\nsbp (oneflow.sbp.sbp or tuple of oneflow.sbp.sbp, optional) – the desired sbp descriptor of returned global tensor. Default: if None, the returned tensor is local one using the argument device.\nrequires_grad (bool, optional) – If autograd should record operations on the returned tensor. Default: False.\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> x = flow.randn(2, 3)\n>>> y = flow.full_like(x, 2.0)\n>>> y\ntensor([[2., 2., 2.],\n        [2., 2., 2.]], dtype=oneflow.float32)\n>>> y = flow.full_like(x, 2, dtype=flow.int32)\n>>> y\ntensor([[2, 2, 2],\n        [2, 2, 2]], dtype=oneflow.int32)\n>>> placement = flow.placement(\"cpu\", ranks=[0])\n>>> y = flow.full_like(x, 5.0, placement=placement, sbp=flow.sbp.broadcast)  # construct global tensor\n>>> y.is_global\nTrue\n\n\n\n"
    },
    {
        "api_name": "oneflow.tensor_scatter_nd_update",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.tensor_scatter_nd_update.html",
        "api_signature": "oneflow.tensor_scatter_nd_update(tensor, indices, updates)",
        "api_description": "This operation creates a new tensor by applying sparse updates to the input tensor.\nThis is similar to an index assignment.\nThis operator is very similar to scatter_nd(), except that the updates are scattered onto an existing\ntensor (as opposed to a zero-tensor).",
        "return_value": "",
        "parameters": "\ntensor – The tensor will be scattered.\nindices – The indices of update. Its type should be flow.int.\nupdate – The update Tensor.\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> tensor = flow.arange(8)\n>>> indices = flow.tensor([[1], [3], [5]])\n>>> updates = flow.tensor([-1, -2, -3])\n>>> flow.tensor_scatter_nd_update(tensor, indices, updates)\ntensor([ 0, -1,  2, -2,  4, -3,  6,  7], dtype=oneflow.int64)\n\n\n\n"
    },
    {
        "api_name": "oneflow.logspace",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.logspace.html",
        "api_signature": "oneflow.logspace(start, end, steps, base=10.0, *, dtype=None, device=None, placement=None, sbp=None, requires_grad=False)",
        "api_description": "Creates a one-dimensional tensor of size steps whose values are evenly\nspaced from \\({{\\text{{base}}}}^{{\\text{{start}}}}\\) to\n\\({{\\text{{base}}}}^{{\\text{{end}}}}\\), inclusive, on a logarithmic scale\nwith base base. That is, the values are:\n\\[(\\text{base}^{\\text{start}},\n\\text{base}^{(\\text{start} + \\frac{\\text{end} - \\text{start}}{ \\text{steps} - 1})},\n\\ldots,\n\\text{base}^{(\\text{start} + (\\text{steps} - 2) * \\frac{\\text{end} - \\text{start}}{ \\text{steps} - 1})},\n\\text{base}^{\\text{end}})\\]",
        "return_value": "",
        "parameters": "\nstart (float) – the starting value for the set of points\nend (float) – the ending value for the set of points\nsteps (int) – size of the constructed tensor\nbase (float, optional) – base of the logarithm function. Default: 10.0.\n\n\nKeyword Arguments\n\ndtype (oneflow.dtype, optional) – the data type to perform the computation in.\nDefault: if None, uses the global default dtype (see oneflow.get_default_dtype())\nwhen both start and end are real,\nand corresponding complex dtype when either is complex.\ndevice (oneflow.device, optional) – the desired device of returned tensor. Default: if None, uses the current device for the default tensor type\nplacement (oneflow.placement, optional) – the desired placement of returned global tensor. Default: if None, the returned tensor is local one using the argument device.\nsbp (oneflow.sbp.sbp or tuple of oneflow.sbp.sbp, optional) – the desired sbp descriptor of returned global tensor. Default: if None, the returned tensor is local one using the argument device.\nrequires_grad (bool, optional) – If autograd should record operations on the returned tensor. Default: False.\n\n\n\nExample:\n>>> import oneflow as flow\n>>> flow.logspace(start=-10, end=10, steps=2)\ntensor([1.0000e-10, 1.0000e+10], dtype=oneflow.float32)\n>>> flow.logspace(start=0.1, end=1.0, steps=5)\ntensor([ 1.2589,  2.1135,  3.5481,  5.9566, 10.0000], dtype=oneflow.float32)\n>>> flow.logspace(start=0.1, end=1.0, steps=1)\ntensor([1.2589], dtype=oneflow.float32)\n>>> flow.logspace(start=2, end=2, steps=1, base=2)\ntensor([4.], dtype=oneflow.float32)\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.argwhere",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.argwhere.html",
        "api_signature": "oneflow.argwhere(input, dtype: Optional[oneflow._oneflow_internal.dtype] = oneflow.int32)",
        "api_description": "This operator finds the indices of input Tensor input elements that are non-zero.\nIt returns a list in which each element is a coordinate that points to a non-zero element in the condition.",
        "return_value": "The result Tensor.\n\n",
        "parameters": "\ninput (oneflow.Tensor) – the input Tensor.\ndtype (Optional[flow.dtype], optional) – The data type of output. Defaults to None.\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import numpy as np\n>>> import oneflow as flow\n>>> x = np.array([[0, 1, 0],\n...            [2, 0, 2]]).astype(np.float32)\n\n>>> input = flow.Tensor(x)\n>>> output = flow.argwhere(input)\n>>> output\ntensor([[0, 1],\n        [1, 0],\n        [1, 2]], dtype=oneflow.int32)\n\n\n\n"
    },
    {
        "api_name": "oneflow.atleast_1d",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.atleast_1d.html",
        "api_signature": "oneflow.atleast_1d(*tensors)",
        "api_description": "",
        "return_value": "The interface is consistent with PyTorch.\nThe documentation is referenced from: https://pytorch.org/docs/1.10/generated/torch.atleast_1d.html.\n\nA Tensor\n\n\n",
        "parameters": "tensors (List[oneflow.Tensor] or oneflow.Tensor) – Tensor or list of tensors to be reshaped\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> x = flow.randn(1)\n>>> flow.atleast_1d(x).shape\noneflow.Size([1])\n>>> x = flow.tensor(0)\n>>> x.shape\noneflow.Size([])\n>>> flow.atleast_1d(x).shape\noneflow.Size([1])\n\n\n\n"
    },
    {
        "api_name": "oneflow.atleast_2d",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.atleast_2d.html",
        "api_signature": "oneflow.atleast_2d(*tensors)",
        "api_description": "",
        "return_value": "The interface is consistent with PyTorch.\nThe documentation is referenced from: https://pytorch.org/docs/1.10/generated/torch.atleast_2d.html.\n\nA Tensor\n\n\n",
        "parameters": "tensors (List[oneflow.Tensor] or oneflow.Tensor) – Tensor or list of tensors to be reshaped\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> x = flow.tensor(0)\n>>> x.shape\noneflow.Size([])\n>>> flow.atleast_2d(x).shape\noneflow.Size([1, 1])\n>>> x = flow.randn(3)\n>>> flow.atleast_2d(x).shape\noneflow.Size([1, 3])\n>>> x = flow.randn(3, 3)\n>>> flow.atleast_2d(x).shape\noneflow.Size([3, 3])\n\n\n\n"
    },
    {
        "api_name": "oneflow.atleast_3d",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.atleast_3d.html",
        "api_signature": "oneflow.atleast_3d(*tensors)",
        "api_description": "",
        "return_value": "The interface is consistent with PyTorch.\nThe documentation is referenced from: https://pytorch.org/docs/1.10/generated/torch.atleast_3d.html.\n\nA Tensor\n\n\n",
        "parameters": "tensors (List[oneflow.Tensor] or oneflow.Tensor) – Tensor or list of tensors to be reshaped\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> x = flow.tensor(0)\n>>> flow.atleast_3d(x).shape\noneflow.Size([1, 1, 1])\n>>> x = flow.randn(3)\n>>> flow.atleast_3d(x).shape\noneflow.Size([1, 3, 1])\n>>> x = flow.randn(3, 4)\n>>> flow.atleast_3d(x).shape\noneflow.Size([3, 4, 1])\n>>> x = flow.randn(3, 4, 5)\n>>> flow.atleast_3d(x).shape\noneflow.Size([3, 4, 5])\n\n\n\n"
    },
    {
        "api_name": "oneflow.cat",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.cat.html",
        "api_signature": "oneflow.cat(tensors, dim=0)",
        "api_description": "Concatenate two or more Tensor s at specified dim.\nAnalogous to numpy.concatenate",
        "return_value": "A Tensor\n\n\n",
        "parameters": "\ninputs – a list of Tensor\ndim – a int.\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> import numpy as np\n\n>>> input1 = flow.tensor(np.random.randn(2, 6, 5, 3), dtype=flow.float32)\n>>> input2 = flow.tensor(np.random.randn(2, 6, 5, 3), dtype=flow.float32)\n>>> input3 = flow.tensor(np.random.randn(2, 6, 5, 3), dtype=flow.float32)\n\n>>> out = flow.cat([input1, input2, input3], dim=1) # equal to using flow.concat()\n>>> out.shape\noneflow.Size([2, 18, 5, 3])\n\n\n\n"
    },
    {
        "api_name": "oneflow.column_stack",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.column_stack.html",
        "api_signature": "oneflow.column_stack(tensors)",
        "api_description": "Creates a new tensor by horizontally stacking the tensors in tensors.\nEquivalent to oneflow.hstack(tensors), tensors with dimensions less than 2 will be reshaped to (t.numel(), 1) before being stacked horizontally.",
        "return_value": "A Tensor\n\n\n",
        "parameters": "tensors – (List[oneflow.Tensor]): sequence of tensors to stack\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> x1 = flow.randn(5)\n>>> x2 = flow.randn(5)\n>>> flow.column_stack([x1, x2]).shape\noneflow.Size([5, 2])\n>>> x1 = flow.randn(2, 5)\n>>> x2 = flow.randn(2, 2)\n>>> flow.column_stack([x1, x2]).shape\noneflow.Size([2, 7])\n\n\n\n"
    },
    {
        "api_name": "oneflow.concat",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.concat.html",
        "api_signature": "oneflow.concat()",
        "api_description": "cat(tensors, dim=0) -> Tensor\nConcatenate two or more Tensor s at specified dim.\nAnalogous to numpy.concatenate",
        "return_value": "A Tensor\n\n\n",
        "parameters": "\ninputs – a list of Tensor\ndim – a int.\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> import numpy as np\n\n>>> input1 = flow.tensor(np.random.randn(2, 6, 5, 3), dtype=flow.float32)\n>>> input2 = flow.tensor(np.random.randn(2, 6, 5, 3), dtype=flow.float32)\n>>> input3 = flow.tensor(np.random.randn(2, 6, 5, 3), dtype=flow.float32)\n\n>>> out = flow.cat([input1, input2, input3], dim=1) # equal to using flow.concat()\n>>> out.shape\noneflow.Size([2, 18, 5, 3])\n\n\n\n"
    },
    {
        "api_name": "oneflow.chunk",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.chunk.html",
        "api_signature": "oneflow.chunk()",
        "api_description": "Splits a tensor into a specific number of chunks. Each chunk is a view of the input tensor. Last chunk will be bigger if the tensor size along the given dimension dim is not divisible by chunks.",
        "return_value": "List of Tensors.\n\n\n",
        "parameters": "\ninput (oneflow.Tensor) – The tensor to split.\nchunks (int) – Number of chunks to return.\ndim (int) – Dimension along which to split the tensor.\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> import numpy as np\n\n>>> arr = np.random.randn(5, 3, 6, 9).astype(np.float32)\n>>> input = flow.tensor(arr)\n>>> output = []\n>>> chunks = 3\n>>> output = flow.chunk(input, chunks=chunks, dim=2)\n>>> out_shape = []\n>>> for i in range(0, chunks):\n...     out_shape.append(output[i].numpy().shape)\n>>> out_shape\n[(5, 3, 2, 9), (5, 3, 2, 9), (5, 3, 2, 9)]\n\n\n\n"
    },
    {
        "api_name": "oneflow.dstack",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.dstack.html",
        "api_signature": "oneflow.dstack(tensors)",
        "api_description": "Stack tensors in tensors depthwish (along third axis).\nThis is equivalent to concatenation tensors in tensors along the third axis after 1-D and 2-D tensors have been reshaped by oneflow.atleast_3d().",
        "return_value": "A Tensor\n\n\n",
        "parameters": "tensors – (List[oneflow.Tensor]): sequence of tensors to stack\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> x1 = flow.randn(2, 3, 4)\n>>> x2 = flow.randn(2, 3, 2)\n>>> flow.dstack([x1, x2]).shape\noneflow.Size([2, 3, 6])\n>>> x = flow.randn(6, 4)\n>>> flow.dstack([x, x]).shape\noneflow.Size([6, 4, 2])\n\n\n\n"
    },
    {
        "api_name": "oneflow.expand",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.expand.html",
        "api_signature": "oneflow.expand(input, *sizes)",
        "api_description": "This operator expand the input tensor to a larger size.\nPassing -1 as the size for a dimension means not changing the size of that dimension.\nTensor can be also expanded to a larger number of dimensions and the new ones will be appended at the front.\nFor the new dimensions, the size cannot be set to -1.",
        "return_value": "The result Tensor.\n\n",
        "parameters": "\ninput (oneflow.Tensor) – the input Tensor.\n*sizes (oneflow.Size or int) – The desired expanded size.\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> import numpy as np\n>>> x = np.array([[[[0, 1]],\n...               [[2, 3]],\n...               [[4, 5]]]]).astype(np.int32)\n>>> input = flow.Tensor(x)\n>>> input.shape\noneflow.Size([1, 3, 1, 2])\n>>> out = input.expand(1, 3, 2, 2)\n>>> out.shape\noneflow.Size([1, 3, 2, 2])\n\n\n\n"
    },
    {
        "api_name": "oneflow.gather",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.gather.html",
        "api_signature": "oneflow.gather(input, dim, index, sparse_grad=False)",
        "api_description": "Gathers values along an axis specified by dim.\nFor a 3-D tensor the output is specified by:\nout[i][j][k] = input[index[i][j][k]][j][k]  # if dim == 0\nout[i][j][k] = input[i][index[i][j][k]][k]  # if dim == 1\nout[i][j][k] = input[i][j][index[i][j][k]]  # if dim == 2\ninput and index must have the same number of dimensions.\nIt is also required that index.size(d) <= input.size(d) for all\ndimensions d != dim.  out will have the same shape as index.",
        "return_value": "",
        "parameters": "\ninput (Tensor) – the source tensor\ndim (int) – the axis along which to index\nindex (LongTensor) – the indices of elements to gather\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> import numpy as np\n>>> input = np.random.randn(3, 4, 3, 5)\n>>> index = np.random.choice(np.arange(3), size=180, replace=True).reshape((3, 4, 3, 5))\n>>> output = flow.gather(flow.Tensor(input), 1, flow.tensor(index, dtype=flow.int64))\n>>> output.shape\noneflow.Size([3, 4, 3, 5])\n\n\n\n"
    },
    {
        "api_name": "oneflow.gather_nd",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.gather_nd.html",
        "api_signature": "oneflow.gather_nd(input, index)",
        "api_description": "This operator is a high-dimensional extension of gather, index is a K-dimensional\ntensor, which is regarded as a index of input Tensor input.\nEach element defines a slice of input:\n\\[output[i_{0},i_{1},...,i_{K-2}] = input[index(i_{0},i_{1},...,i_{K-2})]\\]",
        "return_value": "",
        "parameters": "\ninput – The input Tensor.\nindex – The slice indices.\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> import numpy as np\n>>> input = flow.tensor(np.array([[1, 2,3], [4, 5,6],[7,8,9]]), dtype=flow.float)\n>>> index_1 = flow.tensor(np.array([[0], [2]]), dtype=flow.int)\n>>> out_1 = flow.gather_nd(input,index_1)\n>>> print(out_1.shape)\noneflow.Size([2, 3])\n>>> out_1\ntensor([[1., 2., 3.],\n        [7., 8., 9.]], dtype=oneflow.float32)\n>>> index_2 = flow.tensor(np.array([[0,2], [2,1]]), dtype=flow.int)\n>>> out_2 = flow.gather_nd(input,index_2)\n>>> out_2\ntensor([3., 8.], dtype=oneflow.float32)\n\n\n\n"
    },
    {
        "api_name": "oneflow.batch_gather",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.batch_gather.html",
        "api_signature": "oneflow.batch_gather()",
        "api_description": "Gather the element in batch dims.",
        "return_value": "",
        "parameters": "\nin (Tensor) – the input tensor.\nindices (Tensor) – the indices tensor, its dtype must be int32/64.\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": "Example 1:\n>>> import oneflow as flow\n>>> import numpy as np\n\n>>> x = flow.Tensor(np.array([[1, 2, 3],\n...                           [4, 5, 6]]))\n>>> indices = flow.tensor(np.array([1, 0]).astype(np.int64))\n>>> out = flow.batch_gather(x, indices)\n\ntensor([[4., 5., 6.],\n        [1., 2., 3.]], dtype=oneflow.float32)\n\n\nExample 2:\n>>> import oneflow as flow\n>>> import numpy as np\n\n>>> x = flow.Tensor(np.array([[[1, 2, 3], [4, 5, 6]],\n...                           [[1, 2, 3], [4, 5, 6]]]))\n>>> indices = flow.tensor(np.array([[1, 0],\n...                                 [0, 1]]).astype(np.int64))\n>>> out = flow.batch_gather(x, indices)\n\ntensor([[[4., 5., 6.],\n         [1., 2., 3.]],\n        [[1., 2., 3.],\n         [4., 5., 6.]]], dtype=oneflow.float32)\n\n\n\n"
    },
    {
        "api_name": "oneflow.hsplit",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.hsplit.html",
        "api_signature": "oneflow.hsplit(input, indices_or_sections)",
        "api_description": "The documentation is referenced from:\nSplits input, a tensor with one or more dimensions, into multiple tensors horizontally according to indices_or_sections.\nEach split is a view of input.\nIf input is one dimensional this is equivalent to calling oneflow.tensor_split(input, indices_or_sections, dim=0)\n(the split dimension is zero), and if input has two or more dimensions it’s equivalent to calling\noneflow.tensor_split(input, indices_or_sections, dim=1) (the split dimension is 1), except that if indices_or_sections\nis an integer it must evenly divide the split dimension or a runtime error will be thrown.",
        "return_value": "the output TensorTuple.\n\n",
        "parameters": "\ninput (Tensor) – the input tensor.\nindices_or_sections (int or a list) – See argument in oneflow.tensor_split().\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n\n>>> input = flow.rand(3,4,5,6)\n>>> output = flow.hsplit(input,(1,3))\n>>> output[0].size()\noneflow.Size([3, 1, 5, 6])\n>>> output[1].size()\noneflow.Size([3, 2, 5, 6])\n>>> output[2].size()\noneflow.Size([3, 1, 5, 6])\n\n\n\n"
    },
    {
        "api_name": "oneflow.hstack",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.hstack.html",
        "api_signature": "oneflow.hstack(tensors)",
        "api_description": "Stack tensors in tensors horizontally (column wise).\nThis is equivalent to concatenation tensors in tensors along the first axis for 1-D tensors, and along the second axis for all other tensors.\nWhen there are tensors with dimension less than 1, these tensors will be reshaped by oneflow.atleast_1d() to 1-dims tensors before stacking.",
        "return_value": "A Tensor\n\n\n",
        "parameters": "tensors – (List[oneflow.Tensor]): sequence of tensors to stack\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> x1 = flow.randn(5, 2)\n>>> x2 = flow.randn(5, 3)\n>>> flow.hstack([x1, x2]).shape\noneflow.Size([5, 5])\n>>> x = flow.randn(5)\n>>> flow.hstack([x, x]).shape\noneflow.Size([10])\n\n\n\n"
    },
    {
        "api_name": "oneflow.vsplit",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.vsplit.html",
        "api_signature": "oneflow.vsplit()",
        "api_description": "Splits input, a tensor with two or more dimensions, into multiple tensors vertically according to indices_or_sections.\nEach split is a view of input.\nThis is equivalent to calling oneflow.tensor_split(input, indices_or_sections, dim=0) (the split dimension is 0),\nexcept that if indices_or_sections is an integer it must evenly divide the split dimension or a runtime error will be thrown.\nThe documentation is referenced from:",
        "return_value": "the output TensorTuple.\n\n",
        "parameters": "\ninput (Tensor) – the input tensor.\nindices_or_sections (int or a list) – If indices_or_sections is an integer n , input is split into n sections\nalong dimension dim.If input is divisible by n along dimension dim, each section will be of equal size,\ninput.size (dim) / n. If input is not divisible by n, the sizes of the first int(input.size(dim) % n).\nsections will have size int(input.size(dim) / n) + 1, and the rest will have size int(input.size(dim) / n).\nIf indices_or_sections is a list or tuple of ints, then input is split along dimension dim at each of the indices in\nthe list, tuple or tensor. For instance, indices_or_sections=[2, 3] and dim=0 would result in the tensors\ninput[:2], input[2:3], and input[3:].If indices_or_sections is a tensor, it must be a zero-dimensional or\none-dimensional long tensor on the CPU.\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n\n>>> input = flow.rand(4, 4, 5, 6)\n>>> output = flow.vsplit(input, (1, 3))\n>>> output[0].size()\noneflow.Size([1, 4, 5, 6])\n>>> output[1].size()\noneflow.Size([2, 4, 5, 6])\n>>> output[2].size()\noneflow.Size([1, 4, 5, 6])\n\n\n\n"
    },
    {
        "api_name": "oneflow.vstack",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.vstack.html",
        "api_signature": "oneflow.vstack(tensors)",
        "api_description": "Stack tensors in tensors vertically (row wise).\nThis is equivalent to concatenation tensors in tensors along the first axis.\nWhen there are tensors with dimension less than 2, these tensors will be reshaped by oneflow.atleast_2d() to 2-D tensors before stacking.",
        "return_value": "A Tensor\n\n\n",
        "parameters": "tensors – (List[oneflow.Tensor]): sequence of tensors to stack\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> x1 = flow.randn(2, 5)\n>>> x2 = flow.randn(3, 5)\n>>> flow.vstack([x1, x2]).shape\noneflow.Size([5, 5])\n>>> x = flow.randn(5)\n>>> flow.vstack([x, x]).shape\noneflow.Size([2, 5])\n\n\n\n"
    },
    {
        "api_name": "oneflow.index_select",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.index_select.html",
        "api_signature": "oneflow.index_select(dim, index)",
        "api_description": "Select values along an axis specified by dim.\nindex must be an Int32 Tensor with 1-D.\ndim must be in the range of input Dimensions.\nvalue of index must be in the range of the dim-th of input.",
        "return_value": "",
        "parameters": "\ninput (Tensor) – the source tensor\ndim (int) – the axis along which to index\nindex (Tensor) – the 1-D tensor containing the indices to index\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> input = flow.tensor([[1,2,3],[4,5,6]], dtype=flow.int32)\n>>> input\ntensor([[1, 2, 3],\n        [4, 5, 6]], dtype=oneflow.int32)\n>>> index = flow.tensor([0,1], dtype=flow.int64)\n>>> output = flow.index_select(input, 1, index)\n>>> output\ntensor([[1, 2],\n        [4, 5]], dtype=oneflow.int32)\n>>> output = input.index_select(1, index)\n>>> output\ntensor([[1, 2],\n        [4, 5]], dtype=oneflow.int32)\n\n\n\n"
    },
    {
        "api_name": "oneflow.index_add",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.index_add.html",
        "api_signature": "oneflow.index_add(input, dim, index, source, *, alpha=1, out=None)",
        "api_description": "See oneflow.Tensor.index_add_() for function description.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.masked_select",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.masked_select.html",
        "api_signature": "oneflow.masked_select(input, mask)",
        "api_description": "",
        "return_value": "The shapes of the mask tensor and the input tensor don’t need to match, but they must be broadcastable.\n\n",
        "parameters": "\ninput (Tensor) – the input tensor.\nmask (Tensor) – the tensor containing the binary mask to index with\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> import numpy as np\n\n>>> input = flow.tensor(np.array([[-0.4620, 0.3139], [0.3898, -0.7197], [0.0478, -0.1657]]), dtype=flow.float32)\n>>> mask = input.gt(0.05)\n>>> out = flow.masked_select(input, mask)\n>>> out\ntensor([0.3139, 0.3898], dtype=oneflow.float32)\n\n\n\n"
    },
    {
        "api_name": "oneflow.movedim",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.movedim.html",
        "api_signature": "oneflow.movedim()",
        "api_description": "Moves the dimension(s) of input at the position(s) in source to the position(s) in destination.\nOther dimensions of input that are not explicitly moved remain in their original order and appear at the positions not specified in destination.\nThe documentation is referenced from:",
        "return_value": "the output Tensor.\n\n",
        "parameters": "\ninput (Tensor) – the input tensor.\nsource (int or a list) – Original positions of the dims to move. These must be unique.\ndestination (int or a list) – Destination positions for each of the original dims. These must also be unique.\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> import numpy as np\n\n>>> input = flow.tensor(np.random.randn(2, 3, 4, 5), dtype=flow.float32)\n>>> output = flow.movedim(input, 1, 0)\n>>> output.shape\noneflow.Size([3, 2, 4, 5])\n>>> output = flow.movedim(input, (1, 2), (0, 1))\n>>> output.shape\noneflow.Size([3, 4, 2, 5])\n\n\n\n"
    },
    {
        "api_name": "oneflow.narrow",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.narrow.html",
        "api_signature": "oneflow.narrow(x, dim: int, start: int, length: int)",
        "api_description": "",
        "return_value": "The dimension dim is input from start to start + length.\n\n",
        "parameters": "\ninput – the tensor to narrow.\ndim – the dimension along which to narrow.\nstart – the starting dimension.\nlength – the distance to the ending dimension.\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> input = flow.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n>>> flow.narrow(input, 0, 0, 2)\ntensor([[1, 2, 3],\n        [4, 5, 6]], dtype=oneflow.int64)\n>>> flow.narrow(input, 1, 1, 2)\ntensor([[2, 3],\n        [5, 6],\n        [8, 9]], dtype=oneflow.int64)\n\n\n\n"
    },
    {
        "api_name": "oneflow.nonzero",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nonzero.html",
        "api_signature": "oneflow.nonzero(input, *, out=None, as_tuple=False)",
        "api_description": "",
        "return_value": "input.  Each row in the result contains the indices of a non-zero\nelement in input. The result is sorted lexicographically, with\nthe last index changing the fastest (C-style).\nIf input has \\(n\\) dimensions, then the resulting indices tensor\nout is of size \\((z \\times n)\\), where \\(z\\) is the total number of\nnon-zero elements in the input tensor.\nWhen as_tuple is True:\neach containing the indices (in that dimension) of all non-zero elements of\ninput .\nIf input has \\(n\\) dimensions, then the resulting tuple contains \\(n\\)\ntensors of size \\(z\\), where \\(z\\) is the total number of\nnon-zero elements in the input tensor.\nAs a special case, when input has zero dimensions and a nonzero scalar\nvalue, it is treated as a one-dimensional tensor with one element.\n\nIf as_tuple is False, the output\ntensor containing indices. If as_tuple is True, one 1-D tensor for\neach dimension, containing the indices of each nonzero element along that\ndimension.\n\n",
        "parameters": "input (Tensor) – the input tensor.\n\nKeyword Arguments\nout (Tensor, optional) – the output tensor containing indices\n\n",
        "input_shape": "",
        "notes": "When as_tuple is False (default):  returns a\n2-D tensor where each row is the index for a nonzero value.\nWhen as_tuple is True: returns a tuple of 1-D\nindex tensors, allowing for advanced indexing, so x[x.nonzero(as_tuple=True)]\ngives all nonzero values of tensor x. Of the returned tuple, each index tensor\ncontains nonzero indices for a certain dimension.\nSee below for more details on the two behaviors.\nWhen as_tuple is False (default):",
        "code_example": ""
    },
    {
        "api_name": "oneflow.permute",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.permute.html",
        "api_signature": "oneflow.permute(input, *dims)",
        "api_description": "",
        "return_value": "\n",
        "parameters": "dims (tuple of ints) – The desired ordering of dimensions\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import numpy as np\n>>> import oneflow as flow\n\n>>> input = flow.tensor(np.random.randn(2, 6, 5, 3), dtype=flow.float32)\n>>> output = flow.permute(input, (1, 0, 2, 3)).shape\n>>> output\noneflow.Size([6, 2, 5, 3])\n\n\n\n"
    },
    {
        "api_name": "oneflow.repeat",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.repeat.html",
        "api_signature": "oneflow.repeat(input, sizes)",
        "api_description": "This operator repeat the input tensor to a larger size along the specified dimensions.",
        "return_value": "The result Tensor.\n\n",
        "parameters": "\ninput (oneflow.Tensor) – the input Tensor.\nsizes (flow.Shape or List) – The number of times to repeat this tensor along each dimension.\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> import numpy as np\n>>> np_arr = np.random.randn(5, 3, 6, 9).astype(np.float32)\n>>> input = flow.Tensor(np_arr)\n>>> out = input.repeat(1, 1, 2, 2)\n>>> out.shape\noneflow.Size([5, 3, 12, 18])\n>>> out = input.repeat(2, 1, 1, 2, 2)\n>>> out.shape\noneflow.Size([2, 5, 3, 12, 18])\n\n\n\n"
    },
    {
        "api_name": "oneflow.reshape",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.reshape.html",
        "api_signature": "oneflow.reshape(input, shape: Optional[Sequence[int]] = None)",
        "api_description": "This operator reshapes a Tensor.\nWe can set one dimension in shape as -1, the operator will infer the complete shape.",
        "return_value": "A Tensor has the same type as x.\n\n\n",
        "parameters": "\nx – A Tensor.\nshape – Shape of the output tensor.\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import numpy as np\n>>> import oneflow as flow\n>>> x = np.array(\n...    [[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12], [13, 14, 15, 16]]\n... ).astype(np.float32)\n>>> input = flow.Tensor(x)\n\n>>> y = flow.reshape(input, shape=[2, 2, 2, -1]).shape\n>>> y\noneflow.Size([2, 2, 2, 2])\n\n\n\n"
    },
    {
        "api_name": "oneflow.row_stack",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.row_stack.html",
        "api_signature": "oneflow.row_stack(tensors)",
        "api_description": "Alias of oneflow.vstack().\nStack tensors in tensors vertically (row wise).\nThis is equivalent to concatenation tensors in tensors along the first axis.\nWhen there are tensors with dimension less than 2, these tensors will be reshaped by oneflow.atleast_2d() to 2-D tensors before stacking.",
        "return_value": "A Tensor\n\n\n",
        "parameters": "tensors – (List[oneflow.Tensor]): sequence of tensors to stack\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> x1 = flow.randn(2, 5)\n>>> x2 = flow.randn(3, 5)\n>>> flow.vstack([x1, x2]).shape\noneflow.Size([5, 5])\n>>> x = flow.randn(5)\n>>> flow.vstack([x, x]).shape\noneflow.Size([2, 5])\n\n\n\n"
    },
    {
        "api_name": "oneflow.select",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.select.html",
        "api_signature": "oneflow.select()",
        "api_description": "Slices the self tensor along the selected dimension at the given index. This function returns\na view of the original tensor with the given dimension removed.",
        "return_value": "the output Tensor.\n\n",
        "parameters": "\ninput (Tensor) – the input tensor.\ndim (int) – the dimension to slice.\nselect (int) – the index to select with.\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> input = flow.rand(3, 4, 5)\n>>> out = flow.select(input, 0, 1)\n>>> out.size()\noneflow.Size([4, 5])\n>>> out = flow.select(input, 1, 1)\n>>> out.size()\noneflow.Size([3, 5])\n\n\n\n"
    },
    {
        "api_name": "oneflow.scatter",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.scatter.html",
        "api_signature": "oneflow.scatter(input, dim, index, src, *, reduce=None)",
        "api_description": "This operator writes the elements specified by index along with the axis\ndim from the src into the input.\nTake a 3-D blob as example, the output is specified by:\ninput[index[i][j][k]][j][k] = src[i][j][k]  # if dim == 0\ninput[i][index[i][j][k]][k] = src[i][j][k]  # if dim == 1\ninput[i][j][index[i][j][k]] = src[i][j][k]  # if dim == 2\ninput, index and src (if it is a Tensor) should all have the same number of dimensions.\nIt is also required that index.shape(d) <= src.shape(d) for all dimensions d,\nand that index.shape(d) <= input.shape(d) for all dimensions d != dim.",
        "return_value": "The scatterd Tensor.\n\n",
        "parameters": "\ninput (Tensor) – The input blob.\ndim (int) – The axis along which to index\nindex (Tensor) – The index blob of elements to scatter.\nsrc (Tensor or float) – The source blob whose elements will be scatterd and updated to output.\nreduce (str, optional) – Reduction operation to apply, can be either add or multiply.\n\n\n",
        "input_shape": "",
        "notes": "Warning\nWhen indices are not unique, the behavior is non-deterministic (one of the values from src will be picked arbitrarily)\nand the gradient will be incorrect (it will be propagated to all locations in the source that correspond to the same index)!\nThe backward pass is implemented only for src.shape == index.shape.\nAdditionally accepts an optional reduce argument that allows specification of an optional reduction operation,\nwhich is applied to all values in the tensor src into input at the indicies specified in the index.\nFor each value in src, the reduction operation is applied to an index in input which is specified by its index in src for dimension != dim\nand by the corresponding value in index for dimension = dim.\nGiven a 3-D tensor and reduction using the multiplication operation, input is updated as:\ninput[index[i][j][k]][j][k] *= src[i][j][k]  # if dim == 0\ninput[i][index[i][j][k]][k] *= src[i][j][k]  # if dim == 1\ninput[i][j][index[i][j][k]] *= src[i][j][k]  # if dim == 2\nReducing with the addition operation is the same as using oneflow.scatter_add().",
        "code_example": ">>> import oneflow as flow\n>>> import numpy as np\n\n>>> input = flow.ones((3,5))*2\n>>> index = flow.tensor(np.array([[0,1,2],[0,1,4]], ), dtype=flow.int32)\n>>> src = flow.Tensor(np.array([[0,10,20,30,40],[50,60,70,80,90]]))\n>>> out = flow.scatter(input, 1, index, src)\n>>> out\ntensor([[ 0., 10., 20.,  2.,  2.],\n        [50., 60.,  2.,  2., 70.],\n        [ 2.,  2.,  2.,  2.,  2.]], dtype=oneflow.float32)\n\n\n\n"
    },
    {
        "api_name": "oneflow.scatter_add",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.scatter_add.html",
        "api_signature": "oneflow.scatter_add(input, dim, index, src)",
        "api_description": "This operator scatter the src with addition operation according to index along dim into the input.\nTake a 3-D blob as example, the output is specified by:\ninput[index[i][j][k]][j][k] += src[i][j][k]  # if dim == 0\ninput[i][index[i][j][k]][k] += src[i][j][k]  # if dim == 1\ninput[i][j][index[i][j][k]] += src[i][j][k]  # if dim == 2",
        "return_value": "The scatterd Tensor.\n\n",
        "parameters": "\ninput (Tensor) – The input blob.\ndim (int) – The axis along which to index\nindex (Tensor) – The index blob of elements to scatter.\nsrc (Tensor) – The source blob whose elements will be scatterd and added to output.\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> import numpy as np\n>>> input = flow.ones((3,5))*2\n>>> index = flow.tensor(np.array([[0,1,2],[0,1,4]], ), dtype=flow.int32)\n>>> src = flow.Tensor(np.array([[0,10,20,30,40],[50,60,70,80,90]]))\n>>> out = flow.scatter_add(input, 1, index, src)\n>>> out\ntensor([[ 2., 12., 22.,  2.,  2.],\n        [52., 62.,  2.,  2., 72.],\n        [ 2.,  2.,  2.,  2.,  2.]], dtype=oneflow.float32)\n\n\n\n"
    },
    {
        "api_name": "oneflow.scatter_nd",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.scatter_nd.html",
        "api_signature": "oneflow.scatter_nd(index, update, shape)",
        "api_description": "This operator inserts the elements in update according to the index and create a new Tensor.",
        "return_value": "",
        "parameters": "\nindex – The indices of update. Its type should be flow.int.\nupdate – The update Tensor.\nshape (Sequence[int]) – The constant tensor shape, the constant tensor elements are all zero.\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> import numpy as np\n>>> index = flow.tensor(np.array([[1], [6], [4]]), dtype=flow.int)\n>>> update = flow.tensor(np.array([10.2, 5.1, 12.7]), dtype=flow.float)\n>>> out = flow.scatter_nd(index, update, [8])\n>>> out\ntensor([ 0.0000, 10.2000,  0.0000,  0.0000, 12.7000,  0.0000,  5.1000,  0.0000],\n       dtype=oneflow.float32)\n\n\n\n"
    },
    {
        "api_name": "oneflow.slice",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.slice.html",
        "api_signature": "oneflow.slice(input, slice_tup_list: Sequence[Tuple[int, int, int]])",
        "api_description": "Extracts a slice from a tensor.\nThe slice_tup_list assigns the slice indices in each dimension, the format is (start, stop, step).\nThe operator will slice the tensor according to the slice_tup_list.",
        "return_value": "",
        "parameters": "\ninput – A Tensor.\nslice_tup_list – A list of slice tuple, indicate each dimension slice (start, stop, step).\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import numpy as np\n>>> import oneflow as flow\n>>> input = flow.Tensor(np.random.randn(3, 6, 9).astype(np.float32))\n>>> tup_list = [[None, None, None], [0, 5, 2], [0, 6, 3]]\n>>> y = flow.slice(input, slice_tup_list=tup_list)\n>>> y.shape\noneflow.Size([3, 3, 2])\n\n\n\n"
    },
    {
        "api_name": "oneflow.slice_update",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.slice_update.html",
        "api_signature": "oneflow.slice_update(input, update, slice_tup_list: Sequence[Tuple[int, int, int]])",
        "api_description": "Update a slice of tensor x. Like x[start:stop:step] = update.",
        "return_value": "",
        "parameters": "\nx – A Tensor, whose slice will be updated.\nupdate – A Tensor, indicate the update content.\nslice_tup_list – A list of slice tuple, indicate each dimension slice (start, stop, step).\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import numpy as np\n>>> import oneflow as flow\n\n>>> input = flow.Tensor(np.array([1, 1, 1, 1, 1]).astype(np.float32))\n>>> update = flow.Tensor(np.array([2, 3, 4]).astype(np.float32))\n>>> flow.slice_update(input, update, slice_tup_list=[[1, 4, 1]])\ntensor([1., 2., 3., 4., 1.], dtype=oneflow.float32)\n\n\n\n"
    },
    {
        "api_name": "oneflow.split",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.split.html",
        "api_signature": "oneflow.split()",
        "api_description": "Splits the tensor into chunks.\nIf split_size_or_sections is an integer type, then x will be split into equally sized chunks (if possible).\nLast chunk will be smaller if the tensor size along the given dimension dim is not divisible by split_size.\nIf split_size_or_sections is a list, then x will be split into len(split_size_or_sections) chunks\nwith sizes in dim according to split_size_or_sections.",
        "return_value": "",
        "parameters": "\nx – tensor to split.\nsplit_size_or_sections – size of a single chunk or list of sizes for each chunk.\ndim – dimension along which to split the tensor.\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> a = flow.arange(10).view(5, 2)\n>>> flow.split(a, 2)\n(tensor([[0, 1],\n        [2, 3]], dtype=oneflow.int64), tensor([[4, 5],\n        [6, 7]], dtype=oneflow.int64), tensor([[8, 9]], dtype=oneflow.int64))\n>>> flow.split(a, [1, 4])\n(tensor([[0, 1]], dtype=oneflow.int64), tensor([[2, 3],\n        [4, 5],\n        [6, 7],\n        [8, 9]], dtype=oneflow.int64))\n\n\n\n"
    },
    {
        "api_name": "oneflow.squeeze",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.squeeze.html",
        "api_signature": "oneflow.squeeze()",
        "api_description": "This operator removes the specified dimention which size is 1 of the input Tensor.\nIf the dim is not specified, this operator will remove all the dimention which size is 1 of the input Tensor.\nThe amount of element in return value is the same as Tensor input.",
        "return_value": "The result Tensor.\n\n",
        "parameters": "\ninput (oneflow.Tensor) – the input Tensor.\ndim (int, optinal) – Defaults to None, if given, the input will be squeezed only in this dimension.\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> import numpy as np\n>>> input = flow.tensor(np.array([[[[1, 1, 1]]]]).astype(np.int32))\n>>> input.shape\noneflow.Size([1, 1, 1, 3])\n>>> out = flow.squeeze(input, dim=[1, 2]).shape\n>>> out\noneflow.Size([1, 3])\n\n\n\n"
    },
    {
        "api_name": "oneflow.stack",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.stack.html",
        "api_signature": "oneflow.stack()",
        "api_description": "Concatenates a sequence of tensors along a new dimension.\nThe returned tensor shares the same underlying data with input tensors.\nA dim value within the range [-input.ndimension() - 1, input.ndimension() + 1]\ncan be used. Negative dim will correspond to stack()\napplied at dim = dim + input.ndimension() + 1.",
        "return_value": "A Tensor\n\n\n",
        "parameters": "\ninputs (List[oneflow.Tensor]) – the list of input tensors. Each tensor should have the same shape.\ndim (int) – the index at which to insert the concatenated dimension.\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> import numpy as np\n\n>>> x1 = flow.tensor(np.random.rand(1, 3, 5))\n>>> x2 = flow.tensor(np.random.rand(1, 3, 5))\n>>> y = flow.stack([x1, x2], dim = -1)\n>>> y.shape\noneflow.Size([1, 3, 5, 2])\n\n\n\n"
    },
    {
        "api_name": "oneflow.swapaxes",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.swapaxes.html",
        "api_signature": "oneflow.swapaxes(input, axis0, axis1)",
        "api_description": "This function is equivalent to NumPy’s swapaxes function.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n\n>>> x = flow.tensor([[[0,1],[2,3]],[[4,5],[6,7]]])\n>>> x.shape\noneflow.Size([2, 2, 2])\n>>> flow.swapaxes(x, 0, 1).shape\noneflow.Size([2, 2, 2])\n>>> flow.swapaxes(x, 0, 2).shape\noneflow.Size([2, 2, 2])\n\n\n\n"
    },
    {
        "api_name": "oneflow.swapdims",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.swapdims.html",
        "api_signature": "oneflow.swapdims(input, dim0, dim1)",
        "api_description": "This function is equivalent to torch’s swapdims function.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n\n>>> x = flow.tensor([[[0,1],[2,3]],[[4,5],[6,7]]])\n>>> x\ntensor([[[0, 1],\n         [2, 3]],\n\n        [[4, 5],\n         [6, 7]]], dtype=oneflow.int64)\n>>> flow.swapdims(x, 0, 1)\ntensor([[[0, 1],\n         [4, 5]],\n\n        [[2, 3],\n         [6, 7]]], dtype=oneflow.int64)\n>>> flow.swapdims(x, 0, 2)\ntensor([[[0, 4],\n         [2, 6]],\n\n        [[1, 5],\n         [3, 7]]], dtype=oneflow.int64)\n\n\n\n"
    },
    {
        "api_name": "oneflow.t",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.t.html",
        "api_signature": "oneflow.t()",
        "api_description": "oneflow.t(input) → Tensor.\nExpects input to be <= 2-D tensor and transposes dimensions 0 and 1.\n0-D and 1-D tensors are returned as is. When input is a 2-D tensor this is equivalent to transpose(input, 0, 1).",
        "return_value": "",
        "parameters": "input (oneflow.Tensor) – An input tensor.\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> import numpy as np\n\n>>> x = flow.tensor(np.random.randn(), dtype=flow.float32)\n>>> flow.t(x).shape\noneflow.Size([])\n>>> x = flow.tensor(np.random.randn(3), dtype=flow.float32)\n>>> flow.t(x).shape\noneflow.Size([3])\n>>> x = flow.tensor(np.random.randn(2,3), dtype=flow.float32)\n>>> flow.t(x).shape\noneflow.Size([3, 2])\n\n\n\n"
    },
    {
        "api_name": "oneflow.tile",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.tile.html",
        "api_signature": "oneflow.tile(input, dims)",
        "api_description": "Constructs a tensor by repeating the elements of input.  The dims argument specifies the number\nof repetitions in each dimension.\nIf dims specifies fewer dimensions than input has, then ones are prepended to dims until\nall dimensions are specified.  For example, if input has shape (8, 6, 4, 2) and dims is (2, 2),\nthen dims is treated as (1, 1, 2, 2).\nAnalogously, if input has fewer dimensions than dims specifies, then input is treated as\nif it were unsqueezed at dimension zero until it has as many dimensions as dims specifies.",
        "return_value": "",
        "parameters": "\ninput (oneflow.Tensor) – the tensor whose elements to repeat.\ndims (tuple) – the number of repetitions per dimension.\n\n\n\n",
        "input_shape": "",
        "notes": "This function is similar to NumPy’s tile function.\nThe documentation is referenced from:",
        "code_example": "if it had the shape (1, 1, 4, 2).\n\n>>> import oneflow as flow\n>>> import numpy as np\n\n>>> np_arr = np.random.randn(5, 3, 6, 9).astype(np.float32)\n>>> input = flow.Tensor(np_arr)\n>>> out = input.tile(2,1,2,1)\n>>> out.shape\noneflow.Size([10, 3, 12, 9])\n>>> x = np.random.randn(5, 2, 1)\n>>> input = flow.Tensor(x)\n>>> out = input.tile(3,4)\n>>> out.shape\noneflow.Size([5, 6, 4])\n\n\n\n"
    },
    {
        "api_name": "oneflow.transpose",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.transpose.html",
        "api_signature": "oneflow.transpose()",
        "api_description": "",
        "return_value": "The resulting out tensor shares its underlying storage with the input tensor, so changing the content of one would change the content of the other.\n\nA transposed tensor.\n\n",
        "parameters": "\ninput (oneflow.Tensor) – the input tensor.\ndim0 (int) – the first dimension to be transposed.\ndim1 (int) – the second dimension to be transposed.\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import numpy as np\n>>> import oneflow as flow\n>>> input = flow.tensor(np.random.randn(2, 6, 5, 3), dtype=flow.float32)\n>>> out = flow.transpose(input, 0, 1).shape\n>>> out\noneflow.Size([6, 2, 5, 3])\n\n\n\n"
    },
    {
        "api_name": "oneflow.unbind",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.unbind.html",
        "api_signature": "oneflow.unbind()",
        "api_description": "Removes a tensor dimension.",
        "return_value": "This function is equivalent to PyTorch’s unbind function.\n\n",
        "parameters": "\nx (Tensor) – the tensor to unbind\ndim (int) – dimension to remove\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n\n>>> x = flow.tensor(range(12)).reshape([3,4])\n>>> flow.unbind(x)\n(tensor([0, 1, 2, 3], dtype=oneflow.int64), tensor([4, 5, 6, 7], dtype=oneflow.int64), tensor([ 8,  9, 10, 11], dtype=oneflow.int64))\n>>> flow.unbind(x, 1)\n(tensor([0, 4, 8], dtype=oneflow.int64), tensor([1, 5, 9], dtype=oneflow.int64), tensor([ 2,  6, 10], dtype=oneflow.int64), tensor([ 3,  7, 11], dtype=oneflow.int64))\n\n\n\n"
    },
    {
        "api_name": "oneflow.unsqueeze",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.unsqueeze.html",
        "api_signature": "oneflow.unsqueeze(input, dim)",
        "api_description": "",
        "return_value": "specified position.\nThe returned tensor shares the same underlying data with this tensor.\nA dim value within the range [-input.ndimension() - 1, input.ndimension() + 1)\ncan be used. Negative dim will correspond to unsqueeze()\napplied at dim = dim + input.ndimension() + 1.\n\n",
        "parameters": "\ninput (Tensor) – the input tensor.\ndim (int) – the index at which to insert the singleton dimension\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import numpy as np\n>>> import oneflow as flow\n\n>>> x = flow.randn(2, 3, 4)\n>>> y = x.unsqueeze(2)\n>>> y.shape\noneflow.Size([2, 3, 1, 4])\n\n\n\n"
    },
    {
        "api_name": "oneflow.where",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.where.html",
        "api_signature": "oneflow.where(condition, x=None, y=None)",
        "api_description": "Return a tensor of elements selected from either x or y, depending on condition.\nIf the element in condition is larger than 0,\nit will take the x element, else it will take the y element",
        "return_value": "A tensor of shape equal to the broadcasted shape of condition, x, y\n\n",
        "parameters": "\ncondition (IntTensor) – When 1 (nonzero), yield x, otherwise yield y\nx (Tensor or Scalar) – value (if :attr:x is a scalar) or values selected at indices\nwhere condition is True\ny (Tensor or Scalar) – value (if :attr:x is a scalar) or values selected at indices\nwhere condition is False\n\n\n",
        "input_shape": "",
        "notes": "If x is None and y is None,  flow.where(condition) is\nidentical to flow.nonzero(condition, as_tuple=True).\nThe tensors condition, x, y must be broadcastable.",
        "code_example": ">>> import numpy as np\n>>> import oneflow as flow\n>>> x = flow.tensor(\n...    np.array([[-0.4620, 0.3139], [0.3898, -0.7197], [0.0478, -0.1657]]),\n...    dtype=flow.float32,\n... )\n>>> y = flow.tensor(np.ones(shape=(3, 2)), dtype=flow.float32)\n>>> condition = flow.tensor(np.array([[0, 1], [1, 0], [1, 0]]), dtype=flow.int32)\n>>> out = condition.where(x, y)\n>>> out \ntensor([[1.0000, 0.3139],\n        ...\n        [0.0478, 1.0000]], dtype=oneflow.float32)\n\n\n\n"
    },
    {
        "api_name": "oneflow.tensor_split",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.tensor_split.html",
        "api_signature": "oneflow.tensor_split()",
        "api_description": "Splits a tensor into multiple sub-tensors, all of which are views of input, along dimension\ndim according to the indices or number of sections specified by indices_or_sections .\nThe documentation is referenced from:",
        "return_value": "the output TensorTuple.\n\n",
        "parameters": "\ninput (Tensor) – the input tensor.\nindices_or_sections (int or a list) – If indices_or_sections is an integer n , input is split into n sections\nalong dimension dim.If input is divisible by n along dimension dim, each section will be of equal size,\ninput.size (dim) / n. If input is not divisible by n, the sizes of the first int(input.size(dim) % n).\nsections will have size int(input.size(dim) / n) + 1, and the rest will have size int(input.size(dim) / n).\nIf indices_or_sections is a list or tuple of ints, then input is split along dimension dim at each of the indices in\nthe list, tuple or tensor. For instance, indices_or_sections=[2, 3] and dim=0 would result in the tensors\ninput[:2], input[2:3], and input[3:].If indices_or_sections is a tensor, it must be a zero-dimensional or\none-dimensional long tensor on the CPU.\ndim (int) – dimension along which to split the tensor.\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n\n>>> input = flow.rand(3,4,5)\n>>> output = flow.tensor_split(input,(2,3),2)\n>>> output[0].size()\noneflow.Size([3, 4, 2])\n>>> output[1].size()\noneflow.Size([3, 4, 1])\n>>> output[2].size()\noneflow.Size([3, 4, 2])\n\n\n\n"
    },
    {
        "api_name": "oneflow.seed",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.seed.html",
        "api_signature": "oneflow.seed()",
        "api_description": "Sets the seed for generating random numbers to a non-deterministic\nrandom number. Returns a 64 bit number used to seed the RNG.\nThe documentation is referenced from:",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.manual_seed",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.manual_seed.html",
        "api_signature": "oneflow.manual_seed(seed)",
        "api_description": "Sets the seed for generating random numbers. Returns a\noneflow.Generator object.\nThe documentation is referenced from:",
        "return_value": "",
        "parameters": "seed (int) – The desired seed. Value must be within the inclusive range\n[-0x8000_0000_0000_0000, 0xffff_ffff_ffff_ffff]. Otherwise, a RuntimeError\nis raised. Negative inputs are remapped to positive values with the formula\n0xffff_ffff_ffff_ffff + seed.\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.initial_seed",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.initial_seed.html",
        "api_signature": "oneflow.initial_seed()",
        "api_description": "",
        "return_value": "Python long.\nThe documentation is referenced from:\nhttps://pytorch.org/docs/1.10/_modules/torch/random.html.\n\n",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.get_rng_state",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.get_rng_state.html",
        "api_signature": "oneflow.get_rng_state()",
        "api_description": "Sets the random number generator state.\nThe documentation is referenced from:",
        "return_value": "",
        "parameters": "new_state (oneflow.ByteTensor) – The desired state\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.set_rng_state",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.set_rng_state.html",
        "api_signature": "oneflow.set_rng_state(state)",
        "api_description": "",
        "return_value": "The documentation is referenced from:\nhttps://pytorch.org/docs/1.10/generated/torch.set_rng_state.html.\n\n",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.bernoulli",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.bernoulli.html",
        "api_signature": "oneflow.bernoulli(input, p, *, generator=None, out=None)",
        "api_description": "This operator returns a Tensor with binaray random numbers (0 / 1) from a Bernoulli distribution.",
        "return_value": "",
        "parameters": "\ninput (Tensor) – the input tensor of probability values for the Bernoulli distribution\np (float, optional) – the probability for the Bernoulli distribution. If specified, Bernoulli distribution will use p for sampling, not input\ngenerator (Generator, optional) – a pseudorandom number generator for sampling\nout (Tensor, optional) – the output tensor.\n\n\n\n\n",
        "input_shape": "Input: \\((*)\\). Input can be of any shape\nOutput: \\((*)\\). Output is of the same shape as input\n\n\n\n",
        "notes": "",
        "code_example": ">>> import numpy as np\n>>> import oneflow as flow\n\n>>> arr = np.array(\n...    [\n...        [1.0, 1.0, 1.0],\n...        [1.0, 1.0, 1.0],\n...        [1.0, 1.0, 1.0],\n...    ]\n... )\n>>> x = flow.tensor(arr, dtype=flow.float32)\n>>> y = flow.bernoulli(x)\n>>> y\ntensor([[1., 1., 1.],\n        [1., 1., 1.],\n        [1., 1., 1.]], dtype=oneflow.float32)\n>>> y = flow.bernoulli(x, 1)\n>>> y\ntensor([[1., 1., 1.],\n        [1., 1., 1.],\n        [1., 1., 1.]], dtype=oneflow.float32)\n>>> y = flow.bernoulli(x, p=0)\n>>> y\ntensor([[0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.]], dtype=oneflow.float32)\n\n\n\n"
    },
    {
        "api_name": "oneflow.normal",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.normal.html",
        "api_signature": "oneflow.normal(mean, std, *, generator=None, out=None)",
        "api_description": "",
        "return_value": "whose mean and standard deviation are given.\nThe mean is a tensor with the mean of\neach output element’s normal distribution\nThe std is a tensor with the standard deviation of\neach output element’s normal distribution\nThe shapes of mean and std don’t need to match, but the\ntotal number of elements in each tensor need to be the same.\n\nThe output tensor, with random normal values.\n\n\n",
        "parameters": "\nmean (Tensor) – the tensor of per-element means\nstd (Tensor) – the tensor of per-element standard deviations\n\n\nKeyword Arguments\n\ngenerator (Generator, optional) – Random number generator. Defaults to oneflow::DefaultGenerator if not provided.\nout (Tensor, optional) – Output tensor, will be resized and filled with the result. If not provided, a new tensor is created.\n\n\n\nExample:\n>>> import oneflow as flow\n>>> generator = flow.Generator()\n>>> generator.manual_seed(0) \n<oneflow._oneflow_internal.Generator object at ...>\n>>> z = flow.normal(mean=flow.arange(1., 11.), std=flow.arange(1, 0, -0.1), generator=generator)\n>>> z[:5]\ntensor([3.2122, 3.0468, 3.6192, 4.3387, 5.6261], dtype=oneflow.float32)\n\n\nnormal(mean=0.0, std, *, generator=None, out=None) -> Tensor.\nSimilar to the function above, but the means are shared among all drawn elements.\n\n\nmean (float, optional) – the mean for all distributions\nstd (Tensor) – the tensor of per-element standard deviations\n\n\nKeyword Arguments\n\ngenerator (Generator, optional) – Random number generator. Defaults to oneflow::DefaultGenerator if not provided.\nout (Tensor, optional) – Output tensor, will be resized and filled with the result. If not provided, a new tensor is created.\n\n\n\nExample:\n>>> import oneflow as flow\n>>> flow.normal(mean=0.5, std=flow.arange(1., 6.)).shape\noneflow.Size([5])\n\n\nnormal(mean, std=1.0, *, generator=None, out=None) -> Tensor\nSimilar to the function above, but the standard deviations are shared among all drawn elements.\n\n\nmean (Tensor) – the tensor of per-element means\nstd (float, optional) – the standard deviation\n\n\nKeyword Arguments\n\ngenerator (Generator, optional) – Random number generator. Defaults to oneflow::DefaultGenerator if not provided.\nout (Tensor) – The output tensor\n\n\n\nmean (float) – the mean for all distributions\nstd (float) – the standard deviation for all distributions\nsize (int...) – a sequence of integers defining the shape of the output tensor.\n\n\nKeyword Arguments\n\nout (Tensor, optional) – the output tensor.\nplacement (flow.placement, optional) – The desired device of returned global tensor. If None, will\nconstruct local tensor.\nsbp (flow.sbp, optional) – The desired sbp of returned global tensor. It must be equal with the\nnumbers of placement.\ngenerator (oneflow.Generator, optional) – a pseudorandom number generator for sampling\ndtype (oneflow.dtype, optional) – the desired data type of returned tensor.\nDefault: oneflow.float32.\ndevice – the desired device of returned tensor. Default: cpu.\nrequires_grad (bool, optional) – If autograd should record operations on the returned tensor. Default: False.\n\n\n\nExample:\n>>> import oneflow as flow\n>>> generator = flow.Generator()\n>>> generator.manual_seed(0) \n<oneflow._oneflow_internal.Generator object at ...>\n>>> y = flow.normal(0, 1, 5, generator=generator)\n>>> y\ntensor([2.2122, 1.1631, 0.7740, 0.4838, 1.0434], dtype=oneflow.float32)\n\n\n\n",
        "input_shape": "",
        "notes": "Infers the output shape from input arrays mean and std.\nThe output shape will have a dimensionality equal to the max of mean and std.\nDimensions with size 1 in either mean or std are expanded to match the other.",
        "code_example": ""
    },
    {
        "api_name": "oneflow.rand",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.rand.html",
        "api_signature": "oneflow.rand(*size, *, dtype=None, generator=None, device=None, placement=None, sbp=None, requires_grad=False)",
        "api_description": "",
        "return_value": "The shape of the tensor is defined by the variable argument size.\n\n",
        "parameters": "\nsize (int... or oneflow.Size) – Defining the shape of the output tensor.\nCan be a variable number of arguments or a collection like a list or tuple or oneflow.Size.\ndtype (flow.dtype, optional) – The desired data type of returned tensor. Default: flow.float32.\ngenerator (flow.Generator, optional) – a pseudorandom number generator for sampling\ndevice (flow.device, optional) – The desired device of returned local tensor. If None, uses the\ncurrent device.\nplacement (flow.placement, optional) – The desired device of returned global tensor. If None, will\nconstruct local tensor.\nsbp (flow.sbp, optional) – The desired sbp of returned global tensor. It must be equal with the\nnumbers of placement.\nrequires_grad (bool, optional) – If autograd should record operations on the returned tensor. Default: False.\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> x = flow.rand(3,3) # construct local tensor\n>>> x.shape\noneflow.Size([3, 3])\n>>> x.is_global\nFalse\n>>> placement = flow.placement(\"cpu\", ranks=[0])\n>>> sbp = flow.sbp.broadcast\n>>> x = flow.rand(3, 3, placement=placement, sbp=sbp) # construct global tensor\n>>> x.is_global\nTrue\n\n\n\n"
    },
    {
        "api_name": "oneflow.randint",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.randint.html",
        "api_signature": "oneflow.randint(low=0, high, size, *, dtype=None, generator=None, device=None, placement=None, sbp=None, requires_grad=False)",
        "api_description": "",
        "return_value": "The shape of the tensor is defined by the variable argument size.\nThe interface is consistent with PyTorch.\nThe documentation is referenced from: https://pytorch.org/docs/1.10/generated/torch.randint.html.\n\n",
        "parameters": "\nlow (int, optional) – Lowest integer to be drawn from the distribution. Default: 0.\nhigh (int) – One above the highest integer to be drawn from the distribution.\nsize (tuple or oneflow.Size) – Defining the shape of the output tensor.\nCan be a variable number of arguments or a collection like a list or tuple or oneflow.Size.\n\n\nKeyword Arguments\n\ndtype (oneflow.dtype, optional) – The desired data type of returned tensor. Default: flow.int64.\ngenerator (oneflow.Generator, optional) – \ndevice (oneflow.device, optional) – The desired device of returned local tensor. If None, uses the\ncurrent device.\nplacement (oneflow.placement, optional) – The desired device of returned global tensor. If None, will\nconstruct local tensor.\nsbp (oneflow.sbp, optional) – The desired sbp of returned global tensor. It must be equal with the\nnumbers of placement.\nrequires_grad (bool, optional) – If autograd should record operations on the returned tensor. Default: False.\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> generator = flow.Generator()\n>>> generator.manual_seed(0) \n<oneflow._oneflow_internal.Generator object at ...>\n>>> y = flow.randint(0, 5, (3,3), generator=generator) # construct local tensor\n>>> y\ntensor([[2, 2, 3],\n        [4, 3, 4],\n        [2, 4, 2]], dtype=oneflow.int64)\n>>> y.is_global\nFalse\n>>> placement = flow.placement(\"cpu\", ranks=[0])\n>>> y = flow.randint(0, 5, (3,3), generator=generator, placement=placement, sbp=flow.sbp.broadcast) # construct global tensor\n>>> y.is_global\nTrue\n\n\n\n"
    },
    {
        "api_name": "oneflow.randn",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.randn.html",
        "api_signature": "oneflow.randn(*size, *, dtype=None, generator=None, device=None, placement=None, sbp=None, requires_grad=False)",
        "api_description": "",
        "return_value": "The shape of the tensor is defined by the variable argument size.\n\n",
        "parameters": "\nsize (int... or oneflow.Size) – Defining the shape of the output tensor.\nCan be a variable number of arguments or a collection like a list or tuple or oneflow.Size.\ndtype (flow.dtype, optional) – The desired data type of returned tensor. Default: flow.float32.\ngenerator (flow.Generator, optional) – a pseudorandom number generator for sampling\ndevice (flow.device, optional) – The desired device of returned local tensor. If None, uses the\ncurrent device.\nplacement (flow.placement, optional) – The desired device of returned global tensor. If None, will\nconstruct local tensor.\nsbp (flow.sbp, optional) – The desired sbp of returned global tensor. It must be equal with the\nnumbers of placement.\nrequires_grad (bool, optional) – If autograd should record operations on the returned tensor. Default: False.\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> x = flow.randn(3,3) # construct local tensor\n>>> x.shape\noneflow.Size([3, 3])\n>>> x.is_global\nFalse\n>>> placement = flow.placement(\"cpu\", ranks=[0])\n>>> sbp = flow.sbp.broadcast\n>>> x = flow.randn(3,3,placement=placement,sbp=sbp) # construct global tensor\n>>> x.is_global\nTrue\n\n\n\n"
    },
    {
        "api_name": "oneflow.randperm",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.randperm.html",
        "api_signature": "oneflow.randperm(n, *, generator=None, dtype=torch.int64, device=None, placement=None, sbp=None, requires_grad=False)",
        "api_description": "",
        "return_value": "\n",
        "parameters": "n (int) – the upper bound (exclusive)\n\nKeyword Arguments\n\ngenerator (oneflow.Generator, optional) – a pseudorandom number generator for sampling\ndtype (oneflow.dtype, optional) – the desired data type of returned tensor.\nDefault: oneflow.int64.\ndevice – the desired device of returned tensor. Default: cpu.\nplacement – (flow.placement, optional): The desired device of returned global tensor. If None,\nwill construct local tensor.\nsbp – (flow.sbp, optional): The desired sbp of returned global tensor. It must be equal with the\nnumbers of placement.\nrequires_grad (bool, optional) – If autograd should record operations on the returned tensor. Default: False.\n\n\n\nExample:\n>>> import oneflow as flow\n>>> generator = flow.Generator()\n>>> generator.manual_seed(0) \n<oneflow._oneflow_internal.Generator object at ...>\n>>> y = flow.randperm(5, generator=generator) # construct local tensor\n>>> y\ntensor([2, 4, 3, 0, 1], dtype=oneflow.int64)\n>>> y.is_global\nFalse\n>>> placement = flow.placement(\"cpu\", ranks=[0])\n>>> y = flow.randperm(5, generator=generator, placement=placement, sbp=flow.sbp.broadcast) # construct global tensor\n>>> y.is_global\nTrue\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.multinomial",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.multinomial.html",
        "api_signature": "oneflow.multinomial(input, num_samples, replacement=False, generator=None)",
        "api_description": "",
        "return_value": "from the multinomial probability distribution located in the corresponding row\nof tensor input.\n\n",
        "parameters": "\ninput (Tensor) – the input tensor containing probabilities\nnum_samples (int) – number of samples to draw\nreplacement (bool, optional) – whether to draw with replacement or not\n\n\n\n",
        "input_shape": "",
        "notes": "The rows of input do not need to sum to one (in which case we use\nthe values as weights), but must be non-negative, finite and have\na non-zero sum.\nIndices are ordered from left to right according to when each was sampled\n(first samples are placed in first column).\nIf input is a vector, out is a vector of size num_samples.\nIf input is a matrix with m rows, out is an matrix of shape\n\\((m x num\\_samples)\\).\nIf replacement is True, samples are drawn with replacement.\nIf not, they are drawn without replacement, which means that when a\nsample index is drawn for a row, it cannot be drawn again for that row.\nWhen drawn without replacement, num_samples must be lower than\nnumber of non-zero elements in input (or the min number of non-zero\nelements in each row of input if it is a matrix).",
        "code_example": ">>> import oneflow as flow\n>>> gen = flow.manual_seed(0)\n>>> weights = flow.tensor([0, 10, 3, 0], dtype=flow.float) # create a tensor of weights\n>>> flow.multinomial(weights, 2)\ntensor([1, 2], dtype=oneflow.int64)\n>>> flow.multinomial(weights, 4, replacement=True)\ntensor([1, 2, 1, 1], dtype=oneflow.int64)\n\n\n\n"
    },
    {
        "api_name": "oneflow.save",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.save.html",
        "api_signature": "oneflow.save(obj: Any, path_or_buffer: Union[os.PathLike, BinaryIO, IO[bytes], pathlib.Path], global_dst_rank: Optional[int] = None, save_as_external_data: bool = False)",
        "api_description": "Save an object to a directory.",
        "return_value": "",
        "parameters": "\nobj – The object to be saved\npath_or_buffer – a file-like object (has to implement write and flush) or a string or\nos.PathLike object containing a file name\nglobal_dst_rank (int, optional) – The destination rank for\nsaving global tensors. When specified, whole tensors\nwill be saved by the process whose rank ==\nglobal_src_rank, while other processes will not do any\ndisk I/O.\nsave_as_external_data (bool) – useful only if path_or_buffer is a string or\nos.PathLike object containing a file name\n\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.load",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.load.html",
        "api_signature": "oneflow.load(path: Union[os.PathLike, BinaryIO, IO[bytes], pathlib.Path, str], global_src_rank: Optional[int] = None, map_location: Optional[Union[Callable[[oneflow.Tensor, str], oneflow.Tensor], oneflow._oneflow_internal.device, str, oneflow._oneflow_internal.placement]] = None, *, support_pytorch_format: bool = True)",
        "api_description": "Loads an object saved with oneflow.save() from a directory.",
        "return_value": "The loaded object\n\n\n\n",
        "parameters": "\npath – a file-like object (has to implement read(), readline(), tell(), and seek()),\nor a string or os.PathLike object containing a file name\nglobal_src_rank (int, optional) – The source rank for\nloading global tensors. When specified, only the\nprocess whose rank == global_src_rank will really\nread the files in path, and tensors in the loaded\nobject will be consistent with placement =\nflow.placement(‘cuda’, [global_src_rank])\nmap_location (str, flow.device or flow.placement, callable, optional) – indicates the location where all tensors should be loaded.\nsupport_pytorch_format (bool, optional) – whether to support\nloading the file saved by torch.save. Default: True\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.set_num_threads",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.set_num_threads.html",
        "api_signature": "oneflow.set_num_threads()",
        "api_description": "Sets the number of threads used for intraop parallelism on CPU.\nWarning\nTo ensure that the correct number of threads is used,\nset_num_threads must be called before running eager, eager globe or ddp.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.no_grad",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.no_grad.html",
        "api_signature": null,
        "api_description": "Context-manager that disabled gradient calculation.\nDisabling gradient calculation is useful for inference, when you are sure that\nyou will not call Tensor.backward(). It will reduce memory consumption for computations\nthat would otherwise have requires_grad=True.\nIn this mode, the result of every computation will have requires_grad=False, even when\nthe inputs have requires_grad=True.\nThis context manager is thread local; it will not affect computation in other threads.\nAlso functions as a decorator. (Make sure to instantiate with parenthesis.)\n>>> import oneflow as flow\n>>> x = flow.ones(2, 3, requires_grad=True)\n>>> with flow.no_grad():\n...     y = x * x\n>>> y.requires_grad\nFalse\n>>> @flow.no_grad()\n... def no_grad_func(x):\n...     return x * x\n>>> y = no_grad_func(x)\n>>> y.requires_grad\nFalse\n__init__()¶\nInitialize self.  See help(type(self)) for accurate signature.\nMethods\n__call__(func)\nCall self as a function.\n__delattr__(name, /)\nImplement delattr(self, name).\n__dir__()\nDefault dir() implementation.\n__enter__()\n__eq__(value, /)\nReturn self==value.\n__exit__(exc_type, exc_val, exc_tb)\n__format__(format_spec, /)\nDefault object formatter.\n__ge__(value, /)\nReturn self>=value.\n__getattribute__(name, /)\nReturn getattr(self, name).\n__gt__(value, /)\nReturn self>value.\n__hash__()\nReturn hash(self).\n__init__()\nInitialize self.\n__init_subclass__\nThis method is called when a class is subclassed.\n__le__(value, /)\nReturn self<=value.\n__lt__(value, /)\nReturn self<value.\n__ne__(value, /)\nReturn self!=value.\n__new__(**kwargs)\nCreate and return a new object.\n__reduce__()\nHelper for pickle.\n__reduce_ex__(protocol, /)\nHelper for pickle.\n__repr__()\nReturn repr(self).\n__setattr__(name, value, /)\nImplement setattr(self, name, value).\n__sizeof__()\nSize of object in memory, in bytes.\n__str__()\nReturn str(self).\n__subclasshook__\nAbstract classes can override this to customize issubclass().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.set_grad_enabled",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.set_grad_enabled.html",
        "api_signature": "oneflow.set_grad_enabled(is_train=True)",
        "api_description": "Context-manager that enabled gradient calculation.\nEnables gradient calculation, if it has been disabled via no_grad.\nThis context manager is thread local; it will not affect computation in other threads.\nAlso functions as a decorator. (Make sure to instantiate with parenthesis.)",
        "return_value": "",
        "parameters": "mode (bool) – Flag whether to enable or disable gradient calculation. (default: True)\n\n\n>>> import oneflow as flow\n>>> x = flow.ones(2, 3, requires_grad=True)\n>>> with flow.set_grad_enabled(True):\n...     y = x * x\n>>> y.requires_grad\nTrue\n>>> @flow.set_grad_enabled(False)\n... def no_grad_func(x):\n...     return x * x\n>>> y = no_grad_func(x)\n>>> y.requires_grad\nFalse\n\n\n\n\n__init__(is_train=True)¶\nInitialize self.  See help(type(self)) for accurate signature.\n\nMethods\n\n\n\n\n\n\n__call__(func)\nCall self as a function.\n\n__delattr__(name, /)\nImplement delattr(self, name).\n\n__dir__()\nDefault dir() implementation.\n\n__enter__()\n\n\n__eq__(value, /)\nReturn self==value.\n\n__exit__(exc_type, exc_val, exc_tb)\n\n\n__format__(format_spec, /)\nDefault object formatter.\n\n__ge__(value, /)\nReturn self>=value.\n\n__getattribute__(name, /)\nReturn getattr(self, name).\n\n__gt__(value, /)\nReturn self>value.\n\n__hash__()\nReturn hash(self).\n\n__init__([is_train])\nInitialize self.\n\n__init_subclass__\nThis method is called when a class is subclassed.\n\n__le__(value, /)\nReturn self<=value.\n\n__lt__(value, /)\nReturn self<value.\n\n__ne__(value, /)\nReturn self!=value.\n\n__new__(**kwargs)\nCreate and return a new object.\n\n__reduce__()\nHelper for pickle.\n\n__reduce_ex__(protocol, /)\nHelper for pickle.\n\n__repr__()\nReturn repr(self).\n\n__setattr__(name, value, /)\nImplement setattr(self, name, value).\n\n__sizeof__()\nSize of object in memory, in bytes.\n\n__str__()\nReturn str(self).\n\n__subclasshook__\nAbstract classes can override this to customize issubclass().\n\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.enable_grad",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.enable_grad.html",
        "api_signature": null,
        "api_description": "Context-manager that enabled gradient calculation.\nEnables gradient calculation, if it has been disabled via no_grad.\nThis context manager is thread local; it will not affect computation in other threads.\nAlso functions as a decorator. (Make sure to instantiate with parenthesis.)\n>>> import oneflow as flow\n>>> x = flow.ones(2, 3, requires_grad=True)\n>>> with flow.no_grad():\n...     with flow.enable_grad():\n...         y = x * x\n>>> y.requires_grad\nTrue\n>>> @flow.enable_grad()\n... def no_grad_func(x):\n...     return x * x\n>>> with flow.no_grad():\n...     y = no_grad_func(x)\n>>> y.requires_grad\nTrue\n__init__()¶\nInitialize self.  See help(type(self)) for accurate signature.\nMethods\n__call__(func)\nCall self as a function.\n__delattr__(name, /)\nImplement delattr(self, name).\n__dir__()\nDefault dir() implementation.\n__enter__()\n__eq__(value, /)\nReturn self==value.\n__exit__(exc_type, exc_val, exc_tb)\n__format__(format_spec, /)\nDefault object formatter.\n__ge__(value, /)\nReturn self>=value.\n__getattribute__(name, /)\nReturn getattr(self, name).\n__gt__(value, /)\nReturn self>value.\n__hash__()\nReturn hash(self).\n__init__()\nInitialize self.\n__init_subclass__\nThis method is called when a class is subclassed.\n__le__(value, /)\nReturn self<=value.\n__lt__(value, /)\nReturn self<value.\n__ne__(value, /)\nReturn self!=value.\n__new__(**kwargs)\nCreate and return a new object.\n__reduce__()\nHelper for pickle.\n__reduce_ex__(protocol, /)\nHelper for pickle.\n__repr__()\nReturn repr(self).\n__setattr__(name, value, /)\nImplement setattr(self, name, value).\n__sizeof__()\nSize of object in memory, in bytes.\n__str__()\nReturn str(self).\n__subclasshook__\nAbstract classes can override this to customize issubclass().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.is_grad_enabled",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.is_grad_enabled.html",
        "api_signature": "oneflow.is_grad_enabled()",
        "api_description": "",
        "return_value": "\n",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.inference_mode",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.inference_mode.html",
        "api_signature": "oneflow.inference_mode(mode=True)",
        "api_description": "Context-manager that enables or disables inference mode\nInferenceMode is a new context manager analogous to no_grad to be used when you arecertain\nyour operations will have no interactions with autograd (e.g., model training). Code run\nunder this mode gets better performance by disabling view tracking and version counter bumps.\nThis context manager is thread local; it will not affect computation in other threads.\nAlso functions as a decorator. (Make sure to instantiate with parenthesis.)",
        "return_value": "",
        "parameters": "mode (bool) – Flag whether to enable or disable inference mode. (default: True)\n\n\n>>> import oneflow as flow\n>>> x = flow.ones(2, 3, requires_grad=True)\n>>> with flow.inference_mode():\n...     y = x * x\n>>> y.requires_grad\nFalse\n>>> @flow.inference_mode()\n... def no_grad_func(x):\n...     return x * x\n>>> y = no_grad_func(x)\n>>> y.requires_grad\nFalse\n\n\n\n\n__init__(mode=True)¶\nInitialize self.  See help(type(self)) for accurate signature.\n\nMethods\n\n\n\n\n\n\n__call__(func)\nCall self as a function.\n\n__delattr__(name, /)\nImplement delattr(self, name).\n\n__dir__()\nDefault dir() implementation.\n\n__enter__()\n\n\n__eq__(value, /)\nReturn self==value.\n\n__exit__(exc_type, exc_val, exc_tb)\n\n\n__format__(format_spec, /)\nDefault object formatter.\n\n__ge__(value, /)\nReturn self>=value.\n\n__getattribute__(name, /)\nReturn getattr(self, name).\n\n__gt__(value, /)\nReturn self>value.\n\n__hash__()\nReturn hash(self).\n\n__init__([mode])\nInitialize self.\n\n__init_subclass__\nThis method is called when a class is subclassed.\n\n__le__(value, /)\nReturn self<=value.\n\n__lt__(value, /)\nReturn self<value.\n\n__ne__(value, /)\nReturn self!=value.\n\n__new__(**kwargs)\nCreate and return a new object.\n\n__reduce__()\nHelper for pickle.\n\n__reduce_ex__(protocol, /)\nHelper for pickle.\n\n__repr__()\nReturn repr(self).\n\n__setattr__(name, value, /)\nImplement setattr(self, name, value).\n\n__sizeof__()\nSize of object in memory, in bytes.\n\n__str__()\nReturn str(self).\n\n__subclasshook__\nAbstract classes can override this to customize issubclass().\n\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.abs",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.abs.html",
        "api_signature": "oneflow.abs()",
        "api_description": "Return the absolute value of each element in input tensor:math:y = |x| element-wise.",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> import numpy as np\n\n>>> x = flow.tensor(np.array([-1, 2, -3, 4]).astype(np.float32))\n>>> flow.abs(x)\ntensor([1., 2., 3., 4.], dtype=oneflow.float32)\n\n\n\n"
    },
    {
        "api_name": "oneflow.acos",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.acos.html",
        "api_signature": "oneflow.acos()",
        "api_description": "",
        "return_value": "\n\\[\\text{out}_{i} = \\arccos(\\text{input}_{i})\\]\n\n",
        "parameters": "input (Tensor) – the input tensor.\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> import numpy as np\n\n>>> arr = np.array([0.5, 0.6, 0.7])\n>>> input = flow.tensor(arr, dtype=flow.float32)\n>>> output = flow.acos(input)\n>>> output\ntensor([1.0472, 0.9273, 0.7954], dtype=oneflow.float32)\n\n\n\n"
    },
    {
        "api_name": "oneflow.acosh",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.acosh.html",
        "api_signature": "oneflow.acosh()",
        "api_description": "",
        "return_value": "\n\\[\\text{out}_{i} = \\cosh^{-1}(\\text{input}_{i})\\]\n\n",
        "parameters": "input (Tensor) – the input tensor.\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> import numpy as np\n>>> x1 = flow.tensor(np.array([2, 3, 4]).astype(np.float32))\n>>> out1 = flow.acosh(x1)\n>>> out1\ntensor([1.3170, 1.7627, 2.0634], dtype=oneflow.float32)\n>>> x2 = flow.tensor(np.array([1.5, 2.6, 3.7]).astype(np.float32),device=flow.device('cuda'))\n>>> out2 = flow.acosh(x2)\n>>> out2\ntensor([0.9624, 1.6094, 1.9827], device='cuda:0', dtype=oneflow.float32)\n\n\n\n"
    },
    {
        "api_name": "oneflow.arccos",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.arccos.html",
        "api_signature": "oneflow.arccos()",
        "api_description": "",
        "return_value": "\n\\[\\text{out}_{i} = \\arccos(\\text{input}_{i})\\]\n\n",
        "parameters": "input (Tensor) – the input tensor.\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> import numpy as np\n\n>>> arr = np.array([0.5, 0.6, 0.7])\n>>> input = flow.tensor(arr, dtype=flow.float32)\n>>> output = flow.acos(input)\n>>> output\ntensor([1.0472, 0.9273, 0.7954], dtype=oneflow.float32)\n\n\n\n"
    },
    {
        "api_name": "oneflow.arccosh",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.arccosh.html",
        "api_signature": "oneflow.arccosh()",
        "api_description": "",
        "return_value": "\n\\[\\text{out}_{i} = \\cosh^{-1}(\\text{input}_{i})\\]\n\n",
        "parameters": "input (Tensor) – the input tensor.\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> import numpy as np\n>>> x1 = flow.tensor(np.array([2, 3, 4]).astype(np.float32))\n>>> out1 = flow.acosh(x1)\n>>> out1\ntensor([1.3170, 1.7627, 2.0634], dtype=oneflow.float32)\n>>> x2 = flow.tensor(np.array([1.5, 2.6, 3.7]).astype(np.float32),device=flow.device('cuda'))\n>>> out2 = flow.acosh(x2)\n>>> out2\ntensor([0.9624, 1.6094, 1.9827], device='cuda:0', dtype=oneflow.float32)\n\n\n\n"
    },
    {
        "api_name": "oneflow.add",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.add.html",
        "api_signature": "oneflow.add(input, other, *, alpha=1)",
        "api_description": "Adds other, scaled by alpha, to input. Scalar and broadcast promotation are supported.\n\\[out = input + alpha \\times other\\]",
        "return_value": "the output Tensor.\n\n",
        "parameters": "\ninput (Union[int, float, oneflow.Tensor]) – the input tensor.\nother (Union[int, float, oneflow.Tensor]) – the tensor or number to add to input.\n\n\nKeyword Arguments\nalpha (Number, optional) – the multiplier for other.\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import numpy as np\n>>> import oneflow as flow\n\n# element-wise add\n>>> x = flow.tensor(np.random.randn(2,3), dtype=flow.float32)\n>>> y = flow.tensor(np.random.randn(2,3), dtype=flow.float32)\n>>> out = flow.add(x, y).numpy()\n>>> out.shape\n(2, 3)\n\n# scalar add\n>>> x = 5\n>>> y = flow.tensor(np.random.randn(2,3), dtype=flow.float32)\n>>> out = flow.add(x, y).numpy()\n>>> out.shape\n(2, 3)\n\n# broadcast add\n>>> x = flow.tensor(np.random.randn(1,1), dtype=flow.float32)\n>>> y = flow.tensor(np.random.randn(2,3), dtype=flow.float32)\n>>> out = flow.add(x, y).numpy()\n>>> out.shape\n(2, 3)\n\n# use alpha\n>>> x = flow.zeros(2, 3)\n>>> y = flow.ones(2, 3)\n>>> out = flow.add(x, y, alpha=10)\n>>> out\ntensor([[10., 10., 10.],\n        [10., 10., 10.]], dtype=oneflow.float32)\n\n\n\n"
    },
    {
        "api_name": "oneflow.addcdiv",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.addcdiv.html",
        "api_signature": "oneflow.addcdiv(input, tensor1, tensor2, *, value=1)",
        "api_description": "Performs the element-wise division of tensor1 by tensor2,\nmultiply the result by the scalar value and add it to input.\n\\[\\text{out}_i = \\text{input}_i + \\text{value} \\times \\frac{\\text{tensor1}_i}{\\text{tensor2}_i}\\]\nThe shapes of input, tensor1, and tensor2 must be\nbroadcastable.\nFor inputs of type FloatTensor or DoubleTensor, value must be\na real number, otherwise an integer.",
        "return_value": "",
        "parameters": "\ninput (Tensor) – the tensor to be added\ntensor1 (Tensor) – the numerator tensor\ntensor2 (Tensor) – the denominator tensor\n\n\nKeyword Arguments\nvalue (Number, optional) – multiplier for \\(\\text{{tensor1}} / \\text{{tensor2}}\\)\n\n\nExample:\n>>> import oneflow as flow\n>>> input = flow.tensor([ 0.3810,  1.2774, -0.2972, -0.3719])\n>>> tensor1 = flow.tensor([0.8032,  0.2930, -0.8113, -0.2308])\n>>> tensor2 = flow.tensor([[0.5], [1]])\n>>> output = flow.addcdiv(input, tensor1, tensor2)\n>>> output.shape\noneflow.Size([2, 4])\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.addcmul",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.addcmul.html",
        "api_signature": "oneflow.addcmul(input, tensor1, tensor2, *, value=1)",
        "api_description": "Performs the element-wise multiplication of tensor1 by tensor2, multiply the result\nby the scalar value and add it to input.\nThe documentation is referenced from:\n\\[\\text{out}_i = \\text{input}_i + value \\times\\  \\text{tensor1}_i \\times\\ \\text{tensor2}_i\\]",
        "return_value": "the output Tensor.\n\n",
        "parameters": "\ninput (Tensor) – the tensor to be added.\ntensor1 (Tensor) – the tensor to be multiplied.\ntensor2 (Tensor) – the tensor to be multiplied.\n\n\nKeyword Arguments\nvalue (Number, optional) – multiplier for \\(tensor1 * tensor2\\).\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n\n>>> input = flow.rand(2, 3, 4)\n>>> tensor1 = flow.rand(2, 3, 4)\n>>> tensor2 = flow.rand(2, 3, 4)\n>>> out = flow.addcmul(input, tensor1, tensor2, value=2)\n>>> out.size()\noneflow.Size([2, 3, 4])\n\n\n\n"
    },
    {
        "api_name": "oneflow.asin",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.asin.html",
        "api_signature": "oneflow.asin()",
        "api_description": "",
        "return_value": "\n\\[\\text{out}_{i} = \\sin^{-1}(\\text{input}_{i})\\]\n\n",
        "parameters": "input (Tensor) – the input tensor.\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> import numpy as np\n>>> input = flow.tensor(np.array([-0.5,  0.8, 1.0,  -0.8]), dtype=flow.float32)\n>>> output = flow.asin(input)\n>>> output.shape\noneflow.Size([4])\n>>> output\ntensor([-0.5236,  0.9273,  1.5708, -0.9273], dtype=oneflow.float32)\n>>> input1 = flow.tensor(np.array([[0.8, 1.0], [-0.6, -1.0]]), dtype=flow.float32)\n>>> output1 = input1.asin()\n>>> output1.shape\noneflow.Size([2, 2])\n>>> output1\ntensor([[ 0.9273,  1.5708],\n        [-0.6435, -1.5708]], dtype=oneflow.float32)\n\n\n\n"
    },
    {
        "api_name": "oneflow.asinh",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.asinh.html",
        "api_signature": "oneflow.asinh()",
        "api_description": "",
        "return_value": "\n\\[\\text{out}_{i} = \\sinh^{-1}(\\text{input}_{i})\\]\n\n",
        "parameters": "input (Tensor) – the input tensor.\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> import numpy as np\n>>> input = flow.tensor(np.array([2, 3, 4]), dtype=flow.float32)\n>>> output = flow.asinh(input)\n>>> output.shape\noneflow.Size([3])\n>>> output\ntensor([1.4436, 1.8184, 2.0947], dtype=oneflow.float32)\n\n>>> input1 = flow.tensor(np.array([[-1, 0, -0.4], [5, 7, 0.8]]), dtype=flow.float32)\n>>> output1 = input1.asinh()\n>>> output1.shape\noneflow.Size([2, 3])\n>>> output1\ntensor([[-0.8814,  0.0000, -0.3900],\n        [ 2.3124,  2.6441,  0.7327]], dtype=oneflow.float32)\n\n\n\n"
    },
    {
        "api_name": "oneflow.arcsin",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.arcsin.html",
        "api_signature": "oneflow.arcsin()",
        "api_description": "",
        "return_value": "\n\\[\\text{out}_{i} = \\sin^{-1}(\\text{input}_{i})\\]\n\n",
        "parameters": "input (Tensor) – the input tensor.\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> import numpy as np\n>>> input = flow.tensor(np.array([-0.5,  0.8, 1.0,  -0.8]), dtype=flow.float32)\n>>> output = flow.asin(input)\n>>> output.shape\noneflow.Size([4])\n>>> output\ntensor([-0.5236,  0.9273,  1.5708, -0.9273], dtype=oneflow.float32)\n>>> input1 = flow.tensor(np.array([[0.8, 1.0], [-0.6, -1.0]]), dtype=flow.float32)\n>>> output1 = input1.asin()\n>>> output1.shape\noneflow.Size([2, 2])\n>>> output1\ntensor([[ 0.9273,  1.5708],\n        [-0.6435, -1.5708]], dtype=oneflow.float32)\n\n\n\n"
    },
    {
        "api_name": "oneflow.arcsinh",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.arcsinh.html",
        "api_signature": "oneflow.arcsinh()",
        "api_description": "",
        "return_value": "\n\\[\\text{out}_{i} = \\sinh^{-1}(\\text{input}_{i})\\]\n\n",
        "parameters": "input (Tensor) – the input tensor.\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> import numpy as np\n>>> input = flow.tensor(np.array([2, 3, 4]), dtype=flow.float32)\n>>> output = flow.asinh(input)\n>>> output.shape\noneflow.Size([3])\n>>> output\ntensor([1.4436, 1.8184, 2.0947], dtype=oneflow.float32)\n\n>>> input1 = flow.tensor(np.array([[-1, 0, -0.4], [5, 7, 0.8]]), dtype=flow.float32)\n>>> output1 = input1.asinh()\n>>> output1.shape\noneflow.Size([2, 3])\n>>> output1\ntensor([[-0.8814,  0.0000, -0.3900],\n        [ 2.3124,  2.6441,  0.7327]], dtype=oneflow.float32)\n\n\n\n"
    },
    {
        "api_name": "oneflow.atan",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.atan.html",
        "api_signature": "oneflow.atan()",
        "api_description": "",
        "return_value": "\n\\[\\text{out}_{i} = \\tan^{-1}(\\text{input}_{i})\\]\n\n",
        "parameters": "input (Tensor) – the input tensor.\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> import numpy as np\n>>> input = flow.tensor(np.array([0.5, 0.6, 0.7]), dtype=flow.float32)\n>>> output = flow.atan(input)\n>>> output.shape\noneflow.Size([3])\n\n\n\n"
    },
    {
        "api_name": "oneflow.atanh",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.atanh.html",
        "api_signature": "oneflow.atanh()",
        "api_description": "",
        "return_value": "\n\\[\\text{out}_{i} = \\tanh^{-1}(\\text{input}_{i})\\]\n\n",
        "parameters": "input (Tensor) – the input tensor.\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> import numpy as np\n>>> np_arr = np.array([0.5, 0.6, 0.7]).astype(np.float32)\n>>> input = flow.tensor(np_arr, dtype=flow.float32)\n>>> output = flow.atanh(input)\n>>> output\ntensor([0.5493, 0.6931, 0.8673], dtype=oneflow.float32)\n\n\n\n"
    },
    {
        "api_name": "oneflow.arctan",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.arctan.html",
        "api_signature": "oneflow.arctan()",
        "api_description": "",
        "return_value": "\n\\[\\text{out}_{i} = \\tan^{-1}(\\text{input}_{i})\\]\n\n",
        "parameters": "input (Tensor) – the input tensor.\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> import numpy as np\n>>> input = flow.tensor(np.array([0.5, 0.6, 0.7]), dtype=flow.float32)\n>>> output = flow.atan(input)\n>>> output.shape\noneflow.Size([3])\n\n\n\n"
    },
    {
        "api_name": "oneflow.arctanh",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.arctanh.html",
        "api_signature": "oneflow.arctanh()",
        "api_description": "",
        "return_value": "\n\\[\\text{out}_{i} = \\tanh^{-1}(\\text{input}_{i})\\]\n\n",
        "parameters": "input (Tensor) – the input tensor.\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> import numpy as np\n>>> np_arr = np.array([0.5, 0.6, 0.7]).astype(np.float32)\n>>> input = flow.tensor(np_arr, dtype=flow.float32)\n>>> output = flow.atanh(input)\n>>> output\ntensor([0.5493, 0.6931, 0.8673], dtype=oneflow.float32)\n\n\n\n"
    },
    {
        "api_name": "oneflow.atan2",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.atan2.html",
        "api_signature": "oneflow.atan2()",
        "api_description": "Element-wise arctangent of input{i}/other{i}\nwith consideration of the quadrant. Returns a new tensor with the signed\nangles in radians between vector (other{i},input{i}) and vector (1, 0).\nThe shapes of input and other must be broadcastable.",
        "return_value": "",
        "parameters": "\ninput (Tensor) – the first input tensor.\nother (Tensor) – the second input tensor.\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> import numpy as np\n\n>>> x1 = flow.Tensor(np.array([1,2,3]))\n>>> y1 = flow.Tensor(np.array([3,2,1]))\n>>> x2 = flow.Tensor(np.array([1.53123589,0.54242598,0.15117185]))\n>>> y2 = flow.Tensor(np.array([-0.21906378,0.09467151,-0.75562878]))\n>>> x3 = flow.Tensor(np.array([1,0,-1]))\n>>> y3 = flow.Tensor(np.array([0,1,0]))\n\n>>> flow.atan2(x1,y1).numpy()\narray([0.32175055, 0.7853982 , 1.2490457 ], dtype=float32)\n>>> flow.atan2(x2,y2).numpy()\narray([1.7128955, 1.3980033, 2.9441385], dtype=float32)\n>>> flow.atan2(x3,y3).numpy()\narray([ 1.5707964,  0.       , -1.5707964], dtype=float32)\n\n\n\n"
    },
    {
        "api_name": "oneflow.ceil",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.ceil.html",
        "api_signature": "oneflow.ceil()",
        "api_description": "",
        "return_value": "the smallest integer greater than or equal to each element.\nThe equation is:\n\n\\[\\text{out}_{i} = \\left\\lceil \\text{input}_{i} \\right\\rceil = \\left\\lfloor \\text{input}_{i} \\right\\rfloor + 1\\]\n\nThe result Tensor\n\n",
        "parameters": "input (oneflow.Tensor) – A Tensor.\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> import numpy as np\n>>> x = flow.tensor(np.array([0.1, -2, 3.4]).astype(np.float32))\n>>> y = flow.ceil(x)\n>>> y.shape\noneflow.Size([3])\n>>> y\ntensor([ 1., -2.,  4.], dtype=oneflow.float32)\n>>> x = flow.tensor(np.array([[2.5, 4.6, 0.6],[7.8, 8.3, 9.2]]).astype(np.float32))\n>>> y = x.ceil()\n>>> y.shape\noneflow.Size([2, 3])\n>>> y\ntensor([[ 3.,  5.,  1.],\n        [ 8.,  9., 10.]], dtype=oneflow.float32)\n>>> x = flow.tensor(np.array([[[2.2, 4.4, 6.5],[7.1, 8.2, 9.3]],[[10.6,11.2,12.2],[13.5,14.8,15.9]]]).astype(np.float32))\n>>> y = flow.ceil(x)\n>>> y.shape\noneflow.Size([2, 2, 3])\n>>> y\ntensor([[[ 3.,  5.,  7.],\n         [ 8.,  9., 10.]],\n\n        [[11., 12., 13.],\n         [14., 15., 16.]]], dtype=oneflow.float32)\n\n\n\n"
    },
    {
        "api_name": "oneflow.ceil_",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.ceil_.html",
        "api_signature": "oneflow.ceil_()",
        "api_description": "In-place version of oneflow.ceil()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.clamp",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.clamp.html",
        "api_signature": "oneflow.clamp()",
        "api_description": "Clamp all elements in input into the range [ min, max ] and return\na resulting tensor:\n\\[\\begin{split}y_i = \\begin{cases}\n\\text{min} & \\text{if } x_i < \\text{min} \\\\\nx_i & \\text{if } \\text{min} \\leq x_i \\leq \\text{max} \\\\\n\\text{max} & \\text{if } x_i > \\text{max}\n\\end{cases}\\end{split}\\]\nIf input is of type FloatTensor or DoubleTensor, args min\nand max must be real numbers, otherwise they should be integers.",
        "return_value": "",
        "parameters": "\ninput (Tensor) – the input tensor.\nmin (Number) – lower-bound of the range to be clamped to. Defaults to None.\nmax (Number) – upper-bound of the range to be clamped to. Defaults to None.\nout (Tensor, optional) – the output tensor.\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> import numpy as np\n>>> arr = np.array([0.2, 0.6, -1.5, -0.3])\n>>> input = flow.Tensor(arr)\n>>> output = flow.clamp(input, min=-0.5, max=0.5)\n>>> output\ntensor([ 0.2000,  0.5000, -0.5000, -0.3000], dtype=oneflow.float32)\n\n>>> arr = np.array([0.2, 0.6, -1.5, -0.3])\n>>> input = flow.Tensor(arr)\n>>> output = flow.clamp(input, min=None, max=0.5)\n>>> output\ntensor([ 0.2000,  0.5000, -1.5000, -0.3000], dtype=oneflow.float32)\n\n>>> arr = np.array([0.2, 0.6, -1.5, -0.3])\n>>> input = flow.Tensor(arr)\n>>> output = flow.clamp(input, min=-0.5, max=None)\n>>> output\ntensor([ 0.2000,  0.6000, -0.5000, -0.3000], dtype=oneflow.float32)\n\n\n\n"
    },
    {
        "api_name": "oneflow.clamp_min",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.clamp_min.html",
        "api_signature": "oneflow.clamp_min()",
        "api_description": "Clamp all elements in input which are less than min to min and return\na resulting tensor:\n\\[y_i = \\max(min, x_i)\\]\nIf input is of type FloatTensor or DoubleTensor, args min\nmust be real numbers, otherwise they should be integers.",
        "return_value": "",
        "parameters": "\ninput (Tensor) – the input tensor.\nmin (Number) – lower-bound of the range to be clamped to.\nout (Tensor, optional) – the output tensor.\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> input = flow.Tensor([0.2, 0.6, -1.5, -0.3])\n>>> output = flow.clamp_min(input, min=-0.5)\n>>> output\ntensor([ 0.2000,  0.6000, -0.5000, -0.3000], dtype=oneflow.float32)\n\n>>> input = flow.Tensor([0.2, 0.6, -1.5, -0.3])\n>>> output = flow.clamp_min(input, min=-2)\n>>> output\ntensor([ 0.2000,  0.6000, -1.5000, -0.3000], dtype=oneflow.float32)\n\n>>> input = flow.Tensor([0.2, 0.6, -1.5, -0.3])\n>>> output = flow.clamp_min(input, min=1)\n>>> output\ntensor([1., 1., 1., 1.], dtype=oneflow.float32)\n\n\n\n"
    },
    {
        "api_name": "oneflow.clamp_max",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.clamp_max.html",
        "api_signature": "oneflow.clamp_max()",
        "api_description": "Clamp all elements in input which are greater than max to max and return\na resulting tensor:\n\\[y_i = \\min(max, x_i)\\]\nIf input is of type FloatTensor or DoubleTensor, args max\nmust be real numbers, otherwise they should be integers.",
        "return_value": "",
        "parameters": "\ninput (Tensor) – the input tensor.\nmax (Number) – upper-bound of the range to be clamped to.\nout (Tensor, optional) – the output tensor.\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> input = flow.Tensor([0.2, 0.6, -1.5, -0.3])\n>>> output = flow.clamp_max(input, max=-0.5)\n>>> output\ntensor([-0.5000, -0.5000, -1.5000, -0.5000], dtype=oneflow.float32)\n\n>>> input = flow.Tensor([0.2, 0.6, -1.5, -0.3])\n>>> output = flow.clamp_max(input, max=-2)\n>>> output\ntensor([-2., -2., -2., -2.], dtype=oneflow.float32)\n\n>>> input = flow.Tensor([0.2, 0.6, -1.5, -0.3])\n>>> output = flow.clamp_max(input, max=1)\n>>> output\ntensor([ 0.2000,  0.6000, -1.5000, -0.3000], dtype=oneflow.float32)\n\n\n\n"
    },
    {
        "api_name": "oneflow.clip",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.clip.html",
        "api_signature": "oneflow.clip()",
        "api_description": "Alias for oneflow.clamp().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.cos",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.cos.html",
        "api_signature": "oneflow.cos()",
        "api_description": "",
        "return_value": "\n\\[\\text{out}_{i} = \\cos(\\text{input}_{i})\\]\n\n",
        "parameters": "input (Tensor) – the input tensor.\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> import numpy as np\n>>> arr = np.array([1.4309,  1.2706, -0.8562,  0.9796])\n>>> input = flow.tensor(arr, dtype=flow.float32)\n>>> output = flow.cos(input).numpy()\n\n\n\n"
    },
    {
        "api_name": "oneflow.cosh",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.cosh.html",
        "api_signature": "oneflow.cosh()",
        "api_description": "",
        "return_value": "\n\\[\\text{out}_{i} = \\cosh(\\text{input}_{i})\\]\n\n",
        "parameters": "input (Tensor) – the input tensor.\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import numpy as np\n>>> import oneflow as flow\n\n>>> arr = np.array([ 0.1632,  1.1835, -0.6979, -0.7325])\n>>> input = flow.tensor(arr, dtype=flow.float32)\n>>> output = flow.cosh(input).numpy()\n>>> output\narray([1.0133467, 1.7859949, 1.2535787, 1.2804903], dtype=float32)\n\n\n\n"
    },
    {
        "api_name": "oneflow.div",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.div.html",
        "api_signature": "oneflow.div(x, y, *, rounding_mode=None)",
        "api_description": "Computes the division of input by other for each element, scalar and broadcast promotation are supported.\nThe formula is:\n\\[out = \\frac{input}{other}\\]",
        "return_value": "",
        "parameters": "\ninput (Union[int, float, oneflow.Tensor]) – input.\nother (Union[int, float, oneflow.Tensor]) – other.\n\n\nKeyword Arguments\nrounding_mode (str, optional) – It can be set as \"floor\" (roudning the results down)\nor \"trunc\" (rounding the results towards zero). None for default (no rounding).\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import numpy as np\n>>> import oneflow as flow\n\n# element-wise divide\n>>> input = flow.tensor(np.random.randn(2,3), dtype=flow.float32)\n>>> other = flow.tensor(np.random.randn(2,3), dtype=flow.float32)\n>>> out = flow.div(input,other).numpy()\n>>> out.shape\n(2, 3)\n\n# scalar divide\n>>> input = 5\n>>> other = flow.tensor(np.random.randn(2,3), dtype=flow.float32)\n>>> out = flow.div(input,other).numpy()\n>>> out.shape\n(2, 3)\n\n# broadcast divide\n>>> input = flow.tensor(np.random.randn(1,1), dtype=flow.float32)\n>>> other = flow.tensor(np.random.randn(2,3), dtype=flow.float32)\n>>> out = flow.div(input,other).numpy()\n>>> out.shape\n(2, 3)\n\n# rounding_mode\n>>> x = flow.tensor([ 0.3810,  1.2774, -0.2972, -0.3719,  0.4637])\n>>> flow.div(x, 0.5)\ntensor([ 0.7620,  2.5548, -0.5944, -0.7438,  0.9274], dtype=oneflow.float32)\n>>> flow.div(x, 0.5, rounding_mode=\"floor\")\ntensor([ 0.,  2., -1., -1.,  0.], dtype=oneflow.float32)\n>>> flow.div(x, 0.5, rounding_mode=\"trunc\")\ntensor([0., 2., -0., -0., 0.], dtype=oneflow.float32)\n\n\n\n"
    },
    {
        "api_name": "oneflow.erf",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.erf.html",
        "api_signature": "oneflow.erf()",
        "api_description": "Computes the error function of each element. The error function is defined as follows:\n\\[\\operatorname{erf}(x)=\\frac{2}{\\sqrt{\\pi}} \\int_{0}^{x} e^{-t^{2}} d t\\]",
        "return_value": "The result Tensor\n\n",
        "parameters": "x (oneflow.Tensor) – A Tensor\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> import numpy as np\n\n>>> x = flow.tensor(np.array([0, -1., 10.]), dtype=flow.float32)\n>>> out = flow.erf(x)\n>>> out.shape\noneflow.Size([3])\n>>> out.numpy()\narray([ 0.       , -0.8427008,  1.       ], dtype=float32)\n\n>>> x = flow.tensor(np.array([[0, -1., 10.], [5, 7, 0.8]]), dtype=flow.float32)\n>>> out = flow.erf(x)\n>>> out.shape\noneflow.Size([2, 3])\n>>> out.numpy()\narray([[ 0.        , -0.8427008 ,  1.        ],\n       [ 1.        ,  1.        ,  0.74210095]], dtype=float32)\n\n>>> x = flow.tensor(np.array([[0, -1., 10.], [5, 7, 0.8], [2, 3, 4]]), dtype=flow.float32)\n>>> out = x.erf()\n>>> out.shape\noneflow.Size([3, 3])\n>>> out.numpy()\narray([[ 0.        , -0.8427008 ,  1.        ],\n       [ 1.        ,  1.        ,  0.74210095],\n       [ 0.9953223 ,  0.9999779 ,  1.        ]], dtype=float32)\n\n\n\n"
    },
    {
        "api_name": "oneflow.erfc",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.erfc.html",
        "api_signature": "oneflow.erfc()",
        "api_description": "Computes the complementary error function of each element of input. The complementary error\nfunction is defined as follows:\n\\[\\operatorname{erfc}(x)=1-\\frac{2}{\\sqrt{\\pi}} \\int_{0}^{x} e^{-t^{2}} d t\\]",
        "return_value": "The result Tensor\n\n",
        "parameters": "x (oneflow.Tensor) – A Tensor\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> import numpy as np\n\n>>> x = flow.tensor(np.array([0, -1., 10.]), dtype=flow.float32)\n>>> out = flow.erfc(x)\n>>> out\ntensor([1.0000e+00, 1.8427e+00, 2.8026e-45], dtype=oneflow.float32)\n\n>>> x = flow.tensor(np.array([[0, -1., 10.], [5, 7, 0.8]]), dtype=flow.float32)\n>>> out = flow.erfc(x)\n>>> out\ntensor([[1.0000e+00, 1.8427e+00, 2.8026e-45],\n        [1.5375e-12, 4.1838e-23, 2.5790e-01]], dtype=oneflow.float32)\n\n\n\n"
    },
    {
        "api_name": "oneflow.erfinv",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.erfinv.html",
        "api_signature": "oneflow.erfinv()",
        "api_description": "Computes the inverse error function of input. The inverse error function is defined in the range \\((-1, 1)\\) as:\n\\[\\mathrm{erfinv}(\\mathrm{erf}(x)) = x\\]",
        "return_value": "",
        "parameters": "input (oneflow.Tensor) – the input tensor.\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> import numpy as np\n\n>>> input=flow.tensor(np.random.randn(3,3).astype(np.float32))\n>>> of_out=flow.erfinv(input)\n>>> of_out.shape\noneflow.Size([3, 3])\n\n\n\n"
    },
    {
        "api_name": "oneflow.exp",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.exp.html",
        "api_signature": "oneflow.exp()",
        "api_description": "This operator computes the exponential of Tensor.\nThe equation is:\n\\[out = e^x\\]",
        "return_value": "The result Tensor\n\n",
        "parameters": "x (oneflow.Tensor) – A Tensor\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import numpy as np\n>>> import oneflow as flow\n\n>>> x = flow.tensor(np.array([1, 2, 3]).astype(np.float32), dtype=flow.float32)\n>>> y = flow.exp(x)\n>>> y\ntensor([ 2.7183,  7.3891, 20.0855], dtype=oneflow.float32)\n\n\n\n"
    },
    {
        "api_name": "oneflow.expm1",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.expm1.html",
        "api_signature": "oneflow.expm1()",
        "api_description": "",
        "return_value": "of input.\nThe equation is:\n\n\\[y_{i} = e^{x_{i}} - 1\\]\n\nThe result Tensor\n\n",
        "parameters": "input (oneflow.Tensor) – A Tensor.\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> import numpy as np\n>>> x = flow.tensor(np.array([1, 2, 3]).astype(np.float32))\n>>> y = flow.expm1(x)\n>>> y.shape\noneflow.Size([3])\n>>> y\ntensor([ 1.7183,  6.3891, 19.0855], dtype=oneflow.float32)\n\n>>> x = flow.tensor(np.array([[[2, 4, 6],[7, 8, 9]],[[10,11,12],[13,14,15]]]).astype(np.float32))\n>>> y = flow.expm1(x)\n>>> print(y.shape)\noneflow.Size([2, 2, 3])\n>>> print(y.numpy())\n[[[6.3890562e+00 5.3598152e+01 4.0242880e+02]\n  [1.0956332e+03 2.9799580e+03 8.1020840e+03]]\n\n [[2.2025465e+04 5.9873141e+04 1.6275380e+05]\n  [4.4241238e+05 1.2026032e+06 3.2690165e+06]]]\n\n\n\n"
    },
    {
        "api_name": "oneflow.floor",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.floor.html",
        "api_signature": "oneflow.floor()",
        "api_description": "",
        "return_value": "\n\\[\\text{out}_{i} = \\lfloor \\text{input}_{i} \\rfloor\\]\n\n",
        "parameters": "input (Tensor) – the input tensor.\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> import numpy as np\n>>> input = flow.tensor(np.array([-0.5,  1.5, 0,  0.8]), dtype=flow.float32)\n>>> output = flow.floor(input)\n>>> output.shape\noneflow.Size([4])\n>>> output.numpy()\narray([-1.,  1.,  0.,  0.], dtype=float32)\n\n>>> input1 = flow.tensor(np.array([[0.8, 1.0], [-0.6, 2.5]]), dtype=flow.float32)\n>>> output1 = input1.floor()\n>>> output1.shape\noneflow.Size([2, 2])\n>>> output1.numpy()\narray([[ 0.,  1.],\n       [-1.,  2.]], dtype=float32)\n\n\n\n"
    },
    {
        "api_name": "oneflow.floor_",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.floor_.html",
        "api_signature": "oneflow.floor_()",
        "api_description": "In-place version of oneflow.floor()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.frac",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.frac.html",
        "api_signature": "oneflow.frac()",
        "api_description": "frac(input) → Tensor\nComputes the fractional portion of each element in input.\n\\[\\text{out}_{i} = \\text{input}_{i} - \\left\\lfloor |\\text{input}_{i}| \\right\\rfloor * \\operatorname{sgn}(\\text{input}_{i})\\]",
        "return_value": "The fractional part of the argument.\n\n",
        "parameters": "input – The input Tensor.\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> flow.frac(flow.Tensor([1, 2.50, -3.21]))\ntensor([ 0.0000,  0.5000, -0.2100], dtype=oneflow.float32)\n\n\n\n"
    },
    {
        "api_name": "oneflow.frac_",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.frac_.html",
        "api_signature": "oneflow.frac_()",
        "api_description": "In-place version of oneflow.frac().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.fmod",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.fmod.html",
        "api_signature": "oneflow.fmod(input, other, *, out=None)",
        "api_description": "Computes the element-wise remainder of division.\nThe dividend and divisor may contain both for integer and floating point\nnumbers. The remainder has the same sign as the dividend input.\nSupports broadcasting to a common shape, integer and float inputs.",
        "return_value": "",
        "parameters": "\ninput (Tensor) – the dividend\nother (Tensor or Scalar) – the divisor\n\n\nKeyword Arguments\nout (Tensor, optional) – the output tensor.\n\n\nExample:\n>>> import oneflow as flow\n>>> flow.fmod(flow.tensor([-3., -2, -1, 1, 2, 3], dtype=flow.float32), 2.)\ntensor([-1., -0., -1.,  1.,  0.,  1.], dtype=oneflow.float32)\n>>> flow.fmod(flow.tensor([1, 2, 3, 4, 5.], dtype=flow.float32), 1.5)\ntensor([1.0000, 0.5000, 0.0000, 1.0000, 0.5000], dtype=oneflow.float32)\n>>> flow.fmod(flow.tensor([1, 2, 3, 4., -5]), flow.tensor([4, 2, 1, 3., 1]))\ntensor([1., 0., 0., 1., -0.], dtype=oneflow.float32)\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.gelu",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.gelu.html",
        "api_signature": "oneflow.gelu(x: Tensor)",
        "api_description": "Applies the Gaussian Error Linear Units function:\n\\[\\begin{split}\\\\text{GELU}(x) = x * \\Phi(x)\\end{split}\\]\nwhere \\(\\Phi(x)\\) is the Cumulative Distribution Function for Gaussian Distribution.\nWhen the approximate argument is ‘tanh’, Gelu is estimated with:\n\\[\\begin{split}\\\\text{GELU}(x) = 0.5 * x * (1 + \\\\text{Tanh}(\\sqrt(2 / \\pi) * (x + 0.044715 * x^3)))\\end{split}\\]",
        "return_value": "A Tensor has same shape as the input.\n\n",
        "parameters": "\ninput (oneflow.Tensor) – Input Tensor\napproximate (string, optional) – the gelu approximation algorithm to use:\n'none' | 'tanh'. Default: 'none'\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import numpy as np\n>>> import oneflow as flow\n\n>>> x = np.array([-0.5, 0, 0.5]).astype(np.float32)\n>>> input = flow.tensor(x)\n\n>>> out = flow.gelu(input)\n>>> out\ntensor([-0.1543,  0.0000,  0.3457], dtype=oneflow.float32)\n\n\nSee\nGELU for more details.\n\n"
    },
    {
        "api_name": "oneflow.quick_gelu",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.quick_gelu.html",
        "api_signature": "oneflow.quick_gelu(x: Tensor)",
        "api_description": "Applies GELU approximation that is fast but somewhat inaccurate. See: https://github.com/hendrycks/GELUs\n\\[\\begin{split}\\\\text{QuickGELU}(x) = x * \\\\sigma(1.702x) = x * \\\\frac{1}{1 + \\\\exp(-1.702x)}\\end{split}\\]",
        "return_value": "A Tensor has same shape as the input.\n\n",
        "parameters": "input (oneflow.Tensor) – Input Tensor\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.square_relu",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.square_relu.html",
        "api_signature": "oneflow.square_relu(x: Tensor)",
        "api_description": "Applies the relu^2 activation introduced in https://arxiv.org/abs/2109.08668v2\n\\[\\begin{split}\\\\text{ReLU}(x) = \\\\max(0, x) * \\\\max(0, x)\\end{split}\\]",
        "return_value": "A Tensor has same shape as the input.\n\n",
        "parameters": "input (oneflow.Tensor) – Input Tensor\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.log",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.log.html",
        "api_signature": "oneflow.log()",
        "api_description": "",
        "return_value": "\n\\[y_{i} = \\log_{e} (x_{i})\\]\n\n",
        "parameters": "input (Tensor) – the input tensor.\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> import numpy as np\n>>> arr = np.random.randn(2, 3, 4, 5)\n>>> input = flow.tensor(arr, dtype=flow.float32)\n>>> output = flow.log(input)\n\n\n\n"
    },
    {
        "api_name": "oneflow.log1p",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.log1p.html",
        "api_signature": "oneflow.log1p()",
        "api_description": "",
        "return_value": "\n\\[\\text{out}_{i}=\\log_e(1+\\text{input}_{i})\\]\n",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> import numpy as np\n>>> x = flow.tensor(np.array([1.3, 1.5, 2.7]), dtype=flow.float32)\n>>> out = flow.log1p(x)\n>>> out\ntensor([0.8329, 0.9163, 1.3083], dtype=oneflow.float32)\n\n\n\n"
    },
    {
        "api_name": "oneflow.log2",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.log2.html",
        "api_signature": "oneflow.log2(input)",
        "api_description": "",
        "return_value": "\n\\[y_{i} = \\log2_{e} (x_{i})\\]\n\n",
        "parameters": "input (Tensor) – the input tensor.\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> import numpy as np\n>>> arr = np.random.randn(2, 3, 4, 5)\n>>> input = flow.tensor(arr, dtype=flow.float32)\n>>> output = flow.log2(input)\n\n\n\n"
    },
    {
        "api_name": "oneflow.log10",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.log10.html",
        "api_signature": "oneflow.log10(input)",
        "api_description": "",
        "return_value": "\n\\[y_{i} = \\log10_{e} (x_{i})\\]\n\n",
        "parameters": "input (Tensor) – the input tensor.\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> x = flow.ones(3, 3) * 10\n>>> output = flow.log10(x)\n>>> output\ntensor([[1., 1., 1.],\n        [1., 1., 1.],\n        [1., 1., 1.]], dtype=oneflow.float32)\n\n\n\n"
    },
    {
        "api_name": "oneflow.logical_and",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.logical_and.html",
        "api_signature": "oneflow.logical_and()",
        "api_description": "Computes the element-wise logical AND of the given input tensors.\nZeros are treated as False and nonzeros are treated as True.",
        "return_value": "The output Tensor\n\n",
        "parameters": "\ninput (oneflow.Tensor) – The input Tensor\nother (oneflow.Tensor) – The Tensor to compute AND with\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import numpy as np\n>>> import oneflow as flow\n\n>>> input1 = flow.tensor(np.array([1, 0, 1]).astype(np.float32), dtype=flow.float32)\n>>> input2 = flow.tensor(np.array([1, 1, 0]).astype(np.float32), dtype=flow.float32)\n\n>>> out = flow.logical_and(input1, input2)\n>>> out\ntensor([ True, False, False], dtype=oneflow.bool)\n\n\n\n"
    },
    {
        "api_name": "oneflow.logical_not",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.logical_not.html",
        "api_signature": "oneflow.logical_not()",
        "api_description": "Computes the element-wise logical NOT of the given input tensors.\nZeros are treated as False and nonzeros are treated as True.\n:param input: The input Tensor\n:type input: oneflow.Tensor\n:param other: The Tensor to compute NOT with\n:type other: oneflow.Tensor",
        "return_value": "The output Tensor\n\n",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n\n>>> input = flow.tensor([1, 0, -1], dtype=flow.float32)\n>>> out = flow.logical_not(input)\n>>> out\ntensor([False,  True, False], dtype=oneflow.bool)\n\n\n\n"
    },
    {
        "api_name": "oneflow.logical_or",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.logical_or.html",
        "api_signature": "oneflow.logical_or()",
        "api_description": "Computes the element-wise logical OR of the given input tensors.\nZeros are treated as False and nonzeros are treated as True.",
        "return_value": "The output Tensor\n\n",
        "parameters": "\ninput (oneflow.Tensor) – The input Tensor\nother (oneflow.Tensor) – The Tensor to compute OR with\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import numpy as np\n>>> import oneflow as flow\n\n>>> input1 = flow.tensor(np.array([1, 0, 1]).astype(np.float32), dtype=flow.float32)\n>>> input2 = flow.tensor(np.array([1, 0, 0]).astype(np.float32), dtype=flow.float32)\n\n>>> out = flow.logical_or(input1, input2)\n>>> out\ntensor([ True, False,  True], dtype=oneflow.bool)\n\n\n\n"
    },
    {
        "api_name": "oneflow.logical_xor",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.logical_xor.html",
        "api_signature": "oneflow.logical_xor()",
        "api_description": "Computes the element-wise logical XOR of the given input tensors.\nZeros are treated as False and nonzeros are treated as True.",
        "return_value": "The output Tensor\n\n",
        "parameters": "\ninput (oneflow.Tensor) – The input Tensor\nother (oneflow.Tensor) – The Tensor to compute XOR with\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import numpy as np\n>>> import oneflow as flow\n\n>>> input1 = flow.tensor(np.array([1, 0, 1]).astype(np.float32), dtype=flow.float32)\n>>> input2 = flow.tensor(np.array([1, 0, 0]).astype(np.float32), dtype=flow.float32)\n>>> out = flow.logical_xor(input1, input2)\n>>> out\ntensor([False, False,  True], dtype=oneflow.bool)\n\n\n\n"
    },
    {
        "api_name": "oneflow.bitwise_and",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.bitwise_and.html",
        "api_signature": "oneflow.bitwise_and()",
        "api_description": "Computes the bitwise AND of input and other.\nThe input tensor must be of integral or Boolean types.\nFor bool tensors, it computes the logical AND.",
        "return_value": "The output Tensor\n\n",
        "parameters": "\ninput (oneflow.Tensor) – The input Tensor\nother (oneflow.Tensor) – The Tensor to compute bitwise AND with\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> x = flow.tensor([1, 2, 3])\n>>> flow.bitwise_and(x, 2)\ntensor([0, 2, 2], dtype=oneflow.int64)\n>>> y = flow.tensor([5, 6, 7])\n>>> flow.bitwise_and(x, y)\ntensor([1, 2, 3], dtype=oneflow.int64)\n\n\n\n"
    },
    {
        "api_name": "oneflow.bitwise_or",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.bitwise_or.html",
        "api_signature": "oneflow.bitwise_or()",
        "api_description": "Computes the bitwise OR of input and other.\nThe input tensor must be of integral or Boolean types.\nFor bool tensors, it computes the logical OR.",
        "return_value": "The output Tensor\n\n",
        "parameters": "\ninput (oneflow.Tensor) – The input Tensor\nother (oneflow.Tensor) – The Tensor to compute OR with\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> x = flow.tensor([1, 2, 3])\n>>> flow.bitwise_or(x, 4)\ntensor([5, 6, 7], dtype=oneflow.int64)\n>>> y = flow.tensor([5, 6, 7])\n>>> flow.bitwise_or(x, y)\ntensor([5, 6, 7], dtype=oneflow.int64)\n\n\n\n"
    },
    {
        "api_name": "oneflow.bitwise_xor",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.bitwise_xor.html",
        "api_signature": "oneflow.bitwise_xor()",
        "api_description": "Computes the bitwise XOR of input and other.\nThe input tensor must be of integral or Boolean types.\nFor bool tensors, it computes the logical XOR.",
        "return_value": "The output Tensor\n\n",
        "parameters": "\ninput (oneflow.Tensor) – The input Tensor\nother (oneflow.Tensor) – The Tensor to compute XOR with\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> x = flow.tensor([1, 2, 3])\n>>> flow.bitwise_xor(x, 2)\ntensor([3, 0, 1], dtype=oneflow.int64)\n>>> y = flow.tensor([5, 6, 7])\n>>> flow.bitwise_xor(x, y)\ntensor([4, 4, 4], dtype=oneflow.int64)\n\n\n\n"
    },
    {
        "api_name": "oneflow.bitwise_not",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.bitwise_not.html",
        "api_signature": "oneflow.bitwise_not()",
        "api_description": "Computes the bitwise NOT of input.\nThe input tensor must be of integral or Boolean types.\nFor bool tensors, it computes the logical NOT.",
        "return_value": "The output Tensor\n\n",
        "parameters": "input (oneflow.Tensor) – The input Tensor\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> x = flow.tensor([1, 2, 3])\n>>> flow.bitwise_not(x)\ntensor([-2, -3, -4], dtype=oneflow.int64)\n>>> x = flow.tensor([0, 0, 1]).bool()\n>>> flow.bitwise_not(x)\ntensor([ True,  True, False], dtype=oneflow.bool)\n\n\n\n"
    },
    {
        "api_name": "oneflow.mish",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.mish.html",
        "api_signature": "oneflow.mish(x: Tensor)",
        "api_description": "Applies the element-wise function:\n\\[ext{mish}(x) = x *      ext{tanh}(      ext{softplus}(x))\\]",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import numpy as np\n>>> import oneflow as flow\n\n>>> x = np.array([1, 2, 3]).astype(np.float32)\n>>> input = flow.tensor(x)\n\n>>> out = flow.mish(input)\n>>> out\ntensor([0.8651, 1.9440, 2.9865], dtype=oneflow.float32)\n\n\nSee Mish for more details.\n\n"
    },
    {
        "api_name": "oneflow.mul",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.mul.html",
        "api_signature": "oneflow.mul()",
        "api_description": "Computes the multiplication of input by other for each element, scalar and broadcast promotation are supported.\nThe formula is:\n\\[\\text{out}_i = \\text{input}_i \\times \\text{other}_i\\]",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import numpy as np\n>>> import oneflow as flow\n\n# element-wise multiply\n>>> input = flow.tensor(np.random.randn(2,3), dtype=flow.float32)\n>>> other = flow.tensor(np.random.randn(2,3), dtype=flow.float32)\n>>> out = flow.mul(input,other).numpy()\n>>> out.shape\n(2, 3)\n\n# scalar mutiply\n>>> input = 5\n>>> other = flow.tensor(np.random.randn(2,3), dtype=flow.float32)\n>>> out = flow.mul(input,other).numpy()\n>>> out.shape\n(2, 3)\n\n# broadcast mutiply\n>>> input = flow.tensor(np.random.randn(1,1), dtype=flow.float32)\n>>> other = flow.tensor(np.random.randn(2,3), dtype=flow.float32)\n>>> out = flow.mul(input,other).numpy()\n>>> out.shape\n(2, 3)\n\n\n\n"
    },
    {
        "api_name": "oneflow.neg",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.neg.html",
        "api_signature": "oneflow.neg()",
        "api_description": "This operator computes the negative value of Tensor.",
        "return_value": "The result Tensor\n\n",
        "parameters": "input (oneflow.Tensor) – A Tensor\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import numpy as np\n>>> import oneflow as flow\n\n>>> input = flow.tensor(\n...    np.array([1.0, -1.0, 2.3]).astype(np.float32), dtype=flow.float32\n... )\n>>> out = flow.negative(input)\n>>> out\ntensor([-1.0000,  1.0000, -2.3000], dtype=oneflow.float32)\n\n\n\n"
    },
    {
        "api_name": "oneflow.negative",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.negative.html",
        "api_signature": "oneflow.negative()",
        "api_description": "This operator computes the negative value of Tensor.",
        "return_value": "The result Tensor\n\n",
        "parameters": "input (oneflow.Tensor) – A Tensor\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import numpy as np\n>>> import oneflow as flow\n\n>>> input = flow.tensor(\n...    np.array([1.0, -1.0, 2.3]).astype(np.float32), dtype=flow.float32\n... )\n>>> out = flow.negative(input)\n>>> out\ntensor([-1.0000,  1.0000, -2.3000], dtype=oneflow.float32)\n\n\n\n"
    },
    {
        "api_name": "oneflow.pow",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.pow.html",
        "api_signature": "oneflow.pow()",
        "api_description": "Takes the power of each element in input with exponent and returns a tensor with the result. Exponent can be either a single float number, a single int number, or a tensor with the same shape as input.\nWhen exponent is a scalar value, the operation applied is:\n\\[\\text{out}_i = x_i ^ \\text{exponent}\\]\nWhen exponent is a tensor, the operation applied is:\n\\[\\text{out}_i = x_i ^ {\\text{exponent}_i}\\]",
        "return_value": "The result of variance on the specified axis of input Tensor\n\n",
        "parameters": "\ninput (Tensor) – the input tensor.\nexponent (int, float, Tensor) – the exponent.\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> import numpy as np\n\n>>> x = flow.tensor(np.array([1.0, 2.0, 3.0, 4.0, 5.0, 6.0]), dtype=flow.float32)\n>>> out = flow.pow(x, 2)\n>>> out\ntensor([ 1.,  4.,  9., 16., 25., 36.], dtype=oneflow.float32)\n\n>>> x = flow.tensor(np.array([1.0, 2.0, 3.0, 4.0]), dtype=flow.float32)\n>>> y = flow.tensor(np.array([1.0, 2.0, 3.0, 4.0]), dtype=flow.float32)\n>>> out = flow.pow(x, y)\n>>> out\ntensor([  1.,   4.,  27., 256.], dtype=oneflow.float32)\n\n\n\n"
    },
    {
        "api_name": "oneflow.reciprocal",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.reciprocal.html",
        "api_signature": "oneflow.reciprocal()",
        "api_description": "Computes the safe reciprocal of x. If x is zero, the reciprocal will\nbe also set to zero.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import numpy as np\n>>> import oneflow as flow\n\n>>> x = flow.tensor(np.array([[1, 2, 3], [4, 5, 6]]), dtype=flow.float32)\n>>> out = flow.reciprocal(x)\n>>> out.numpy()\narray([[1.        , 0.5       , 0.33333334],\n       [0.25      , 0.2       , 0.16666667]], dtype=float32)\n\n\n\n"
    },
    {
        "api_name": "oneflow.round",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.round.html",
        "api_signature": "oneflow.round()",
        "api_description": "This operator rounds the value of Blob to the nearest integer.",
        "return_value": "The result Tensor\n\n",
        "parameters": "input (oneflow.Tensor) – A Tensor\n\n",
        "input_shape": "",
        "notes": "This function implements the “round half to even” to break ties when a number is equidistant from two integers (e.g. round(2.5) is 2).",
        "code_example": ">>> import oneflow as flow\n>>> import numpy as np\n>>> x1 = flow.tensor(np.array([1.49999, 1.500001, 2.7]).astype(np.float32))\n>>> out1 = flow.round(x1)\n>>> out1.numpy()\narray([1., 2., 3.], dtype=float32)\n>>> x2 = flow.tensor(np.array([2.499999, 7.5000001, 5.3, 6.8]).astype(np.float32))\n>>> out2 = flow.round(x2)\n>>> out2.numpy()\narray([2., 8., 5., 7.], dtype=float32)\n\n\n\n"
    },
    {
        "api_name": "oneflow.round_",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.round_.html",
        "api_signature": "oneflow.round_()",
        "api_description": "In-place version of oneflow.round().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.rsqrt",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.rsqrt.html",
        "api_signature": "oneflow.rsqrt()",
        "api_description": "",
        "return_value": "the elements of input.\n\n\\[\\text{out}_{i} = \\frac{1}{\\sqrt{\\text{input}_{i}}}\\]\n\n",
        "parameters": "input – the input tensor.\n\n\n>>> import oneflow as flow\n>>> import numpy as np\n\n>>> a = flow.tensor(np.array([1.0, 2.0, 3.0]), dtype=flow.float32)\n>>> out = flow.rsqrt(a).numpy()\n>>> out\narray([1.        , 0.70710677, 0.57735026], dtype=float32)\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.selu",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.selu.html",
        "api_signature": "oneflow.selu(x: Tensor)",
        "api_description": "Applies element-wise function\n\\[ext{SELU}(x) = scale * (\\max(0,x) + \\min(0, \u0007lpha * (\\exp(x) - 1)))`, with :math:`\u0007lpha=1.6732632423543772848170429916717` and  :math:`scale=1.0507009873554804934193349852946`.\\]\nSee SELU for more details.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import numpy as np\n>>> import oneflow as flow\n\n>>> x = np.array([1, 2, 3]).astype(np.float32)\n>>> input = flow.tensor(x)\n>>> out = flow.nn.functional.selu(input)\n>>> out\ntensor([1.0507, 2.1014, 3.1521], dtype=oneflow.float32)\n\n\n\n"
    },
    {
        "api_name": "oneflow.softmax",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.softmax.html",
        "api_signature": "oneflow.softmax(x: Tensor, dim: int)",
        "api_description": "Softmax is defined as:\n\\[\\begin{split}\\text{Softmax}(x_{i}) = \\frac{\\\\exp(x_i)}{\\sum_j \\exp(x_j)}\\end{split}\\]\nSee Softmax for more details.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.softplus",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.softplus.html",
        "api_signature": "oneflow.softplus(x: Tensor, beta: double = 1, threshold: double = 20)",
        "api_description": "Applies the element-wise function:\n\\[\\text{Softplus}(x) = \\frac{1}{\\beta} * \\log(1 + \\exp(\\beta * x))\\]\nFor numerical stability the implementation reverts to the linear function\nwhen \\(input \\times \\beta > threshold\\).\nSee Softplus for more details.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.softsign",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.softsign.html",
        "api_signature": "oneflow.softsign(x: Tensor)",
        "api_description": "The formula is:\n\\[softsign(x) = \\frac{x}{1 + |x|}\\]",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import numpy as np\n>>> import oneflow as flow\n\n>>> x = np.array([1, 2, 3]).astype(np.float32)\n>>> input = flow.tensor(x)\n>>> out = flow.nn.functional.softsign(input)\n>>> out\ntensor([0.5000, 0.6667, 0.7500], dtype=oneflow.float32)\n\n\nSee Softsign for more details.\n\n"
    },
    {
        "api_name": "oneflow.silu",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.silu.html",
        "api_signature": "oneflow.silu(x: Tensor)",
        "api_description": "The formula is:\n\\[ext{silu}(x) = x * sigmoid(x)\\]",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import numpy as np\n>>> import oneflow as flow\n\n>>> x = np.array([1, 2, 3]).astype(np.float32)\n>>> input = flow.tensor(x)\n>>> out = flow.silu(input)\n>>> out\ntensor([0.7311, 1.7616, 2.8577], dtype=oneflow.float32)\n\n\nSee SiLU for more details.\n\n"
    },
    {
        "api_name": "oneflow.sigmoid",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.sigmoid.html",
        "api_signature": "oneflow.sigmoid(input)",
        "api_description": "Applies the element-wise function \\(\\text{Sigmoid}(x) = \\frac{1}{1 + \\exp(-x)}\\)\nSee Sigmoid for more details.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import numpy as np\n>>> import oneflow as flow\n\n>>> x = np.array([0.81733328, 0.43621480, 0.10351428])\n>>> input = flow.tensor(x, dtype=flow.float32)\n>>> out = flow.nn.functional.sigmoid(input)\n>>> out\ntensor([0.6937, 0.6074, 0.5259], dtype=oneflow.float32)\n\n\n\n"
    },
    {
        "api_name": "oneflow.sign",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.sign.html",
        "api_signature": "oneflow.sign()",
        "api_description": "Computes the sign of Tensor.\n\\[\\text{out}_{i}  = \\text{sgn}(\\text{input}_{i})\\]",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> import numpy as np\n>>> x1 = flow.tensor(np.array([-2, 0, 2]).astype(np.float32))\n>>> out1 = flow.sign(x1)\n>>> out1.numpy()\narray([-1.,  0.,  1.], dtype=float32)\n>>> x2 = flow.tensor(np.array([-3.2, -4.5, 5.8]).astype(np.float32),device=flow.device('cuda'))\n>>> out2 = flow.sign(x2)\n>>> out2.numpy()\narray([-1., -1.,  1.], dtype=float32)\n\n\n\n"
    },
    {
        "api_name": "oneflow.sin",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.sin.html",
        "api_signature": "oneflow.sin()",
        "api_description": "",
        "return_value": "sin(x: Tensor) -> Tensor\n\n\\[\\text{y}_{i} = \\sin(\\text{x}_{i})\\]\n\n",
        "parameters": "x (Tensor) – the input tensor.\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ".. code-block:: python\n>>> import oneflow as flow\n>>> import numpy as np\n>>> x1 = flow.tensor(np.array([-0.5461,  0.1347, -2.7266, -0.2746]).astype(np.float32))\n>>> y1 = flow.sin(x1)\n>>> y1\ntensor([-0.5194,  0.1343, -0.4032, -0.2712], dtype=oneflow.float32)\n\n\n>>> x2 = flow.tensor(np.array([-1.4, 2.6, 3.7]).astype(np.float32), device=flow.device('cuda'))\n>>> y2 = flow.sin(x2)\n>>> y2\ntensor([-0.9854,  0.5155, -0.5298], device='cuda:0', dtype=oneflow.float32)\n\n\n\n"
    },
    {
        "api_name": "oneflow.sinh",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.sinh.html",
        "api_signature": "oneflow.sinh()",
        "api_description": "",
        "return_value": "\n\\[\\text{out}_{i} = \\sinh(\\text{input}_{i})\\]\n\n",
        "parameters": "input (Tensor) – the input tensor.\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import numpy as np\n>>> import oneflow as flow\n\n>>> x1 = flow.tensor(np.array([1, 2, 3]), dtype=flow.float32)\n>>> x2 = flow.tensor(np.array([1.53123589,0.54242598,0.15117185]), dtype=flow.float32)\n>>> x3 = flow.tensor(np.array([1,0,-1]), dtype=flow.float32)\n\n>>> flow.sinh(x1).numpy()\narray([ 1.1752012,  3.6268604, 10.017875 ], dtype=float32)\n>>> flow.sinh(x2).numpy()\narray([2.20381  , 0.5694193, 0.1517483], dtype=float32)\n>>> flow.sinh(x3).numpy()\narray([ 1.1752012,  0.       , -1.1752012], dtype=float32)\n\n\n\n"
    },
    {
        "api_name": "oneflow.sin_",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.sin_.html",
        "api_signature": "oneflow.sin_()",
        "api_description": "In-place version of oneflow.sin()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.sqrt",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.sqrt.html",
        "api_signature": "oneflow.sqrt()",
        "api_description": "",
        "return_value": "\n\\[\\text{out}_{i} = \\sqrt{\\text{input}_{i}}\\]\n\n",
        "parameters": "input – the input tensor.\n\n\n>>> import oneflow as flow\n>>> import numpy as np\n\n>>> arr = np.array([1.0, 2.0, 3.0])\n>>> input = flow.tensor(arr, dtype=flow.float32)\n>>> output = flow.sqrt(input).numpy()\n>>> output\narray([1.       , 1.4142135, 1.7320508], dtype=float32)\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.square",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.square.html",
        "api_signature": "oneflow.square()",
        "api_description": "",
        "return_value": "\n\\[\\text{out}_{i} = \\sqrt{\\text{input}_{i}}\\]\n\n",
        "parameters": "input – the input tensor.\n\n\n>>> import oneflow as flow\n>>> import numpy as np\n\n>>> arr = np.array([1.0, 2.0, 3.0])\n>>> input = flow.tensor(arr, dtype=flow.float32)\n>>> output = flow.square(input).numpy()\n>>> output\narray([1., 4., 9.], dtype=float32)\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.sub",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.sub.html",
        "api_signature": "oneflow.sub()",
        "api_description": "Computes the subtraction of input by other for each element, scalar and broadcast promotation are supported.\nThe formula is:\n\\[out = input - other\\]",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import numpy as np\n>>> import oneflow as flow\n\n# element-wise subtract\n>>> input = flow.tensor(np.random.randn(2,3), dtype=flow.float32)\n>>> other = flow.tensor(np.random.randn(2,3), dtype=flow.float32)\n>>> out = flow.sub(input,other).numpy()\n>>> out.shape\n(2, 3)\n\n# scalar subtract\n>>> input = 5\n>>> other = flow.tensor(np.random.randn(2,3), dtype=flow.float32)\n>>> out = flow.sub(input,other).numpy()\n>>> out.shape\n(2, 3)\n\n# broadcast subtract\n>>> input = flow.tensor(np.random.randn(1,1), dtype=flow.float32)\n>>> other = flow.tensor(np.random.randn(2,3), dtype=flow.float32)\n>>> out = flow.sub(input,other).numpy()\n>>> out.shape\n(2, 3)\n\n\n\n"
    },
    {
        "api_name": "oneflow.tan",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.tan.html",
        "api_signature": "oneflow.tan()",
        "api_description": "",
        "return_value": "\n\\[\\text{out}_{i} = \\tan(\\text{input}_{i})\\]\n\n",
        "parameters": "input (Tensor) – the input tensor.\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> import numpy as np\n>>> np_arr = np.array([-1/4*np.pi, 0, 1/4*np.pi]).astype(np.float32)\n>>> input = flow.tensor(np_arr, dtype=flow.float32)\n>>> output = flow.tan(input)\n>>> output\ntensor([-1.,  0.,  1.], dtype=oneflow.float32)\n\n\n\n"
    },
    {
        "api_name": "oneflow.tanh",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.tanh.html",
        "api_signature": "oneflow.tanh(x: Tensor)",
        "api_description": "The equation is:\n\\[out = \\frac{e^x-e^{-x}}{e^x+e^{-x}}\\]\nSee Tanh for more details.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.trunc",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.trunc.html",
        "api_signature": "oneflow.trunc(input)",
        "api_description": "",
        "return_value": "the elements of input.\n\n",
        "parameters": "input (Tensor) – the input tensor.\n\n\nExample:\n>>> import oneflow as flow\n>>> a = flow.tensor([ 3.4742,  0.5466, -0.8008, -0.9079])\n>>> flow.trunc(a)\ntensor([3., 0., -0., -0.], dtype=oneflow.float32)\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.floor_divide",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.floor_divide.html",
        "api_signature": "oneflow.floor_divide()",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.lerp",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.lerp.html",
        "api_signature": "oneflow.lerp(start, end, weight)",
        "api_description": "Does a linear interpolation of two tensors start and end based on a scalar or tensor weight and returns the result.\nThe shapes of start` and end must be broadcastable. If weight is a tensor, then the shapes of weight, start, and end must be broadcastable.\n\\[out_{i} = start_{i} + weight_{i} * (end_{i} - start_{i})\\]",
        "return_value": "",
        "parameters": "\nstart (oneflow.Tensor) – the tensor with the starting points.\nend (oneflow.Tensor) – the tensor with the ending points.\nweight (float or oneflow.Tensor) – the weight for the interpolation formula.\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> start = flow.arange(1., 5.)\n>>> end = flow.empty(4).fill_(10)\n>>> flow.lerp(start, end, 0.5)\ntensor([5.5000, 6.0000, 6.5000, 7.0000], dtype=oneflow.float32)\n>>> flow.lerp(start, end, flow.full_like(start, 0.5))\ntensor([5.5000, 6.0000, 6.5000, 7.0000], dtype=oneflow.float32)\n\n\n\n"
    },
    {
        "api_name": "oneflow.lerp_",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.lerp_.html",
        "api_signature": "oneflow.lerp_()",
        "api_description": "In-place version of oneflow.lerp()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.quantile",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.quantile.html",
        "api_signature": "oneflow.quantile(input, q, dim=None, keepdim=False, *, interpolation='linear', out=None)",
        "api_description": "Computes the q-th quantiles of each row of the input tensor along the dimension dim.\nTo compute the quantile, we map q in [0, 1] to the range of indices [0, n] to find the location\nof the quantile in the sorted input. If the quantile lies between two data points a < b with\nindices i and j in the sorted order, result is computed according to the given\ninterpolation method as follows:\nlinear: a + (b - a) * fraction, where fraction is the fractional part of the computed quantile index.\nlower: a.\nhigher: b.\nnearest: a or b, whichever’s index is closer to the computed quantile index (rounding down for .5 fractions).\nmidpoint: (a + b) / 2.\nIf q is a 1D tensor, the first dimension of the output represents the quantiles and has size\nequal to the size of q, the remaining dimensions are what remains from the reduction.",
        "return_value": "",
        "parameters": "\ninput (oneflow.Tensor) – the input Tensor.\nq (float or oneflow.Tensor) – a scalar or 1D tensor of values in the range [0, 1].\ndim (int, optional) – the dimension to reduce. Default is None.\nkeepdim (bool, optional) – whether the output tensor has dim retained or not. Default is False\ninterpolation (str, optional) – interpolation method to use when the desired quantile lies between two data points.\nCan be linear, lower, higher, midpoint and nearest.\nDefault is linear.\nout (oneflow.Tensor, optional) – the output Tensor.\n\n\n\n",
        "input_shape": "",
        "notes": "By default dim is None resulting in the input tensor being flattened before computation.",
        "code_example": ">>> import oneflow as flow\n>>> a = flow.arange(8.)\n>>> q = flow.tensor([0.25, 0.5, 0.75])\n>>> flow.quantile(a, q, dim=0, keepdim=True)\ntensor([[1.7500],\n        [3.5000],\n        [5.2500]], dtype=oneflow.float32)\n>>> a = flow.arange(4.)\n>>> flow.quantile(a, 0.6, interpolation=\"linear\")\ntensor(1.8000, dtype=oneflow.float32)\n>>> flow.quantile(a, 0.6, interpolation=\"lower\")\ntensor(1., dtype=oneflow.float32)\n>>> flow.quantile(a, 0.6, interpolation=\"higher\")\ntensor(2., dtype=oneflow.float32)\n>>> flow.quantile(a, 0.6, interpolation=\"midpoint\")\ntensor(1.5000, dtype=oneflow.float32)\n>>> flow.quantile(a, 0.6, interpolation=\"nearest\")\ntensor(2., dtype=oneflow.float32)\n>>> flow.quantile(a, 0.4, interpolation=\"nearest\")\ntensor(1., dtype=oneflow.float32)\n\n\n\n"
    },
    {
        "api_name": "oneflow.argmax",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.argmax.html",
        "api_signature": "oneflow.argmax()",
        "api_description": "The op computes the index with the largest value of a Tensor at specified axis.",
        "return_value": "A Tensor(dtype=int64) contains the index with the largest value of input\n\n",
        "parameters": "\ninput (oneflow.Tensor) – Input Tensor\ndim (int, optional) – dimension to be calculated. Defaults to the last dim (-1)\nkeepdim (bool optional) – whether the output tensor has dim retained or not. Ignored if dim=None.\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n\n>>> input = flow.tensor([[1, 3, 8, 7, 2],\n...            [1, 9, 4, 3, 2]], dtype=flow.float32)\n>>> output = flow.argmax(input)\n>>> output\ntensor(6, dtype=oneflow.int64)\n>>> output = flow.argmax(input, dim=1)\n>>> output\ntensor([2, 1], dtype=oneflow.int64)\n\n\n\n"
    },
    {
        "api_name": "oneflow.argmin",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.argmin.html",
        "api_signature": "oneflow.argmin()",
        "api_description": "The op computes the index with the largest value of a Tensor at specified axis.",
        "return_value": "A Tensor(dtype=int64) contains the index with the largest value of input\n\n",
        "parameters": "\ninput (oneflow.Tensor) – Input Tensor\ndim (int, optional) – dimension to be calculated. Defaults to the last dim (-1)\nkeepdim (bool optional) – whether the output tensor has dim retained or not. Ignored if dim=None.\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n\n>>> input = flow.tensor([[4, 3, 1, 0, 2],\n...            [5, 9, 7, 6, 8]], dtype=flow.float32)\n>>> output = flow.argmin(input)\n>>> output\ntensor(3, dtype=oneflow.int64)\n>>> output = flow.argmin(input, dim=1)\n>>> output\ntensor([3, 0], dtype=oneflow.int64)\n\n\n\n"
    },
    {
        "api_name": "oneflow.amax",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.amax.html",
        "api_signature": "oneflow.amax(input, dim=None, keepdim=False)",
        "api_description": "",
        "return_value": "This function is equivalent to PyTorch’s amax function.\n\nMaximum of the input tensor\n\n",
        "parameters": "\ninput (oneflow.Tensor) – the input Tensor.\ndim (int or List of int, optional) – the dimension or the dimensions to reduce. Dim is None by default.\nkeepdim (bool, optional) – whether to retain the dimension. keepdim is False by default.\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n\n>>> x = flow.tensor([[[0,1],[2,3]],[[4,5],[6,7]]])\n>>> flow.amax(x, 1)\ntensor([[2, 3],\n        [6, 7]], dtype=oneflow.int64)\n>>> flow.amax(x, 0)\ntensor([[4, 5],\n        [6, 7]], dtype=oneflow.int64)\n>>> flow.amax(x)\ntensor(7, dtype=oneflow.int64)\n>>> flow.amax(x, 0, True)\ntensor([[[4, 5],\n         [6, 7]]], dtype=oneflow.int64)\n\n\n\n"
    },
    {
        "api_name": "oneflow.amin",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.amin.html",
        "api_signature": "oneflow.amin(input, dim, keepdim=False)",
        "api_description": "",
        "return_value": "If keepdim is True, the output tensor is of the same size as input except in the dimension(s) dim where it is of size 1. Otherwise, dim is squeezed (see oneflow.squeeze()), resulting in the output tensor having 1 (or len(dim)) fewer dimension(s).\nThis function is equivalent to PyTorch’s amin function.\nThe documentation is referenced from: https://pytorch.org/docs/1.10/generated/torch.amin.html.\n\n",
        "parameters": "\ninput (oneflow.Tensor) – the input Tensor.\ndim (int, Tuple[int]) – the dimension or dimensions to reduce.\nkeepdim (bool) – whether the output tensor has dim retained or not.\n\n\n\nExample:\n>>> import oneflow as flow\n\n>>> x = flow.tensor([[[0,1],[2,3]],[[4,5],[6,7]]])\n>>> flow.amin(x, 1)\ntensor([[0, 1],\n        [4, 5]], dtype=oneflow.int64)\n>>> flow.amin(x, 0)\ntensor([[0, 1],\n        [2, 3]], dtype=oneflow.int64)\n>>> flow.amin(x)\ntensor(0, dtype=oneflow.int64)\n>>> flow.amin(x, 0, True)\ntensor([[[0, 1],\n         [2, 3]]], dtype=oneflow.int64)\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.any",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.any.html",
        "api_signature": "oneflow.any(input, dim=None, keepdim=False)",
        "api_description": "For each row of input in the given dimension dim, returns True if any element in the row evaluate to True and False otherwise. If the dimension is None, compute if any elements in the input tensor to true.\nIf keepdim is True, the output tensor is of the same size as input except in the dimension(s) dim where it is of size 1. Otherwise, dim is squeezed oneflow.squeeze(), resulting in the output tensor having 1 (or len(dim)) fewer dimension(s).",
        "return_value": "",
        "parameters": "\ninput (oneflow.Tensor) – the Input Tensor\ndim (int, optional) – the dimension to reduce. Default: None\nkeepdim (bool, optional) – whether the output tensor has dim retained or not. Default: False\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n\n>>> input = flow.Tensor([[1, 2, 3], [4, 5, 6]]) < 4\n>>> input\ntensor([[ True,  True,  True],\n        [False, False, False]], dtype=oneflow.bool)\n>>> flow.any(input)\ntensor(True, dtype=oneflow.bool)\n>>> flow.any(input, 0)\ntensor([True, True, True], dtype=oneflow.bool)\n>>> flow.any(input, 0, True)\ntensor([[True, True, True]], dtype=oneflow.bool)\n\n\n\n"
    },
    {
        "api_name": "oneflow.max",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.max.html",
        "api_signature": "oneflow.max(input, dim=None, keepdim=False)",
        "api_description": "Computes the maximum value of all elements in the input tensor.",
        "return_value": "If dim is None, returns\nthe maximum value of all elements in the input tensor. Otherwise, returns a tuple of Tensor (values, indices),\nwhere the values are the maximum value of all elements in the input tensor,\nthe indices are the indices of the elements in the original input tensor.\n\n",
        "parameters": "\ninput (oneflow.Tensor) – the Input Tensor\ndim (int, optional) – the dimension to reduce. Default: None\nkeepdim (bool, optional) – whether the output tensor has dim retained or not. Default: False\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n\n>>> input = flow.Tensor([[4, 1, 5], [2, 6, 3]])\n>>> flow.max(input)\ntensor(6., dtype=oneflow.float32)\n>>> result = flow.max(input, dim=1)\n>>> result.values\ntensor([5., 6.], dtype=oneflow.float32)\n>>> result.indices\ntensor([2, 1], dtype=oneflow.int64)\n\n\n\n"
    },
    {
        "api_name": "oneflow.min",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.min.html",
        "api_signature": "oneflow.min(input, dim=None, keepdim=False)",
        "api_description": "Computes the minimum value of all elements in the input tensor.",
        "return_value": "If dim is None, returns\nthe minimum value of all elements in the input tensor. Otherwise, returns a tuple of Tensor (values, indices),\nwhere the values are the minimum value of all elements in the input tensor,\nthe indices are the indices of the elements in the original input tensor.\n\n",
        "parameters": "\ninput (oneflow.Tensor) – the Input Tensor\ndim (int, optional) – the dimension to reduce. Default: None\nkeepdim (bool, optional) – whether the output tensor has dim retained or not. Default: False\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n\n>>> input = flow.Tensor([[4, 1, 5], [2, 6, 3]])\n>>> flow.min(input)\ntensor(1., dtype=oneflow.float32)\n>>> result = flow.min(input, dim=1)\n>>> result.values\ntensor([1., 2.], dtype=oneflow.float32)\n>>> result.indices\ntensor([1, 0], dtype=oneflow.int64)\n\n\n\n"
    },
    {
        "api_name": "oneflow.mean",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.mean.html",
        "api_signature": "oneflow.mean(input, dim=None, keepdim=False)",
        "api_description": "Computes the mean of row of elements in a tensor in the given dimension. If the dimension is None, mean of all elements will be caculated.\nIf keepdim is True, the output tensor is of the same size as input except in the dimension(s) dim where it is of size 1. Otherwise, dim is squeezed oneflow.squeeze(), resulting in the output tensor having 1 (or len(dim)) fewer dimension(s).",
        "return_value": "",
        "parameters": "\ninput (oneflow.Tensor) – the Input Tensor\ndim (int or tuple of ints, optional) – the dimension to reduce. Default: None\nkeepdim (bool, optional) – whether the output tensor has dim retained or not. Default: False\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n\n>>> input = flow.Tensor([[1, 2, 3], [4, 5, 6]])\n>>> flow.mean(input)\ntensor(3.5000, dtype=oneflow.float32)\n>>> flow.mean(input, dim=0)\ntensor([2.5000, 3.5000, 4.5000], dtype=oneflow.float32)\n>>> flow.mean(input, dim=1)\ntensor([2., 5.], dtype=oneflow.float32)\n\n\n\n"
    },
    {
        "api_name": "oneflow.median",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.median.html",
        "api_signature": "oneflow.median(input)",
        "api_description": "",
        "return_value": "The documentation is referenced from:\nhttps://pytorch.org/docs/1.10/generated/torch.median.html#torch.median\n\nin the dimension dim, and indices contains the index of the median values found in the dimension dim.\nBy default, dim is the last dimension of the input tensor.\nIf keepdim is True, the output tensors are of the same size\nas input except in the dimension dim where they are of size 1.\nOtherwise, dim is squeezed (see flow.squeeze()), resulting in\nthe outputs tensor having 1 fewer dimension than input.\n\n",
        "parameters": "input (Tensor) – the input tensor.\n\n\n\ninput (Tensor) – the input tensor.\ndim (int) – the dimension to reduce.\nkeepdim (bool) – whether the output tensor has dim retained or not.\n\n\n\n",
        "input_shape": "",
        "notes": "The median is not unique for input tensors with an even number\nof elements. In this case the lower of the two medians is returned.\nThe median is not unique for input tensors with an even number\nof elements in the dimension dim. In this case the lower of the\ntwo medians is returned.",
        "code_example": ">>> import oneflow as flow\n>>> x = flow.tensor((1, 2, -1), dtype=flow.float32)\n>>> flow.median(x)\ntensor(1., dtype=oneflow.float32)\n\n\n\n\noneflow.median(input, dim=- 1, keepdim=False, *, out=None)\n\n>>> import oneflow as flow\n>>> a = flow.tensor([[ 0.2505, -0.3982, -0.9948,  0.3518, -1.3131],\n...    [ 0.3180, -0.6993,  1.0436,  0.0438,  0.2270],\n...    [-0.2751,  0.7303,  0.2192,  0.3321,  0.2488],\n...    [ 1.0778, -1.9510,  0.7048,  0.4742, -0.7125]])\n>>> result=flow.median(a, 1)\n>>> result.values\ntensor([-0.3982,  0.2270,  0.2488,  0.4742], dtype=oneflow.float32)\n>>> result.indices\ntensor([1, 4, 4, 3], dtype=oneflow.int64)\n\n\n\n"
    },
    {
        "api_name": "oneflow.mode",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.mode.html",
        "api_signature": "oneflow.mode(input, dim=- 1, keepdim=False)",
        "api_description": "",
        "return_value": "the input tensor in the given dimension dim, i.e. a value which appears most often in\nthat row, and indices is the index location of each mode value found.\nBy default, dim is the last dimension of the input tensor.\nIf keepdim is True, the output tensors are of the same size\nas input except in the dimension dim where they are of size 1.\nOtherwise, dim is squeezed (see flow.squeeze()), resulting in\nthe outputs tensor having 1 fewer dimension than input.\n\nthe result tuple of two output\ntensors (values, indices)\n\n",
        "parameters": "\ninput (Tensor) – the input tensor.\ndim (int) – the dimension to reduce. Default: -1\nkeepdim (bool) – whether the output tensor has dim retained or not. Default: False\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n\n>>> x = flow.tensor([6, 2, 5, 3, 3, 5, 4, 3])\n>>> result = flow.mode(x)\n>>> result.values\ntensor(3, dtype=oneflow.int64)\n>>> result.indices\ntensor(7, dtype=oneflow.int64)\n>>> x = flow.Tensor([[2, 1, 2, 3], [2, 4, 3, 3]])\n>>> result = flow.mode(x, dim=0)\n>>> result.values\ntensor([2., 1., 2., 3.], dtype=oneflow.float32)\n>>> result.indices\ntensor([1, 0, 0, 1], dtype=oneflow.int64)\n\n\n\n"
    },
    {
        "api_name": "oneflow.prod",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.prod.html",
        "api_signature": "oneflow.prod(input, dim=None, keepdim=False)",
        "api_description": "Computes the product of row of elements in a tensor in the given dimension. If the dimension is None, product of all elements will be caculated.\nIf keepdim is True, the output tensor is of the same size as input except in the dimension(s) dim where it is of size 1. Otherwise, dim is squeezed oneflow.squeeze(), resulting in the output tensor having 1 (or len(dim)) fewer dimension(s).",
        "return_value": "",
        "parameters": "\ninput (oneflow.Tensor) – the Input Tensor\ndim (int or tuple of ints, optional) – the dimension to reduce. Default: None\nkeepdim (bool, optional) – whether the output tensor has dim retained or not. Default: False\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n\n>>> input = flow.Tensor([[1, 2, 3], [4, 5, 6]])\n>>> flow.prod(input)\ntensor(720., dtype=oneflow.float32)\n>>> flow.prod(input, dim=0)\ntensor([ 4., 10., 18.], dtype=oneflow.float32)\n>>> flow.prod(input, dim=1)\ntensor([  6., 120.], dtype=oneflow.float32)\n\n\n\n"
    },
    {
        "api_name": "oneflow.nansum",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nansum.html",
        "api_signature": "oneflow.nansum(input, dim, keepdim=False, *, dtype=None)",
        "api_description": "",
        "return_value": "treating Not a Numbers (NaNs) as zero. If dim is a list of dimensions,\nreduce over all of them.\nIf keepdim is True, the output tensor is of the same size as input except\nin the dimension(s) dim where it is of size 1.\nOtherwise, dim is squeezed (see oneflow.squeeze()),\nresulting in the output tensor having 1 (or len(dim)) fewer dimension(s).\nThe interface is consistent with PyTorch.\nThe documentation is referenced from: https://pytorch.org/docs/1.10/generated/torch.nansum.html.\n\n",
        "parameters": "\ninput (oneflow.Tensor) – the Input Tensor\ndim (int, optional) – the dimension to reduce. Default: None\nkeepdim (bool, optional) – whether the output tensor has dim retained or not. Default: False\ndtype (oneflow.dtype, optional) – the desired data type of returned tensor.\nIf specified, the input tensor is casted to dtype before the operation is performed.\nThis is useful for preventing data type overflows. Default: None.\n\n\n\nExample:\n>>> import oneflow as flow\n>>> x = flow.tensor([1., 2., float(\"nan\")])\n>>> flow.nansum(x)\ntensor(3., dtype=oneflow.float32)\n>>> x = flow.tensor([[1., float(\"nan\")], [float(\"nan\"), 2]])\n>>> flow.nansum(x, dim=1)\ntensor([1., 2.], dtype=oneflow.float32)\n>>> x = flow.tensor([float(\"nan\") for i in range(3)])\n>>> flow.nansum(x)\ntensor(0., dtype=oneflow.float32)\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.std",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.std.html",
        "api_signature": "oneflow.std()",
        "api_description": "",
        "return_value": "dimension dim. If dim is a list of dimensions,\nreduce over all of them.\nIf keepdim is True, the output tensor is of the same size as input except in\nthe dimension(s) dim where it is of size 1. Otherwise, dim is squeezed,\nresulting in the output tensor having 1 (or len(dim)) fewer dimension(s).\nIf unbiased is False, then the standard-deviation will be calculated\nvia the biased estimator. Otherwise, Bessel’s correction will be used.\n\n",
        "parameters": "\ninput (Tensor) – the input tensor.\ndim (int or tuple of ints) – the dimension or dimensions to reduce.\nunbiased (bool) – whether to use the unbiased estimation or not\nkeepdim (bool) – whether the output tensor has dim retained or not.\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> import numpy as np\n\n>>> arr = np.array([1.0, 2.0, 3.0])\n>>> input = flow.tensor(arr)\n>>> output = flow.std(input, dim=0).numpy()\n>>> output\narray(1.)\n\n\n\n"
    },
    {
        "api_name": "oneflow.sum",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.sum.html",
        "api_signature": "oneflow.sum(input, dim=None, keepdim=False)",
        "api_description": "Computes the sum of row of elements in a tensor in the given dimension. If the dimension is None, sum of all elements will be caculated.\nIf keepdim is True, the output tensor is of the same size as input except in the dimension(s) dim where it is of size 1. Otherwise, dim is squeezed oneflow.squeeze(), resulting in the output tensor having 1 (or len(dim)) fewer dimension(s).",
        "return_value": "",
        "parameters": "\ninput (oneflow.Tensor) – the Input Tensor\ndim (int or tuple of ints, optional) – the dimension to reduce. Default: None\nkeepdim (bool, optional) – whether the output tensor has dim retained or not. Default: False\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n\n>>> input = flow.Tensor([[1, 2, 3], [4, 5, 6]])\n>>> flow.sum(input)\ntensor(21., dtype=oneflow.float32)\n>>> flow.sum(input, dim=0)\ntensor([5., 7., 9.], dtype=oneflow.float32)\n>>> flow.sum(input, dim=1)\ntensor([ 6., 15.], dtype=oneflow.float32)\n\n\n\n"
    },
    {
        "api_name": "oneflow.logsumexp",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.logsumexp.html",
        "api_signature": "oneflow.logsumexp(input, dim, keepdim=False)",
        "api_description": "",
        "return_value": "tensor in the given dimension dim. The computation is numerically\nstabilized.\nFor summation index \\(j\\) given by dim and other indices \\(i\\), the result is\n\n\\[\\text{logsumexp}(x)_{{i}} = \\log \\sum_j \\exp(x_{{ij}})\\]\nThe interface is consistent with PyTorch.\nThe documentation is referenced from: https://pytorch.org/docs/1.12/generated/torch.logsumexp.html.\n\n",
        "parameters": "\ninput (oneflow.Tensor) – the Input Tensor\ndim (int or tuple of ints) – the dimension or dimensions to reduce.\nkeepdim (bool, optional) – whether the output tensor has dim retained or not. Default: False\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n\n>>> input = flow.Tensor([[1, 2, 3], [4, 5, 6]])\n>>> flow.logsumexp(input, 0)\ntensor([4.0486, 5.0486, 6.0486], dtype=oneflow.float32)\n>>> flow.logsumexp(input, 1)\ntensor([3.4076, 6.4076], dtype=oneflow.float32)\n\n\n\n"
    },
    {
        "api_name": "oneflow.var",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.var.html",
        "api_signature": "oneflow.var()",
        "api_description": "",
        "return_value": "If keepdim is True, the output tensor is of the same size as input except in the dimension(s) dim\nwhere it is of size 1. Otherwise, dim is squeezed (see flow.squeeze()), resulting in the output\ntensor having 1 (or len(dim)) fewer dimension(s).\n\nThe result of variance on the specified axis of input Tensor\n\n",
        "parameters": "\ninput (Tensor) – the input tensor.\ndim (int or tuple of ints) – the dimension or dimensions to reduce. Defaults to None.\nunbiased (bool, optional) – whether to use Bessel’s correction (\\(\\delta N = 1\\)). Defaults to True.\nkeepdim (bool, optional) – whether the output tensor has dim retained or not. Defaults to False.\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import numpy as np\n>>> import oneflow as flow\n\n>>> input = flow.tensor(np.random.randn(2, 3, 4, 5))\n>>> output = flow.var(input, 1, True)\n\n\n\n"
    },
    {
        "api_name": "oneflow.norm",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.norm.html",
        "api_signature": "oneflow.norm(input, p='fro', dim=None, keepdim=False, dtype=None)",
        "api_description": "",
        "return_value": "The documentation is referenced from: https://pytorch.org/docs/1.10/generated/torch.norm.html.\n\nWarning\nUse oneflow.linalg.norm(), instead, or oneflow.linalg.vector_norm()\nwhen computing vector norms and oneflow.linalg.matrix_norm() when\ncomputing matrix norms. Note, however, the signature for these functions\nis slightly different than the signature for oneflow.norm.\n\n\n",
        "parameters": "\ninput (Tensor) – The input tensor. Its data type must be either a floating\npoint or complex type. For complex inputs, the norm is calculated using the\nabsolute value of each element. If the input is complex and neither\ndtype nor out is specified, the result’s data type will\nbe the corresponding floating point type (e.g. float if input is\ncomplexfloat).\np (int, float, inf, -inf, 'fro', 'nuc', optional) – the order of norm. Default: 'fro'\nThe following norms can be calculated:\n\n\n\n\n\n\n\nord\nmatrix norm\nvector norm\n\n\n\n’fro’\nFrobenius norm\n–\n\n‘nuc’\nnuclear norm\n–\n\nNumber\n–\nsum(abs(x)**p)**(1./p)\n\n\n\nThe vector norm can be calculated across any number of dimensions.\nThe corresponding dimensions of input are flattened into\none dimension, and the norm is calculated on the flattened\ndimension.\nFrobenius norm produces the same result as p=2 in all cases\nexcept when dim is a list of three or more dims, in which\ncase Frobenius norm throws an error.\nNuclear norm can only be calculated across exactly two dimensions.\n\ndim (int, tuple of ints, list of ints, optional) – Specifies which dimension or dimensions of input to\ncalculate the norm across. If dim is None, the norm will\nbe calculated across all dimensions of input. If the norm\ntype indicated by p does not support the specified number of\ndimensions, an error will occur.\nkeepdim (bool, optional) – whether the output tensors have dim\nretained or not. Ignored if dim = None and\nout = None. Default: False\ndtype (oneflow.dtype, optional) – the desired data type of\nreturned tensor. If specified, the input tensor is casted to\ndtype while performing the operation. Default: None.\n\n\n\n\n",
        "input_shape": "",
        "notes": "Even though p='fro' supports any number of dimensions, the true\nmathematical definition of Frobenius norm only applies to tensors with\nexactly two dimensions. oneflow.linalg.norm() with ord='fro' aligns\nwith the mathematical definition, since it can only be applied across\nexactly two dimensions.\nExample:\n>>> import oneflow as flow\n>>> a = flow.arange(9, dtype= flow.float) - 4\n>>> b = a.reshape((3, 3))\n>>> flow.norm(a)\ntensor(7.7460, dtype=oneflow.float32)\n>>> flow.norm(b)\ntensor(7.7460, dtype=oneflow.float32)\n>>> flow.norm(a, float('inf'))\ntensor(4., dtype=oneflow.float32)\n>>> flow.norm(b, float('inf'))\ntensor(9., dtype=oneflow.float32)\n>>> c = flow.tensor([[ 1, 2, 3],[-1, 1, 4]] , dtype= flow.float)\n>>> flow.norm(c, dim=0)\ntensor([1.4142, 2.2361, 5.0000], dtype=oneflow.float32)\n>>> flow.norm(c, dim=1)\ntensor([3.7417, 4.2426], dtype=oneflow.float32)\n>>> flow.norm(c, p=1, dim=1)\ntensor([6., 6.], dtype=oneflow.float32)\n>>> d = flow.arange(8, dtype= flow.float).reshape(2,2,2)\n>>> flow.norm(d, dim=(1,2))\ntensor([ 3.7417, 11.2250], dtype=oneflow.float32)\n>>> flow.norm(d[0, :, :]), flow.norm(d[1, :, :])\n(tensor(3.7417, dtype=oneflow.float32), tensor(11.2250, dtype=oneflow.float32))",
        "code_example": ""
    },
    {
        "api_name": "oneflow.all",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.all.html",
        "api_signature": "oneflow.all(input, dim=None, keepdim=False)",
        "api_description": "For each row of input in the given dimension dim, returns True if all element in the row evaluate to True and False otherwise. If the dimension is None, compute if all elements in the input tensor to true.\nIf keepdim is True, the output tensor is of the same size as input except in the dimension(s) dim where it is of size 1. Otherwise, dim is squeezed oneflow.squeeze(), resulting in the output tensor having 1 (or len(dim)) fewer dimension(s).",
        "return_value": "",
        "parameters": "\ninput (oneflow.Tensor) – the Input Tensor\ndim (int, optional) – the dimension to reduce. Default: None\nkeepdim (bool, optional) – whether the output tensor has dim retained or not. Default: False\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n\n>>> input = flow.Tensor([[1, 2, 3], [4, 5, 6]]) < 4\n>>> input\ntensor([[ True,  True,  True],\n        [False, False, False]], dtype=oneflow.bool)\n>>> flow.all(input)\ntensor(False, dtype=oneflow.bool)\n>>> flow.all(input, 1)\ntensor([ True, False], dtype=oneflow.bool)\n>>> flow.all(input, 1, True)\ntensor([[ True],\n        [False]], dtype=oneflow.bool)\n\n\n\n"
    },
    {
        "api_name": "oneflow.argsort",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.argsort.html",
        "api_signature": "oneflow.argsort()",
        "api_description": "This operator sorts the input Tensor at specified dim and returns the indices of the sorted Tensor.",
        "return_value": "The indices of the sorted Tensor.\n\n",
        "parameters": "\ninput (oneflow.Tensor) – the input Tensor.\ndim (int, optional) – the dimension to be sorted. Defaults to the last dim (-1).\ndescending (bool, optional) – controls the sorting order (ascending or descending).\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import numpy as np\n>>> import oneflow as flow\n>>> x = np.array([[10, 2, 9, 3, 7],\n...               [1, 9, 4, 3, 2]]).astype(\"float32\")\n>>> input = flow.Tensor(x)\n>>> output = flow.argsort(input)\n>>> output\ntensor([[1, 3, 4, 2, 0],\n        [0, 4, 3, 2, 1]], dtype=oneflow.int32)\n>>> output = flow.argsort(input, descending=True)\n>>> output\ntensor([[0, 2, 4, 3, 1],\n        [1, 2, 3, 4, 0]], dtype=oneflow.int32)\n>>> output = flow.argsort(input, dim=0)\n>>> output\ntensor([[1, 0, 1, 0, 1],\n        [0, 1, 0, 1, 0]], dtype=oneflow.int32)\n\n\n\n"
    },
    {
        "api_name": "oneflow.eq",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.eq.html",
        "api_signature": "oneflow.eq(input, other)",
        "api_description": "Computes element-wise equality.\nThe second argument can be a number or a tensor whose shape is broadcastable with the first argument.",
        "return_value": "\nA boolean tensor that is True where input is equal to other and False elsewhere\n\n\n\n\n",
        "parameters": "\ninput (oneflow.Tensor) – the tensor to compare\nother (oneflow.Tensor, float or int) – the target to compare\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> import numpy as np\n\n>>> input = flow.tensor(np.array([2, 3, 4, 5]), dtype=flow.float32)\n>>> other = flow.tensor(np.array([2, 3, 4, 1]), dtype=flow.float32)\n\n>>> y = flow.eq(input, other)\n>>> y\ntensor([ True,  True,  True, False], dtype=oneflow.bool)\n\n\n\n"
    },
    {
        "api_name": "oneflow.equal",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.equal.html",
        "api_signature": "oneflow.equal(input, other)",
        "api_description": "True if two tensors have the same size and elements, False otherwise.",
        "return_value": "A boolean value\n\n\n",
        "parameters": "\ninput (oneflow.Tensor) – the tensor to compare\nother (oneflow.Tensor) – the target to compare\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> import numpy as np\n\n>>> input = flow.tensor(np.array([2, 3, 4, 5]), dtype=flow.float32)\n>>> other = flow.tensor(np.array([2, 3, 4, 1]), dtype=flow.float32)\n\n>>> y = flow.equal(input, other)\n>>> y\nFalse\n\n>>> y = flow.equal(input, input)\n>>> y\nTrue\n\n\n\n"
    },
    {
        "api_name": "oneflow.gt",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.gt.html",
        "api_signature": "oneflow.gt()",
        "api_description": "",
        "return_value": "\nA Tensor with bool type.\n\n",
        "parameters": "\ninput (oneflow.Tensor) – A Tensor\nother (oneflow.Tensor) – A Tensor\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import numpy as np\n>>> import oneflow as flow\n\n>>> input1 = flow.tensor(np.random.randn(2, 6, 5, 3), dtype=flow.float32)\n>>> input2 = flow.tensor(np.random.randn(2, 6, 5, 3), dtype=flow.float32)\n\n>>> out = flow.gt(input1, input2).shape\n>>> out\noneflow.Size([2, 6, 5, 3])\n\n\n\n"
    },
    {
        "api_name": "oneflow.isinf",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.isinf.html",
        "api_signature": "oneflow.isinf(input)",
        "api_description": "Tests if each element of input is infinite (positive or negative infinity) or not.",
        "return_value": "A boolean tensor that is True where input is infinite and False elsewhere.\n\n\nExample:\n>>> import oneflow as flow\n>>> flow.isinf(flow.tensor([1, float('inf'), 2, float('-inf'), float('nan')]))\ntensor([False,  True, False,  True, False], dtype=oneflow.bool)\n\n\n\n",
        "parameters": "input (Tensor) – the input tensor.\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.isnan",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.isnan.html",
        "api_signature": "oneflow.isnan(input)",
        "api_description": "",
        "return_value": "\nA boolean tensor that is True where input is NaN and False elsewhere.\n\n\nExample:\n>>> import oneflow as flow\n>>> flow.isnan(flow.tensor([1, float('nan'), 2]))\ntensor([False,  True, False], dtype=oneflow.bool)\n\n\n\n",
        "parameters": "input (Tensor) – the input tensor.\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.le",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.le.html",
        "api_signature": "oneflow.le(input, other)",
        "api_description": "",
        "return_value": "\nA Tensor with bool type.\n\n",
        "parameters": "\ninput (oneflow.Tensor) – A Tensor\nother (oneflow.Tensor) – A Tensor\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import numpy as np\n>>> import oneflow as flow\n\n>>> input1 = flow.tensor(np.array([1, 2, 3]).astype(np.float32), dtype=flow.float32)\n>>> input2 = flow.tensor(np.array([1, 1, 4]).astype(np.float32), dtype=flow.float32)\n\n>>> out = flow.le(input1, input2)\n>>> out\ntensor([ True, False,  True], dtype=oneflow.bool)\n\n\n\n"
    },
    {
        "api_name": "oneflow.lt",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.lt.html",
        "api_signature": "oneflow.lt(input, other)",
        "api_description": "",
        "return_value": "\nA Tensor with bool type.\n\n",
        "parameters": "\ninput (oneflow.Tensor) – A Tensor\nother (oneflow.Tensor) – A Tensor\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import numpy as np\n>>> import oneflow as flow\n\n>>> input1 = flow.tensor(np.array([1, 2, 3]).astype(np.float32), dtype=flow.float32)\n>>> input2 = flow.tensor(np.array([1, 2, 4]).astype(np.float32), dtype=flow.float32)\n\n>>> out = flow.lt(input1, input2)\n>>> out\ntensor([False, False,  True], dtype=oneflow.bool)\n\n\n\n"
    },
    {
        "api_name": "oneflow.ne",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.ne.html",
        "api_signature": "oneflow.ne(input, other)",
        "api_description": "Computes element-wise not equality.\nThe second argument can be a number or a tensor whose shape is broadcastable with the first argument.",
        "return_value": "\nA boolean tensor that is True where input is not equal to other and False elsewhere\n\n\n\n\n",
        "parameters": "\ninput (oneflow.Tensor) – the tensor to compare\nother (oneflow.Tensor, float or int) – the target to compare\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> import numpy as np\n\n>>> input = flow.tensor(np.array([2, 3, 4, 5]), dtype=flow.float32)\n>>> other = flow.tensor(np.array([2, 3, 4, 1]), dtype=flow.float32)\n\n>>> y = flow.ne(input, other)\n>>> y\ntensor([False, False, False,  True], dtype=oneflow.bool)\n\n\n\n"
    },
    {
        "api_name": "oneflow.sort",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.sort.html",
        "api_signature": "oneflow.sort()",
        "api_description": "Sorts the elements of the input tensor along a given dimension in ascending order by value.",
        "return_value": "A tuple of (values, indices), where\nwhere the values are the sorted values and the indices are the indices of the elements\nin the original input tensor.\n\n",
        "parameters": "\ninput (oneflow.Tensor) – the input Tensor.\ndim (int, optional) – the dimension to be sorted. Defaults to the last dim (-1).\ndescending (bool, optional) – controls the sorting order (ascending or descending).\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> import numpy as np\n>>> x = np.array([[1, 3, 8, 7, 2], [1, 9, 4, 3, 2]], dtype=np.float32)\n>>> input = flow.Tensor(x)\n>>> result = flow.sort(input)\n>>> result.values\ntensor([[1., 2., 3., 7., 8.],\n        [1., 2., 3., 4., 9.]], dtype=oneflow.float32)\n>>> result.indices\ntensor([[0, 4, 1, 3, 2],\n        [0, 4, 3, 2, 1]], dtype=oneflow.int32)\n>>> result = flow.sort(input, descending=True)\n>>> result.values\ntensor([[8., 7., 3., 2., 1.],\n        [9., 4., 3., 2., 1.]], dtype=oneflow.float32)\n>>> result.indices\ntensor([[2, 3, 1, 4, 0],\n        [1, 2, 3, 4, 0]], dtype=oneflow.int32)\n>>> result = flow.sort(input, dim=0)\n>>> result.values\ntensor([[1., 3., 4., 3., 2.],\n        [1., 9., 8., 7., 2.]], dtype=oneflow.float32)\n>>> result.indices\ntensor([[0, 0, 1, 1, 0],\n        [1, 1, 0, 0, 1]], dtype=oneflow.int32)\n\n\n\n"
    },
    {
        "api_name": "oneflow.topk",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.topk.html",
        "api_signature": "oneflow.topk()",
        "api_description": "Finds the values and indices of the k largest entries at specified axis.",
        "return_value": "A tuple of (values, indices), where\nthe indices are the indices of the elements in the original input tensor.\n\n",
        "parameters": "\ninput (oneflow.Tensor) – Input Tensor\nk (int) – the k in “top-k”\ndim (int, optional) – the dimension to sort along. Defaults to the last dim (-1)\nlargest (bool, optional) – controls whether to return largest or smallest elements\nsorted (bool, optional) – controls whether to return the elements in sorted order (Only Support True Now!)\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> import numpy as np\n>>> x = np.array([[1, 3, 8, 7, 2], [1, 9, 4, 3, 2]], dtype=np.float32)\n>>> result = flow.topk(flow.Tensor(x), k=3, dim=1)\n>>> result.values\ntensor([[8., 7., 3.],\n        [9., 4., 3.]], dtype=oneflow.float32)\n>>> result.indices\ntensor([[2, 3, 1],\n        [1, 2, 3]], dtype=oneflow.int64)\n>>> result.values.shape\noneflow.Size([2, 3])\n>>> result.indices.shape\noneflow.Size([2, 3])\n>>> result = flow.topk(flow.Tensor(x), k=2, dim=1, largest=False)\n>>> result.values\ntensor([[1., 2.],\n        [1., 2.]], dtype=oneflow.float32)\n>>> result.indices\ntensor([[0, 4],\n        [0, 4]], dtype=oneflow.int64)\n>>> result.values.shape\noneflow.Size([2, 2])\n>>> result.indices.shape\noneflow.Size([2, 2])\n\n\n\n"
    },
    {
        "api_name": "oneflow.ge",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.ge.html",
        "api_signature": "oneflow.ge()",
        "api_description": "",
        "return_value": "\nA Tensor with bool type.\n\n",
        "parameters": "\ninput (oneflow.Tensor) – A Tensor\nother (oneflow.Tensor) – A Tensor\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import numpy as np\n>>> import oneflow as flow\n\n>>> input1 = flow.tensor(np.array([1, 2, 3]).astype(np.float32), dtype=flow.float32)\n>>> input2 = flow.tensor(np.array([1, 1, 4]).astype(np.float32), dtype=flow.float32)\n\n>>> out = flow.ge(input1, input2)\n>>> out\ntensor([ True,  True, False], dtype=oneflow.bool)\n\n\n\n"
    },
    {
        "api_name": "oneflow.greater",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.greater.html",
        "api_signature": "oneflow.greater()",
        "api_description": "",
        "return_value": "\nA Tensor with bool type.\n\n",
        "parameters": "\ninput (oneflow.Tensor) – A Tensor\nother (oneflow.Tensor) – A Tensor\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import numpy as np\n>>> import oneflow as flow\n\n>>> input1 = flow.tensor(np.random.randn(2, 6, 5, 3), dtype=flow.float32)\n>>> input2 = flow.tensor(np.random.randn(2, 6, 5, 3), dtype=flow.float32)\n\n>>> out = flow.gt(input1, input2).shape\n>>> out\noneflow.Size([2, 6, 5, 3])\n\n\n\n"
    },
    {
        "api_name": "oneflow.greater_equal",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.greater_equal.html",
        "api_signature": "oneflow.greater_equal()",
        "api_description": "",
        "return_value": "\nA Tensor with bool type.\n\n",
        "parameters": "\ninput (oneflow.Tensor) – A Tensor\nother (oneflow.Tensor) – A Tensor\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import numpy as np\n>>> import oneflow as flow\n\n>>> input1 = flow.tensor(np.array([1, 2, 3]).astype(np.float32), dtype=flow.float32)\n>>> input2 = flow.tensor(np.array([1, 1, 4]).astype(np.float32), dtype=flow.float32)\n\n>>> out = flow.ge(input1, input2)\n>>> out\ntensor([ True,  True, False], dtype=oneflow.bool)\n\n\n\n"
    },
    {
        "api_name": "oneflow.maximum",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.maximum.html",
        "api_signature": "oneflow.maximum()",
        "api_description": "Computes the element-wise maximum of x and y.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n\n>>> x = flow.tensor((1, 2, -1), dtype=flow.float32)\n>>> y = flow.tensor((3, 0, 4), dtype=flow.float32)\n>>> flow.maximum(x, y)\ntensor([3., 2., 4.], dtype=oneflow.float32)\n\n>>> x = flow.tensor((1,), dtype=flow.float32)\n>>> y = flow.tensor((3, 0, 4), dtype=flow.float32)\n>>> flow.maximum(x, y)\ntensor([3., 1., 4.], dtype=oneflow.float32)\n\n\n\n"
    },
    {
        "api_name": "oneflow.minimum",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.minimum.html",
        "api_signature": "oneflow.minimum()",
        "api_description": "Computes the element-wise minimum of x and y.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n\n>>> x = flow.tensor((1, 2, -1), dtype=flow.float32)\n>>> y = flow.tensor((3, 0, 4), dtype=flow.float32)\n>>> flow.minimum(x, y)\ntensor([ 1.,  0., -1.], dtype=oneflow.float32)\n\n>>> x = flow.tensor((1,), dtype=flow.float32)\n>>> y = flow.tensor((3, 0, 4), dtype=flow.float32)\n>>> flow.minimum(x, y)\ntensor([1., 0., 1.], dtype=oneflow.float32)\n\n\n\n"
    },
    {
        "api_name": "oneflow.not_equal",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.not_equal.html",
        "api_signature": "oneflow.not_equal()",
        "api_description": "ne(input, other) -> Tensor\nComputes element-wise not equality.\nThe second argument can be a number or a tensor whose shape is broadcastable with the first argument.",
        "return_value": "\nA boolean tensor that is True where input is not equal to other and False elsewhere\n\n\n\n\n",
        "parameters": "\ninput (oneflow.Tensor) – the tensor to compare\nother (oneflow.Tensor, float or int) – the target to compare\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> import numpy as np\n\n>>> input = flow.tensor(np.array([2, 3, 4, 5]), dtype=flow.float32)\n>>> other = flow.tensor(np.array([2, 3, 4, 1]), dtype=flow.float32)\n\n>>> y = flow.ne(input, other)\n>>> y\ntensor([False, False, False,  True], dtype=oneflow.bool)\n\n\n\n"
    },
    {
        "api_name": "oneflow.isclose",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.isclose.html",
        "api_signature": "oneflow.isclose(input, other, atol=1e-08, rtol=1e-05, equal_nan=False)",
        "api_description": "",
        "return_value": "input is “close” to the corresponding element of other.\nCloseness is defined as:\n\n\\[\\lvert \\text{input} - \\text{other} \\rvert \\leq \\texttt{atol} + \\texttt{rtol} \\times \\lvert \\text{other} \\rvert\\]\n\nA Tensor with bool type.\n\n",
        "parameters": "\ninput (oneflow.Tensor) – first tensor to compare\nother (oneflow.Tensor) – second tensor to compare\natol (float, optional) – absolute tolerance. Default: 1e-08\nrtol (float, optional) – relative tolerance. Default: 1e-05\nequal_nan (bool, optional) – if True, then two NaN s will be considered equal. Default: False\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n\n>>> flow.isclose(flow.tensor((1., 2, 3)), flow.tensor((1 + 1e-10, 3, 4)))\ntensor([ True, False, False], dtype=oneflow.bool)\n\n>>> flow.isclose(flow.tensor((float('inf'), 4)), flow.tensor((float('inf'), 6)), rtol=.5)\ntensor([True, True], dtype=oneflow.bool)\n\n\n\n"
    },
    {
        "api_name": "oneflow.allclose",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.allclose.html",
        "api_signature": "oneflow.allclose(input, other, rtol=1e-05, atol=1e-08, equal_nan=False)",
        "api_description": "This function checks if input and other satisfy the condition:\n\\[\\lvert \\text{input} - \\text{other} \\rvert \\leq \\texttt{atol} + \\texttt{rtol} \\times \\lvert \\text{other} \\rvert\\]\nelementwise, for all elements of input and other. The behaviour of this function is analogous to\nnumpy.allclose",
        "return_value": "A Tensor with bool type.\n\n",
        "parameters": "\ninput (oneflow.Tensor) – first tensor to compare\nother (oneflow.Tensor) – second tensor to compare\natol (float, optional) – absolute tolerance. Default: 1e-08\nrtol (float, optional) – relative tolerance. Default: 1e-05\nequal_nan (bool, optional) – if True, then two NaN s will be considered equal. Default: False\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n\n>>> flow.allclose(flow.tensor([10000., 1e-07]), flow.tensor([10000.1, 1e-08]))\nFalse\n>>> flow.allclose(flow.tensor([10000., 1e-08]), flow.tensor([10000.1, 1e-09]))\nTrue\n>>> flow.allclose(flow.tensor([1.0, float('nan')]), flow.tensor([1.0, float('nan')]))\nFalse\n>>> flow.allclose(flow.tensor([1.0, float('nan')]), flow.tensor([1.0, float('nan')]), equal_nan=True)\nTrue\n\n\n\n"
    },
    {
        "api_name": "oneflow.hann_window",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.hann_window.html",
        "api_signature": "oneflow.hann_window(window_length, periodic=True, *, device=None, placement=None, sbp=None, dtype=None, requires_grad=False)",
        "api_description": "Hann window function.\n\\[w[n] = \\frac{1}{2}\\ \\left[1 - \\cos \\left( \\frac{2 \\pi n}{N - 1} \\right)\\right] =\n\\sin^2 \\left( \\frac{\\pi n}{N - 1} \\right),\\]\nwhere \\(N\\) is the full window size.\nThe input window_length is a positive integer controlling the\nreturned window size. periodic flag determines whether the returned\nwindow trims off the last duplicate value from the symmetric window. Therefore, if periodic is true, the \\(N\\) in\nabove formula is in fact \\(\\text{window_length} + 1\\). Also, we always have\noneflow.hann_window(L, periodic=True) equal to\noneflow.hann_window(L + 1, periodic=False)[:-1]).",
        "return_value": "A 1-D tensor of size \\((\\text{{window_length}},)\\) containing the window\n\n",
        "parameters": "\nwindow_length (int) – the size of returned window\nperiodic (bool, optional) – If True, returns a window to be used as periodic\nfunction. If False, return a symmetric window.\n\n\nKeyword Arguments\n\ndtype (oneflow.dtype, optional) – the data type to perform the computation in.\nDefault: if None, uses the global default dtype (see oneflow.get_default_dtype())\nwhen both start and end are real,\nand corresponding complex dtype when either is complex.\ndevice (oneflow.device, optional) – the desired device of returned tensor. Default: if None, uses the current device for the default tensor type\nplacement (oneflow.placement, optional) – the desired placement of returned global tensor. Default: if None, the returned tensor is local one using the argument device.\nsbp (oneflow.sbp.sbp or tuple of oneflow.sbp.sbp, optional) – the desired sbp descriptor of returned global tensor. Default: if None, the returned tensor is local one using the argument device.\nrequires_grad (bool, optional) – If autograd should record operations on the returned tensor. Default: False.\n\n\n",
        "input_shape": "",
        "notes": "If window_length \\(=1\\), the returned window contains a single value 1.",
        "code_example": ""
    },
    {
        "api_name": "oneflow.adaptive_avg_pool1d",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.adaptive_avg_pool1d.html",
        "api_signature": "oneflow.adaptive_avg_pool1d(input, output_size)",
        "api_description": "Applies a 1D adaptive average pooling over an input signal composed of several input planes.\nSee oneflow.nn.AdaptiveAvgPool1d",
        "return_value": "",
        "parameters": "\ninput – input tensor\noutput_size – the target output size (single integer)\n\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.adaptive_avg_pool2d",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.adaptive_avg_pool2d.html",
        "api_signature": "oneflow.adaptive_avg_pool2d(input, output_size, data_format=None)",
        "api_description": "Applies a 2D adaptive average pooling over an input signal composed of several input planes.\nSee oneflow.nn.AdaptiveAvgPool2d",
        "return_value": "",
        "parameters": "\ninput – input tensor\noutput_size – the target output size (single integer or double-integer tuple)\n\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.adaptive_avg_pool3d",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.adaptive_avg_pool3d.html",
        "api_signature": "oneflow.adaptive_avg_pool3d(input, output_size)",
        "api_description": "Applies a 3D adaptive average pooling over an input signal composed of several input planes.\nSee oneflow.nn.AdaptiveAvgPool3d",
        "return_value": "",
        "parameters": "\ninput – input tensor\noutput_size – the target output size (single integer or triple-integer tuple)\n\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.broadcast_like",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.broadcast_like.html",
        "api_signature": "oneflow.broadcast_like()",
        "api_description": "This operator broadcast tensor x to like_tensor according to the broadcast_axes.",
        "return_value": "Broadcasted input Tensor.\n\n",
        "parameters": "\nx (Tensor) – The input Tensor.\nlike_tensor (Tensor) – The like Tensor.\nbroadcast_axes (Optional[Sequence], optional) – The axes you want to broadcast. Defaults to None.\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n\n>>> x = flow.randn(3, 1, 1)\n>>> like_tensor = flow.randn(3, 4, 5)\n>>> broadcast_tensor = flow.broadcast_like(x, like_tensor, broadcast_axes=[1, 2])\n>>> broadcast_tensor.shape\noneflow.Size([3, 4, 5])\n\n\n\n"
    },
    {
        "api_name": "oneflow.cast",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.cast.html",
        "api_signature": "oneflow.cast()",
        "api_description": "The operation takes input tensor x and casts it to the output with dtype",
        "return_value": "A Tensor with specific dtype.\n\n",
        "parameters": "\nx (oneflow.Tensor) – A Tensor\ndtype (flow.dtype) – Data type of the output tensor\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> import numpy as np\n>>> np_arr = np.random.randn(2, 3, 4, 5).astype(np.float32)\n>>> input = flow.tensor(np_arr, dtype=flow.float32)\n>>> output = flow.cast(input, flow.int8)\n>>> np.array_equal(output.numpy(), np_arr.astype(np.int8))\nTrue\n\n\n\n"
    },
    {
        "api_name": "oneflow.cumprod",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.cumprod.html",
        "api_signature": "oneflow.cumprod(input, dim)",
        "api_description": "This operator computes the cumulative product of input elements in the given dimension.\nThe equation is:\n$$\ny_{i}=x_{0}*x_{1}*…*x_{i}\n$$",
        "return_value": "The result tensor with cumprod result.\n\n",
        "parameters": "\ninput (Tensor) – the input tensor.\ndim (int) – the dimension to do cumsum whose valid range is [-N, N-1), and the N is tensor’s dimensions\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> input=flow.tensor([1, 2, 3])\n>>> flow.cumprod(input, dim=0)\ntensor([1, 2, 6], dtype=oneflow.int64)\n\n\n\n"
    },
    {
        "api_name": "oneflow.cumsum",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.cumsum.html",
        "api_signature": "oneflow.cumsum(input, dim)",
        "api_description": "This operator computes the cumulative sum of input elements in the given dimension.\nThe equation is:\n$$\ny_{i}=x_{0}+x_{1}+…+x_{i}\n$$",
        "return_value": "The result tensor with cumsum result.\n\n",
        "parameters": "\ninput (Tensor) – the input ND tensor.\ndim (int) – the dimension to do cumsum, valid range is [-N, N-1), N is tensor’s dimensions\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> input = flow.ones(3, 3)\n>>> dim = 1\n>>> flow.cumsum(input, dim)\ntensor([[1., 2., 3.],\n        [1., 2., 3.],\n        [1., 2., 3.]], dtype=oneflow.float32)\n\n\n\n"
    },
    {
        "api_name": "oneflow.diag",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.diag.html",
        "api_signature": "oneflow.diag()",
        "api_description": "If input is a vector (1-D tensor), then returns a 2-D square tensor with the elements of input as the diagonal.\nIf input is a matrix (2-D tensor), then returns a 1-D tensor with diagonal elements of input.",
        "return_value": "the output Tensor.\n\n",
        "parameters": "\ninput (Tensor) – the input tensor.\ndiagonal (Optional[int], 0) – The diagonal to consider.\nIf diagonal = 0, it is the main diagonal. If diagonal > 0, it is above the main diagonal. If diagonal < 0, it is below the main diagonal. Defaults to 0.\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> import numpy as np\n>>> arr = np.array(\n...     [\n...        [1.0, 2.0, 3.0],\n...        [4.0, 5.0, 6.0],\n...        [7.0, 8.0, 9.0],\n...     ]\n... )\n\n>>> input = flow.tensor(arr, dtype=flow.float32)\n>>> flow.diag(input)\ntensor([1., 5., 9.], dtype=oneflow.float32)\n\n\n\n"
    },
    {
        "api_name": "oneflow.diagonal",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.diagonal.html",
        "api_signature": "oneflow.diagonal(input, offset, dim1, dim2)",
        "api_description": "",
        "return_value": "appended as a dimension at the end of the shape.\n\nthe output Tensor.\n\n",
        "parameters": "\ninput (Tensor) – the input tensor.Must be at least 2-dimensional.\noffset (Optional[int], 0) – which diagonal to consider. Default: 0 (main diagonal)\ndim1 (Optional[int], 0) – first dimension with respect to which to take diagonal. Default: 0\ndim2 (Optional[int], 1) – second dimension with respect to which to take diagonal. Default: 1\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n\n>>> input = flow.randn(2,  3,  4)\n>>> output = flow.diagonal(input, offset=1, dim1=1, dim2=0)\n>>> output.shape\noneflow.Size([4, 1])\n\n\n\n"
    },
    {
        "api_name": "oneflow.einsum",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.einsum.html",
        "api_signature": "oneflow.einsum(equation, *operands)",
        "api_description": "Sums the product of the elements of the input operands along dimensions specified using a notation\nbased on the Einstein summation convention.\nEinsum allows computing many common multi-dimensional linear algebraic array operations by representing them\nin a short-hand format based on the Einstein summation convention, given by equation. The details of\nthis format are described below, but the general idea is to label every dimension of the input operands\nwith some subscript and define which subscripts are part of the output. The output is then computed by summing\nthe product of the elements of the operands along the dimensions whose subscripts are not part of the\noutput. For example, matrix multiplication can be computed using einsum as flow.einsum(“ij,jk->ik”, A, B).\nHere, j is the summation subscript and i and k the output subscripts (see section below for more details on why).\nEquation:\nThe equation string specifies the subscripts (letters in [a-zA-Z]) for each dimension of\nthe input operands in the same order as the dimensions, separating subcripts for each operand by a\ncomma (‘,’), e.g. ‘ij,jk’ specify subscripts for two 2D operands. The dimensions labeled with the same subscript\nmust be broadcastable, that is, their size must either match or be 1. The exception is if a subscript is\nrepeated for the same input operand, in which case the dimensions labeled with this subscript for this operand\nmust match in size and the operand will be replaced by its diagonal along these dimensions. The subscripts that\nappear exactly once in the equation will be part of the output, sorted in increasing alphabetical order.\nThe output is computed by multiplying the input operands element-wise, with their dimensions aligned based\non the subscripts, and then summing out the dimensions whose subscripts are not part of the output.\nOptionally, the output subscripts can be explicitly defined by adding an arrow (‘->’) at the end of the equation\nfollowed by the subscripts for the output. For instance, the following equation computes the transpose of a\nmatrix multiplication: ‘ij,jk->ki’. The output subscripts must appear at least once for some input operand and\nat most once for the output.\nEllipsis (‘…’) can be used in place of subscripts to broadcast the dimensions covered by the ellipsis.\nEach input operand may contain at most one ellipsis which will cover the dimensions not covered by subscripts,\ne.g. for an input operand with 5 dimensions, the ellipsis in the equation ‘ab…c’ cover the third and fourth\ndimensions. The ellipsis does not need to cover the same number of dimensions across the operands but the\n‘shape’ of the ellipsis (the size of the dimensions covered by them) must broadcast together. If the output is not\nexplicitly defined with the arrow (‘->’) notation, the ellipsis will come first in the output (left-most dimensions),\nbefore the subscript labels that appear exactly once for the input operands. e.g. the following equation implements\nbatch matrix multiplication ‘…ij,…jk’.\nA few final notes: the equation may contain whitespaces between the different elements (subscripts, ellipsis,\narrow and comma) but something like ‘…’ is not valid. An empty string ‘’ is valid for scalar operands.",
        "return_value": "",
        "parameters": "\nequation (String) – The subscripts for the Einstein summation.\n*operands (oneflow.Tensor) – The tensors to compute the Einstein summation of.\n\n\n\n",
        "input_shape": "",
        "notes": "flow.einsum handles ellipsis (‘…’) differently from NumPy in that it allows dimensions\ncovered by the ellipsis to be summed over, that is, ellipsis are not required to be part of the output.\nThis function does not optimize the given expression, so a different formula for the same computation may\nrun faster or consume less memory. Projects like opt_einsum (https://optimized-einsum.readthedocs.io/en/stable/)\ncan optimize the formula for you.",
        "code_example": ">>> import oneflow as flow\n\n# trace\n>>> flow.einsum('ii', flow.arange(4*4).reshape(4,4).to(flow.float32))\ntensor(30., dtype=oneflow.float32)\n\n# diagonal\n>>> flow.einsum('ii->i', flow.arange(4*4).reshape(4,4).to(flow.float32))\ntensor([ 0.,  5., 10., 15.], dtype=oneflow.float32)\n\n# outer product\n>>> x = flow.arange(5).to(flow.float32)\n>>> y = flow.arange(4).to(flow.float32)\n>>> flow.einsum('i,j->ij', x, y)\ntensor([[ 0.,  0.,  0.,  0.],\n        [ 0.,  1.,  2.,  3.],\n        [ 0.,  2.,  4.,  6.],\n        [ 0.,  3.,  6.,  9.],\n        [ 0.,  4.,  8., 12.]], dtype=oneflow.float32)\n\n# batch matrix multiplication\n>>> As = flow.arange(3*2*5).reshape(3,2,5).to(flow.float32)\n>>> Bs = flow.arange(3*5*4).reshape(3,5,4).to(flow.float32)\n>>> flow.einsum('bij,bjk->bik', As, Bs).shape\noneflow.Size([3, 2, 4])\n\n# batch permute\n>>> A = flow.randn(2, 3, 4, 5)\n>>> flow.einsum('...ij->...ji', A).shape\noneflow.Size([2, 3, 5, 4])\n\n# bilinear\n>>> A = flow.randn(3,5,4)\n>>> l = flow.randn(2,5)\n>>> r = flow.randn(2,4)\n>>> flow.einsum('bn,anm,bm->ba', l, A, r).shape\noneflow.Size([2, 3])\n\n\n\n"
    },
    {
        "api_name": "oneflow.flatten",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.flatten.html",
        "api_signature": "oneflow.flatten()",
        "api_description": "Flattens a contiguous range of dims into a tensor.",
        "return_value": "",
        "parameters": "\nstart_dim – first dim to flatten (default = 0).\nend_dim – last dim to flatten (default = -1).\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import numpy as np\n>>> import oneflow as flow\n>>> input = flow.randn(32, 1, 5, 5)\n>>> output = flow.flatten(input, start_dim=1)\n>>> output.shape\noneflow.Size([32, 25])\n\n\n\n"
    },
    {
        "api_name": "oneflow.flip",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.flip.html",
        "api_signature": "oneflow.flip(input, dims)",
        "api_description": "Reverse the order of a n-D tensor along given axis in dims.",
        "return_value": "",
        "parameters": "\ninput (Tensor) – the input tensor\ndims (a list or tuple) – axis to flip on\n\n\n\n",
        "input_shape": "",
        "notes": "flow.flip makes a copy of input’s data. This is different from NumPy’s np.flip,\nwhich returns a view in constant time. Since copying a tensor’s data is more work than viewing that data,\nflow.flip is expected to be slower than np.flip.",
        "code_example": ">>> import oneflow as flow\n>>> import numpy as np\n\n>>> np_arr = np.arange(0, 8).reshape((2, 2, 2)).astype(np.float32)\n>>> input = flow.Tensor(np_arr)\n>>> input.shape\noneflow.Size([2, 2, 2])\n>>> out = flow.flip(input, [0, 1])\n>>> out\ntensor([[[6., 7.],\n         [4., 5.]],\n\n        [[2., 3.],\n         [0., 1.]]], dtype=oneflow.float32)\n\n\n\n"
    },
    {
        "api_name": "oneflow.in_top_k",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.in_top_k.html",
        "api_signature": "oneflow.in_top_k(targets, predictions, k)",
        "api_description": "Says whether the targets are in the top K predictions.",
        "return_value": "A Tensor of type bool. Computed Precision at k as a bool Tensor.\n\n",
        "parameters": "\ntargets (Tensor) – the target tensor of type int32 or int64.\npredictions (Tensor) – the predictions tensor of type float32 .\nk (int) – Number of top elements to look at for computing precision.\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> import numpy as np\n>>> targets1 = flow.tensor(np.array([3, 1]), dtype=flow.int32)\n>>> predictions1 = flow.tensor(np.array([[0.0, 1.0, 2.0, 3.0], [3.0, 2.0, 1.0, 0.0],]), dtype=flow.float32)\n>>> out1 = flow.in_top_k(targets1, predictions1, k=1)\n>>> out1\ntensor([ True, False], dtype=oneflow.bool)\n>>> out2 = flow.in_top_k(targets1, predictions1, k=2)\n>>> out2\ntensor([True, True], dtype=oneflow.bool)\n>>> targets2 = flow.tensor(np.array([3, 1]), dtype=flow.int32, device=flow.device('cuda'))\n>>> predictions2 = flow.tensor(np.array([[0.0, 1.0, 2.0, 3.0], [3.0, 2.0, 1.0, 0.0],]), dtype=flow.float32, device=flow.device('cuda'))\n>>> out3 = flow.in_top_k(targets2, predictions2, k=1)\n>>> out3\ntensor([ True, False], device='cuda:0', dtype=oneflow.bool)\n\n\n\n"
    },
    {
        "api_name": "oneflow.meshgrid",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.meshgrid.html",
        "api_signature": "oneflow.meshgrid(*tensors, indexing='ij')",
        "api_description": "Take \\(N\\) tensors, each of which can be either scalar or 1-dimensional\nvector, and create \\(N\\) N-dimensional grids, where the \\(i\\) th grid is defined by\nexpanding the \\(i\\) th input over dimensions defined by other inputs.\nThe documentation is referenced from:",
        "return_value": "If the input has \\(k\\) tensors of size\n\\((N_1,), (N_2,), \\ldots , (N_k,)\\), then the output would also have \\(k\\) tensors,\nwhere all tensors are of size \\((N_1, N_2, \\ldots , N_k)\\).\n\n",
        "parameters": "\ntensors (list of Tensor) – list of scalars or 1 dimensional tensors. Scalars will be\ntreated as tensors of size \\((1,)\\) automatically.\nindexing ((string, optional) – the indexing mode, either “xy” or “ij”, defaults to “ij”.\nIf “ij” is selected, the dimensions are in the same order as the cardinality of the inputs.\nIf “xy” is selected, the first dimension corresponds to the cardinality of\nthe second input and the second dimension corresponds to the cardinality of the first input.\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import numpy as np\n>>> import oneflow as flow\n\n>>> input1 = flow.tensor(np.array([2, 2, 3]), dtype=flow.float32)\n>>> input2 = flow.tensor(np.array([4, 5, 6]), dtype=flow.float32)\n>>> of_x, of_y = flow.meshgrid(input1, input2)\n>>> of_x\ntensor([[2., 2., 2.],\n        [2., 2., 2.],\n        [3., 3., 3.]], dtype=oneflow.float32)\n>>> of_y\ntensor([[4., 5., 6.],\n        [4., 5., 6.],\n        [4., 5., 6.]], dtype=oneflow.float32)\n\n\n\n"
    },
    {
        "api_name": "oneflow.nms",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nms.html",
        "api_signature": "oneflow.nms(boxes, scores, iou_threshold: float)",
        "api_description": "Performs non-maximum suppression (NMS) on the boxes according\nto their intersection-over-union (IoU).\nNMS iteratively removes lower scoring boxes which have an\nIoU greater than iou_threshold with another (higher scoring)\nbox.",
        "return_value": "int64 tensor with the indices of the elements that have been kept by NMS, sorted in decreasing order of scores\n\n",
        "parameters": "\nboxes (Tensor[N, 4]) – boxes to perform NMS on. They\nare expected to be in (x1, y1, x2, y2) format with 0 <= x1 < x2 and\n0 <= y1 < y2.\nscores (Tensor[N]) – scores for each one of the boxes\niou_threshold (float) – discards all overlapping boxes with IoU > iou_threshold\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.roc_auc_score",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.roc_auc_score.html",
        "api_signature": "oneflow.roc_auc_score()",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.roll",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.roll.html",
        "api_signature": "oneflow.roll(input, shifts, dims=None)",
        "api_description": "Roll the tensor along the given dimension(s).\nElements that are shifted beyond the last position are re-introduced at the first position.\nIf a dimension is not specified, the tensor will be flattened before rolling and then restored to the original shape.",
        "return_value": "The result Tensor.\n\n",
        "parameters": "\ninput (oneflow.Tensor) – the input Tensor.\nshifts (int or tuple of ints) – The number of places by which the elements of the tensor are shifted.\nIf shifts is a tuple, dims must be a tuple of the same size,\nand each dimension will be rolled by the corresponding value.\ndims (int or tuple of ints) – Axis along which to roll.\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> import numpy as np\n>>> x = np.array([[1, 2],\n...               [3, 4],\n...               [5, 6],\n...               [7, 8]])\n>>> input = flow.Tensor(x)\n>>> input.shape\noneflow.Size([4, 2])\n>>> out = flow.roll(input, 1, 0)\n>>> out\ntensor([[7., 8.],\n        [1., 2.],\n        [3., 4.],\n        [5., 6.]], dtype=oneflow.float32)\n>>> input.roll(-1, 1)\ntensor([[2., 1.],\n        [4., 3.],\n        [6., 5.],\n        [8., 7.]], dtype=oneflow.float32)\n\n\n\n"
    },
    {
        "api_name": "oneflow.searchsorted",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.searchsorted.html",
        "api_signature": "oneflow.searchsorted()",
        "api_description": "Find the indices from the innermost dimension of sorted_sequence such that, if the corresponding values\nin values were inserted before the indices, the order of the corresponding innermost dimension within\nsorted_sequence would be preserved. Return a new tensor with the same size as values. If right is False\n(default), then the left boundary of sorted_sequence is closed. More formally, the returned index\nsatisfies the following rules:\nsorted_sequence\nright\nreturned index satisfies\n1-D\nFalse\nsorted_sequence[i-1] < values[m][n]…[l][x] <= sorted_sequence[i]\n1-D\nTrue\nsorted_sequence[i-1] <= values[m][n]…[l][x] < sorted_sequence[i]\nN-D\nFalse\nsorted_sequence[m][n]…[l][i-1] < values[m][n]…[l][x]<= sorted_sequence[m][n]…[l][i]\nN-D\nTrue\nsorted_sequence[m][n]…[l][i-1] <= values[m][n]…[l][x]sorted_sequence[m][n]…[l][i]",
        "return_value": "",
        "parameters": "\nsorted_sequence (Tensor) – N-D or 1-D tensor, containing monotonically increasing sequence on the\ninnermost dimension.\nvalues (Tensor or Scalar) – N-D tensor or a Scalar containing the search value(s).\nout_int32 (bool optional) – indicate the output data type. torch.int32 if True, torch.int64 otherwise.\nDefault value is False, i.e. default output data type is torch.int64.\nright (bool optional) – if False, return the first suitable location that is found. If True, return the\nlast such index. If no suitable index found, return 0 for non-numerical value\n(eg. nan, inf) or the size of innermost dimension within sorted_sequence (one\npass the last index of the innermost dimension). In other words, if False, gets\nthe lower bound index for each value in values on the corresponding innermost\ndimension of the sorted_sequence. If True, gets the upper bound index instead.\nDefault value is False.\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> sorted_sequence = flow.tensor([[1, 3, 5, 7, 9], [2, 4, 6, 8, 10]])\n>>> sorted_sequence\ntensor([[ 1,  3,  5,  7,  9],\n        [ 2,  4,  6,  8, 10]], dtype=oneflow.int64)\n>>> values = flow.tensor([[3, 6, 9], [3, 6, 9]])\n>>> values\ntensor([[3, 6, 9],\n        [3, 6, 9]], dtype=oneflow.int64)\n>>> flow.searchsorted(sorted_sequence, values)\ntensor([[1, 3, 4],\n        [1, 2, 4]], dtype=oneflow.int64)\n>>> flow.searchsorted(sorted_sequence, values, right=True)\ntensor([[2, 3, 5],\n        [1, 3, 4]], dtype=oneflow.int64)\n>>> sorted_sequence_1d = flow.tensor([1, 3, 5, 7, 9])\n>>> sorted_sequence_1d\ntensor([1, 3, 5, 7, 9], dtype=oneflow.int64)\n>>> flow.searchsorted(sorted_sequence_1d, values)\ntensor([[1, 3, 4],\n        [1, 3, 4]], dtype=oneflow.int64)\n\n\n\n"
    },
    {
        "api_name": "oneflow.tensordot",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.tensordot.html",
        "api_signature": "oneflow.tensordot(a, b, dims=Union[int, Tensor, Tuple[List[int], List[int]], List[List[int]]], out=None)",
        "api_description": "Compute tensor dot along given dimensions.\nGiven two tensors a and b, and dims which represent two lists containing dim indices, tensordot traverses the two\nlists and calculate the tensor dot along every dim pair.",
        "return_value": "The result tensor\n\n",
        "parameters": "\na (oneflow.Tensor) – The input tensor to compute tensordot\nb (oneflow.Tensor) – The input tensor to compute tensordot\ndims (int or list or tuple or oneflow.Tensor) – The dims to calculate tensordot.\nIf it’s an integer or oneflow.Tensor with only one element,\nthe last dims of tensor a and the first dims of tensor b will be calculated.\nIf it’s a list or tuple or oneflow.Tensor with more than one element,\nit must contain two array-like object, which represent the dims of tensor a and tensor b to be calculated.\nout (oneflow.Tensor) – The tensor to save result (NOT IMPLEMENTED YET)\n\n\n",
        "input_shape": "",
        "notes": "Three common use cases are:\ndims = 0 : tensor product \\(a \\otimes b\\)\ndims = 1 : tensor dot product \\(a \\cdot b\\)\ndims = 2 : (default) tensor double contraction \\(a : b\\)\nThe part of documentation is referenced from https://numpy.org/doc/stable/reference/generated/numpy.tensordot.html.\nThe operation is equivalent to the series of operations:\nPermute the dimensions of the tensor A that require tensordot to the end\nPermute the dimensions of the tensor B that require tensordot to the start\nReshape the permuted tensor A into a 2-dimensional tensor, where the size of the 0th dimension is the product of the dimensions that do not require dot product, and the size of the 1st dimension is the product of the dimensions that require dot product\nReshape the permuted tensor B into a 2-dimensional tensor, where the size of the 0th dimension is the product of the dimensions that require dot product, and the size of the 1st dimension is the product of the dimensions that do not require dot product\nCalculate the matrix multiplication of reshaped tensor A and reshaped tensor B\nReshape the result of matrix multiplication, the target shape is the concatenation of the dimensions that do not require tensordot of tensor A and B\nThis series of operations can be equivalently represented by the following code:\n>>> import oneflow as flow\n>>> a = flow.randn(2, 4, 3)\n>>> b = flow.randn(3, 4, 2)\n>>> dims = [[0, 2], [2, 0]]\n>>> permuted_a = a.permute(1, 0, 2) # 0, 2 are the dimensions requiring tensordot and are placed in the end in permuting\n>>> permuted_b = b.permute(2, 0, 1) # 2, 0 are the dimensions requiring tensordot and are placed at the beginning in permuting\n>>> reshaped_a = permuted_a.reshape(4, 2 * 3) # 4 is the dimensions of a that do not require tensordot\n>>> reshaped_b = permuted_b.reshape(2 * 3, 4) # 4 is the dimensions of a that do not require tensordot\n>>> matmul_result = flow.matmul(reshaped_a, reshaped_b)\n>>> result = matmul_result.reshape(4, 4) # 4, 4 are the concatentation of dimensions that do not require tensordot of a and b\n>>> flow.all(result == flow.tensordot(a, b, dims))\ntensor(True, dtype=oneflow.bool)",
        "code_example": ">>> import oneflow as flow\n>>> a = flow.randn(3, 4, 5)\n>>> b = flow.randn(4, 5, 6)\n>>> flow.tensordot(a, b, dims=2).shape\noneflow.Size([3, 6])\n>>> b = flow.randn(5, 6, 7)\n>>> flow.tensordot(a, b, dims=1).shape\noneflow.Size([3, 4, 6, 7])\n>>> b = flow.randn(3, 4, 7)\n>>> flow.tensordot(a, b, dims=[[0, 1], [0, 1]]).shape\noneflow.Size([5, 7])\n\n\n\n"
    },
    {
        "api_name": "oneflow.tril",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.tril.html",
        "api_signature": "oneflow.tril()",
        "api_description": "",
        "return_value": "the other elements of the result tensor out are set to 0.\n\n",
        "parameters": "\ninput (Tensor) – the input tensor.\ndiagonal (int, optional) – the diagonal to specify.\n\n\n\n",
        "input_shape": "",
        "notes": "if diagonal = 0, the diagonal of the returned tensor will be the main diagonal,\nif diagonal > 0, the diagonal of the returned tensor will be above the main diagonal,\nif diagonal < 0, the diagonal of the returned tensor will be below the main diagonal.",
        "code_example": ">>> import oneflow as flow\n>>> import numpy as np\n\n>>> x = flow.tensor(np.ones(shape=(3, 3)).astype(np.float32))\n>>> flow.tril(x)\ntensor([[1., 0., 0.],\n        [1., 1., 0.],\n        [1., 1., 1.]], dtype=oneflow.float32)\n\n\n\n"
    },
    {
        "api_name": "oneflow.repeat_interleave",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.repeat_interleave.html",
        "api_signature": "oneflow.repeat_interleave(input, repeats, dim=None, *, output_size=None)",
        "api_description": "Repeat elements of a tensor.\nWarning\nThis is different from oneflow.Tensor.repeat() but similar to numpy.repeat.",
        "return_value": "Repeated tensor which has the same shape as input, except along the given axis.\n\n",
        "parameters": "\ninput (oneflow.Tensor) – the input Tensor.\nrepeats (Tensor or int) – The number of repetitions for each element.\nrepeats is broadcasted to fit the shape of the given axis.\ndim (int, optional) – The dimension along which to repeat values.\nBy default, use the flattened input array, and return a flat output\narray.\n\n\nKeyword Arguments\noutput_size (int, optional) – Total output size for the given axis\n( e.g. sum of repeats). If given, it will avoid stream syncronization\nneeded to calculate output shape of the tensor.\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> x = flow.tensor([1, 2, 3])\n>>> y = flow.tensor([[1, 2], [3, 4]])\n>>> flow.repeat_interleave(y, 2)\ntensor([1, 1, 2, 2, 3, 3, 4, 4], dtype=oneflow.int64)\n>>> flow.repeat_interleave(y, 3, dim=1)\ntensor([[1, 1, 1, 2, 2, 2],\n        [3, 3, 3, 4, 4, 4]], dtype=oneflow.int64)\n>>> flow.repeat_interleave(y, flow.tensor([1, 2]), dim=0)\ntensor([[1, 2],\n        [3, 4],\n        [3, 4]], dtype=oneflow.int64)\n>>> flow.repeat_interleave(y, flow.tensor([1, 2]), dim=0, output_size=3)\ntensor([[1, 2],\n        [3, 4],\n        [3, 4]], dtype=oneflow.int64)\n\n\n\n"
    },
    {
        "api_name": "oneflow.triu",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.triu.html",
        "api_signature": "oneflow.triu()",
        "api_description": "",
        "return_value": "the other elements of the result tensor out are set to 0.\n\n",
        "parameters": "\ninput (Tensor) – the input tensor.\ndiagonal (int, optional) – the diagonal to consider\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> import numpy as np\n\n>>> x = flow.tensor(np.ones(shape=(3, 3)).astype(np.float32))\n>>> flow.triu(x)\ntensor([[1., 1., 1.],\n        [0., 1., 1.],\n        [0., 0., 1.]], dtype=oneflow.float32)\n\n\n\n"
    },
    {
        "api_name": "oneflow.cross",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.cross.html",
        "api_signature": "oneflow.cross(input, other, dim=None)",
        "api_description": "",
        "return_value": "Supports input of float and double dtypes.\nAlso supports batches of vectors, for which it computes the product along the dimension dim.\nIn this case, the output has the same batch dimensions as the inputs.\nIf dim is not given, it defaults to the first dimension found with the size 3. Note that this might be unexpected.\nThe documentation is referenced from: https://pytorch.org/docs/1.11/generated/torch.cross.html\n\nWarning\nThis function may change in a future PyTorch release to match the default behaviour in oneflow.linalg.cross(). We recommend using oneflow.linalg.cross().\n\n\n",
        "parameters": "\ninput (Tensor) – the first input tensor.\nother (Tensor) – the second input tensor.\ndim (int, optional) – the dimension to take the cross-product in. Default: None\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.bincount",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.bincount.html",
        "api_signature": "oneflow.bincount()",
        "api_description": "oneflow.bincount(input, weights=None, minlength=0) → Tensor\nCount the frequency of each value in an array of non-negative ints.\nThe number of bins (size 1) is one larger than the largest value in input unless input is empty,\nin which case the result is a tensor of size 0. If minlength is specified,\nthe number of bins is at least minlength and if input is empty,\nthen the result is tensor of size minlength filled with zeros.\nIf n is the value at position i, out[n] += weights[i] if weights is specified else out[n] += 1.",
        "return_value": "",
        "parameters": "\ninput (oneflow.Tensor) – 1-d int Tensor\nweights (oneflow.Tensor) – optional, weight for each value in the input tensor. Should be of same size as input tensor.\nminlength (int) – optional, minimum number of bins. Should be non-negative.\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> x = flow.tensor([1, 2, 4, 6])\n>>> flow.bincount(x)\ntensor([0, 1, 1, 0, 1, 0, 1], dtype=oneflow.int64)\n>>> x = flow.tensor([1, 2, 1])\n>>> weights = flow.tensor([0.1, 0.2, 0.15])\n>>> flow.bincount(x, weights=weights)\ntensor([0.0000, 0.2500, 0.2000], dtype=oneflow.float32)\n>>> flow.bincount(x, weights=weights, minlength=4)\ntensor([0.0000, 0.2500, 0.2000, 0.0000], dtype=oneflow.float32)\n\n\n\n"
    },
    {
        "api_name": "oneflow.broadcast_shapes",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.broadcast_shapes.html",
        "api_signature": "oneflow.broadcast_shapes(*shapes)",
        "api_description": "The documentation is referenced from:\nSimilar to oneflow.broadcast_tensors() but for shapes.\nThis is equivalent to flow.broadcast_tensors(*map(flow.empty, shapes))[0].shape\nbut avoids the need create to intermediate tensors.\nThis is useful for broadcasting tensors of common batch shape but different rightmost shape,\ne.g. to broadcast mean vectors with covariance matrices.",
        "return_value": "A shape compatible with all input shapes.\n\nRaises\nRuntimeError – If shapes are incompatible.\n\n\nExample:\n>>> import oneflow as flow\n>>> flow.broadcast_shapes((2,), (3, 1), (1, 1, 1))\noneflow.Size([1, 3, 2])\n\n\n\n",
        "parameters": "*shapes (flow.Size) – Shapes of tensors.\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.broadcast_tensors",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.broadcast_tensors.html",
        "api_signature": "oneflow.broadcast_tensors(*tensors)",
        "api_description": "The documentation is referenced from:\nBroadcasts the given tensors according to broadcasting-semantics.",
        "return_value": "",
        "parameters": "*tensors – any number of tensors of the same type\n\n\n\nWarning\nMore than one element of a broadcasted tensor may refer to a single\nmemory location. As a result, in-place operations (especially ones that\nare vectorized) may result in incorrect behavior. If you need to write\nto the tensors, please clone them first.\n\nExample:\n>>> import oneflow as flow\n>>> x = flow.arange(3).view(1, 3)\n>>> y = flow.arange(2).view(2, 1)\n>>> a, b = flow.broadcast_tensors(x, y)\n>>> a.size()\noneflow.Size([2, 3])\n>>> a\ntensor([[0, 1, 2],\n        [0, 1, 2]], dtype=oneflow.int64)\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.broadcast_to",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.broadcast_to.html",
        "api_signature": "oneflow.broadcast_to(input, shape)",
        "api_description": "The documentation is referenced from:\nBroadcasts input to the shape shape. Equivalent to calling input.expand(shape). See oneflow.expand() for details.",
        "return_value": "",
        "parameters": "\ninput (oneflow.Tensor) – the input tensor.\nshape (list, tuple, or oneflow.Size) – the new shape.\n\n\n\nExample:\n>>> import oneflow as flow\n>>> x = flow.tensor([1, 2, 3])\n>>> flow.broadcast_to(x, (3, 3))\ntensor([[1, 2, 3],\n        [1, 2, 3],\n        [1, 2, 3]], dtype=oneflow.int64)\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.unique",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.unique.html",
        "api_signature": "oneflow.unique(input, sorted=True, return_inverse=False, return_counts=False, dtype=oneflow.int32)",
        "api_description": "",
        "return_value": "The documentation is referenced from: https://pytorch.org/docs/1.10/generated/torch.unique.html.\n\n\noutput (Tensor): the output list of unique scalar elements.\ninverse_indices (Tensor): (optional) if return_inverse is True,\nthere will be an additional returned tensor (same shape as input) representing\nthe indices for where elements in the original input map to in the output;\notherwise, this function will only return a single tensor.\ncounts (Tensor): (optional) if return_counts is True, there will be an additional\nreturned tensor (same shape as output or output.size(dim), if dim was specified)\nrepresenting the number of occurrences for each unique value or tensor.\n\n\n\n",
        "parameters": "\ninput (Tensor) – The input tensor.\nsorted (bool) – Whether to sort the unique elements in ascending order before returning as output.\nreturn_inverse (bool) – Whether to also return the indices for where elements in the original input ended up in the returned unique list.\nreturn_counts (bool) – Whether to also return the counts for each unique element.\ndtype (flow.dtype) – Dtype of the returned indices and counts.\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> x = flow.tensor([3, 1, 2, 0 ,2])\n>>> flow.unique(x)\ntensor([0, 1, 2, 3], dtype=oneflow.int64)\n>>> flow.unique(x, sorted=False)\ntensor([3, 1, 2, 0], dtype=oneflow.int64)\n>>> results, indices = flow.unique(x, return_inverse=True)\n>>> indices\ntensor([3, 1, 2, 0, 2], dtype=oneflow.int32)\n>>> results, counts = flow.unique(x, return_counts=True)\n>>> counts\ntensor([1, 1, 2, 1], dtype=oneflow.int32)\n>>> results, indices = flow.unique(x, return_inverse=True, dtype=flow.long)\n>>> indices\ntensor([3, 1, 2, 0, 2], dtype=oneflow.int64)\n\n\n\n"
    },
    {
        "api_name": "oneflow.addmm",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.addmm.html",
        "api_signature": "oneflow.addmm(beta=1, input, alpha=1, mat1, mat2, out=None)",
        "api_description": "Performs a matrix multiplication of the matrices mat1 and mat2.\nThe matrix input is added to the final result.\nIf mat1 is a \\((n \\times m)\\) tensor, mat2 is a\n\\((m \\times p)\\) tensor, then input must be\nbroadcastable with a \\((n \\times p)\\) tensor\nand out will be a \\((n \\times p)\\) tensor.\nalpha and beta are scaling factors on matrix-vector product between\nmat1 and mat2 and the added matrix input respectively.\n\\[\\text{out} = \\beta\\ \\text{input} + \\alpha\\ (\\text{mat1}_i \\mathbin{@} \\text{mat2}_i)\\]\nFor inputs of type float or double, arguments beta and\nalpha must be real numbers, otherwise they should be integers.",
        "return_value": "",
        "parameters": "\nbeta (Number, optional) – multiplier for input (\\(\\beta\\))\ninput (Tensor) – matrix to be added\nalpha (Number, optional) – multiplier for \\(mat1 @ mat2\\) (\\(\\alpha\\))\nmat1 (Tensor) – the first matrix to be multiplied\nmat2 (Tensor) – the second matrix to be multiplied\nout (Tensor, optional) – the output tensor.\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import numpy as np\n>>> import oneflow as flow\n>>> input = flow.tensor(np.array([[1,2,4],[5,11,9.1]]))\n>>> mat1 = flow.tensor(np.array([[7.3,1.9,7.3],[10.2,1,5.5]]))\n>>> mat2 = flow.tensor(np.array([[7.3,1.9,7.3],[10.2,1,5.5],[3.7,2.2,8.1]]))\n>>> output = flow.addmm(input, mat1, mat2)\n>>> output\ntensor([[100.6800,  33.8300, 126.8700],\n        [110.0100,  43.4800, 133.6100]], dtype=oneflow.float64)\n>>> output.shape\noneflow.Size([2, 3])\n\n\n>>> input2 = flow.tensor(np.array([1.7]))\n>>> mat1 = flow.tensor(np.array([[1,2],[5,9.1],[7.7,1.4]]))\n>>> mat2 = flow.tensor(np.array([[1,2,3.7],[5,9.1,6.8]]))\n>>> output2 = flow.addmm(input2, mat1, mat2, alpha=1, beta=2)\n>>> output2\ntensor([[14.4000, 23.6000, 20.7000],\n        [53.9000, 96.2100, 83.7800],\n        [18.1000, 31.5400, 41.4100]], dtype=oneflow.float64)\n>>> output2.shape\noneflow.Size([3, 3])\n\n\n\n"
    },
    {
        "api_name": "oneflow.bmm",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.bmm.html",
        "api_signature": "oneflow.bmm()",
        "api_description": "Performs a batch matrix-matrix product of matrices stored in input and mat2.\ninput and mat2 must be 3-D tensors each containing the same number of matrices.\nIf input is a (b x n x m) tensor, mat2 is a (b x m x p) tensor, out will be a (b x n x p) tensor.",
        "return_value": "",
        "parameters": "\ninput (oneflow.Tensor) – the first batch of matrices to be multiplied\nmat2 (oneflow.Tensor) – the second batch of matrices to be multiplied\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> import numpy as np\n>>> input1 = flow.randn(10, 3, 4)\n>>> input2 = flow.randn(10, 4, 5)\n>>> of_out = flow.bmm(input1, input2)\n>>> of_out.shape\noneflow.Size([10, 3, 5])\n\n\n\n"
    },
    {
        "api_name": "oneflow.baddbmm",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.baddbmm.html",
        "api_signature": "oneflow.baddbmm(input, batch1, batch2, *, beta=1, alpha=1, out=None)",
        "api_description": "Performs a batch matrix-matrix product of matrices in batch1 and batch2.\ninput is added to the final result.\nbatch1 and batch2 must be 3-D tensors each containing the same\nnumber of matrices.\nIf batch1 is a \\((b \\times n \\times m)\\) tensor, batch2 is a\n\\((b \\times m \\times p)\\) tensor, then input must be\nbroadcastable with a\n\\((b \\times n \\times p)\\) tensor and out will be a\n\\((b \\times n \\times p)\\) tensor.\n\\[\\text{out}_i = \\beta\\ \\text{input}_i + \\alpha\\ (\\text{batch1}_i \\mathbin{@} \\text{batch2}_i)\\]\nIf beta is 0, then input will be ignored, and nan and inf in it will not be propagated.\nFor inputs of type FloatTensor or DoubleTensor, arguments beta and\nalpha must be real numbers, otherwise they should be integers.\nArgs:\ninput (Tensor): the tensor to be added\nbatch1 (Tensor): the first batch of matrices to be multiplied\nbatch2 (Tensor): the second batch of matrices to be multiplied\nKeyword Arguments\nbeta (Number, optional) – multiplier for input (\\(\\beta\\))\nalpha (Number, optional) – multiplier for \\(\\text{{batch1}} \\mathbin{{@}} \\text{{batch2}}\\) (\\(\\alpha\\))",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> input = flow.randn(10, 3, 5)\n>>> batch1 = flow.randn(10, 3, 4)\n>>> batch2 = flow.randn(10, 4, 5)\n>>> of_out = flow.baddbmm(input, batch1, batch2)\n>>> of_out.shape\noneflow.Size([10, 3, 5])\n\n\n\n"
    },
    {
        "api_name": "oneflow.dot",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.dot.html",
        "api_signature": "oneflow.dot()",
        "api_description": "This operator computes the dot product of tensor input and other.\nThe equation is:\n$$\n\\sum_{i=1}^{n}(x[i] * y[i])\n$$",
        "return_value": "",
        "parameters": "\ninput (Tensor) – first tensor in the dot product.\nother (Tensor) – second tensor in the dot product.\n\n\n\n\n",
        "input_shape": "input: Input must be 1D.\nother: Other must be 1D.\n\n\n\n",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> flow.dot(flow.Tensor([2, 3]), flow.Tensor([2, 1]))\ntensor(7., dtype=oneflow.float32)\n\n\n\n"
    },
    {
        "api_name": "oneflow.matmul",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.matmul.html",
        "api_signature": "oneflow.matmul(input, other)",
        "api_description": "This operator applies matrix multiplication to two Tensor.",
        "return_value": "The result Tensor\n\n",
        "parameters": "\na (oneflow.Tensor) – A Tensor\nb (oneflow.Tensor) – A Tensor\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> import numpy as np\n>>> input1 = flow.tensor(np.random.randn(2, 6), dtype=flow.float32)\n>>> input2 = flow.tensor(np.random.randn(6, 5), dtype=flow.float32)\n>>> of_out = flow.matmul(input1, input2)\n>>> of_out.shape\noneflow.Size([2, 5])\n\n\n\n"
    },
    {
        "api_name": "oneflow.mm",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.mm.html",
        "api_signature": "oneflow.mm(input, mat2)",
        "api_description": "Performs a matrix multiplication of the matrices input and mat2.\nIf input is a \\((n \\times m)\\) tensor, mat2 is a\n\\((m \\times p)\\) tensor, out will be a \\((n \\times p)\\) tensor.",
        "return_value": "The result Tensor\n\n",
        "parameters": "\ninput (oneflow.Tensor) – the first matrix to be matrix multiplied\nmat2 (oneflow.Tensor) – the second matrix to be matrix multiplied\n\n\n",
        "input_shape": "",
        "notes": "This function does not broadcast.\nFor broadcasting matrix products, see oneflow.matmul().",
        "code_example": ">>> import oneflow as flow\n>>> mat1 = flow.randn(2, 3)\n>>> mat2 = flow.randn(3, 3)\n>>> of_out = flow.mm(mat1, mat2)\n>>> of_out.shape\noneflow.Size([2, 3])\n\n\n\n"
    },
    {
        "api_name": "oneflow.mv",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.mv.html",
        "api_signature": "oneflow.mv(input, vec)",
        "api_description": "Performs a matrix-vector product of the matrix input and the vector vec.\nIf input is a \\((n \\times m)\\) tensor, vec is a\n1-D tensor of size m, out will be a 1-D tensor of size n.",
        "return_value": "the output Tensor\n\n",
        "parameters": "\ninput (oneflow.Tensor) – matrix to be matrix multiplied\nvec (oneflow.Tensor) – vector to be matrix multiplied\n\n\n",
        "input_shape": "",
        "notes": "This function does not broadcast.",
        "code_example": ">>> import oneflow as flow\n>>> mat = flow.randn(2, 3)\n>>> vec = flow.randn(3)\n>>> out = flow.mv(mat, vec)\n>>> out.shape\noneflow.Size([2])\n\n\n\n"
    },
    {
        "api_name": "oneflow.nn.Parameter",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.Parameter.html",
        "api_signature": null,
        "api_description": "__init__(*args, **kwargs)¶\nInitialize self.  See help(type(self)) for accurate signature.\nMethods\n__abs__()\nabs(self)\n__add__(value, /)\nReturn self+value.\n__and__(value, /)\nReturn self&value.\n__array__([dtype])\nTensor.numpy() → numpy.ndarray\n__bool__()\nis_nonzero(input) -> (bool)\n__contains__(element)\nCheck if element is present in tensor\n__delattr__(name, /)\nImplement delattr(self, name).\n__delitem__(key, /)\nDelete self[key].\n__dir__()\nDefault dir() implementation.\n__eq__(value, /)\nReturn self==value.\n__float__()\n__floordiv__(value, /)\nReturn self//value.\n__format__(format_spec)\nDefault object formatter.\n__ge__(value, /)\nReturn self>=value.\n__getattribute__(name, /)\nReturn getattr(self, name).\n__getitem__(key, /)\nReturn self[key].\n__getstate__()\n__gt__(value, /)\nReturn self>value.\n__hash__()\nReturn hash(self).\n__iadd__(other)\n__imul__(value, /)\nReturn self*=value.\n__index__()\n__init__(*args, **kwargs)\nInitialize self.\n__init_subclass__\nThis method is called when a class is subclassed.\n__int__()\n__invert__()\n~self\n__ipow__(value, /)\nReturn self**=value.\n__isub__(value, /)\nReturn self-=value.\n__itruediv__(value, /)\nReturn self/=value.\n__le__(value, /)\nReturn self<=value.\n__len__()\nReturn len(self).\n__lt__(value, /)\nReturn self<value.\n__matmul__(value, /)\nReturn self@value.\n__mod__(value, /)\nReturn self%value.\n__mul__(value, /)\nReturn self*value.\n__ne__(value, /)\nReturn self!=value.\n__neg__()\n-self\n__new__(**kwargs)\nCreate and return a new object.\n__or__(value, /)\nReturn self|value.\n__pow__(value[, mod])\nReturn pow(self, value, mod).\n__radd__(value, /)\nReturn value+self.\n__rand__(value, /)\nReturn value&self.\n__reduce__()\nHelper for pickle.\n__reduce_ex__(protocol, /)\nHelper for pickle.\n__repr__()\nReturn repr(self).\n__rfloordiv__(value, /)\nReturn value//self.\n__rmatmul__(value, /)\nReturn value@self.\n__rmod__(value, /)\nReturn value%self.\n__rmul__(value, /)\nReturn value*self.\n__ror__(value, /)\nReturn value|self.\n__rpow__(value[, mod])\nReturn pow(value, self, mod).\n__rsub__(value, /)\nReturn value-self.\n__rtruediv__(value, /)\nReturn value/self.\n__rxor__(value, /)\nReturn value^self.\n__setattr__(name, value, /)\nImplement setattr(self, name, value).\n__setitem__(key, value, /)\nSet self[key] to value.\n__setstate__(pickle_dict)\n__sizeof__()\nSize of object in memory, in bytes.\n__str__()\nReturn str(self).\n__sub__(value, /)\nReturn self-value.\n__subclasshook__\nAbstract classes can override this to customize issubclass().\n__truediv__(value, /)\nReturn self/value.\n__xor__(value, /)\nReturn self^value.\n_copy_from_numpy\n_copy_to_numpy\n_meta_repr()\n_register_post_grad_accumulation_hook\n_register_storage_delete_hook\n_zero_grad_\nabs\nSee oneflow.abs()\nacos\nSee oneflow.acos()\nacosh\nSee oneflow.acosh()\nadd(other, *[, alpha])\nSee oneflow.add()\nadd_(other, *[, alpha])\nIn-place version of oneflow.Tensor.add().\naddcdiv\nSee oneflow.addcdiv()\naddcdiv_\nIn-place version of oneflow.Tensor.addcdiv()\naddcmul\nSee oneflow.addcmul()\naddcmul_\nIn-place version of oneflow.Tensor.addcmul().\naddmm(mat1, mat2[, alpha, beta])\nSee oneflow.addmm()\nall([dim, keepdim])\nSee oneflow.all()\nallclose(other[, atol, rtol, equal_nan])\namax\nSee oneflow.amax()\namin\nSee oneflow.amin()\nany([dim, keepdim])\nSee oneflow.any()\narccos\nSee oneflow.arccos()\narccosh\nSee oneflow.arccosh()\narcsin\nSee oneflow.arcsin()\narcsinh\nSee oneflow.arcsinh()\narctan\nSee oneflow.arctan()\narctanh\nSee oneflow.arctanh()\nargmax\nSee oneflow.argmax()\nargmin\nSee oneflow.argmin()\nargsort([dim, descending])\nSee oneflow.argsort()\nargwhere()\nSee oneflow.argwhere()\nas_strided(size, stride[, storage_offset])\nSee oneflow.as_strided()\nas_strided_(size, stride[, storage_offset])\nIn-place version of oneflow.Tensor.as_strided()\nasin\nSee oneflow.asin()\nasinh\nSee oneflow.asinh()\natan\nSee oneflow.atan()\natan2\nSee oneflow.atan2()\natanh\nSee oneflow.atanh()\nbackward([gradient, retain_graph, create_graph])\nComputes the gradient of current tensor w.r.t. graph leaves.\nbaddbmm\nSee oneflow.baddbmm()\nbernoulli\nSee oneflow.bernoulli()\nbernoulli_\nThe inplace version of oneflow.Tensor.bernoulli_().\nbfloat16\nbincount\nSee oneflow.bincount()\nbitwise_and\nSee oneflow.bitwise_and()\nbitwise_not\nbitwise_or\nSee oneflow.bitwise_or()\nbitwise_xor\nSee oneflow.bitwise_xor()\nbmm\nSee oneflow.bmm()\nbool\nTensor.bool() is equivalent to Tensor.to(oneflow.bool).\nbroadcast_to\nSee oneflow.broadcast_to()\nbyte\nself.byte() is equivalent to self.to(oneflow.uint8).\ncast\nSee oneflow.cast()\nceil\nSee oneflow.ceil()\nceil_\nSee oneflow.ceil_()\ncheck_meta_consistency\nchunk\nSee oneflow.chunk()\nclamp\nSee oneflow.clamp().\nclamp_\nInplace version of oneflow.Tensor.clamp().\nclamp_max\nclamp_max_\nclamp_min\nclamp_min_\nclip\nAlias for oneflow.Tensor.clamp().\nclip_\nAlias for oneflow.Tensor.clamp_().\nclone\nSee oneflow.clone()\nconj()\nconj_physical()\ncontiguous\ncontiguous_\ncopy_(other)\nCopies the elements from src into self tensor and returns self.\ncos\nSee oneflow.cos()\ncosh\nSee oneflow.cosh()\ncpu",
        "return_value": "\ncross(other[, dim])\nSee oneflow.cross()\n\ncuda\n\ncumprod(dim[, dtype])\nSee oneflow.cumprod()\n\ncumsum(dim[, dtype])\nSee oneflow.cumsum()\n\ndata_ptr\n\n\ndetach\n\n\ndiag\nSee oneflow.diag()\n\ndiagonal\nSee oneflow.diagonal()\n\ndigamma\nSee oneflow.digamma()\n\ndim\nTensor.dim() → int\n\ndiv\nSee oneflow.div()\n\ndiv_(value)\nIn-place version of oneflow.Tensor.div().\n\ndot\nSee oneflow.dot()\n\ndouble\nTensor.double() is equivalent to Tensor.to(flow.float64).\n\nelement_size\nTensor.element_size() → int\n\neq(other)\nSee oneflow.eq()\n\nequal\nSee oneflow.equal()\n\nerf()\nSee oneflow.erf()\n\nerfc()\nSee oneflow.erfc()\n\nerfinv\nSee oneflow.erfinv()\n\nerfinv_\nInplace version of oneflow.erfinv()\n\nexp\nSee oneflow.exp()\n\nexp2\nSee oneflow.exp2()\n\nexpand()\nSee oneflow.expand()\n\nexpand_as(other)\nExpand this tensor to the same size as other.\n\nexpm1\nSee oneflow.expm1()\n\nexponential_([lambd, generator])\n\n\nfill_\nTensor.fill_(value) → Tensor\n\nflatten\nSee oneflow.flatten()\n\nflip\nSee oneflow.flip()\n\nfloat\nTensor.float() is equivalent to Tensor.to(flow.float32).\n\nfloor\nSee oneflow.floor()\n\nfloor_\nSee oneflow.floor_()\n\nfloor_divide\n\n\nfmod(other)\nSee oneflow.fmod()\n\nfrac\nSee oneflow.frac().\n\nfrac_\nIn-place version of oneflow.Tensor.frac().\n\ngather(dim, index)\nSee oneflow.gather()\n\nge\nSee oneflow.ge()\n\ngelu\n\n\nget_device()\nFor CUDA tensors, this function returns the device ordinal of the GPU on which the tensor resides.\n\nglobal_id\n\n\nglobal_to_global([placement, sbp, grad_sbp, …])\nPerforms Tensor placement and/or sbp conversion.\n\ngt\nSee oneflow.gt()\n\ngt_(value)\nIn-place version of oneflow.Tensor.gt().\n\nhalf\nself.half() is equivalent to self.to(dtype=oneflow.float16).\n\nimag()\n\n\nin_top_k(targets, predictions, k)\nSee oneflow.in_top_k()\n\nindex_add(dim, index, source[, alpha])\n\n\nindex_add_(dim, index, source, *[, alpha])\nThe interface is consistent with PyTorch.\n\nindex_select(dim, index)\nSee oneflow.index_select()\n\nint\nTensor.int() is equivalent to Tensor.to(flow.int32).\n\ninverse\nSee oneflow.linalg.inv()\n\nis_consistent()\n\n\nis_contiguous()\n\nis_floating_point\nSee oneflow.is_floating_point()\n\nis_offloaded()\nDetermine whether the tensor has been moved to CPU memory and the CUDA device memory has been released.\n\nis_pinned()\n\nis_view\n\n\nisclose\n\n\nisinf\nSee oneflow.isinf()\n\nisnan\nSee oneflow.isnan()\n\nitem\n\nkaiming_normal_([a, mode, nonlinearity, …])\n\n\nkaiming_uniform_([a, mode, nonlinearity, …])\n\n\nle\nSee oneflow.le()\n\nlerp\nSee oneflow.lerp()\n\nlerp_\nSee oneflow.lerp_()\n\nload\nLoad tensor data stored on the host (CPU) back to GPU memory.\n\nlocal_to_global([placement, sbp, …])\nCreates a global tensor from a local tensor.\n\nlog\nSee oneflow.log()\n\nlog10\nSee oneflow.log10()\n\nlog1p\nSee oneflow.log1p()\n\nlog2\nSee oneflow.log2()\n\nlog_softmax\n\n\nlogaddexp(other)\n\n\nlogical_and()\nSee oneflow.logical_and()\n\nlogical_not()\nSee oneflow.logical_not()\n\nlogical_or()\nSee oneflow.logical_or()\n\nlogical_xor()\nSee oneflow.logical_xor()\n\nlogsumexp\nSee oneflow.logsumexp()\n\nlong\nTensor.long() is equivalent to Tensor.to(flow.int64).\n\nlt\nSee oneflow.lt()\n\nmasked_fill\nSee oneflow.masked_fill()\n\nmasked_fill_\nIn-place version of oneflow.Tensor.masked_fill().\n\nmasked_select(mask)\nSee oneflow.masked_select()\n\nmatmul\nSee oneflow.matmul()\n\nmax(dim, index)\nSee oneflow.max()\n\nmaximum\nSee oneflow.maximum()\n\nmean([dim, keepdim])\nSee oneflow.mean()\n\nmedian\nSee oneflow.median()\n\nmin(dim, index)\nSee oneflow.min()\n\nminimum\nSee oneflow.minimum()\n\nmish\nSee oneflow.mish()\n\nmm\nSee oneflow.mm()\n\nmode\nSee oneflow.mode()\n\nmul(value)\nSee oneflow.mul()\n\nmul_(value)\nIn-place version of oneflow.Tensor.mul().\n\nmv\nSee oneflow.mv()\n\nnansum\nSee oneflow.nansum()\n\nnarrow\nSee oneflow.narrow()\n\nndimension\n\n\nne\nSee oneflow.ne()\n\nneg\nSee oneflow.neg()\n\nnegative\nSee oneflow.negative()\n\nnelement\nTensor.nelement() → int\n\nnew\nConstructs a new tensor of the same data type and device (or placemant and sbp) as self tensor.\n\nnew_empty(*size[, dtype, device, placement, …])\n\nnew_full(size, fill_value[, dtype, device, …])\n\nnew_ones()\nSee oneflow.new_ones()\n\nnew_tensor(data[, dtype, device, …])\n\n\nnew_zeros([size, dtype, device, placement, …])\n\nnms(scores, iou_threshold)\nSee oneflow.nms()\n\nnonzero(input[, as_tuple])\nSee oneflow.nonzero()\n\nnorm([p, dim, keepdim, dtype])\nSee oneflow.norm()\n\nnormal_([mean, std, generator])\nFills self tensor with elements samples from the normal distribution parameterized by mean and std.\n\nnumel\nSee oneflow.numel()\n\nnumpy([dtype])\nTensor.numpy() → numpy.ndarray\n\noffload\nTransfer tensor data from GPU memory back to host (CPU) memory.\n\northogonal_([gain])\n\n\npermute\nSee oneflow.permute()\n\npin_memory()\nCopies the tensor to pinned memory, if it’s not already pinned.\n\npow\nSee oneflow.pow()\n\nprod([dim, keepdim])\nSee oneflow.prod()\n\nquantile\nSee oneflow.quantile()\n\nreal()\n\n\nreciprocal\nSee oneflow.reciprocal()\n\nregister_hook(hook)\nRegisters a backward hook.\n\nrelu\nSee oneflow.relu()\n\nrelu_\n\n\nrepeat(*size)\nSee oneflow.repeat()\n\nrepeat_interleave(repeats[, dim, output_size])\nSee oneflow.repeat_interleave()\n\nrequires_grad_([requires_grad])\nSets this tensor’s requires_grad attribute in-place.\n\nreshape\nSee oneflow.reshape()\n\nreshape_as(other)\n\nretain_grad\nEnables this Tensor to have their grad populated during backward().\n\nroll\nSee oneflow.roll()\n\nround\nSee oneflow.round()\n\nround_\nSee oneflow.round_()\n\nrsqrt\nSee oneflow.rsqrt()\n\nscatter(dim, index, src, *[, reduce])\nSee oneflow.scatter()\n\nscatter_(dim, index, src, *[, reduce])\nInplace version of oneflow.Tensor.scatter()\n\nscatter_add\nSee oneflow.scatter_add()\n\nscatter_add_(dim, index, src)\nInplace version of oneflow.Tensor.scatter_add()\n\nselu\nSee oneflow.selu()\n\nsigmoid\nSee oneflow.sigmoid()\n\nsign\nSee oneflow.sign()\n\nsilu\nSee oneflow.silu()\n\nsin()\nSee oneflow.sin()\n\nsin_\nSee oneflow.sin_()\n\nsinh\nSee oneflow.sinh()\n\nsize\n\nsoftmax\nSee oneflow.softmax()\n\nsoftplus\nSee oneflow.softplus()\n\nsoftsign\nSee oneflow.softsign()\n\nsort([dim, descending])\nSee oneflow.sort()\n\nsplit\nSee oneflow.split()\n\nsqrt\nSee oneflow.sqrt()\n\nsquare\nSee oneflow.square()\n\nsqueeze([dim])\nSee oneflow.squeeze()\n\nsqueeze_([dim])\nIn-place version of oneflow.Tensor.squeeze()\n\nstd\nSee oneflow.std()\n\nstorage()\n\n\nstorage_offset()\n\nstride\n\n\nsub\nSee oneflow.sub()\n\nsub_(value)\nIn-place version of oneflow.Tensor.sub().\n\nsum([dim, keepdim])\nSee oneflow.sum()\n\nswapaxes\nSee oneflow.swapaxes()\n\nswapdims\nSee oneflow.swapdims()\n\nt\nSee oneflow.t()\n\ntan\nSee oneflow.tan()\n\ntanh\nSee oneflow.tanh()\n\ntile(*dims)\nSee oneflow.tile()\n\nto(*args, **kwargs)\nPerforms Tensor dtype and/or device conversion.\n\nto_consistent(*args, **kwargs)\nThis interface is no longer available, please use oneflow.Tensor.to_global() instead.\n\nto_global([placement, sbp])\nCreates a global tensor if this tensor is a local tensor, otherwise performs Tensor placement and/or sbp conversion.\n\nto_local(**kwargs)\n\nto_numpy\n\n\ntolist()\n\ntopk\nSee oneflow.topk()\n\ntranspose\nSee oneflow.transpose()\n\ntril\nSee oneflow.tril()\n\ntril_\n\n\ntriu\nSee oneflow.triu()\n\ntriu_\n\n\ntrunc\nSee oneflow.trunc()\n\ntrunc_normal_([mean, std, a, b])\n\n\ntype([dtype, non_blocking])\n\ntype_as\n\nunbind\nSee oneflow.unbind()\n\nunfold\n\nuniform_([a, b])\nTensor.uniform_(from=0, to=1) → Tensor\n\nunique\nSee oneflow.unique()\n\nunsqueeze(dim)\nSee oneflow.unsqueeze()\n\nunsqueeze_(dim)\nIn-place version of oneflow.Tensor.unsqueeze()\n\nvar\nSee oneflow.var()\n\nview\n\nview_as(other)\nExpand this tensor to the same size as other.\n\nwhere([x, y])\nSee oneflow.where()\n\nxavier_normal_([gain])\n\n\nxavier_uniform_([gain])\n\n\nzero_()\nFills self tensor with zeros.\n\n\n\nAttributes\n\n\n\n\n\n\nT\nIs this Tensor with its dimensions reversed.\n\n_ref_index\n\n\n_ref_tensor\n\n\n_tensor_buffer_shapes_and_dtypes\n\n\ndata\n\n\ndevice\nIs the oneflow.device where this Tensor is, which is invalid for global tensor.\n\ndtype\n\n\ngrad\nReturn the gradient calculated by autograd functions.\n\ngrad_fn\nReturn the function that created this tensor if it’s requires_grad is True.\n\nis_cpu\n\n\nis_cuda\nIs True if the Tensor is stored on the GPU, False otherwise.\n\nis_eager\n\n\nis_global\nReturn whether this Tensor is a global tensor.\n\nis_lazy\nReturn whether this Tensor is a lazy tensor.\n\nis_leaf\nAll Tensors that have requires_grad which is False will be leaf Tensors by convention.\n\nis_local\n\n\nlayout\n\n\nndim\nSee oneflow.Tensor.dim()\n\nplacement\nIs the oneflow.placement where this Tensor is, which is invalid for local tensor.\n\nrequires_grad\nIs True if gradient need to be computed for this Tensor, False otherwise.\n\nsbp\nIs the oneflow.sbp representing that how the data of the global tensor is distributed, which is invalid for local tensor.\n\nshape\n\n\n\n\n\n",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.nn.Module",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.Module.html",
        "api_signature": null,
        "api_description": "Base class for all neural network modules.\nThe documentation is referenced from:\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in\na tree structure. You can assign the submodules as regular attributes:\nimport oneflow.nn as nn\nimport oneflow.nn.functional as F\nclass Model(nn.Module):\ndef __init__(self):\nsuper().__init__()\nself.conv1 = nn.Conv2d(1, 20, 5)\nself.conv2 = nn.Conv2d(20, 20, 5)\ndef forward(self, x):\nx = F.relu(self.conv1(x))\nreturn F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their\nparameters converted too when you call to(), etc.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "As per the example above, an __init__() call to the parent class\nmust be made before assignment on the child.\nVariables\ntraining (bool) – Boolean represents whether this module is in training or\nevaluation mode.",
        "code_example": ""
    },
    {
        "api_name": "oneflow.nn.Sequential",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.Sequential.html",
        "api_signature": "oneflow.nn.Sequential(*args: Any)",
        "api_description": "A sequential container.\nModules will be added to it in the order they are passed in the constructor.\nAlternatively, an ordered dict of modules can also be passed in.\nTo make it easier to understand, here is a small example:\n>>> import oneflow.nn as nn\n>>> from collections import OrderedDict\n>>> nn.Sequential(nn.Conv2d(1,20,5), nn.ReLU(), nn.Conv2d(20,64,5), nn.ReLU())\nSequential(\n(0): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))\n(1): ReLU()\n(2): Conv2d(20, 64, kernel_size=(5, 5), stride=(1, 1))\n(3): ReLU()\n)\n>>> nn.Sequential(OrderedDict([\n...    ('conv1', nn.Conv2d(1,20,5)),\n...    ('relu1', nn.ReLU()),\n...    ('conv2', nn.Conv2d(20,64,5)),\n...    ('relu2', nn.ReLU())\n... ]))\nSequential(\n(conv1): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))\n(relu1): ReLU()\n(conv2): Conv2d(20, 64, kernel_size=(5, 5), stride=(1, 1))\n(relu2): ReLU()\n)",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.nn.ModuleList",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.ModuleList.html",
        "api_signature": "oneflow.nn.ModuleList(modules: Optional[Iterable[oneflow.nn.modules.module.Module]] = None)",
        "api_description": "Holds submodules in a list.\nModuleList can be indexed like a regular Python list, but\nmodules it contains are properly registered, and will be visible by all\nModule methods.",
        "return_value": "",
        "parameters": "modules (iterable, optional) – an iterable of modules to add\n\n\n>>> import oneflow.nn as nn\n\n>>> class MyModule(nn.Module):\n...    def __init__(self):\n...        super(MyModule, self).__init__()\n...        self.linears = nn.ModuleList([nn.Linear(10, 10) for i in range(10)])\n...    def forward(self, x):\n...        # ModuleList can act as an iterable, or be indexed using ints\n...        for i, l in enumerate(self.linears):\n...            x = self.linears[i // 2](x) + l(x)\n...        return x\n\n>>> model = MyModule()\n>>> model.linears\nModuleList(\n  (0): Linear(in_features=10, out_features=10, bias=True)\n  (1): Linear(in_features=10, out_features=10, bias=True)\n  (2): Linear(in_features=10, out_features=10, bias=True)\n  (3): Linear(in_features=10, out_features=10, bias=True)\n  (4): Linear(in_features=10, out_features=10, bias=True)\n  (5): Linear(in_features=10, out_features=10, bias=True)\n  (6): Linear(in_features=10, out_features=10, bias=True)\n  (7): Linear(in_features=10, out_features=10, bias=True)\n  (8): Linear(in_features=10, out_features=10, bias=True)\n  (9): Linear(in_features=10, out_features=10, bias=True)\n)\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.nn.ModuleDict",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.ModuleDict.html",
        "api_signature": "oneflow.nn.ModuleDict(modules: Optional[Mapping[str, oneflow.nn.modules.module.Module]] = None)",
        "api_description": "Holds submodules in a dictionary.\nModuleDict can be indexed like a regular Python dictionary,\nbut modules it contains are properly registered, and will be visible by all\nModule methods.\nModuleDict is an ordered dictionary that respects\nthe order of insertion, and\nin update(), the order of the merged\nOrderedDict, dict (started from Python 3.6) or another\nModuleDict (the argument to\nupdate()).",
        "return_value": "",
        "parameters": "modules (iterable, optional) – a mapping (dictionary) of (string: module)\nor an iterable of key-value pairs of type (string, module)\n\n\n>>> import oneflow.nn as nn\n\n>>> class MyModule(nn.Module):\n...    def __init__(self):\n...        super(MyModule, self).__init__()\n...        self.choices = nn.ModuleDict({\n...                'conv': nn.Conv2d(10, 10, 3),\n...                'pool': nn.MaxPool2d(3)\n...        })\n...        self.activations = nn.ModuleDict([\n...                ['lrelu', nn.LeakyReLU()],\n...                ['prelu', nn.PReLU()]\n...        ])\n\n...    def forward(self, x, choice, act):\n...        x = self.choices[choice](x)\n...        x = self.activations[act](x)\n...        return x\n\n>>> model = MyModule()\n>>> model.choices\nModuleDict(\n  (conv): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n  (pool): MaxPool2d(kernel_size=(3, 3), stride=(3, 3), padding=(0, 0), dilation=(1, 1))\n)\n\n\n\n",
        "input_shape": "",
        "notes": "types (e.g., Python’s plain dict before Python version 3.6) does not\npreserve the order of the merged mapping.",
        "code_example": ""
    },
    {
        "api_name": "oneflow.nn.ParameterList",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.ParameterList.html",
        "api_signature": "oneflow.nn.ParameterList(parameters=None)",
        "api_description": "Holds parameters in a list.\nParameterList can be indexed like a regular Python\nlist, but parameters it contains are properly registered, and will be\nvisible by all Module methods.",
        "return_value": "",
        "parameters": "parameters (iterable, optional) – an iterable of Parameter to add\n\n\n>>> import oneflow as flow\n>>> import oneflow.nn as nn\n\n>>> class MyModule(nn.Module):\n...    def __init__(self):\n...        super(MyModule, self).__init__()\n...        self.params = nn.ParameterList([nn.Parameter(flow.randn(10, 10)) for i in range(10)])\n...\n...    def forward(self, x):\n...        # ParameterList can act as an iterable, or be indexed using ints\n...        for i, p in enumerate(self.params):\n...            x = self.params[i // 2].mm(x) + p.mm(x)\n...        return x\n\n>>> model = MyModule()\n>>> model.params\nParameterList(\n    (0): Parameter containing: [<class 'oneflow.nn.Parameter'> of size 10x10]\n    (1): Parameter containing: [<class 'oneflow.nn.Parameter'> of size 10x10]\n    (2): Parameter containing: [<class 'oneflow.nn.Parameter'> of size 10x10]\n    (3): Parameter containing: [<class 'oneflow.nn.Parameter'> of size 10x10]\n    (4): Parameter containing: [<class 'oneflow.nn.Parameter'> of size 10x10]\n    (5): Parameter containing: [<class 'oneflow.nn.Parameter'> of size 10x10]\n    (6): Parameter containing: [<class 'oneflow.nn.Parameter'> of size 10x10]\n    (7): Parameter containing: [<class 'oneflow.nn.Parameter'> of size 10x10]\n    (8): Parameter containing: [<class 'oneflow.nn.Parameter'> of size 10x10]\n    (9): Parameter containing: [<class 'oneflow.nn.Parameter'> of size 10x10]\n)\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.nn.ParameterDict",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.ParameterDict.html",
        "api_signature": "oneflow.nn.ParameterDict(parameters=None)",
        "api_description": "Holds parameters in a dictionary.\nParameterDict can be indexed like a regular Python dictionary, but parameters it\ncontains are properly registered, and will be visible by all Module methods.\nParameterDict is an ordered dictionary that respects\nthe order of insertion, and\nin update(), the order of the merged OrderedDict\nor another ParameterDict (the argument to\nupdate()).",
        "return_value": "",
        "parameters": "parameters (iterable, optional) – a mapping (dictionary) of\n(string : Parameter) or an iterable of key-value pairs\nof type (string, Parameter)\n\n\n>>> import oneflow as flow\n>>> import oneflow.nn as nn\n\n>>> class MyModule(nn.Module):\n...    def __init__(self):\n...        super(MyModule, self).__init__()\n...        self.params = nn.ParameterDict({\n...                'left': nn.Parameter(flow.randn(5, 10)),\n...                'right': nn.Parameter(flow.randn(5, 10))\n...        })\n...\n...    def forward(self, x, choice):\n...        x = self.params[choice].mm(x)\n...        return x\n\n>>> model = MyModule()\n>>> model.params\nParameterDict(\n    (left): Parameter containing: [<class 'oneflow.nn.Parameter'> of size 5x10]\n    (right): Parameter containing: [<class 'oneflow.nn.Parameter'> of size 5x10]\n)\n\n\n\n",
        "input_shape": "",
        "notes": "types (e.g., Python’s plain dict) does not preserve the order of the\nmerged mapping.",
        "code_example": ""
    },
    {
        "api_name": "oneflow.nn.Module.add_module",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.Module.add_module.html",
        "api_signature": "Module.add_module(name, module)",
        "api_description": "Adds a child module to the current module.\nThe module can be accessed as an attribute using the given name.",
        "return_value": "",
        "parameters": "\nname (string) – name of the child module. The child module can be\naccessed from this module using the given name\nmodule (Module) – child module to be added to the module.\n\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.nn.Module.apply",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.Module.apply.html",
        "api_signature": "Module.apply(fn)",
        "api_description": "Applies fn recursively to every submodule (as returned by .children())\nas well as self. Typical use includes initializing the parameters of a model.",
        "return_value": "self\n\n",
        "parameters": "fn (Module -> None) – function to be applied to each submodule\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.nn.Module.buffers",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.Module.buffers.html",
        "api_signature": "Module.buffers(recurse=True)",
        "api_description": "",
        "return_value": "\n",
        "parameters": "recurse (bool) – if True, then yields buffers of this module\nand all submodules. Otherwise, yields only buffers that\nare direct members of this module.\n\nYields\noneflow.Tensor – module buffer\n\n\nExample:\n>>> for buf in model.buffers(): \n...     print(type(buf), buf.size()) \n<class 'oneflow.Tensor'> oneflow.Size([10])\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.nn.Module.children",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.Module.children.html",
        "api_signature": "Module.children()",
        "api_description": "",
        "return_value": "\nYields\nModule – a child module\n\n\nExample:\n>>> import oneflow.nn as nn\n>>> l1 = nn.Linear(2, 2)\n>>> l2 = nn.Linear(2, 2)\n>>> net = nn.Sequential(l1, l2)\n>>> for idx, m in enumerate(net.children()):\n...     print(idx, '->', m)\n0 -> Linear(in_features=2, out_features=2, bias=True)\n1 -> Linear(in_features=2, out_features=2, bias=True)\n\n\n\n",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.nn.Module.cpu",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.Module.cpu.html",
        "api_signature": "Module.cpu()",
        "api_description": "Moves all model parameters and buffers to the CPU.",
        "return_value": "self\n\n",
        "parameters": "",
        "input_shape": "",
        "notes": "This method modifies the module in-place.",
        "code_example": ""
    },
    {
        "api_name": "oneflow.nn.Module.cuda",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.Module.cuda.html",
        "api_signature": "Module.cuda(device=None)",
        "api_description": "Moves all model parameters and buffers to the GPU.\nThis also makes associated parameters and buffers different objects. So\nit should be called before constructing optimizer if the module will\nlive on GPU while being optimized.",
        "return_value": "self\n\n",
        "parameters": "device (int, optional) – if specified, all parameters will be\ncopied to that device\n\n",
        "input_shape": "",
        "notes": "This method modifies the module in-place.",
        "code_example": ""
    },
    {
        "api_name": "oneflow.nn.Module.double",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.Module.double.html",
        "api_signature": "Module.double()",
        "api_description": "Casts all floating point parameters and buffers to double datatype.",
        "return_value": "self\n\n",
        "parameters": "",
        "input_shape": "",
        "notes": "This method modifies the module in-place.",
        "code_example": ""
    },
    {
        "api_name": "oneflow.nn.Module.train",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.Module.train.html",
        "api_signature": "Module.train(mode=True)",
        "api_description": "Sets the module in training mode.\nThis has any effect only on certain modules. See documentations of\nparticular modules for details of their behaviors in training/evaluation\nmode, if they are affected, e.g. Dropout, BatchNorm1d,\netc.",
        "return_value": "self\n\n",
        "parameters": "mode (bool) – whether to set training mode (True) or evaluation\nmode (False). Default: True.\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.nn.Module.eval",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.Module.eval.html",
        "api_signature": "Module.eval()",
        "api_description": "Sets the module in evaluation mode.\nThis has any effect only on certain modules. See documentations of\nparticular modules for details of their behaviors in training/evaluation\nmode, if they are affected, e.g. Dropout, BatchNorm1d,\netc.\nThis is equivalent with self.train(False).",
        "return_value": "self\n\n",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.nn.Module.extra_repr",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.Module.extra_repr.html",
        "api_signature": "Module.extra_repr()",
        "api_description": "Set the extra representation of the module\nTo print customized extra information, you should re-implement\nthis method in your own modules. Both single-line and multi-line\nstrings are acceptable.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.nn.Module.float",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.Module.float.html",
        "api_signature": "Module.float()",
        "api_description": "Casts all floating point parameters and buffers to float datatype.",
        "return_value": "self\n\n",
        "parameters": "",
        "input_shape": "",
        "notes": "This method modifies the module in-place.",
        "code_example": ""
    },
    {
        "api_name": "oneflow.nn.Module.forward",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.Module.forward.html",
        "api_signature": "Module.forward(*args, **kwargs)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.nn.Module.load_state_dict",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.Module.load_state_dict.html",
        "api_signature": "Module.load_state_dict(state_dict, strict=True)",
        "api_description": "Copies parameters and buffers from state_dict into\nthis module and its descendants. If strict is True, then\nthe keys of state_dict must exactly match the keys returned\nby this module’s state_dict() function.",
        "return_value": "\nmissing_keys is a list of str containing the missing keys\nunexpected_keys is a list of str containing the unexpected keys\n\n\n\n",
        "parameters": "\nstate_dict (dict) – a dict containing parameters and\npersistent buffers.\nstrict (bool, optional) – whether to strictly enforce that the keys\nin state_dict match the keys returned by this module’s\nstate_dict() function. Default: True\n\n\n",
        "input_shape": "",
        "notes": "If a parameter or buffer is registered as None and its corresponding key\nexists in state_dict, load_state_dict() will raise a\nRuntimeError.",
        "code_example": ""
    },
    {
        "api_name": "oneflow.nn.Module.modules",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.Module.modules.html",
        "api_signature": "Module.modules()",
        "api_description": "",
        "return_value": "\nYields\nModule – a module in the network\n\n\n\n",
        "parameters": "",
        "input_shape": "",
        "notes": "Duplicate modules are returned only once. In the following\nexample, l will be returned only once.\nExample:\n>>> import oneflow.nn as nn\n>>> l = nn.Linear(2, 2)\n>>> net = nn.Sequential(l, l)\n>>> for idx, m in enumerate(net.modules()):\n...     print(idx, '->', m)\n0 -> Sequential(\n(0): Linear(in_features=2, out_features=2, bias=True)\n(1): Linear(in_features=2, out_features=2, bias=True)\n)\n1 -> Linear(in_features=2, out_features=2, bias=True)",
        "code_example": ""
    },
    {
        "api_name": "oneflow.nn.Module.named_buffers",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.Module.named_buffers.html",
        "api_signature": "Module.named_buffers(prefix='', recurse=True)",
        "api_description": "",
        "return_value": "name of the buffer as well as the buffer itself.\n\n",
        "parameters": "\nprefix (str) – prefix to prepend to all buffer names.\nrecurse (bool) – if True, then yields buffers of this module\nand all submodules. Otherwise, yields only buffers that\nare direct members of this module.\n\n\nYields\n(string, oneflow.Tensor) – Tuple containing the name and buffer\n\n\nExample:\n>>> for name, buf in self.named_buffers(): \n...    if name in ['running_var']: \n...        print(buf.size()) \n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.nn.Module.named_children",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.Module.named_children.html",
        "api_signature": "Module.named_children()",
        "api_description": "",
        "return_value": "the name of the module as well as the module itself.\n\nYields\n(string, Module) – Tuple containing a name and child module\n\n\nExample:\n>>> for name, module in model.named_children(): \n...     if name in ['conv4', 'conv5']: \n...         print(module) \n\n\n\n",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.nn.Module.named_modules",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.Module.named_modules.html",
        "api_signature": "Module.named_modules(memo=None, prefix='')",
        "api_description": "",
        "return_value": "both the name of the module as well as the module itself.\n\n",
        "parameters": "\nmemo – a memo to store the set of modules already added to the result\nprefix – a prefix that will be added to the name of the module\n\n\nYields\n(string, Module) – Tuple of name and module\n\n\n\n",
        "input_shape": "",
        "notes": "Duplicate modules are returned only once. In the following\nexample, l will be returned only once.\nExample:\n>>> import oneflow.nn as nn\n>>> l = nn.Linear(2, 2)\n>>> net = nn.Sequential(l, l)\n>>> for idx, m in enumerate(net.named_modules()):\n...     print(idx, '->', m)\n0 -> ('', Sequential(\n(0): Linear(in_features=2, out_features=2, bias=True)\n(1): Linear(in_features=2, out_features=2, bias=True)\n))\n1 -> ('0', Linear(in_features=2, out_features=2, bias=True))",
        "code_example": ""
    },
    {
        "api_name": "oneflow.nn.Module.named_parameters",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.Module.named_parameters.html",
        "api_signature": "Module.named_parameters(prefix='', recurse=True)",
        "api_description": "",
        "return_value": "name of the parameter as well as the parameter itself.\n\n",
        "parameters": "\nprefix (str) – prefix to prepend to all parameter names.\nrecurse (bool) – if True, then yields parameters of this module\nand all submodules. Otherwise, yields only parameters that\nare direct members of this module.\n\n\nYields\n(string, Parameter) – Tuple containing the name and parameter\n\n\nExample:\n>>> for name, param in self.named_parameters(): \n...    if name in ['bias']: \n...        print(param.size()) \n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.nn.Module.parameters",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.Module.parameters.html",
        "api_signature": "Module.parameters(recurse=True)",
        "api_description": "",
        "return_value": "This is typically passed to an optimizer.\n\n",
        "parameters": "recurse (bool) – if True, then yields parameters of this module\nand all submodules. Otherwise, yields only parameters that\nare direct members of this module.\n\nYields\nParameter – module parameter\n\n\nExample:\n>>> for param in model.parameters(): \n...     print(type(param), param.size()) \n<class 'oneflow.Tensor'> oneflow.Size([10])\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.nn.Module.register_buffer",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.Module.register_buffer.html",
        "api_signature": "Module.register_buffer(name, tensor, persistent=True)",
        "api_description": "Adds a buffer to the module.\nThis is typically used to register a buffer that should not to be\nconsidered a model parameter. For example, BatchNorm’s running_mean\nis not a parameter, but is part of the module’s state. Buffers, by\ndefault, are persistent and will be saved alongside parameters. This\nbehavior can be changed by setting persistent to False. The\nonly difference between a persistent buffer and a non-persistent buffer\nis that the latter will not be a part of this module’s\nstate_dict.\nBuffers can be accessed as attributes using given names.",
        "return_value": "",
        "parameters": "\nname (string) – name of the buffer. The buffer can be accessed\nfrom this module using the given name\ntensor (Tensor or None) – buffer to be registered. If None, then operations\nthat run on buffers, such as cuda, are ignored. If None,\nthe buffer is not included in the module’s state_dict.\npersistent (bool) – whether the buffer is part of this module’s\nstate_dict.\n\n\n\nExample:\n>>> self.register_buffer('running_mean', oneflow.zeros(num_features)) \n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.nn.Module.register_forward_hook",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.Module.register_forward_hook.html",
        "api_signature": "Module.register_forward_hook(hook)",
        "api_description": "Registers a forward hook on the module.\nThe hook will be called every time after forward() has computed an output.\nIt should have the following signature:\nhook(module, input, output) -> None or modified output\nThe input contains only the positional arguments given to the module.\nKeyword arguments won’t be passed to the hooks and only to the forward.\nThe hook can modify the output. It can modify the input inplace but\nit will not have effect on forward since this is called after\nforward() is called.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.nn.Module.register_forward_pre_hook",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.Module.register_forward_pre_hook.html",
        "api_signature": "Module.register_forward_pre_hook(hook)",
        "api_description": "Registers a forward pre-hook on the module.\nThe hook will be called every time before forward() is invoked.\nIt should have the following signature:\nhook(module, input) -> None or modified input\nThe input contains only the positional arguments given to the module.\nKeyword arguments won’t be passed to the hooks and only to the forward.\nThe hook can modify the input. User can either return a tuple or a\nsingle modified value in the hook. We will wrap the value into a tuple\nif a single value is returned(unless that value is already a tuple).",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.nn.Module.register_backward_hook",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.Module.register_backward_hook.html",
        "api_signature": "Module.register_backward_hook(hook: Callable[[oneflow.nn.modules.module.Module, Union[Tuple[oneflow.Tensor, …], oneflow.Tensor], Union[Tuple[oneflow.Tensor, …], oneflow.Tensor]], Union[None, oneflow.Tensor]])",
        "api_description": "Registers a backward hook on the module.\nThis function is deprecated in favor of register_full_backward_hook() and\nthe behavior of this function will change in future versions.",
        "return_value": "a handle that can be used to remove the added hook by calling\nhandle.remove()\n\n",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.nn.Module.register_full_backward_hook",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.Module.register_full_backward_hook.html",
        "api_signature": "Module.register_full_backward_hook(hook: Callable[[oneflow.nn.modules.module.Module, Union[Tuple[oneflow.Tensor, …], oneflow.Tensor], Union[Tuple[oneflow.Tensor, …], oneflow.Tensor]], Union[None, oneflow.Tensor]])",
        "api_description": "Registers a backward hook on the module.\nThe hook will be called every time the gradients with respect to module\ninputs are computed. The hook should have the following signature:\nhook(module, grad_input, grad_output) -> TensorTuple or None\nThe grad_input and grad_output are oneflow.TensorTuple that contain the gradients\nwith respect to the inputs and outputs respectively. The hook should\nnot modify its arguments, but it can optionally return a new gradient with\nrespect to the input that will be used in place of grad_input in\nsubsequent computations. grad_input will only correspond to the inputs given\nas positional arguments and all kwarg arguments are ignored. Entries\nin grad_input and grad_output will be None for all non-Tensor\narguments.\nFor technical reasons, when this hook is applied to a Module, its forward function will\nreceive a view of each Tensor passed to the Module. Similarly the caller will receive a view\nof each Tensor returned by the Module’s forward function.\nWarning\nModifying inputs or outputs inplace is not allowed when using backward hooks and\nwill raise an error.",
        "return_value": "a handle that can be used to remove the added hook by calling\nhandle.remove()\n\n",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.nn.Module.register_state_dict_pre_hook",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.Module.register_state_dict_pre_hook.html",
        "api_signature": "Module.register_state_dict_pre_hook(hook)",
        "api_description": "These hooks will be called with arguments: self, prefix,\nand keep_vars before calling state_dict on self. The registered\nhooks can be used to perform pre-processing before the state_dict\ncall is made.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.nn.Module.register_parameter",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.Module.register_parameter.html",
        "api_signature": "Module.register_parameter(name, param)",
        "api_description": "Adds a parameter to the module.\nThe parameter can be accessed as an attribute using given name.",
        "return_value": "",
        "parameters": "\nname (string) – name of the parameter. The parameter can be accessed\nfrom this module using the given name\nparam (Parameter or None) – parameter to be added to the module. If\nNone, then operations that run on parameters, such as cuda,\nare ignored. If None, the parameter is not included in the\nmodule’s state_dict.\n\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.nn.Module.requires_grad_",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.Module.requires_grad_.html",
        "api_signature": "Module.requires_grad_(requires_grad: bool = True)",
        "api_description": "Change if autograd should record operations on parameters in this\nmodule.\nThis method sets the parameters’ requires_grad attributes\nin-place.\nThis method is helpful for freezing part of the module for finetuning\nor training parts of a model individually (e.g., GAN training).",
        "return_value": "self\n\n",
        "parameters": "requires_grad (bool) – whether autograd should record operations on\nparameters in this module. Default: True.\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.nn.Module.state_dict",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.Module.state_dict.html",
        "api_signature": "Module.state_dict(destination=None, prefix='', keep_vars=False)",
        "api_description": "",
        "return_value": "Both parameters and persistent buffers (e.g. running averages) are\nincluded. Keys are corresponding parameter and buffer names.\na dictionary containing a whole state of the module\n\n",
        "parameters": "\n\ndestination (dict, optional) – Deprecated. This dict is returned\nwith the module state saved in it. It should also have an\nattribute _metadata: dict to save metadata of the module\nstate. If it’s not provided, an OrderedDict is created and\nreturned. Default: None\nprefix (str, optional) – a prefix added to parameter and buffer\nnames to compose the keys in dict. Default: ''\nkeep_vars (bool, optional) – by default the Tensor s\nreturned in the state dict are detached from autograd. If it’s\nset to True, detaching is not performed. Default: False\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.nn.Module.to",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.Module.to.html",
        "api_signature": "Module.to(device: Optional[Union[oneflow._oneflow_internal.device, str, int]] = ..., dtype: Optional[oneflow._oneflow_internal.dtype] = ...)",
        "api_description": "Module.to(dtype: oneflow._oneflow_internal.dtype) → T\nModule.to(tensor: oneflow.Tensor) → T\nMoves and/or casts the parameters and buffers.\nThis can be called as\nto(device=None, dtype=None)\noneflow.nn.to(dtype)\noneflow.nn.to(memory_format=None)\noneflow.nn.to(tensor)\nIts signature is similar to oneflow.Tensor.to(), but only accepts\nfloating point dtypes. In addition, this method will\nonly cast the floating point parameters and buffers to dtype\n(if given). The integral parameters and buffers will be moved\ndevice, if that is given, but with dtypes unchanged.\nSee below for examples.",
        "return_value": "self\n\n",
        "parameters": "\ndevice (oneflow.device) – the desired device of the parameters\nand buffers in this module\ndtype (oneflow.dtype) – the desired floating point dtype of\nthe parameters and buffers in this module\nmemory_format (oneflow.memory_format) – the desired memory\nformat for 4D parameters and buffers in this module (keyword\nonly argument)\ntensor (oneflow.Tensor) – Tensor whose dtype and device are the desired\ndtype and device for all parameters and buffers in this module\n\n\n",
        "input_shape": "",
        "notes": "This method modifies the module in-place.",
        "code_example": ""
    },
    {
        "api_name": "oneflow.nn.Module.zero_grad",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.Module.zero_grad.html",
        "api_signature": "Module.zero_grad(set_to_none=False)",
        "api_description": "Sets gradients of all model parameters to zero. See similar function\nunder oneflow.optim.Optimizer for more context.",
        "return_value": "",
        "parameters": "set_to_none (bool) – instead of setting to zero, set the grads to None.\nSee oneflow.optim.Optimizer.zero_grad() for details.\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.nn.Conv1d",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.Conv1d.html",
        "api_signature": "oneflow.nn.Conv1d(in_channels: int, out_channels: int, kernel_size: Union[int, Tuple[int]], stride: Union[int, Tuple[int]] = 1, padding: Union[str, int, Tuple[int]] = 0, dilation: Union[int, Tuple[int]] = 1, groups: int = 1, bias: bool = True, padding_mode: str = 'zeros', device=None, dtype=None)",
        "api_description": "Applies a 1D convolution over an input signal composed of several input\nplanes.\nIn the simplest case, the output value of the layer with input size\n\\((N, C_{\\text{in}}, L)\\) and output \\((N, C_{\\text{out}}, L_{\\text{out}})\\) can be\nprecisely described as:\n\\[\\text{out}(N_i, C_{\\text{out}_j}) = \\text{bias}(C_{\\text{out}_j}) +\n\\sum_{k = 0}^{C_{in} - 1} \\text{weight}(C_{\\text{out}_j}, k)\n\\star \\text{input}(N_i, k)\\]\nwhere \\(\\star\\) is the valid cross-correlation operator,\n\\(N\\) is a batch size, \\(C\\) denotes a number of channels,\n\\(L\\) is a length of signal sequence.\nstride controls the stride for the cross-correlation, a single\nnumber or a one-element tuple.\npadding controls the amount of padding applied to the input. It\ncan be either a string {{‘valid’, ‘same’}} or a tuple of ints giving the\namount of implicit padding applied on both sides.\ndilation controls the spacing between the kernel points; also\nknown as the à trous algorithm. It is harder to describe, but this link\nhas a nice visualization of what dilation does.",
        "return_value": "",
        "parameters": "\nin_channels (int) – Number of channels in the input image\nout_channels (int) – Number of channels produced by the convolution\nkernel_size (int or tuple) – Size of the convolving kernel\nstride (int or tuple, optional) – Stride of the convolution. Default: 1\npadding (int, tuple or str, optional) – Padding added to both sides of\nthe input. Default: 0\npadding_mode (string, optional) – 'zeros'. Default: 'zeros'\ndilation (int or tuple, optional) – Spacing between kernel\nelements. Default: 1\ngroups (int, optional) – Number of blocked connections from input\nchannels to output channels. Default: 1\nbias (bool, optional) – If True, adds a learnable bias to the\noutput. Default: True\n\n\n\n\n",
        "input_shape": "Input: \\((N, C_{in}, L_{in})\\)\nOutput: \\((N, C_{out}, L_{out})\\) where\n\n\\[L_{out} = \\left\\lfloor\\frac{L_{in} + 2 \\times \\text{padding} - \\text{dilation}\n          \\times (\\text{kernel\\_size} - 1) - 1}{\\text{stride}} + 1\\right\\rfloor\\]\n\n\n\n\n\n\nweight¶\nthe learnable weights of the module of shape\n\\((\\text{out\\_channels},\n\\frac{\\text{in\\_channels}}{\\text{groups}}, \\text{kernel\\_size})\\).\nThe values of these weights are sampled from\n\\(\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})\\) where\n\\(k = \\frac{groups}{C_\\text{in} * \\text{kernel\\_size}}\\)\n\nType\nTensor\n\n\n\n\n\nbias¶\nthe learnable bias of the module of shape\n(out_channels). If bias is True, then the values of these weights are\nsampled from \\(\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})\\) where\n\\(k = \\frac{groups}{C_\\text{in} * \\text{kernel\\_size}}\\)\n\nType\nTensor\n\n\n\n",
        "notes": "padding='valid' is the same as no padding. padding='same' pads\nthe input so the output has the shape as the input. However, this mode\ndoesn’t support any stride values other than 1.",
        "code_example": ">>> import numpy as np\n>>> import oneflow as flow\n>>> import oneflow.nn as nn\n\n>>> arr = np.random.randn(20, 16, 50)\n>>> input = flow.Tensor(arr)\n>>> m = nn.Conv1d(16, 33, 3, stride=2)\n>>> output = m(input)\n\n\n\n"
    },
    {
        "api_name": "oneflow.nn.Conv2d",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.Conv2d.html",
        "api_signature": "oneflow.nn.Conv2d(in_channels: int, out_channels: int, kernel_size: Union[int, Tuple[int, int]], stride: Union[int, Tuple[int, int]] = 1, padding: Union[str, int, Tuple[int, int]] = 0, dilation: Union[int, Tuple[int, int]] = 1, groups: int = 1, bias: bool = True, padding_mode: str = 'zeros', device=None, dtype=None)",
        "api_description": "Applies a 2D convolution over an input signal composed of several input\nplanes.\nIn the simplest case, the output value of the layer with input size\n\\((N, C_{\\text{in}}, H, W)\\) and output \\((N, C_{\\text{out}}, H_{\\text{out}}, W_{\\text{out}})\\)\ncan be precisely described as:\n\\[\\text{out}(N_i, C_{\\text{out}_j}) = \\text{bias}(C_{\\text{out}_j}) +\n\\sum_{k = 0}^{C_{\\text{in}} - 1} \\text{weight}(C_{\\text{out}_j}, k) \\star \\text{input}(N_i, k)\\]\nwhere \\(\\star\\) is the valid 2D cross-correlation operator,\n\\(N\\) is a batch size, \\(C\\) denotes a number of channels,\n\\(H\\) is a height of input planes in pixels, and \\(W\\) is\nwidth in pixels.\nstride controls the stride for the cross-correlation, a single\nnumber or a tuple.\npadding controls the amount of implicit padding on both\nsides for padding number of points for each dimension.\ndilation controls the spacing between the kernel points; also\nknown as the à trous algorithm. It is harder to describe, but this link\nhas a nice visualization of what dilation does.\ngroups controls the connections between inputs and outputs.\nin_channels and out_channels must both be divisible by\ngroups. For example,\nAt groups=1, all inputs are convolved to all outputs.\nAt groups=2, the operation becomes equivalent to having two conv\nlayers side by side, each seeing half the input channels\nand producing half the output channels, and both subsequently\nconcatenated.\nAt groups= in_channels, each input channel is convolved with\nits own set of filters (of size\n\\(\\frac{\\text{out_channels}}{\\text{in_channels}}\\)).,\nThe parameters kernel_size, stride, padding, dilation can either be:\na single int – in which case the same value is used for the height and width dimension\na tuple of two ints – in which case, the first int is used for the height dimension,\nand the second int for the width dimension",
        "return_value": "",
        "parameters": "\nin_channels (int) – Number of channels in the input image\nout_channels (int) – Number of channels produced by the convolution\nkernel_size (int or tuple) – Size of the convolving kernel\nstride (int or tuple, optional) – Stride of the convolution. Default: 1\npadding (int or tuple, optional) – Zero-padding added to both sides of\nthe input. Default: 0\npadding_mode (string, optional) – 'zeros'. Default: 'zeros'\ndilation (int or tuple, optional) – Spacing between kernel elements. Default: 1\ngroups (int, optional) – Number of blocked connections from input\nchannels to output channels. Default: 1\nbias (bool, optional) – If True, adds a learnable bias to the\noutput. Default: True\n\n\n\n\n",
        "input_shape": "Input: \\((N, C_{in}, H_{in}, W_{in})\\)\nOutput: \\((N, C_{out}, H_{out}, W_{out})\\) where\n\n\\[H_{out} = \\left\\lfloor\\frac{H_{in}  + 2 \\times \\text{padding}[0] - \\text{dilation}[0]\n          \\times (\\text{kernel_size}[0] - 1) - 1}{\\text{stride}[0]} + 1\\right\\rfloor\\]\n\n\\[W_{out} = \\left\\lfloor\\frac{W_{in}  + 2 \\times \\text{padding}[1] - \\text{dilation}[1]\n          \\times (\\text{kernel_size}[1] - 1) - 1}{\\text{stride}[1]} + 1\\right\\rfloor\\]\n\n\n\nAttr:\n\nweight (Tensor): the learnable weights of the module of shape\\((\\text{out_channels}, \\frac{\\text{in_channels}}{\\text{groups}},\\)\n\\(\\text{kernel_size[0]}, \\text{kernel_size[1]})\\).\nThe values of these weights are sampled from\n\\(\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})\\) where\n\\(k = \\frac{groups}{C_\\text{in} * \\prod_{i=0}^{1}\\text{kernel_size}[i]}\\)\n\n\n\n\nbias (Tensor):   the learnable bias of the module of shape(out_channels). If bias is True,\nthen the values of these weights are\nsampled from \\(\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})\\) where\n\\(k = \\frac{groups}{C_\\text{in} * \\prod_{i=0}^{1}\\text{kernel_size}[i]}\\)\n\n\n\n\n\n\n",
        "notes": "When groups == in_channels and out_channels == K * in_channels,\nwhere K is a positive integer, this operation is also known as a “depthwise convolution”.\nIn other words, for an input of size \\((N, C_{in}, L_{in})\\),\na depthwise convolution with a depthwise multiplier K can be performed with the arguments\n\\((C_\\text{in}=C_\\text{in}, C_\\text{out}=C_\\text{in} \\times \\text{K}, ..., \\text{groups}=C_\\text{in})\\).",
        "code_example": ">>> import numpy as np\n>>> import oneflow as flow\n>>> import oneflow.nn as nn\n\n>>> arr = np.random.randn(20, 16, 50, 100)\n>>> input = flow.Tensor(arr)\n>>> m = nn.Conv2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2), dilation=(3, 1))\n>>> output = m(input)\n\n\n\n"
    },
    {
        "api_name": "oneflow.nn.Conv3d",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.Conv3d.html",
        "api_signature": "oneflow.nn.Conv3d(in_channels: int, out_channels: int, kernel_size: Union[int, Tuple[int, int, int]], stride: Union[int, Tuple[int, int, int]] = 1, padding: Union[str, int, Tuple[int, int, int]] = 0, dilation: Union[int, Tuple[int, int, int]] = 1, groups: int = 1, bias: bool = True, padding_mode: str = 'zeros', device=None, dtype=None)",
        "api_description": "Applies a 3D convolution over an input signal composed of several input\nplanes.\nIn the simplest case, the output value of the layer with input size \\((N, C_{in}, D, H, W)\\)\nand output \\((N, C_{out}, D_{out}, H_{out}, W_{out})\\) can be precisely described as:\n\\[out(N_i, C_{out_j}) = bias(C_{out_j}) +\n\\sum_{k = 0}^{C_{in} - 1} weight(C_{out_j}, k) \\star input(N_i, k)\\]\nwhere \\(\\star\\) is the valid 3D cross-correlation operator\nstride controls the stride for the cross-correlation.\npadding controls the amount of padding applied to the input. It\ncan be either a string {{‘valid’, ‘same’}} or a tuple of ints giving the\namount of implicit padding applied on both sides.\ndilation controls the spacing between the kernel points; also known as the à trous algorithm.\nIt is harder to describe, but this link has a nice visualization of what dilation does.\nThe parameters kernel_size, stride, padding, dilation can either be:\na single int – in which case the same value is used for the depth, height and width dimension\na tuple of three ints – in which case, the first int is used for the depth dimension,\nthe second int for the height dimension and the third int for the width dimension",
        "return_value": "",
        "parameters": "\nin_channels (int) – Number of channels in the input image\nout_channels (int) – Number of channels produced by the convolution\nkernel_size (int or tuple) – Size of the convolving kernel\nstride (int or tuple, optional) – Stride of the convolution. Default: 1\npadding (int, tuple or str, optional) – Padding added to all six sides of\nthe input. Default: 0\npadding_mode (string, optional) – 'zeros'. Default: 'zeros'\ndilation (int or tuple, optional) – Spacing between kernel elements. Default: 1\ngroups (int, optional) – Number of blocked connections from input channels to output channels. Default: 1\nbias (bool, optional) – If True, adds a learnable bias to the output. Default: True\n\n\n\n\n",
        "input_shape": "Input: \\((N, C_{in}, D_{in}, H_{in}, W_{in})\\)\nOutput: \\((N, C_{out}, D_{out}, H_{out}, W_{out})\\) where\n\n\\[D_{out} = \\left\\lfloor\\frac{D_{in} + 2 \\times \\text{padding}[0] - \\text{dilation}[0]\n      \\times (\\text{kernel\\_size}[0] - 1) - 1}{\\text{stride}[0]} + 1\\right\\rfloor\\]\n\n\\[H_{out} = \\left\\lfloor\\frac{H_{in} + 2 \\times \\text{padding}[1] - \\text{dilation}[1]\n      \\times (\\text{kernel\\_size}[1] - 1) - 1}{\\text{stride}[1]} + 1\\right\\rfloor\\]\n\n\\[W_{out} = \\left\\lfloor\\frac{W_{in} + 2 \\times \\text{padding}[2] - \\text{dilation}[2]\n      \\times (\\text{kernel\\_size}[2] - 1) - 1}{\\text{stride}[2]} + 1\\right\\rfloor\\]\n\n\n\n\n\n\nweight¶\nthe learnable weights of the module of shape\n\\((\\text{out\\_channels}, \\frac{\\text{in\\_channels}}{\\text{groups}},\\)\n\\(\\text{kernel\\_size[0]}, \\text{kernel\\_size[1]}, \\text{kernel\\_size[2]})\\).\nThe values of these weights are sampled from\n\\(\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})\\) where\n\\(k = \\frac{groups}{C_\\text{in} * \\prod_{i=0}^{2}\\text{kernel\\_size}[i]}\\)\n\nType\nTensor\n\n\n\n\n\nbias¶\nthe learnable bias of the module of shape (out_channels). If bias is True,\nthen the values of these weights are\nsampled from \\(\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})\\) where\n\\(k = \\frac{groups}{C_\\text{in} * \\prod_{i=0}^{2}\\text{kernel\\_size}[i]}\\)\n\nType\nTensor\n\n\n\n",
        "notes": "padding='valid' is the same as no padding. padding='same' pads\nthe input so the output has the shape as the input. However, this mode\ndoesn’t support any stride values other than 1.",
        "code_example": ">>> import numpy as np\n>>> import oneflow as flow\n>>> import oneflow.nn as nn\n\n>>> arr = np.random.randn(1, 2, 5, 5, 5)\n>>> input = flow.Tensor(arr)\n>>> m = nn.Conv3d(2, 4, kernel_size=3, stride=1)\n>>> output = m(input)\n\n\n\n"
    },
    {
        "api_name": "oneflow.nn.ConvTranspose1d",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.ConvTranspose1d.html",
        "api_signature": "oneflow.nn.ConvTranspose1d(in_channels: int, out_channels: int, kernel_size: Union[int, Tuple[int]], stride: Union[int, Tuple[int]] = 1, padding: Union[int, Tuple[int]] = 0, output_padding: Union[int, Tuple[int]] = 0, groups: int = 1, bias: bool = True, dilation: Union[int, Tuple[int]] = 1, padding_mode: str = 'zeros')",
        "api_description": "Applies a 1D transposed convolution operator over an input image\ncomposed of several input planes.\nThis module can be seen as the gradient of Conv1d with respect to its input.\nIt is also known as a fractionally-strided convolution or\na deconvolution (although it is not an actual deconvolution operation).\nThis module supports TensorFloat32.\nstride controls the stride for the cross-correlation.\npadding controls the amount of implicit zero padding on both\nsides for dilation * (kernel_size - 1) - padding number of points. See note\nbelow for details.\noutput_padding controls the additional size added to one side\nof the output shape. See note below for details.\ndilation controls the spacing between the kernel points; also known as the à trous algorithm.\nIt is harder to describe, but this link has a nice visualization of what dilation does.",
        "return_value": "",
        "parameters": "\nin_channels (int) – Number of channels in the input image\nout_channels (int) – Number of channels produced by the convolution\nkernel_size (int or tuple) – Size of the convolving kernel\nstride (int or tuple, optional) – Stride of the convolution. Default: 1\npadding (int or tuple, optional) – dilation * (kernel_size - 1) - padding zero-padding\nwill be added to both sides of the input. Default: 0\noutput_padding (int or tuple, optional) – Additional size added to one side\nof the output shape. Default: 0\ngroups (int, optional) – Number of blocked connections from input channels to output channels. Default: 1\nbias (bool, optional) – If True, adds a learnable bias to the output. Default: True\ndilation (int or tuple, optional) – Spacing between kernel elements. Default: 1\n\n\n\n\n",
        "input_shape": "Input: \\((N, C_{in}, L_{in})\\)\nOutput: \\((N, C_{out}, L_{out})\\) where\n\n\\[L_{out} = (L_{in} - 1) \\times \\text{stride} - 2 \\times \\text{padding} + \\text{dilation}\n          \\times (\\text{kernel_size} - 1) + \\text{output_padding} + 1\\]\n\n\n\n\n\n\nweight¶\nthe learnable weights of the module of shape\n\\((\\\\text{in\\_channels}, \\frac{\\\\text{out\\\\_channels}}{\\text{groups}},\\)\n\\(\\\\text{kernel\\\\_size})\\).\nThe values of these weights are sampled from\n\\(\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})\\) where\n\\(k = \\frac{groups}{C_\\text{out} * \\\\text{kernel\\\\_size}}\\)\n\nType\nTensor\n\n\n\n\n\nbias¶\nthe learnable bias of the module of shape (out_channels).\nIf bias is True, then the values of these weights are\nsampled from \\(\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})\\) where\n\\(k = \\frac{groups}{C_\\text{out} * \\\\text{kernel\\\\_size}}\\)\n\nType\nTensor\n\n\n\n\n",
        "notes": "The padding argument effectively adds dilation * (kernel_size - 1) - padding\namount of zero padding to both sizes of the input. This is set so that\nwhen a Conv1d and a ConvTranspose1d\nare initialized with same parameters, they are inverses of each other in\nregard to the input and output shapes. However, when stride > 1,\nConv1d maps multiple input shapes to the same output\nshape. output_padding is provided to resolve this ambiguity by\neffectively increasing the calculated output shape on one side. Note\nthat output_padding is only used to find output shape, but does\nnot actually add zero-padding to output.\nIn some circumstances when using the CUDA backend with CuDNN, this operator\nmay select a nondeterministic algorithm to increase performance. If this is\nundesirable, you can try to make the operation deterministic (potentially at\na performance cost) by setting torch.backends.cudnn.deterministic =\nTrue.\nPlease see the notes on randomness for background.",
        "code_example": ""
    },
    {
        "api_name": "oneflow.nn.ConvTranspose2d",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.ConvTranspose2d.html",
        "api_signature": "oneflow.nn.ConvTranspose2d(in_channels: int, out_channels: int, kernel_size: Union[int, Tuple[int, int]], stride: Union[int, Tuple[int, int]] = 1, padding: Union[int, Tuple[int, int]] = 0, output_padding: Union[int, Tuple[int, int]] = 0, groups: int = 1, bias: bool = True, dilation: int = 1, padding_mode: str = 'zeros')",
        "api_description": "Applies a 2D transposed convolution operator over an input image composed of several input planes.\nThis module can be seen as the gradient of Conv2d with respect to its input.\nIt is also known as a fractionally-strided convolution or\na deconvolution (although it is not an actual deconvolution operation).",
        "return_value": "",
        "parameters": "\nin_channels (int) – Number of channels in the input image\nout_channels (int) – Number of channels produced by the convolution\nkernel_size (int or tuple) – Size of the convolving kernel\nstride (int or tuple, optional) – Stride of the convolution. Default: 1\npadding (int or tuple, optional) – dilation * (kernel_size - 1) - padding zero-padding\nwill be added to both sides of each dimension in the input. Default: 0\noutput_padding (int or tuple, optional) – Additional size added to one side\nof each dimension in the output shape. Default: 0\ngroups (int, optional) – Number of blocked connections from input channels to output channels. Default: 1\nbias (bool, optional) – If True, adds a learnable bias to the output. Default: True\ndilation (int or tuple, optional) – Spacing between kernel elements. Default: 1\n\n\n\n\n",
        "input_shape": "Input: \\((N, C_{in}, H_{in}, W_{in})\\)\nOutput: \\((N, C_{out}, H_{out}, W_{out})\\) where\n\n\n\\[ \\begin{align}\\begin{aligned}H_{out} = (H_{in} - 1) \\times \\text{stride}[0] - 2 \\times \\text{padding}[0] + \\text{dilation}[0]\\\\          \\times (\\text{kernel_size}[0] - 1) + \\text{output_padding}[0] + 1\\end{aligned}\\end{align} \\]\n\n\\[ \\begin{align}\\begin{aligned}W_{out} = (W_{in} - 1) \\times \\text{stride}[1] - 2 \\times \\text{padding}[1] + \\text{dilation}[1]\\\\          \\times (\\text{kernel_size}[1] - 1) + \\text{output_padding}[1] + 1\\end{aligned}\\end{align} \\]\n\n\n\n\nweight¶\nthe learnable weights of the module of shape\n\\((\\text{in_channels}, \\frac{\\text{out_channels}}{\\text{groups}},\\)\n\\(\\text{kernel_size[0]}, \\text{kernel_size[1]})\\).\nThe values of these weights are sampled from\n\\(\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})\\) where\n\\(k = \\frac{groups}{C_\\text{out} * \\prod_{i=0}^{1}\\text{kernel_size}[i]}\\)\n\nType\nTensor\n\n\n\n\n\nbias¶\nthe learnable bias of the module of shape (out_channels)\nIf bias is True, then the values of these weights are\nsampled from \\(\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})\\) where\n\\(k = \\frac{groups}{C_\\text{out} * \\prod_{i=0}^{1}\\text{kernel_size}[i]}\\)\n\nType\nTensor\n\n\n\n",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.nn.ConvTranspose3d",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.ConvTranspose3d.html",
        "api_signature": "oneflow.nn.ConvTranspose3d(in_channels: int, out_channels: int, kernel_size: Union[int, Tuple[int, int, int]], stride: Union[int, Tuple[int, int, int]] = 1, padding: Union[int, Tuple[int, int, int]] = 0, output_padding: Union[int, Tuple[int, int, int]] = 0, groups: int = 1, bias: bool = True, dilation: Union[int, Tuple[int, int, int]] = 1, padding_mode: str = 'zeros')",
        "api_description": "Applies a 3D transposed convolution operator over an input image composed of several input\nplanes.\nThe transposed convolution operator multiplies each input value element-wise by a learnable kernel,\nand sums over the outputs from all input feature planes.\nThis module can be seen as the gradient of Conv3d with respect to its input.\nIt is also known as a fractionally-strided convolution or\na deconvolution (although it is not an actual deconvolution operation).\nThis module supports TensorFloat32.\nstride controls the stride for the cross-correlation.\npadding controls the amount of implicit zero padding on both\nsides for dilation * (kernel_size - 1) - padding number of points. See note\nbelow for details.\noutput_padding controls the additional size added to one side\nof the output shape. See note below for details.\ndilation controls the spacing between the kernel points; also known as the à trous algorithm.\nIt is harder to describe, but this link has a nice visualization of what dilation does.\nThe parameters kernel_size, stride, padding, output_padding\ncan either be:\na single int – in which case the same value is used for the depth, height and width dimensions\na tuple of three ints – in which case, the first int is used for the depth dimension,\nthe second int for the height dimension and the third int for the width dimension",
        "return_value": "",
        "parameters": "\nin_channels (int) – Number of channels in the input image\nout_channels (int) – Number of channels produced by the convolution\nkernel_size (int or tuple) – Size of the convolving kernel\nstride (int or tuple, optional) – Stride of the convolution. Default: 1\npadding (int or tuple, optional) – dilation * (kernel_size - 1) - padding zero-padding\nwill be added to both sides of each dimension in the input. Default: 0\noutput_padding (int or tuple, optional) – Additional size added to one side\nof each dimension in the output shape. Default: 0\ngroups (int, optional) – Number of blocked connections from input channels to output channels. Default: 1\nbias (bool, optional) – If True, adds a learnable bias to the output. Default: True\ndilation (int or tuple, optional) – Spacing between kernel elements. Default: 1\n\n\n\n\n",
        "input_shape": "Input: \\((N, C_{in}, D_{in}, H_{in}, W_{in})\\)\nOutput: \\((N, C_{out}, D_{out}, H_{out}, W_{out})\\) where\n\n\n\\[D_{out} = (D_{in} - 1) \\times \\text{stride}[0] - 2 \\times \\text{padding}[0] + \\text{dilation}[0]\n          \\times (\\text{kernel_size}[0] - 1) + \\text{output_padding}[0] + 1\\]\n\n\\[H_{out} = (H_{in} - 1) \\times \\text{stride}[1] - 2 \\times \\text{padding}[1] + \\text{dilation}[1]\n          \\times (\\text{kernel_size}[1] - 1) + \\text{output_padding}[1] + 1\\]\n\n\\[W_{out} = (W_{in} - 1) \\times \\text{stride}[2] - 2 \\times \\text{padding}[2] + \\text{dilation}[2]\n          \\times (\\text{kernel_size}[2] - 1) + \\text{output_padding}[2] + 1\\]\n\n\n\n\nweight¶\nthe learnable weights of the module of shape\n\\((\\text{in_channels}, \\frac{\\text{out_channels}}{\\text{groups}},\\)\n\\(\\text{kernel_size[0]}, \\text{kernel_size[1]}, \\text{kernel_size[2]})\\).\nThe values of these weights are sampled from\n\\(\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})\\) where\n\\(k = \\frac{groups}{C_\\text{out} * \\prod_{i=0}^{2}\\text{kernel_size}[i]}\\)\n\nType\nTensor\n\n\n\n\n\nbias¶\nthe learnable bias of the module of shape (out_channels)\nIf bias is True, then the values of these weights are\nsampled from \\(\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})\\) where\n\\(k = \\frac{groups}{C_\\text{out} * \\prod_{i=0}^{2}\\text{kernel_size}[i]}\\)\n\nType\nTensor\n\n\n\n",
        "notes": "The padding argument effectively adds dilation * (kernel_size - 1) - padding\namount of zero padding to both sizes of the input. This is set so that\nwhen a Conv3d and a ConvTranspose3d\nare initialized with same parameters, they are inverses of each other in\nregard to the input and output shapes. However, when stride > 1,\nConv3d maps multiple input shapes to the same output\nshape. output_padding is provided to resolve this ambiguity by\neffectively increasing the calculated output shape on one side. Note\nthat output_padding is only used to find output shape, but does\nnot actually add zero-padding to output.",
        "code_example": ""
    },
    {
        "api_name": "oneflow.nn.Unfold",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.Unfold.html",
        "api_signature": "oneflow.nn.Unfold(kernel_size, dilation=1, padding=0, stride=1)",
        "api_description": "This op extracts elements in a local window from input tensor, it also called img2col.\nConsider a batched input tensor of shape \\((N, C, *)\\),\nwhere \\(N\\) is the batch dimension, \\(C\\) is the channel dimension,\nand \\(*\\) represent arbitrary spatial dimensions. This operation flattens\neach sliding kernel_size-sized block within the spatial dimensions\nof input into a column (i.e., last dimension) of a 3-D output\ntensor of shape \\((N, C \\times \\prod(\\text{kernel_size}), L)\\), where\n\\(C \\times \\prod(\\text{kernel_size})\\) is the total number of values\nwithin each block (a block has \\(\\prod(\\text{kernel_size})\\) spatial\nlocations each containing a \\(C\\)-channeled vector), and \\(L\\) is\nthe total number of such blocks:\n\\[L = \\prod_d \\left\\lfloor\\frac{\\text{spatial_size}[d] + 2 \\times \\text{padding}[d] %\n- \\text{dilation}[d] \\times (\\text{kernel_size}[d] - 1) - 1}{\\text{stride}[d]} + 1\\right\\rfloor,\\]\nwhere \\(\\text{spatial_size}\\) is formed by the spatial dimensions\nof input (\\(*\\) above), and \\(d\\) is over all spatial\ndimensions.\nTherefore, indexing output at the last dimension (column dimension)\ngives all values within a certain block.\nThe padding, stride and dilation arguments specify\nhow the sliding blocks are retrieved.\nstride controls the stride for the sliding blocks.\npadding controls the amount of implicit zero-paddings on both\nsides for padding number of points for each dimension before\nreshaping.\ndilation controls the spacing between the kernel points; also known as\nthe à trous algorithm.",
        "return_value": "",
        "parameters": "\nkernel_size (int or tuple) – the size of the sliding blocks\nstride (int or tuple, optional) – the stride of the sliding blocks in the input\nspatial dimensions. Default: 1\npadding (int or tuple, optional) – implicit zero padding to be added on\nboth sides of input. Default: 0\ndilation (int or tuple, optional) – a parameter that controls the\nstride of elements within the\nneighborhood. Default: 1\n\n\n\n\nIf kernel_size, dilation, padding or\nstride is an int or a tuple of length 1, their values will be\nreplicated across all spatial dimensions.\nFor the case of two input spatial dimensions this operation is sometimes\ncalled im2col.\n\n\n",
        "input_shape": "Input: \\((N, C, *)\\)\nOutput: \\((N, C \\times \\prod(\\text{kernel_size}), L)\\) as described above\n\n\n\n",
        "notes": "Fold calculates each combined value in the resulting\nlarge tensor by summing all values from all containing blocks.\nUnfold extracts the values in the local blocks by\ncopying from the large tensor. So, if the blocks overlap, they are not\ninverses of each other.\nIn general, folding and unfolding operations are related as\nfollows. Consider Fold and\nUnfold instances created with the same\nparameters:\n>>> fold_params = dict(kernel_size=..., dilation=..., padding=..., stride=...)\n>>> fold = nn.Fold(output_size=..., **fold_params)\n>>> unfold = nn.Unfold(**fold_params)\nThen for any (supported) input tensor the following\nequality holds:\n::fold(unfold(input)) == divisor * input\nwhere divisor is a tensor that depends only on the shape\nand dtype of the input:\n>>> input_ones = oneflow.ones(input.shape, dtype=input.dtype)\n>>> divisor = fold(unfold(input_ones))\nWhen the divisor tensor contains no zero elements, then\nfold and unfold operations are inverses of each\nother (up to constant divisor).\nWarning\nCurrently, only 4-D input tensors (batched image-like tensors) are\nsupported.",
        "code_example": ">>> import oneflow as flow\n>>> import numpy as np\n\n>>> x_tensor = flow.Tensor(np.random.randn(1, 1, 4, 4))\n>>> unfold = flow.nn.Unfold(kernel_size=3, padding=1)\n>>> out = unfold(x_tensor)\n>>> out.shape\noneflow.Size([1, 9, 16])\n\n\n\n"
    },
    {
        "api_name": "oneflow.nn.Fold",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.Fold.html",
        "api_signature": "oneflow.nn.Fold(output_size, kernel_size, dilation=1, padding=0, stride=1)",
        "api_description": "Combines an array of sliding local blocks into a large containing\ntensor, it also called col2img\nConsider a batched input tensor containing sliding local blocks,\ne.g., patches of images, of shape \\((N, C \\times  \\prod(\\text{kernel_size}), L)\\),\nwhere \\(N\\) is batch dimension, \\(C \\times \\prod(\\text{kernel_size})\\)\nis the number of values within a block (a block has \\(\\prod(\\text{kernel_size})\\)\nspatial locations each containing a \\(C\\)-channeled vector), and\n\\(L\\) is the total number of blocks. (This is exactly the\nsame specification as the output shape of Unfold.) This\noperation combines these local blocks into the large output tensor\nof shape \\((N, C, \\text{output_size}[0], \\text{output_size}[1], \\dots)\\)\nby summing the overlapping values. Similar to Unfold, the\narguments must satisfy\n\\[L = \\prod_d \\left\\lfloor\\frac{\\text{output_size}[d] + 2 \\times \\text{padding}[d] %\n- \\text{dilation}[d] \\times (\\text{kernel_size}[d] - 1) - 1}{\\text{stride}[d]} + 1\\right\\rfloor,\\]\nwhere \\(d\\) is over all spatial dimensions.\noutput_size describes the spatial shape of the large containing\ntensor of the sliding local blocks. It is useful to resolve the ambiguity\nwhen multiple input shapes map to same number of sliding blocks, e.g.,\nwith stride > 0.\nThe padding, stride and dilation arguments specify\nhow the sliding blocks are retrieved.\nstride controls the stride for the sliding blocks.\npadding controls the amount of implicit zero-paddings on both\nsides for padding number of points for each dimension before\nreshaping.\ndilation controls the spacing between the kernel points; also known as\nthe à trous algorithm.",
        "return_value": "",
        "parameters": "\noutput_size (int or tuple) – the shape of the spatial dimensions of the\noutput (i.e., output.sizes()[2:])\nkernel_size (int or tuple) – the size of the sliding blocks\nstride (int or tuple) – the stride of the sliding blocks in the input\nspatial dimensions. Default: 1\npadding (int or tuple, optional) – implicit zero padding to be added on\nboth sides of input. Default: 0\ndilation (int or tuple, optional) – a parameter that controls the\nstride of elements within the\nneighborhood. Default: 1\n\n\n\n\nIf output_size, kernel_size, dilation,\npadding or stride is an int or a tuple of length 1 then\ntheir values will be replicated across all spatial dimensions.\nFor the case of two output spatial dimensions this operation is sometimes\ncalled col2im.\n\n\n",
        "input_shape": "Input: \\((N, C \\times \\prod(\\text{kernel_size}), L)\\) or \\((C \\times \\prod(\\text{kernel_size}), L)\\)\nOutput: \\((N, C, \\text{output_size}[0], \\text{output_size}[1], \\dots)\\)\nor \\((C, \\text{output_size}[0], \\text{output_size}[1], \\dots)\\) as described above\n\n\n\n",
        "notes": "Fold calculates each combined value in the resulting\nlarge tensor by summing all values from all containing blocks.\nUnfold extracts the values in the local blocks by\ncopying from the large tensor. So, if the blocks overlap, they are not\ninverses of each other.\nIn general, folding and unfolding operations are related as\nfollows. Consider Fold and\nUnfold instances created with the same\nparameters:\n>>> fold_params = dict(kernel_size=..., dilation=..., padding=..., stride=...)\n>>> fold = nn.Fold(output_size=..., **fold_params)\n>>> unfold = nn.Unfold(**fold_params)\nThen for any (supported) input tensor the following\nequality holds:\nfold(unfold(input)) == divisor * input\nwhere divisor is a tensor that depends only on the shape\nand dtype of the input:\n>>> input_ones = oneflow.ones(input.shape, dtype=input.dtype)\n>>> divisor = fold(unfold(input_ones))\nWhen the divisor tensor contains no zero elements, then\nfold and unfold operations are inverses of each\nother (up to constant divisor).\nWarning\nCurrently, only unbatched (3D) or batched (4D) image-like output tensors are supported.",
        "code_example": ">>> import oneflow as flow\n>>> import numpy as np\n\n>>> x_tensor = flow.Tensor(np.random.randn(1, 9, 16))\n>>> fold = flow.nn.Fold(output_size=(4, 4), kernel_size=3, padding=1)\n>>> out = fold(x_tensor)\n>>> out.shape\noneflow.Size([1, 1, 4, 4])\n\n\n\n"
    },
    {
        "api_name": "oneflow.nn.MaxPool1d",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.MaxPool1d.html",
        "api_signature": "oneflow.nn.MaxPool1d(kernel_size: Union[int, Tuple[int]], stride: Optional[Union[int, Tuple[int]]] = None, padding: Union[int, Tuple[int]] = 0, dilation: Union[int, Tuple[int]] = 1, return_indices: bool = False, ceil_mode: bool = False)",
        "api_description": "Applies a 1D max pooling over an input signal composed of several input planes.\nIn the simplest case, the output value of the layer with input size \\((N, C, L)\\)\nand output \\((N, C, L_{out})\\) can be precisely described as:\n\\[out(N_i, C_j, k) = \\max_{m=0, \\ldots, \\text{kernel\\_size} - 1}\ninput(N_i, C_j, stride \\times k + m)\\]\nIf padding is non-zero, then the input is implicitly padded with minimum value on both sides\nfor padding number of points. dilation is the stride between the elements within the\nsliding window. This link has a nice visualization of the pooling parameters.",
        "return_value": "",
        "parameters": "\nkernel_size – The size of the sliding window, must be > 0.\nstride – The stride of the sliding window, must be > 0. Default value is kernel_size.\npadding – Implicit negative infinity padding to be added on both sides, must be >= 0 and <= kernel_size / 2.\ndilation – The stride between elements within a sliding window, must be > 0.\nreturn_indices – If True, will return the argmax along with the max values.\nceil_mode – If True, will use ceil instead of floor to compute the output shape. This\nensures that every element in the input tensor is covered by a sliding window.\n\n\n\n\n",
        "input_shape": "Input: \\((N, C, L_{in})\\)\nOutput: \\((N, C, L_{out})\\), where\n\n\\[L_{out} = \\left\\lfloor \\frac{L_{in} + 2 \\times \\text{padding} - \\text{dilation}\n      \\times (\\text{kernel_size} - 1) - 1}{\\text{stride}} + 1\\right\\rfloor\\]\n\n\n\n\n",
        "notes": "When ceil_mode=True, sliding windows are allowed to go off-bounds if they start within the left padding\nor the input. Sliding windows that would start in the right padded region are ignored.",
        "code_example": "import oneflow as flow\nimport numpy as np\n\nof_maxpool1d = flow.nn.MaxPool1d(kernel_size=3, padding=1, stride=1)\nx = flow.Tensor(np.random.randn(1, 4, 4))\ny = of_maxpool1d(x)\ny.shape\noneflow.Size([1, 4, 4])\n\n\n\n"
    },
    {
        "api_name": "oneflow.nn.MaxPool2d",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.MaxPool2d.html",
        "api_signature": "oneflow.nn.MaxPool2d(kernel_size: Union[int, Tuple[int, int]], stride: Optional[Union[int, Tuple[int, int]]] = None, padding: Union[int, Tuple[int, int]] = 0, dilation: Union[int, Tuple[int, int]] = 1, return_indices: bool = False, ceil_mode: bool = False)",
        "api_description": "Applies a 2D max pooling over an input signal composed of several input planes.\nIn the simplest case, the output value of the layer with input size \\((N, C, H, W)\\),\noutput \\((N, C, H_{out}, W_{out})\\) and kernel_size \\((kH, kW)\\)\ncan be precisely described as:\n\\[\\begin{split}\\begin{aligned}\nout(N_i, C_j, h, w) ={} & \\max_{m=0, \\ldots, kH-1} \\max_{n=0, \\ldots, kW-1} \\\\\n& \\text{input}(N_i, C_j, \\text{stride[0]} \\times h + m,\n\\text{stride[1]} \\times w + n)\n\\end{aligned}\\end{split}\\]\nIf padding is non-zero, then the input is implicitly minimum value padded on both sides\nfor padding number of points. dilation controls the spacing between the kernel points.\nIt is harder to describe, but this link has a nice visualization of what dilation does.",
        "return_value": "",
        "parameters": "\nkernel_size – the size of the window to take a max over\nstride – the stride of the window. Default value is kernel_size\npadding – implicit minimum value padding to be added on both sides\ndilation – a parameter that controls the stride of elements in the window\nreturn_indices – if True, will return the max indices along with the outputs.\nUseful for torch.nn.MaxUnpool2d later\nceil_mode – when True, will use ceil instead of floor to compute the output shape\n\n\n\n\n",
        "input_shape": "Input: \\((N, C, H_{in}, W_{in})\\)\nOutput: \\((N, C, H_{out}, W_{out})\\), where\n\n\\[H_{out} = \\left\\lfloor\\frac{H_{in} + 2 * \\text{padding[0]} - \\text{dilation[0]}\n      \\times (\\text{kernel_size[0]} - 1) - 1}{\\text{stride[0]}} + 1\\right\\rfloor\\]\n\n\\[W_{out} = \\left\\lfloor\\frac{W_{in} + 2 * \\text{padding[1]} - \\text{dilation[1]}\n      \\times (\\text{kernel_size[1]} - 1) - 1}{\\text{stride[1]}} + 1\\right\\rfloor\\]\n\n\n\n\n",
        "notes": "When ceil_mode=True, sliding windows are allowed to go off-bounds if they start within the left padding\nor the input. Sliding windows that would start in the right padded region are ignored.\nThe parameters kernel_size, stride, padding, dilation can either be:\na single int – in which case the same value is used for the height and width dimension\na tuple of two ints – in which case, the first int is used for the height dimension,\nand the second int for the width dimension",
        "code_example": "import oneflow as flow\nimport numpy as np\n\nm = flow.nn.MaxPool2d(kernel_size=3, padding=1, stride=1)\nx = flow.Tensor(np.random.randn(1, 4, 4, 4))\ny = m(x)\ny.shape\noneflow.Size([1, 4, 4, 4])\n\n\n\n"
    },
    {
        "api_name": "oneflow.nn.MaxPool3d",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.MaxPool3d.html",
        "api_signature": "oneflow.nn.MaxPool3d(kernel_size: Union[int, Tuple[int, int, int]], stride: Optional[Union[int, Tuple[int, int, int]]] = None, padding: Union[int, Tuple[int, int, int]] = 0, dilation: Union[int, Tuple[int, int, int]] = 1, return_indices: bool = False, ceil_mode: bool = False)",
        "api_description": "Applies a 3D max pooling over an input signal composed of several input planes.\nIn the simplest case, the output value of the layer with input size \\((N, C, D, H, W)\\),\noutput \\((N, C, D_{out}, H_{out}, W_{out})\\) and kernel_size \\((kD, kH, kW)\\)\ncan be precisely described as:\n\\[\\begin{split}\\begin{aligned}\n\\text{out}(N_i, C_j, d, h, w) ={} & \\max_{k=0, \\ldots, kD-1} \\max_{m=0, \\ldots, kH-1} \\max_{n=0, \\ldots, kW-1} \\\\\n& \\text{input}(N_i, C_j, \\text{stride[0]} \\times d + k,\n\\text{stride[1]} \\times h + m, \\text{stride[2]} \\times w + n)\n\\end{aligned}\\end{split}\\]\nIf padding is non-zero, then the input is implicitly minimum value on both sides\nfor padding number of points. dilation controls the spacing between the kernel points.\nIt is harder to describe, but this link has a nice visualization of what dilation does.",
        "return_value": "",
        "parameters": "\nkernel_size – the size of the window to take a max over\nstride – the stride of the window. Default value is kernel_size\npadding – implicit minimum value padding to be added on all three sides\ndilation – a parameter that controls the stride of elements in the window\nreturn_indices – if True, will return the max indices along with the outputs.\nUseful for torch.nn.MaxUnpool3d later\nceil_mode – when True, will use ceil instead of floor to compute the output shape\n\n\n\n\n",
        "input_shape": "Input: \\((N, C, D_{in}, H_{in}, W_{in})\\)\nOutput: \\((N, C, D_{out}, H_{out}, W_{out})\\), where\n\n\\[D_{out} = \\left\\lfloor\\frac{D_{in} + 2 \\times \\text{padding}[0] - \\text{dilation}[0] \\times\n  (\\text{kernel_size}[0] - 1) - 1}{\\text{stride}[0]} + 1\\right\\rfloor\\]\n\n\\[H_{out} = \\left\\lfloor\\frac{H_{in} + 2 \\times \\text{padding}[1] - \\text{dilation}[1] \\times\n  (\\text{kernel_size}[1] - 1) - 1}{\\text{stride}[1]} + 1\\right\\rfloor\\]\n\n\\[W_{out} = \\left\\lfloor\\frac{W_{in} + 2 \\times \\text{padding}[2] - \\text{dilation}[2] \\times\n  (\\text{kernel_size}[2] - 1) - 1}{\\text{stride}[2]} + 1\\right\\rfloor\\]\n\n\n\n\n",
        "notes": "When ceil_mode=True, sliding windows are allowed to go off-bounds if they start within the left padding\nor the input. Sliding windows that would start in the right padded region are ignored.\nThe parameters kernel_size, stride, padding, dilation can either be:\na single int – in which case the same value is used for the depth, height and width dimension\na tuple of three ints – in which case, the first int is used for the depth dimension,\nthe second int for the height dimension and the third int for the width dimension",
        "code_example": "import oneflow as flow\nimport numpy as np\n\nof_maxpool3d = flow.nn.MaxPool3d(kernel_size=3, padding=1, stride=1)\nx = flow.Tensor(np.random.randn(1, 4, 4, 4, 4))\ny = of_maxpool3d(x)\ny.shape\noneflow.Size([1, 4, 4, 4, 4])\n\n\n\n"
    },
    {
        "api_name": "oneflow.nn.MaxUnpool1d",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.MaxUnpool1d.html",
        "api_signature": "oneflow.nn.MaxUnpool1d(kernel_size: Union[int, Tuple[int]], stride: Optional[Union[int, Tuple[int]]] = None, padding: Optional[Union[int, Tuple[int]]] = 0)",
        "api_description": "Computes a partial inverse of MaxPool1d.\nMaxPool1d is not fully invertible, since the non-maximal values are lost.\nMaxUnpool1d takes in as input the output of MaxPool1d\nincluding the indices of the maximal values and computes a partial inverse\nin which all non-maximal values are set to zero.",
        "return_value": "",
        "parameters": "\nkernel_size (int or tuple) – Size of the max pooling window.\nstride (int or tuple) – Stride of the max pooling window.\nIt is set to kernel_size by default.\npadding (int or tuple) – Padding that was added to the input\n\n\n\n\nInputs:\ninput: the input Tensor to invert\nindices: the indices given out by MaxPool1d\noutput_size (optional): the targeted output size\n\n\n",
        "input_shape": "Input: \\((N, C, H_{in})\\).\nOutput: \\((N, C, H_{out})\\), where\n\n\\[H_{out} = (H_{in} - 1) \\times \\text{stride}[0] - 2 \\times \\text{padding}[0] + \\text{kernel\\_size}[0]\\]\nor as given by output_size in the call operator\n\n\n\n\n",
        "notes": "MaxPool1d can map several input sizes to the same output\nsizes. Hence, the inversion process can get ambiguous.\nTo accommodate this, you can provide the needed output size\nas an additional argument output_size in the forward call.\nSee the Inputs and Example below.\nWhen indices contains elements out of the output_size range,\nan RuntimeError will be raised on the cpu and an indeterminate\nresult will be calculated on the cuda.",
        "code_example": ">>> import oneflow as flow\n>>> pool = flow.nn.MaxPool1d(2, stride=2, return_indices=True)\n>>> unpool = flow.nn.MaxUnpool1d(2, stride=2)\n>>> input = flow.tensor([[[1., 2, 3, 4, 5, 6, 7, 8]]])\n>>> output, indices = pool(input)\n>>> unpool(output, indices)\ntensor([[[0., 2., 0., 4., 0., 6., 0., 8.]]], dtype=oneflow.float32)\n>>> # Example showcasing the use of output_size\n>>> input = flow.tensor([[[1., 2, 3, 4, 5, 6, 7, 8, 9]]])\n>>> output, indices = pool(input)\n>>> unpool(output, indices, output_size=input.size())\ntensor([[[0., 2., 0., 4., 0., 6., 0., 8., 0.]]], dtype=oneflow.float32)\n>>> unpool(output, indices)\ntensor([[[0., 2., 0., 4., 0., 6., 0., 8.]]], dtype=oneflow.float32)\n\n\n\n"
    },
    {
        "api_name": "oneflow.nn.MaxUnpool2d",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.MaxUnpool2d.html",
        "api_signature": "oneflow.nn.MaxUnpool2d(kernel_size: Union[int, Tuple[int, int]], stride: Optional[Union[int, Tuple[int, int]]] = None, padding: Optional[Union[int, Tuple[int, int]]] = 0)",
        "api_description": "Computes a partial inverse of MaxPool2d.\nMaxPool2d is not fully invertible, since the non-maximal values are lost.\nMaxUnpool2d takes in as input the output of MaxPool2d\nincluding the indices of the maximal values and computes a partial inverse\nin which all non-maximal values are set to zero.",
        "return_value": "",
        "parameters": "\nkernel_size (int or tuple) – Size of the max pooling window.\nstride (int or tuple) – Stride of the max pooling window.\nIt is set to kernel_size by default.\npadding (int or tuple) – Padding that was added to the input\n\n\n\n\nInputs:\ninput: the input Tensor to invert\nindices: the indices given out by MaxPool2d\noutput_size (optional): the targeted output size\n\n\n",
        "input_shape": "Input: \\((N, C, H_{in}, W_{in})\\) .\nOutput: \\((N, C, H_{out}, W_{out})\\), where\n\n\\[H_{out} = (H_{in} - 1) \\times \\text{stride[0]} - 2 \\times \\text{padding[0]} + \\text{kernel\\_size[0]}\\]\n\n\\[W_{out} = (W_{in} - 1) \\times \\text{stride[1]} - 2 \\times \\text{padding[1]} + \\text{kernel\\_size[1]}\\]\nor as given by output_size in the call operator\n\n\n\n\n",
        "notes": "MaxPool2d can map several input sizes to the same output\nsizes. Hence, the inversion process can get ambiguous.\nTo accommodate this, you can provide the needed output size\nas an additional argument output_size in the forward call.\nSee the Inputs and Example below.\nWhen indices contains elements out of the output_size range,\nan RuntimeError will be raised on the cpu and an indeterminate\nresult will be calculated on the cuda.",
        "code_example": ">>> import oneflow as flow\n>>> pool = flow.nn.MaxPool2d(2, stride=2, return_indices=True)\n>>> unpool = flow.nn.MaxUnpool2d(2, stride=2)\n>>> input = flow.tensor([[[[ 1.,  2,  3,  4],\n...                         [ 5,  6,  7,  8],\n...                         [ 9, 10, 11, 12],\n...                         [13, 14, 15, 16]]]])\n>>> output, indices = pool(input)\n>>> unpool(output, indices) \ntensor([[[[ 0.,  0.,  0.,  0.],\n        [ 0.,  6.,  0.,  8.],\n        [ 0.,  0.,  0.,  0.],\n        [ 0., 14.,  0., 16.]]]], dtype=oneflow.float32)\n>>> # specify a different output size than input size\n>>> unpool(output, indices, output_size=flow.Size([1, 1, 5, 5])) \ntensor([[[[ 0.,  0.,  0.,  0.,  0.],\n        [ 6.,  0.,  8.,  0.,  0.],\n        [ 0.,  0.,  0., 14.,  0.],\n        [16.,  0.,  0.,  0.,  0.],\n        [ 0.,  0.,  0.,  0.,  0.]]]], dtype=oneflow.float32)\n\n\n\n"
    },
    {
        "api_name": "oneflow.nn.MaxUnpool3d",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.MaxUnpool3d.html",
        "api_signature": "oneflow.nn.MaxUnpool3d(kernel_size: Union[int, Tuple[int, int, int]], stride: Optional[Union[int, Tuple[int, int, int]]] = None, padding: Optional[Union[int, Tuple[int, int, int]]] = 0)",
        "api_description": "Computes a partial inverse of MaxPool3d.\nMaxPool3d is not fully invertible, since the non-maximal values are lost.\nMaxUnpool3d takes in as input the output of MaxPool3d\nincluding the indices of the maximal values and computes a partial inverse\nin which all non-maximal values are set to zero.",
        "return_value": "",
        "parameters": "\nkernel_size (int or tuple) – Size of the max pooling window.\nstride (int or tuple) – Stride of the max pooling window.\nIt is set to kernel_size by default.\npadding (int or tuple) – Padding that was added to the input\n\n\n\n\nInputs:\ninput: the input Tensor to invert\nindices: the indices given out by MaxPool3d\noutput_size (optional): the targeted output size\n\n\n",
        "input_shape": "Input: \\((N, C, D_{in}, H_{in}, W_{in})\\).\nOutput: \\((N, C, D_{out}, H_{out}, W_{out})\\), where\n\n\\[D_{out} = (D_{in} - 1) \\times \\text{stride[0]} - 2 \\times \\text{padding[0]} + \\text{kernel\\_size[0]}\\]\n\n\\[H_{out} = (H_{in} - 1) \\times \\text{stride[1]} - 2 \\times \\text{padding[1]} + \\text{kernel\\_size[1]}\\]\n\n\\[W_{out} = (W_{in} - 1) \\times \\text{stride[2]} - 2 \\times \\text{padding[2]} + \\text{kernel\\_size[2]}\\]\nor as given by output_size in the call operator\n\n\n\n\n",
        "notes": "MaxPool3d can map several input sizes to the same output\nsizes. Hence, the inversion process can get ambiguous.\nTo accommodate this, you can provide the needed output size\nas an additional argument output_size in the forward call.\nSee the Inputs section below.\nWhen indices contains elements out of the output_size range,\nan RuntimeError will be raised on the cpu and an indeterminate\nresult will be calculated on the cuda.",
        "code_example": ">>> import oneflow as flow\n>>> # pool of square window of size=3, stride=2\n>>> pool = flow.nn.MaxPool3d(3, stride=2, return_indices=True)\n>>> unpool = flow.nn.MaxUnpool3d(3, stride=2)\n>>> output, indices = pool(flow.randn(20, 16, 51, 33, 15))\n>>> unpooled_output = unpool(output, indices)\n>>> unpooled_output.size()\noneflow.Size([20, 16, 51, 33, 15])\n\n\n\n"
    },
    {
        "api_name": "oneflow.nn.AdaptiveAvgPool1d",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.AdaptiveAvgPool1d.html",
        "api_signature": "oneflow.nn.AdaptiveAvgPool1d(output_size: Union[int, Tuple[int]])",
        "api_description": "Applies a 1D adaptive average pooling over an input signal composed of several input planes.\nThe output size is H, for any input size.\nThe number of output features is equal to the number of input planes.",
        "return_value": "",
        "parameters": "output_size – the target output size H\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import numpy as np\n>>> import oneflow as flow\n>>> import oneflow.nn as nn\n\n>>> m = nn.AdaptiveAvgPool1d(5)\n>>> input = flow.Tensor(np.random.randn(1, 64, 8))\n>>> output = m(input)\n>>> output.size()\noneflow.Size([1, 64, 5])\n\n\n\n"
    },
    {
        "api_name": "oneflow.nn.AdaptiveAvgPool2d",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.AdaptiveAvgPool2d.html",
        "api_signature": "oneflow.nn.AdaptiveAvgPool2d(output_size, data_format=None)",
        "api_description": "Applies a 2D adaptive average pooling over an input signal composed of several input planes.\nThe output is of size H x W, for any input size.\nThe number of output features is equal to the number of input planes.",
        "return_value": "",
        "parameters": "output_size – the target output size of the image of the form H x W.\nCan be a tuple (H, W) or a single H for a square image H x H.\nH and W can be either a int, or None which means the size will\nbe the same as that of the input.\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import numpy as np\n>>> import oneflow as flow\n>>> import oneflow.nn as nn\n\n>>> m = nn.AdaptiveAvgPool2d((5,7))\n>>> input = flow.Tensor(np.random.randn(1, 64, 8, 9))\n>>> output = m(input)\n>>> output.size()\noneflow.Size([1, 64, 5, 7])\n\n>>> m = nn.AdaptiveAvgPool2d(7)\n>>> input = flow.Tensor(np.random.randn(1, 64, 10, 9))\n>>> output = m(input)\n>>> output.size()\noneflow.Size([1, 64, 7, 7])\n\n>>> m = nn.AdaptiveAvgPool2d((None, 7))\n>>> input = flow.Tensor(np.random.randn(1, 64, 10, 9))\n>>> output = m(input)\n>>> output.size()\noneflow.Size([1, 64, 10, 7])\n\n\n\n"
    },
    {
        "api_name": "oneflow.nn.AdaptiveAvgPool3d",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.AdaptiveAvgPool3d.html",
        "api_signature": "oneflow.nn.AdaptiveAvgPool3d(output_size)",
        "api_description": "Applies a 3D adaptive average pooling over an input signal composed of several input planes.\nThe output is of size D x H x W, for any input size.\nThe number of output features is equal to the number of input planes.",
        "return_value": "",
        "parameters": "output_size – the target output size of the form D x H x W.\nCan be a tuple (D, H, W) or a single number D for a cube D x D x D.\nD, H and W can be either a int, or None which means the size will\nbe the same as that of the input.\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import numpy as np\n>>> import oneflow as flow\n>>> import oneflow.nn as nn\n\n>>> m = nn.AdaptiveAvgPool3d((5,7,9))\n>>> input = flow.Tensor(np.random.randn(1, 64, 8, 9, 10))\n>>> output = m(input)\n>>> output.size()\noneflow.Size([1, 64, 5, 7, 9])\n\n>>> m = nn.AdaptiveAvgPool3d(7)\n>>> input = flow.Tensor(np.random.randn(1, 64, 10, 9, 8))\n>>> output = m(input)\n>>> output.size()\noneflow.Size([1, 64, 7, 7, 7])\n\n>>> m = nn.AdaptiveAvgPool3d((7, None, None))\n>>> input = flow.Tensor(np.random.randn(1, 64, 10, 9, 8))\n>>> output = m(input)\n>>> output.size()\noneflow.Size([1, 64, 7, 9, 8])\n\n\n\n"
    },
    {
        "api_name": "oneflow.nn.AdaptiveMaxPool1d",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.AdaptiveMaxPool1d.html",
        "api_signature": "oneflow.nn.AdaptiveMaxPool1d(output_size, return_indices: bool = False)",
        "api_description": "Applies a 1D adaptive max pooling over an input signal composed of several input planes.\nThe documentation is referenced from:\nThe output size is \\(L_{out}\\), for any input size.\nThe number of output features is equal to the number of input planes.",
        "return_value": "",
        "parameters": "\noutput_size – the target output size \\(L_{out}\\).\nreturn_indices – if True, will return the indices along with the outputs.\nDefault: False\n\n\n\n\n",
        "input_shape": "Input: \\((N, C, L_{in})\\).\nOutput: \\((N, C, L_{out})\\), where \\(L_{out}=\\text{output_size}\\).\n\n\n\n",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.nn.AdaptiveMaxPool2d",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.AdaptiveMaxPool2d.html",
        "api_signature": "oneflow.nn.AdaptiveMaxPool2d(output_size, return_indices=False, data_format=None)",
        "api_description": "Applies a 2D adaptive max pooling over an input signal composed of several input planes.\nThe documentation is referenced from:\nThe output is of size \\(H_{out} \\times W_{out}\\), for any input size.\nThe number of output features is equal to the number of input planes.",
        "return_value": "",
        "parameters": "\noutput_size – the target output size of the image of the form \\(H_{out} \\times W_{out}\\).\nCan be a tuple \\((H_{out}, W_{out})\\) or a single \\(H_{out}\\) for a\nsquare image \\(H_{out} \\times H_{out}\\). \\(H_{out}\\) and \\(W_{out}\\)\nshould be a int.\nreturn_indices – if True, will return the indices along with the outputs.\nDefault: False\n\n\n\n\n",
        "input_shape": "Input: \\((N, C, H_{in}, W_{in})\\).\nOutput: \\((N, C, H_{out}, W_{out})\\), where\n\\((H_{out}, W_{out})=\\text{output_size}\\).\n\n\n\n",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.nn.AdaptiveMaxPool3d",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.AdaptiveMaxPool3d.html",
        "api_signature": "oneflow.nn.AdaptiveMaxPool3d(output_size, return_indices: bool = False)",
        "api_description": "Applies a 3D adaptive max pooling over an input signal composed of several input planes.\nThe documentation is referenced from:\nThe output is of size \\(D_{out} \\times H_{out} \\times W_{out}\\), for any input size.\nThe number of output features is equal to the number of input planes.",
        "return_value": "",
        "parameters": "\noutput_size – the target output size of the image of the form \\(D_{out} \\times H_{out} \\times W_{out}\\).\nCan be a tuple \\((D_{out}, H_{out}, W_{out})\\) or a single\n\\(D_{out}\\) for a cube \\(D_{out} \\times D_{out} \\times D_{out}\\).\n\\(D_{out}\\), \\(H_{out}\\) and \\(W_{out}\\) should be a\nint.\nreturn_indices – if True, will return the indices along with the outputs.\nDefault: False\n\n\n\n\n",
        "input_shape": "Input: \\((N, C, D_{in}, H_{in}, W_{in})\\).\nOutput: \\((N, C, D_{out}, H_{out}, W_{out})\\),\nwhere \\((D_{out}, H_{out}, W_{out})=\\text{output_size}\\).\n\n\n\n",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.nn.AvgPool1d",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.AvgPool1d.html",
        "api_signature": "oneflow.nn.AvgPool1d(kernel_size: Union[int, Tuple[int, int]], stride: Optional[Union[int, Tuple[int, int]]] = None, padding: Union[int, Tuple[int, int]] = 0, ceil_mode: bool = False, count_include_pad: bool = True)",
        "api_description": "Applies a 1D average pooling over an input signal composed of several input planes.\nIn the simplest case, the output value of the layer with input size \\((N, C, H, W)\\),\noutput \\((N, C, H_{out}, W_{out})\\) and kernel_size \\(k\\)\ncan be precisely described as:\n\\[\\begin{split}out(N_i, C_j, l)  = \\\\frac{1}{k} \\\\sum_{m=0}^{k-1}\ninput(N_i, C_j, stride[0] \\\\times h + m, stride*l + m)\\end{split}\\]\nIf padding is non-zero, then the input is implicitly zero-padded on both sides for padding number of points.\nThe parameters kernel_size, stride, padding can each be an int or a one-element tuple.",
        "return_value": "",
        "parameters": "\nkernel_size – the size of the window.\nstrides – the stride of the window. Default value is kernel_size.\npadding – implicit zero padding to be added on both sides.\nceil_mode – when True, will use ceil instead of floor to compute the output shape.\ncount_include_pad – when True, will include the zero-padding in the averaging calculation.\n\n\n\n",
        "input_shape": "",
        "notes": "When ceil_mode=True, sliding windows are allowed to go off-bounds if they start within the left padding or the\ninput. Sliding windows that would start in the right padded region are ignored.",
        "code_example": "import oneflow as flow\nimport numpy as np\n\nm = flow.nn.AvgPool1d(kernel_size=3, padding=1, stride=1)\nx = flow.tensor(np.random.randn(1, 4, 4))\ny = m(x)\ny.shape\noneflow.Size([1, 4, 4])\n\n\n\n"
    },
    {
        "api_name": "oneflow.nn.AvgPool2d",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.AvgPool2d.html",
        "api_signature": "oneflow.nn.AvgPool2d(kernel_size: Union[int, Tuple[int, int]], stride: Optional[Union[int, Tuple[int, int]]] = None, padding: Union[int, Tuple[int, int]] = 0, ceil_mode: bool = False, count_include_pad: bool = True, divisor_override: int = 0)",
        "api_description": "Performs the 2d-average pooling on the input.\nIn the simplest case, the output value of the layer with input size \\((N, C, H, W)\\),\noutput \\((N, C, H_{out}, W_{out})\\) and kernel_size \\((kH, kW)\\)\ncan be precisely described as:\n\\[out(N_i, C_j, h, w)  = \\frac{1}{kH * kW} \\sum_{m=0}^{kH-1} \\sum_{n=0}^{kW-1}\ninput(N_i, C_j, stride[0] \\times h + m, stride[1] \\times w + n)\\]",
        "return_value": "",
        "parameters": "\nkernel_size (Union[int, Tuple[int, int]]) – An int or list of ints that has length 1, 2. The size of the window for each dimension of the input Tensor.\nstrides (Union[int, Tuple[int, int]]) – An int or list of ints that has length 1, 2. The stride of the sliding window for each dimension of the input Tensor.\npadding (Tuple[int, int]) – An int or list of ints that has length 1, 2. Implicit zero padding to be added on both sides.\nceil_mode (bool, default to False) – When True, will use ceil instead of floor to compute the output shape.\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": "import oneflow as flow\nimport numpy as np\n\nm = flow.nn.AvgPool2d(kernel_size=3, padding=1, stride=1)\nx = flow.tensor(np.random.randn(1, 4, 4, 4))\ny = m(x)\ny.shape\noneflow.Size([1, 4, 4, 4])\n\n\n\n"
    },
    {
        "api_name": "oneflow.nn.AvgPool3d",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.AvgPool3d.html",
        "api_signature": "oneflow.nn.AvgPool3d(kernel_size: Union[int, Tuple[int, int, int]], stride: Optional[Union[int, Tuple[int, int, int]]] = None, padding: Union[int, Tuple[int, int, int]] = 0, ceil_mode: bool = False, count_include_pad: bool = True, divisor_override: int = 0)",
        "api_description": "Applies a 3D average pooling over an input signal composed of several input planes.\nIn the simplest case, the output value of the layer with input size \\((N, C, D, H, W)\\),\noutput \\((N, C, D_{out}, H_{out}, W_{out})\\) and kernel_size \\((kD, kH, kW)\\)\ncan be precisely described as:\n\\[\\begin{split}out(N_i, C_j, d, h, w)  = \\\\frac{1}{kD * kH * kW } \\\\sum_{k=0}^{kD-1} \\\\sum_{m=0}^{kH-1} \\\\sum_{n=0}^{kW-1}\ninput(N_i, C_j, stride[0] \\\\times d + k, stride[1] \\\\times h + m, stride[2] \\\\times w + n)\\end{split}\\]\nIf padding is non-zero, then the input is implicitly zero-padded on all three sides for padding number of points.",
        "return_value": "",
        "parameters": "\nkernel_size – the size of the window.\nstrides – the stride of the window. Default value is kernel_size.\npadding – implicit zero padding to be added on all three sides.\nceil_mode – when True, will use ceil instead of floor to compute the output shape.\ncount_include_pad – when True, will include the zero-padding in the averaging calculation.\ndivisor_override – if specified, it will be used as divisor, otherwise kernel_size will be used.\n\n\n\n\n",
        "input_shape": "Input: \\((N, C, D_{in}, H_{in}, W_{in})\\)\nOutput: \\((N, C, D_{out}, H_{out}, W_{out})\\), where\n\n\\[\\begin{split}D_{out} = \\\\left\\\\lfloor\\\\frac{D_{in} + 2 \\\\times \\\\text{padding}[0] - \\\\text{kernel_size}[0]}{\\\\text{stride}[0]} + 1\\\\right\\\\rfloor\\end{split}\\]\n\n\\[\\begin{split}H_{out} = \\\\left\\\\lfloor\\\\frac{H_{in} + 2 \\\\times \\\\text{padding}[1] - \\\\text{kernel_size}[1]}{\\\\text{stride}[1]} + 1\\\\right\\\\rfloor\\end{split}\\]\n\n\\[\\begin{split}W_{out} = \\\\left\\\\lfloor\\\\frac{W_{in} + 2 \\\\times \\\\text{padding}[2] - \\\\text{kernel_size}[2]}{\\\\text{stride}[2]} + 1\\\\right\\\\rfloor\\end{split}\\]\n\n\n\n\n",
        "notes": "When ceil_mode=True, sliding windows are allowed to go off-bounds if they start within the left padding or the\ninput. Sliding windows that would start in the right padded region are ignored.",
        "code_example": "import oneflow as flow\nimport numpy as np\n\nm = flow.nn.AvgPool3d(kernel_size=(2,2,2),padding=(0,0,0),stride=(1,1,1))\nx = flow.tensor(np.random.randn(9, 7, 11, 32, 20))\ny = m(x)\ny.shape\noneflow.Size([9, 7, 10, 31, 19])\n\n\n\n"
    },
    {
        "api_name": "oneflow.nn.ConstantPad1d",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.ConstantPad1d.html",
        "api_signature": "oneflow.nn.ConstantPad1d(padding)",
        "api_description": "Pads the input tensor boundaries with a constant value.\nFor N-dimensional padding, use torch.nn.functional.pad().",
        "return_value": "",
        "parameters": "\npadding (int, list, tuple) – the size of the padding. If is int, uses the same\npadding in both boundaries. If a 2-tuple, uses\n(\\(\\text{padding_left}\\), \\(\\text{padding_right}\\))\nvalue (int, float) – The constant value used for padding. Defaults to 0.\n\n\n\n\n",
        "input_shape": "Input: \\((N, C, W_{in})\\)\nOutput: \\((N, C, W_{out})\\) where\n\\(W_{out} = W_{in} + \\text{padding\\_left} + \\text{padding\\_right}\\)\n\n\n\n\n",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> import numpy as np\n\n>>> input = flow.tensor(np.arange(8).reshape(2,2,2).astype(np.float32))\n>>> m = flow.nn.ConstantPad1d(padding=[1, 2], value=9.9999)\n>>> output = m(input)\n>>> output\ntensor([[[9.9999, 0.0000, 1.0000, 9.9999, 9.9999],\n         [9.9999, 2.0000, 3.0000, 9.9999, 9.9999]],\n\n        [[9.9999, 4.0000, 5.0000, 9.9999, 9.9999],\n         [9.9999, 6.0000, 7.0000, 9.9999, 9.9999]]], dtype=oneflow.float32)\n\n\n\n"
    },
    {
        "api_name": "oneflow.nn.ConstantPad2d",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.ConstantPad2d.html",
        "api_signature": "oneflow.nn.ConstantPad2d(padding)",
        "api_description": "This operator pads the input with constant value that user specifies.\nUser can set the amount of padding by setting the parameter paddings.\nThe documentation is referenced from:",
        "return_value": "",
        "parameters": "\npadding (int, tuple, list) – the size of the padding.\nIf is int, uses the same padding in all boundaries.\nIf a 4-tuple, uses\n(\\(\\mathrm{padding_{left}}\\), \\(\\mathrm{padding_{right}}\\), \\(\\mathrm{padding_{top}}\\), \\(\\mathrm{padding_{bottom}}\\))\nvalue (int, float) – The constant value used for padding. Defaults to 0.\n\n\n\n\n",
        "input_shape": "Input: \\((N, C, H_{in}, W_{in})\\)\nOutput: \\((N, C, H_{out}, W_{out})\\) where\n\\(H_{out} = H_{in} + \\mathrm{padding_{top}} + \\mathrm{padding_{bottom}}\\)\n\\(W_{out} = W_{in} + \\mathrm{padding_{left}} + \\mathrm{padding_{right}}\\)\n\n\n\n\n",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> import numpy as np\n\n>>> m = flow.nn.ConstantPad2d((2, 2, 1, 1), 1)\n>>> input = flow.tensor(np.arange(18).reshape((1, 2, 3, 3)).astype(np.float32))\n>>> output = m(input)\n>>> output.shape\noneflow.Size([1, 2, 5, 7])\n>>> output\ntensor([[[[ 1.,  1.,  1.,  1.,  1.,  1.,  1.],\n          [ 1.,  1.,  0.,  1.,  2.,  1.,  1.],\n          [ 1.,  1.,  3.,  4.,  5.,  1.,  1.],\n          [ 1.,  1.,  6.,  7.,  8.,  1.,  1.],\n          [ 1.,  1.,  1.,  1.,  1.,  1.,  1.]],\n\n         [[ 1.,  1.,  1.,  1.,  1.,  1.,  1.],\n          [ 1.,  1.,  9., 10., 11.,  1.,  1.],\n          [ 1.,  1., 12., 13., 14.,  1.,  1.],\n          [ 1.,  1., 15., 16., 17.,  1.,  1.],\n          [ 1.,  1.,  1.,  1.,  1.,  1.,  1.]]]], dtype=oneflow.float32)\n\n\n\n"
    },
    {
        "api_name": "oneflow.nn.ConstantPad3d",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.ConstantPad3d.html",
        "api_signature": "oneflow.nn.ConstantPad3d(padding)",
        "api_description": "Pads the input tensor boundaries with a constant value.\nFor N-dimensional padding, use flow.nn.functional.pad().",
        "return_value": "",
        "parameters": "\npadding (int, list, tuple) – the size of the padding. If is int, uses the same\npadding in all boundaries. If a 6-tuple, uses\n(\\(\\text{padding_left}\\), \\(\\text{padding_right}\\),\n\\(\\text{padding_top}\\), \\(\\text{padding_bottom}\\),\n\\(\\text{padding_front}\\), \\(\\text{padding_back}\\))\nvalue (int, float) – The constant value used for padding. Defaults to 0.\n\n\n\n\n",
        "input_shape": "Input: \\((N, C, D_{in}, H_{in}, W_{in})\\)\nOutput: \\((N, C, D_{out}, H_{out}, W_{out})\\) where\n\\(D_{out} = D_{in} + \\text{padding_front} + \\text{padding_back}\\)\n\\(H_{out} = H_{in} + \\text{padding_top} + \\text{padding_bottom}\\)\n\\(W_{out} = W_{in} + \\text{padding_left} + \\text{padding_right}\\)\n\n\n\n\n",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.nn.ReflectionPad1d",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.ReflectionPad1d.html",
        "api_signature": "oneflow.nn.ReflectionPad1d(padding)",
        "api_description": "This operator pads the input tensor using the reflection of the input boundary.\nThe documentation is referenced from:",
        "return_value": "\n",
        "parameters": "padding (Union[int,tuple]) – The size or bundary of padding, if is int uses the same padding in all dimension; if 4-dims tuple, uses \\((\\text{padding}_{\\text{left}}, \\text{padding}_{\\text{right}}, \\text{padding}_{\\text{top}}, \\text{padding}_{\\text{bottom}} )\\)\n\n",
        "input_shape": "Input: \\((C, W_{in})\\) or \\((N, C, W_{in})\\).\nOutput: \\((C, W_{out})\\) or \\((N, C, W_{out})\\), where\n\\(W_{out} = W_{in} + \\text{padding_left} + \\text{padding_right}\\)\n\n\n\n\n",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> import numpy as np\n>>> input = flow.tensor(np.arange(18).reshape((2, 3, 3)).astype(np.float32))\n>>> m = flow.nn.ReflectionPad1d((2, 2))\n>>> out = m(input)\n>>> out\ntensor([[[ 2.,  1.,  0.,  1.,  2.,  1.,  0.],\n         [ 5.,  4.,  3.,  4.,  5.,  4.,  3.],\n         [ 8.,  7.,  6.,  7.,  8.,  7.,  6.]],\n\n        [[11., 10.,  9., 10., 11., 10.,  9.],\n         [14., 13., 12., 13., 14., 13., 12.],\n         [17., 16., 15., 16., 17., 16., 15.]]], dtype=oneflow.float32)\n\n\n\n"
    },
    {
        "api_name": "oneflow.nn.ReflectionPad2d",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.ReflectionPad2d.html",
        "api_signature": "oneflow.nn.ReflectionPad2d(padding)",
        "api_description": "This operator pads the input tensor using the reflection of the input boundary.\nThe documentation is referenced from:",
        "return_value": "\n",
        "parameters": "padding (Union[int,tuple]) – The size or bundary of padding, if is int uses the same padding in all dimension; if 4-dims tuple, uses \\((\\text{padding}_{\\text{left}}, \\text{padding}_{\\text{right}}, \\text{padding}_{\\text{top}}, \\text{padding}_{\\text{bottom}} )\\)\n\n",
        "input_shape": "Input: \\((N, C, H_{\\text{in}}, W_{\\text{in}})\\) or \\((C, H_{in}, W_{in})\\)\nOutput: \\((N, C, H_{\\text{out}}, W_{\\text{out}})\\) or \\((C, H_{out}, W_{out})\\) where\n\\(H_{\\text{out}} = H_{\\text{in}} + \\text{padding}_{\\text{top}} + \\text{padding}_{\\text{bottom}}\\)\n\\(W_{\\text{out}} = W_{\\text{in}} + \\text{padding}_{\\text{left}} + \\text{padding}_{\\text{right}}\\)\n\n\n\n\n",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> import numpy as np\n>>> input = flow.tensor(np.arange(18).reshape((1, 2, 3, 3)).astype(np.float32))\n>>> m = flow.nn.ReflectionPad2d((2, 2, 1, 1))\n>>> out = m(input)\n>>> out\ntensor([[[[ 5.,  4.,  3.,  4.,  5.,  4.,  3.],\n          [ 2.,  1.,  0.,  1.,  2.,  1.,  0.],\n          [ 5.,  4.,  3.,  4.,  5.,  4.,  3.],\n          [ 8.,  7.,  6.,  7.,  8.,  7.,  6.],\n          [ 5.,  4.,  3.,  4.,  5.,  4.,  3.]],\n\n         [[14., 13., 12., 13., 14., 13., 12.],\n          [11., 10.,  9., 10., 11., 10.,  9.],\n          [14., 13., 12., 13., 14., 13., 12.],\n          [17., 16., 15., 16., 17., 16., 15.],\n          [14., 13., 12., 13., 14., 13., 12.]]]], dtype=oneflow.float32)\n\n\n\n"
    },
    {
        "api_name": "oneflow.nn.ReplicationPad1d",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.ReplicationPad1d.html",
        "api_signature": "oneflow.nn.ReplicationPad1d(padding)",
        "api_description": "Pads the input tensor using replication of the input boundary.\nThe documentation is referenced from:\nFor N-dimensional padding, use oneflow.nn.functional.pad().",
        "return_value": "",
        "parameters": "padding (int, tuple) – the size of the padding. If is int, uses the same\npadding in all boundaries. If a 2-tuple, uses\n(\\(\\text{padding_left}\\), \\(\\text{padding_right}\\))\n\n\n\n",
        "input_shape": "Input: \\((C, W_{in})\\) or \\((N, C, W_{in})\\).\nOutput: \\((C, W_{out})\\) or \\((N, C, W_{out})\\), where\n\\(W_{out} = W_{in} + \\text{padding_left} + \\text{padding_right}\\)\n\n\n\n\n",
        "notes": "",
        "code_example": ">>> import numpy as np\n>>> import oneflow as flow\n>>> m = flow.nn.ReplicationPad1d((2, 2))\n>>> input = flow.tensor(np.arange(18).reshape((2, 3, 3)).astype(np.float32))\n>>> out = m(input)\n>>> out\ntensor([[[ 0.,  0.,  0.,  1.,  2.,  2.,  2.],\n         [ 3.,  3.,  3.,  4.,  5.,  5.,  5.],\n         [ 6.,  6.,  6.,  7.,  8.,  8.,  8.]],\n\n        [[ 9.,  9.,  9., 10., 11., 11., 11.],\n         [12., 12., 12., 13., 14., 14., 14.],\n         [15., 15., 15., 16., 17., 17., 17.]]], dtype=oneflow.float32)\n\n\n\n"
    },
    {
        "api_name": "oneflow.nn.ReplicationPad2d",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.ReplicationPad2d.html",
        "api_signature": "oneflow.nn.ReplicationPad2d(padding)",
        "api_description": "Pads the input tensor using the replication of the input boundary.\nThe documentation is referenced from:",
        "return_value": "",
        "parameters": "padding (Union[int, tuple, list]) – the size of the padding. If is int, uses the same padding in all boundaries. If a 4-tuple, uses (\\(\\mathrm{padding_{left}}\\), \\(\\mathrm{padding_{right}}\\), \\(\\mathrm{padding_{top}}\\), \\(\\mathrm{padding_{bottom}}\\))\n\n\n\n",
        "input_shape": "Input: \\((N, C, H_{\\text{in}}, W_{\\text{in}})\\) or \\((C, H_{in}, W_{in})\\)\nOutput: \\((N, C, H_{\\text{out}}, W_{\\text{out}})\\) or \\((C, H_{out}, W_{out})\\) where\n\n\\(H_{out} = H_{in} + \\mathrm{padding_{top}} + \\mathrm{padding_{bottom}}\\)\n\\(W_{out} = W_{in} + \\mathrm{padding_{left}} + \\mathrm{padding_{right}}\\)\n\n\n\n\n\n",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> import numpy as np\n>>> m = flow.nn.ReplicationPad2d((2, 2, 1, 1))\n>>> input = flow.tensor(np.arange(18).reshape((1, 2, 3, 3)).astype(np.float32))\n>>> input_int = flow.tensor(np.arange(18).reshape((1, 2, 3, 3)).astype(np.int32))\n>>> output = m(input)\n>>> output.shape\noneflow.Size([1, 2, 5, 7])\n>>> output\ntensor([[[[ 0.,  0.,  0.,  1.,  2.,  2.,  2.],\n          [ 0.,  0.,  0.,  1.,  2.,  2.,  2.],\n          [ 3.,  3.,  3.,  4.,  5.,  5.,  5.],\n          [ 6.,  6.,  6.,  7.,  8.,  8.,  8.],\n          [ 6.,  6.,  6.,  7.,  8.,  8.,  8.]],\n\n         [[ 9.,  9.,  9., 10., 11., 11., 11.],\n          [ 9.,  9.,  9., 10., 11., 11., 11.],\n          [12., 12., 12., 13., 14., 14., 14.],\n          [15., 15., 15., 16., 17., 17., 17.],\n          [15., 15., 15., 16., 17., 17., 17.]]]], dtype=oneflow.float32)\n\n\n\n"
    },
    {
        "api_name": "oneflow.nn.ZeroPad2d",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.ZeroPad2d.html",
        "api_signature": "oneflow.nn.ZeroPad2d(padding)",
        "api_description": "Pads the input tensor boundaries with zero. User can set the amount of padding by setting the parameter paddings.\nThe documentation is referenced from:",
        "return_value": "",
        "parameters": "padding (Union[int, tuple]) – the size of the padding. If is int, uses the same padding in all boundaries. If a 4-tuple, uses (\\(\\mathrm{padding_{left}}\\), \\(\\mathrm{padding_{right}}\\), \\(\\mathrm{padding_{top}}\\), \\(\\mathrm{padding_{bottom}}\\))\n\n\n\n",
        "input_shape": "Input: \\((N, C, H_{in}, W_{in})\\)\nOutput: \\((N, C, H_{out}, W_{out})\\) where\n\n\\(H_{out} = H_{in} + \\mathrm{padding_{top}} + \\mathrm{padding_{bottom}}\\)\n\\(W_{out} = W_{in} + \\mathrm{padding_{left}} + \\mathrm{padding_{right}}\\)\n\n\n\n\n\n",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> import numpy as np\n>>> m1 = flow.nn.ZeroPad2d(2)\n>>> m2 = flow.nn.ZeroPad2d((1,2,2,0))\n>>> input = flow.tensor(np.arange(18).reshape((1, 2, 3, 3)).astype(np.float32))\n>>> output = m1(input)\n>>> output.shape\noneflow.Size([1, 2, 7, 7])\n>>> output\ntensor([[[[ 0.,  0.,  0.,  0.,  0.,  0.,  0.],\n          [ 0.,  0.,  0.,  0.,  0.,  0.,  0.],\n          [ 0.,  0.,  0.,  1.,  2.,  0.,  0.],\n          [ 0.,  0.,  3.,  4.,  5.,  0.,  0.],\n          [ 0.,  0.,  6.,  7.,  8.,  0.,  0.],\n          [ 0.,  0.,  0.,  0.,  0.,  0.,  0.],\n          [ 0.,  0.,  0.,  0.,  0.,  0.,  0.]],\n\n         [[ 0.,  0.,  0.,  0.,  0.,  0.,  0.],\n          [ 0.,  0.,  0.,  0.,  0.,  0.,  0.],\n          [ 0.,  0.,  9., 10., 11.,  0.,  0.],\n          [ 0.,  0., 12., 13., 14.,  0.,  0.],\n          [ 0.,  0., 15., 16., 17.,  0.,  0.],\n          [ 0.,  0.,  0.,  0.,  0.,  0.,  0.],\n          [ 0.,  0.,  0.,  0.,  0.,  0.,  0.]]]], dtype=oneflow.float32)\n>>> output = m2(input)\n>>> output\ntensor([[[[ 0.,  0.,  0.,  0.,  0.,  0.],\n          [ 0.,  0.,  0.,  0.,  0.,  0.],\n          [ 0.,  0.,  1.,  2.,  0.,  0.],\n          [ 0.,  3.,  4.,  5.,  0.,  0.],\n          [ 0.,  6.,  7.,  8.,  0.,  0.]],\n\n         [[ 0.,  0.,  0.,  0.,  0.,  0.],\n          [ 0.,  0.,  0.,  0.,  0.,  0.],\n          [ 0.,  9., 10., 11.,  0.,  0.],\n          [ 0., 12., 13., 14.,  0.,  0.],\n          [ 0., 15., 16., 17.,  0.,  0.]]]], dtype=oneflow.float32)\n\n\n\n"
    },
    {
        "api_name": "oneflow.nn.ELU",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.ELU.html",
        "api_signature": "oneflow.nn.ELU(alpha: float = 1.0, inplace: bool = False)",
        "api_description": "Applies the element-wise function\\(\\text{ELU}(x) = \\begin{cases}x & \\text{ if } x \\gt 0  \\\\\\alpha*(exp(x)-1) & \\text{ if } x \\le 0 \\\\\\end{cases}\\)",
        "return_value": "",
        "parameters": "\nalpha – the \\(\\alpha\\) value for the ELU formulation. Default: 1.0\ninplace – can optionally do the operation in-place. Default: False\n\n\n\n\n",
        "input_shape": "Input: \\((N, *)\\) where * means, any number of additional\ndimensions\nOutput: \\((N, *)\\), same shape as the input\n\n\n\n",
        "notes": "",
        "code_example": ">>> import numpy as np\n>>> import oneflow as flow\n\n>>> x = np.array([-0.5, 0, 0.5]).astype(np.float32)\n>>> input = flow.Tensor(x)\n>>> elu = flow.nn.ELU()\n\n>>> out = elu(input)\n>>> out\ntensor([-0.3935,  0.0000,  0.5000], dtype=oneflow.float32)\n\n\n\n"
    },
    {
        "api_name": "oneflow.nn.Hardshrink",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.Hardshrink.html",
        "api_signature": "oneflow.nn.Hardshrink(lambd: float = 0.5, inplace: bool = False)",
        "api_description": "The Hardshrink activation.\nThe formula is:\n\\[\\begin{split}\\text{Hardshrink}(x) =\n\\begin{cases}\nx, & \\text{ if } x > \\lambda \\\\\nx, & \\text{ if } x < -\\lambda \\\\\n0, & \\text{ otherwise }\n\\end{cases}\\end{split}\\]",
        "return_value": "",
        "parameters": "\nlambd – the \\(\\lambda\\) value for the Hardshrink formulation. Default: 0.5\ninplace – can optionally do the operation in-place. Default: False\n\n\n\n\n",
        "input_shape": "Input: \\((N, *)\\) where * means, any number of additional\ndimensions\nOutput: \\((N, *)\\), same shape as the input\n\n\n\n",
        "notes": "",
        "code_example": ">>> import numpy as np\n>>> import oneflow as flow\n>>> x = np.array([-1.1, 0, 0.2, 0.5]).astype(np.float32)\n>>> input = flow.Tensor(x)\n>>> hardshrink = flow.nn.Hardshrink(lambd=0.5)\n>>> out = hardshrink(input)\n>>> out\ntensor([-1.1000,  0.0000,  0.0000,  0.0000], dtype=oneflow.float32)\n\n\n\n"
    },
    {
        "api_name": "oneflow.nn.Hardsigmoid",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.Hardsigmoid.html",
        "api_signature": "oneflow.nn.Hardsigmoid(inplace: bool = False)",
        "api_description": "Applies the element-wise function:\n\\[\\begin{split}\\text{Hardsigmoid}(x) = \\begin{cases}\n0 & \\text{ if } x \\le -3  \\\\\n1 & \\text{ if } x \\ge +3 \\\\\n\\frac{x}{6} + \\frac{1}{2} & \\text{ otherwise } \\\\\n\\end{cases}\\end{split}\\]",
        "return_value": "",
        "parameters": "inplace – can optionally do the operation in-place. Default: False\n\n\n\n",
        "input_shape": "Input: \\((N, *)\\) where * means, any number of additional\ndimensions\nOutput: \\((N, *)\\), same shape as the input\n\n\n\n",
        "notes": "",
        "code_example": ">>> import numpy as np\n>>> import oneflow as flow\n\n>>> x = np.array([-0.5, 0, 0.5]).astype(np.float32)\n>>> input = flow.Tensor(x)\n>>> hardsigmoid = flow.nn.Hardsigmoid()\n\n>>> out = hardsigmoid(input)\n>>> out\ntensor([0.4167, 0.5000, 0.5833], dtype=oneflow.float32)\n\n\n\n"
    },
    {
        "api_name": "oneflow.nn.Hardswish",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.Hardswish.html",
        "api_signature": "oneflow.nn.Hardswish(inplace: bool = False)",
        "api_description": "Applies the hardswish function, element-wise, as described in the paper Searching for MobileNetV3.\n\\[\\begin{split}\\text{Hardswish}(x) = \\begin{cases}\n0 & \\text{ if } x \\le -3  \\\\\nx & \\text{ if } x \\ge +3 \\\\\nx*(x+3)/6 & \\text{ otherwise } \\\\\n\\end{cases}\\end{split}\\]",
        "return_value": "",
        "parameters": "inplace – can optionally do the operation in-place. Default: False\n\n\n\n",
        "input_shape": "Input: \\((N, *)\\) where * means, any number of additional\ndimensions\nOutput: \\((N, *)\\), same shape as the input\n\n\n\n>>> import numpy as np\n>>> import oneflow as flow\n\n>>> x = np.array([-0.5, 0, 0.5]).astype(np.float32)\n>>> input = flow.Tensor(x)\n>>> hardswish = flow.nn.Hardswish()\n\n>>> out = hardswish(input)\n>>> out\ntensor([-0.2083,  0.0000,  0.2917], dtype=oneflow.float32)\n\n\n\n",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.nn.Hardtanh",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.Hardtanh.html",
        "api_signature": "oneflow.nn.Hardtanh(min_val: float = - 1, max_val: float = 1, inplace: bool = False, min_value: Optional[float] = None, max_value: Optional[float] = None)",
        "api_description": "Applies the HardTanh function element-wise\nHardTanh is defined as:\n\\[\\begin{split}\\text{HardTanh}(x) = \\begin{cases}\n1 & \\text{ if } x > 1 \\\\\n-1 & \\text{ if } x < -1 \\\\\nx & \\text{ otherwise } \\\\\n\\end{cases}\\end{split}\\]\nThe range of the linear region \\([-1, 1]\\) can be adjusted using\nmin_val and max_val.",
        "return_value": "",
        "parameters": "\nmin_val – minimum value of the linear region range. Default: -1\nmax_val – maximum value of the linear region range. Default: 1\ninplace – can optionally do the operation in-place. Default: False\n\n\n\nKeyword arguments min_value and max_value\nhave been deprecated in favor of min_val and max_val.\n\n",
        "input_shape": "Input: \\((N, *)\\) where * means, any number of additional\ndimensions\nOutput: \\((N, *)\\), same shape as the input\n\n\n\n",
        "notes": "",
        "code_example": ">>> import numpy as np\n>>> import oneflow as flow\n\n>>> m = flow.nn.Hardtanh()\n>>> arr = np.array([0.2, 0.3, 3.0, 4.0])\n>>> x = flow.Tensor(arr)\n>>> out = m(x)\n>>> out\ntensor([0.2000, 0.3000, 1.0000, 1.0000], dtype=oneflow.float32)\n\n\n\n"
    },
    {
        "api_name": "oneflow.nn.LeakyReLU",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.LeakyReLU.html",
        "api_signature": "oneflow.nn.LeakyReLU(negative_slope: float = 0.01, inplace: bool = False)",
        "api_description": "Applies the element-wise function:\n\\[\\begin{split}\\text{LeakyRELU}(x) = \\begin{cases}\nx, & \\text{ if } x \\geq 0 \\\\\n\\text{negative_slope} \\times x, & \\text{ otherwise }\n\\end{cases}\\end{split}\\]",
        "return_value": "",
        "parameters": "\nnegative_slope – Controls the angle of the negative slope. Default: 1e-2\ninplace – can optionally do the operation in-place. Default: False\n\n\n\n\n",
        "input_shape": "Input: \\((N, *)\\) where * means, any number of additional\ndimensions\nOutput: \\((N, *)\\), same shape as the input\n\n\n\n",
        "notes": "",
        "code_example": ">>> import numpy as np\n>>> import oneflow as flow\n\n>>> m = flow.nn.LeakyReLU(0.1)\n>>> arr = np.array([0.2, 0.3, 3.0, 4.0])\n>>> x = flow.Tensor(arr)\n>>> out = m(x)\n>>> out\ntensor([0.2000, 0.3000, 3.0000, 4.0000], dtype=oneflow.float32)\n\n\n\n"
    },
    {
        "api_name": "oneflow.nn.LogSigmoid",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.LogSigmoid.html",
        "api_signature": null,
        "api_description": "Applies the element-wise function:\n\\[\\text{LogSigmoid}(x) = \\log\\left(\\frac{ 1 }{ 1 + \\exp(-x)}\\right)\\]",
        "return_value": "",
        "parameters": "",
        "input_shape": "Input: \\((N, *)\\) where * means, any number of additional\ndimensions\nOutput: \\((N, *)\\), same shape as the input\n\n\n\n",
        "notes": "",
        "code_example": ">>> import numpy as np\n>>> import oneflow as flow\n\n>>> x = np.array([-0.5, 0, 0.5]).astype(np.float32)\n>>> input = flow.Tensor(x)\n>>> logsigmoid = flow.nn.LogSigmoid()\n\n>>> out = logsigmoid(input)\n>>> out\ntensor([-0.9741, -0.6931, -0.4741], dtype=oneflow.float32)\n\n\n\n"
    },
    {
        "api_name": "oneflow.nn.PReLU",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.PReLU.html",
        "api_signature": "oneflow.nn.PReLU(num_parameters: int = 1, init: float = 0.25, device=None, dtype=None)",
        "api_description": "Applies the element-wise function:\n\\[PReLU(x) = \\max(0,x) + a * \\min(0,x)\\]\nHere \\(a\\) is a learnable parameter. When called without arguments, nn.PReLU() uses a single\nparameter \\(a\\) across all input channels. If called with nn.PReLU(nChannels),\na separate \\(a\\) is used for each input channel.",
        "return_value": "",
        "parameters": "\nnum_parameters (int) – number of \\(a\\) to learn.\nAlthough it takes an int as input, there is only two values are legitimate:\n1, or the number of channels at input. Default: 1\ninit (float) – the initial value of \\(a\\). Default: 0.25\n\n\n\n\n",
        "input_shape": "Input: \\((N, *)\\) where * means, any number of additional\ndimensions\nOutput: \\((N, *)\\), same shape as the input\n\n\nAttr:\nweight (Tensor): the learnable weights of shape (num_parameters).\n\n\n\n>>> import numpy as np\n>>> import oneflow as flow\n\n>>> m = flow.nn.PReLU()\n>>> input = flow.tensor(np.asarray([[[[1, -2], [3, 4]]]]), dtype=flow.float32)\n>>> print(m(input).numpy())\n[[[[ 1.  -0.5]\n   [ 3.   4. ]]]]\n\n\n\n",
        "notes": "weight decay should not be used when learning \\(a\\) for good performance.\nChannel dim is the 2nd dim of input. When input has dims < 2, then there is\nno channel dim and the number of channels = 1.",
        "code_example": ""
    },
    {
        "api_name": "oneflow.nn.ReLU",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.ReLU.html",
        "api_signature": "oneflow.nn.ReLU(inplace: bool = False)",
        "api_description": "Applies the rectified linear unit function element-wise:\n\\(\\text{ReLU}(x) = (x)^+ = \\max(0, x)\\)",
        "return_value": "",
        "parameters": "inplace – can optionally do the operation in-place. Default: False\n\n\n\n",
        "input_shape": "Input: \\((N, *)\\) where * means, any number of additional\ndimensions\nOutput: \\((N, *)\\), same shape as the input\n\n\n\n",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> import numpy as np\n>>> relu = flow.nn.ReLU()\n>>> ndarr = np.asarray([1, -2, 3])\n>>> x = flow.Tensor(ndarr)\n>>> relu(x)\ntensor([1., 0., 3.], dtype=oneflow.float32)\n\n\n\n"
    },
    {
        "api_name": "oneflow.nn.ReLU6",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.ReLU6.html",
        "api_signature": "oneflow.nn.ReLU6(inplace: bool = False)",
        "api_description": "Applies the element-wise function:\n\\[\\begin{split}\\text{Relu6}(x) = \\begin{cases}\n6 & \\text{ if } x > 6 \\\\\n0 & \\text{ if } x < 0 \\\\\nx & \\text{ otherwise } \\\\\n\\end{cases}\\end{split}\\]",
        "return_value": "",
        "parameters": "inplace – can optionally do the operation in-place. Default: False\n\n\n\n",
        "input_shape": "Input: \\((N, *)\\) where * means, any number of additional\ndimensions\nOutput: \\((N, *)\\), same shape as the input\n\n\n\n",
        "notes": "",
        "code_example": ">>> import numpy as np\n>>> import oneflow as flow\n\n>>> x = np.array([-0.5, 0, 0.5]).astype(np.float32)\n>>> input = flow.Tensor(x)\n>>> relu6 = flow.nn.ReLU6()\n\n>>> out = relu6(input)\n>>> out\ntensor([0.0000, 0.0000, 0.5000], dtype=oneflow.float32)\n\n\n\n"
    },
    {
        "api_name": "oneflow.nn.SELU",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.SELU.html",
        "api_signature": "oneflow.nn.SELU(inplace: bool = False)",
        "api_description": "Applies the element-wise function:\nThe formula is:\n\\[\\text{SELU}(x) = \\text{scale} * (\\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1)))\\]\nwith \\(\\alpha = 1.6732632423543772848170429916717\\) and\n\\(\\text{scale} = 1.0507009873554804934193349852946\\).\nWarning\nWhen using kaiming_normal or kaiming_normal_ for initialisation,\nnonlinearity='linear' should be used instead of nonlinearity='selu'\nin order to get Self-Normalizing Neural Networks.\nSee torch.nn.init.calculate_gain() for more information.\nMore details can be found in the paper Self-Normalizing Neural Networks.",
        "return_value": "",
        "parameters": "",
        "input_shape": "Input: \\((N, *)\\) where * means, any number of additional\ndimensions\nOutput: \\((N, *)\\), same shape as the input\n\n\n\n",
        "notes": "",
        "code_example": ">>> import numpy as np\n>>> import oneflow as flow\n>>> x = np.array([1, 2, 3]).astype(np.float32)\n>>> input = flow.Tensor(x)\n>>> selu = flow.nn.SELU()\n>>> out = selu(input)\n>>> out\ntensor([1.0507, 2.1014, 3.1521], dtype=oneflow.float32)\n\n\n\n"
    },
    {
        "api_name": "oneflow.nn.CELU",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.CELU.html",
        "api_signature": "oneflow.nn.CELU(alpha: float = 1.0, inplace: bool = False)",
        "api_description": "Applies the element-wise function:\n\\[\\begin{split}\\text{CELU}(x, \\alpha) = \\begin{cases}\nx & \\text{ if } x \\ge 0  \\\\\n\\alpha*(exp(\\frac{x}{\\alpha})-1) & \\text{ otherwise } \\\\\n\\end{cases}\\end{split}\\]",
        "return_value": "",
        "parameters": "\nalpha – the \\(\\alpha\\) value for the CELU formulation. Default: 1.0\ninplace – can optionally do the operation in-place. Default: False\n\n\n\n\n",
        "input_shape": "Input: \\((N, *)\\) where * means, any number of additional\ndimensions\nOutput: \\((N, *)\\), same shape as the input\n\n\n\n",
        "notes": "",
        "code_example": ">>> import numpy as np\n>>> import oneflow as flow\n\n>>> x = np.array([-0.5, 0, 0.5]).astype(np.float32)\n>>> input = flow.Tensor(x)\n>>> celu = flow.nn.CELU(alpha=0.5)\n\n>>> out = celu(input)\n>>> out\ntensor([-0.3161,  0.0000,  0.5000], dtype=oneflow.float32)\n\n\n\n"
    },
    {
        "api_name": "oneflow.nn.GELU",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.GELU.html",
        "api_signature": "oneflow.nn.GELU(approximate='none')",
        "api_description": "Applies the Gaussian Error Linear Units function:\n\\[\\text{GELU}(x) = x * \\Phi(x)\\]\nwhere \\(\\Phi(x)\\) is the Cumulative Distribution Function for Gaussian Distribution.\nWhen the approximate argument is ‘tanh’, Gelu is estimated with:\n\\[\\text{GELU}(x) = 0.5 * x * (1 + \\text{Tanh}(\\sqrt(2 / \\pi) * (x + 0.044715 * x^3)))\\]",
        "return_value": "A Tensor has same shape as the input.\n\n",
        "parameters": "\ninput (oneflow.Tensor) – Input Tensor\napproximate (string, optional) – the gelu approximation algorithm to use:\n'none' | 'tanh'. Default: 'none'\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import numpy as np\n>>> import oneflow as flow\n\n>>> x = np.array([-0.5, 0, 0.5]).astype(np.float32)\n>>> input = flow.Tensor(x)\n>>> gelu = flow.nn.GELU()\n\n>>> out = gelu(input)\n>>> out\ntensor([-0.1543,  0.0000,  0.3457], dtype=oneflow.float32)\n\n\n\n"
    },
    {
        "api_name": "oneflow.nn.QuickGELU",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.QuickGELU.html",
        "api_signature": null,
        "api_description": "Applies GELU approximation that is fast but somewhat inaccurate. See: https://github.com/hendrycks/GELUs\n\\[\\text{QuickGELU}(x) = x * \\sigma(1.702x) = x * \\frac{1}{1 + \\exp(-1.702x)}\\]",
        "return_value": "A Tensor has same shape as the input.\n\n",
        "parameters": "input (oneflow.Tensor) – Input Tensor\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n\n>>> input = flow.Tensor([-0.5, 0, 0.5])\n>>> gelu = flow.nn.QuickGELU()\n\n>>> out = gelu(input)\n>>> out\ntensor([-0.1496,  0.0000,  0.3504], dtype=oneflow.float32)\n\n\n\n"
    },
    {
        "api_name": "oneflow.nn.SquareReLU",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.SquareReLU.html",
        "api_signature": null,
        "api_description": "Applies the relu^2 activation introduced in https://arxiv.org/abs/2109.08668v2\n\\[:math:`\\text{SquareReLU}(x) = \\max(0, x) * \\max(0, x)`\\]",
        "return_value": "A Tensor has same shape as the input.\n\n",
        "parameters": "input (oneflow.Tensor) – Input Tensor\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import numpy as np\n>>> import oneflow as flow\n\n>>> x = np.array([-0.5, 0, 0.5]).astype(np.float32)\n>>> input = flow.Tensor(x)\n>>> square_relu = flow.nn.SquareReLU()\n\n>>> out = square_relu(input)\n>>> out\ntensor([0.0000, 0.0000, 0.2500], dtype=oneflow.float32)\n\n\n\n"
    },
    {
        "api_name": "oneflow.nn.SiLU",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.SiLU.html",
        "api_signature": "oneflow.nn.SiLU(inplace: bool = False)",
        "api_description": "SiLU(Swish) activation:\n\\[\\text{SiLU}(x) = x * sigmoid(x)\\]",
        "return_value": "",
        "parameters": "",
        "input_shape": "Input: \\((N, *)\\) where * means, any number of additional\ndimensions\nOutput: \\((N, *)\\), same shape as the input\n\n\n\n",
        "notes": "See Gaussian Error Linear Units (GELUs)\nwhere the SiLU (Sigmoid Linear Unit) was originally coined, and see\nSigmoid-Weighted Linear Units for Neural Network Function Approximation\nin Reinforcement Learning and Swish:\na Self-Gated Activation Function\nwhere the SiLU was experimented with later.",
        "code_example": ">>> import numpy as np\n>>> import oneflow as flow\n\n\n>>> x = np.array([1, 2, 3]).astype(np.float32)\n>>> input = flow.Tensor(x)\n>>> silu = flow.nn.SiLU()\n>>> out = silu(input)\n>>> out\ntensor([0.7311, 1.7616, 2.8577], dtype=oneflow.float32)\n\n\n\n"
    },
    {
        "api_name": "oneflow.nn.Sigmoid",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.Sigmoid.html",
        "api_signature": null,
        "api_description": "Applies the element-wise function:\n\\[\\text{Sigmoid}(x) = \\sigma(x) = \\frac{1}{1 + \\exp(-x)}\\]",
        "return_value": "",
        "parameters": "",
        "input_shape": "Input: \\((N, *)\\) where * means, any number of additional\ndimensions\nOutput: \\((N, *)\\), same shape as the input\n\n\n\n",
        "notes": "",
        "code_example": ">>> import numpy as np\n>>> import oneflow as flow\n\n>>> x = flow.Tensor(np.array([0.81733328, 0.43621480, 0.10351428]))\n>>> m = flow.nn.Sigmoid()\n>>> out = m(x)\n>>> out\ntensor([0.6937, 0.6074, 0.5259], dtype=oneflow.float32)\n\n\n\n"
    },
    {
        "api_name": "oneflow.nn.Mish",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.Mish.html",
        "api_signature": "oneflow.nn.Mish(inplace: bool = False)",
        "api_description": "Applies the element-wise function:\n\\[\\text{Mish}(x) = x * \\text{Tanh}(\\text{Softplus}(x))\\]",
        "return_value": "",
        "parameters": "",
        "input_shape": "Input: \\((N, *)\\) where * means, any number of additional\ndimensions\nOutput: \\((N, *)\\), same shape as the input\n\n\n\n",
        "notes": "See Mish: A Self Regularized Non-Monotonic Neural Activation Function",
        "code_example": ">>> import numpy as np\n>>> import oneflow as flow\n\n>>> x = np.array([1, 2, 3]).astype(np.float32)\n>>> input = flow.Tensor(x)\n>>> mish = flow.nn.Mish()\n\n>>> out = mish(input)\n>>> out\ntensor([0.8651, 1.9440, 2.9865], dtype=oneflow.float32)\n\n\n\n"
    },
    {
        "api_name": "oneflow.nn.Softplus",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.Softplus.html",
        "api_signature": "oneflow.nn.Softplus(beta: int = 1, threshold: int = 20)",
        "api_description": "Applies the element-wise function:\n\\[\\text{Softplus}(x) = \\frac{1}{\\beta} * \\log(1 + \\exp(\\beta * x))\\]\nSoftPlus is a smooth approximation to the ReLU function and can be used\nto constrain the output of a machine to always be positive.\nFor numerical stability the implementation reverts to the linear function\nwhen \\(input \\times \\beta > threshold\\).",
        "return_value": "",
        "parameters": "\nbeta – the \\(\\beta\\) value for the Softplus formulation. Default: 1\nthreshold – values above this revert to a linear function. Default: 20\n\n\n\n\n",
        "input_shape": "Input: \\((N, *)\\) where * means, any number of additional\ndimensions\nOutput: \\((N, *)\\), same shape as the input\n\n\n\n",
        "notes": "",
        "code_example": ">>> import numpy as np\n>>> import oneflow as flow\n\n>>> x = np.array([-0.5, 0, 0.5]).astype(np.float32)\n>>> input = flow.Tensor(x)\n>>> softplus = flow.nn.Softplus()\n\n>>> out = softplus(input)\n>>> out\ntensor([0.4741, 0.6931, 0.9741], dtype=oneflow.float32)\n\n\n\n"
    },
    {
        "api_name": "oneflow.nn.Softshrink",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.Softshrink.html",
        "api_signature": "oneflow.nn.Softshrink(lambd: float = 0.5, inplace: bool = False)",
        "api_description": "The Softshrink activation.\nThe formula is:\n\\[\\begin{split}\\text{Softshrink}(x) =\n\\begin{cases}\nx - \\lambd, & \\text{ if } x > \\lambda \\\\\nx + \\lambd, & \\text{ if } x < -\\lambda \\\\\n0, & \\text{ otherwise }\n\\end{cases}\\end{split}\\]",
        "return_value": "",
        "parameters": "\nlambd – the \\(\\lambda\\) value for the Softshrink formulation. Default: 0.5\ninplace – can optionally do the operation in-place. Default: False\n\n\n\n\n",
        "input_shape": "Input: \\((N, *)\\) where * means, any number of additional\ndimensions\nOutput: \\((N, *)\\), same shape as the input\n\n\n\n",
        "notes": "",
        "code_example": ">>> import numpy as np\n>>> import oneflow as flow\n>>> x = np.array([-1, 0, 0.2, 0.5]).astype(np.float32)\n>>> input = flow.Tensor(x)\n>>> softshrink = flow.nn.Softshrink(lambd=0.5)\n>>> out = softshrink(input)\n>>> out\ntensor([-0.5000,  0.0000,  0.0000,  0.0000], dtype=oneflow.float32)\n\n\n\n"
    },
    {
        "api_name": "oneflow.nn.Softsign",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.Softsign.html",
        "api_signature": "oneflow.nn.Softsign(inplace: bool = False)",
        "api_description": "The SoftSign activation.\nThe formula is:\n\\[SoftSign(x) = \\frac{x}{1 + |x|}\\]",
        "return_value": "",
        "parameters": "",
        "input_shape": "Input: \\((N, *)\\) where * means, any number of additional\ndimensions\nOutput: \\((N, *)\\), same shape as the input\n\n\n\n",
        "notes": "",
        "code_example": ">>> import numpy as np\n>>> import oneflow as flow\n>>> x = np.array([1, 2, 3]).astype(np.float32)\n>>> input = flow.Tensor(x)\n>>> softsign = flow.nn.Softsign()\n>>> out = softsign(input)\n>>> out\ntensor([0.5000, 0.6667, 0.7500], dtype=oneflow.float32)\n\n\n\n"
    },
    {
        "api_name": "oneflow.nn.Tanh",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.Tanh.html",
        "api_signature": null,
        "api_description": "This operator computes the hyperbolic tangent value of Tensor.\nThe equation is:\n\\[out = \\frac{e^x-e^{-x}}{e^x+e^{-x}}\\]",
        "return_value": "The result Tensor\n\n",
        "parameters": "input (oneflow.Tensor) – A Tensor\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import numpy as np\n>>> import oneflow as flow\n\n>>> x = np.array([-1, 0, 1]).astype(np.float32)\n>>> input = flow.Tensor(x)\n>>> tanh = flow.nn.Tanh()\n>>> out = tanh(input)\n>>> out\ntensor([-0.7616,  0.0000,  0.7616], dtype=oneflow.float32)\n\n\n\n"
    },
    {
        "api_name": "oneflow.nn.Threshold",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.Threshold.html",
        "api_signature": "oneflow.nn.Threshold(threshold: float, value: float)",
        "api_description": "The Threshold Activation. Return x if x is greater than threshold, else return value.\nThe formula is:\n\\[\\begin{split}\\text{Threshold}(x) =\n\\begin{cases}\nx, & \\text{ if } x > \\text{ threshold } \\\\\n\\text{value }, & \\text{ otherwise }\n\\end{cases}\\end{split}\\]",
        "return_value": "The result tensor\n\n",
        "parameters": "\nthreshold (float) – The threshold value for the Threshold formulation\nvalue (float) – The value value for the Threshold formulation\n\n\n\n\n",
        "input_shape": "Input: \\((N, *)\\) where * means, any number of additional dimensions\nOutput: \\((N, *)\\), same shape as the input\n\n\n\n\n",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> import numpy as np\n>>> x = np.array([-1, 0, 0.5, 1]).astype(np.float32)\n>>> input = flow.Tensor(x)\n>>> th = flow.nn.Threshold(threshold=0.5, value=0.2)\n>>> out = th(input)\n>>> out\ntensor([0.2000, 0.2000, 0.2000, 1.0000], dtype=oneflow.float32)\n\n\n\n"
    },
    {
        "api_name": "oneflow.nn.GLU",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.GLU.html",
        "api_signature": "oneflow.nn.GLU(dim: Optional[int] = - 1)",
        "api_description": "The GLU activation.",
        "return_value": "",
        "parameters": "\ninput (Tensor, float) – input tensor.\ndim (int, optional) – dimension on which to split the input. Default: -1\n\n\n\n\n",
        "input_shape": "Input: \\((\\ast_1, N, \\ast_2)\\) where * means, any number of additional\ndimensions\nOutput: \\((\\ast_1, M, \\ast_2)\\) where \\(M=N/2\\)\n\n\n\nThe formula is:\n\n\\[GLU(input) = GLU(a, b) = a \\otimes sigmoid(b)\\]\n\n",
        "notes": "where input is split in half along dim to form a and b, ⊗ is the element-wise product between matrices.",
        "code_example": ">>> import oneflow as flow\n>>> import oneflow.nn as nn\n>>> m = nn.GLU()\n>>> x = flow.tensor([[1, 2, 3, 4], [5, 6, 7, 8]], dtype=flow.float32)\n>>> y = m(x)\n>>> y\ntensor([[0.9526, 1.9640],\n        [4.9954, 5.9980]], dtype=oneflow.float32)\n\n\n\n"
    },
    {
        "api_name": "oneflow.nn.Softmax",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.Softmax.html",
        "api_signature": "oneflow.nn.Softmax(dim: Optional[int] = None)",
        "api_description": "Applies the Softmax function to an n-dimensional input Tensor\nrescaling them so that the elements of the n-dimensional output Tensor\nlie in the range [0,1] and sum to 1.\nSoftmax is defined as:\n\\[\\text{Softmax}(x_{i}) = \\frac{\\exp(x_i)}{\\sum_j \\exp(x_j)}\\]\nWhen the input Tensor is a sparse tensor then the unspecifed\nvalues are treated as -inf.",
        "return_value": "a Tensor of the same dimension and shape as the input with\nvalues in the range [0, 1]\n\n",
        "parameters": "dim (int) – A dimension along which Softmax will be computed (so every slice\nalong dim will sum to 1).\n\n\n",
        "input_shape": "Input: \\((*)\\) where * means, any number of additional\ndimensions\nOutput: \\((*)\\), same shape as the input\n\n\n\n\n",
        "notes": "",
        "code_example": ">>> import numpy as np\n>>> import oneflow as flow\n\n>>> m = flow.nn.Softmax(dim = 2)\n>>> x = flow.Tensor(\n...    np.array(\n...        [[[-0.46716809,  0.40112534,  0.61984003],\n...        [-1.31244969, -0.42528763,  1.47953856]]]\n...    )\n... )\n>>> out = m(x)\n>>> out\ntensor([[[0.1575, 0.3754, 0.4671],\n         [0.0507, 0.1230, 0.8263]]], dtype=oneflow.float32)\n\n\n\n"
    },
    {
        "api_name": "oneflow.nn.LogSoftmax",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.LogSoftmax.html",
        "api_signature": "oneflow.nn.LogSoftmax(dim: Optional[int] = None)",
        "api_description": "Applies the LogSoftmax function to an n-dimensional\ninput Tensor.\nThe LogSoftmax formulation can be simplified as:\n\\[\\text{LogSoftmax}(x_{i}) = \\log\\left(\\frac{\\exp(x_i) }{ \\sum_j \\exp(x_j)} \\right) = x_i - \\log({ \\sum_j \\exp(x_j)})\\]",
        "return_value": "",
        "parameters": "dim (int) – A dimension along which LogSoftmax will be computed.\n\n\n\n",
        "input_shape": "Input: \\((N, *)\\) where * means, any number of additional\ndimensions\nOutput: \\((N, *)\\), same shape as the input\n\n\n\n",
        "notes": "",
        "code_example": ">>> import numpy as np\n>>> import oneflow as flow\n\n>>> m = flow.nn.LogSoftmax(dim=1)\n>>> x = flow.Tensor(\n...    np.array(\n...        [[ 0.4296, -1.1957,  2.5463],\n...        [ 1.2552, -1.5747,  0.6923]]\n...    )\n... )\n>>> out = m(x)\n>>> out\ntensor([[-2.2513, -3.8766, -0.1346],\n        [-0.4877, -3.3176, -1.0506]], dtype=oneflow.float32)\n\n\n\n"
    },
    {
        "api_name": "oneflow.nn.BatchNorm1d",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.BatchNorm1d.html",
        "api_signature": "oneflow.nn.BatchNorm1d(num_features, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)",
        "api_description": "Applies Batch Normalization over a 2D or 3D input (a mini-batch of 1D\ninputs with optional additional channel dimension) as described in the paper\nBatch Normalization: Accelerating Deep Network Training by Reducing\nInternal Covariate Shift .\n\\[y = \\frac{x - \\mathrm{E}[x]}{\\sqrt{\\mathrm{Var}[x] + \\epsilon}} * \\gamma + \\beta\\]\nThe mean and standard-deviation are calculated per-dimension over\nthe mini-batches and \\(\\gamma\\) and \\(\\beta\\) are learnable parameter vectors\nof size C (where C is the input size). By default, the elements of \\(\\gamma\\) are set\nto 1 and the elements of \\(\\beta\\) are set to 0. The standard-deviation is calculated\nvia the biased estimator, equivalent to oneflow.var(input, unbiased=False).\nAlso by default, during training this layer keeps running estimates of its\ncomputed mean and variance, which are then used for normalization during\nevaluation. The running estimates are kept with a default momentum\nof 0.1.\nIf track_running_stats is set to False, this layer then does not\nkeep running estimates, and batch statistics are instead used during\nevaluation time as well.",
        "return_value": "",
        "parameters": "\nnum_features – \\(C\\) from an expected input of size\n\\((N, C, L)\\) or \\(L\\) from input of size \\((N, L)\\)\neps – a value added to the denominator for numerical stability.\nDefault: 1e-5\nmomentum – the value used for the running_mean and running_var\ncomputation. Can be set to None for cumulative moving average\n(i.e. simple average). Default: 0.1\naffine – a boolean value that when set to True, this module has\nlearnable affine parameters. Default: True\ntrack_running_stats – a boolean value that when set to True, this\nmodule tracks the running mean and variance, and when set to False,\nthis module does not track such statistics, and initializes statistics\nbuffers running_mean and running_var as None.\nWhen these buffers are None, this module always uses batch statistics.\nin both training and eval modes. Default: True\n\n\n\n\n",
        "input_shape": "Input: \\((N, C)\\) or \\((N, C, L)\\)\nOutput: \\((N, C)\\) or \\((N, C, L)\\) (same shape as input)\n\n\n\n",
        "notes": "This momentum argument is different from one used in optimizer\nclasses and the conventional notion of momentum. Mathematically, the\nupdate rule for running statistics here is\n\\(\\hat{x}_\\text{new} = (1 - \\text{momentum}) \\times \\hat{x} + \\text{momentum} \\times x_t\\),\nwhere \\(\\hat{x}\\) is the estimated statistic and \\(x_t\\) is the\nnew observed value.\nBecause the Batch Normalization is done over the C dimension, computing statistics\non (N, L) slices, it’s common terminology to call this Temporal Batch Normalization.",
        "code_example": ">>> import oneflow as flow\n>>> import numpy as np\n\n>>> x = flow.Tensor(np.random.randn(20, 100))\n>>> m = flow.nn.BatchNorm1d(100)\n>>> y = m(x)\n\n\n\n"
    },
    {
        "api_name": "oneflow.nn.BatchNorm2d",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.BatchNorm2d.html",
        "api_signature": "oneflow.nn.BatchNorm2d(num_features, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)",
        "api_description": "Applies Batch Normalization over a 4D input (a mini-batch of 2D inputs\nwith additional channel dimension) as described in the paper\nBatch Normalization: Accelerating Deep Network Training by Reducing\nInternal Covariate Shift .\n\\[y = \\frac{x - \\mathrm{E}[x]}{ \\sqrt{\\mathrm{Var}[x] + \\epsilon}} * \\gamma + \\beta\\]\nThe mean and standard-deviation are calculated per-dimension over\nthe mini-batches and \\(\\gamma\\) and \\(\\beta\\) are learnable parameter vectors\nof size C (where C is the input size). By default, the elements of \\(\\gamma\\) are set\nto 1 and the elements of \\(\\beta\\) are set to 0. The standard-deviation is calculated\nvia the biased estimator, equivalent to oneflow.var(input, unbiased=False).\nAlso by default, during training this layer keeps running estimates of its\ncomputed mean and variance, which are then used for normalization during\nevaluation. The running estimates are kept with a default momentum\nof 0.1.\nIf track_running_stats is set to False, this layer then does not\nkeep running estimates, and batch statistics are instead used during\nevaluation time as well.",
        "return_value": "",
        "parameters": "\nnum_features – \\(C\\) from an expected input of size\n\\((N, C, H, W)\\)\neps – a value added to the denominator for numerical stability.\nDefault: 1e-5\nmomentum – the value used for the running_mean and running_var\ncomputation. Can be set to None for cumulative moving average\n(i.e. simple average). Default: 0.1\naffine – a boolean value that when set to True, this module has\nlearnable affine parameters. Default: True\ntrack_running_stats – a boolean value that when set to True, this\nmodule tracks the running mean and variance, and when set to False,\nthis module does not track such statistics, and initializes statistics\nbuffers running_mean and running_var as None.\nWhen these buffers are None, this module always uses batch statistics.\nin both training and eval modes. Default: True\n\n\n\n\n",
        "input_shape": "Input: \\((N, C, H, W)\\)\nOutput: \\((N, C, H, W)\\) (same shape as input)\n\n\n\n",
        "notes": "This momentum argument is different from one used in optimizer\nclasses and the conventional notion of momentum. Mathematically, the\nupdate rule for running statistics here is\n\\(\\hat{x}_\\text{new} = (1 - \\text{momentum}) \\times \\hat{x} + \\text{momentum} \\times x_t\\),\nwhere \\(\\hat{x}\\) is the estimated statistic and \\(x_t\\) is the\nnew observed value.\nBecause the Batch Normalization is done over the C dimension, computing statistics\non (N, H, W) slices, it’s common terminology to call this Spatial Batch Normalization.",
        "code_example": ">>> import oneflow as flow\n>>> import numpy as np\n\n>>> x = flow.Tensor(np.random.randn(4, 2, 8, 3))\n>>> m = flow.nn.BatchNorm2d(num_features=2, eps=1e-5, momentum=0.1)\n>>> y = m(x)\n\n\n\n"
    },
    {
        "api_name": "oneflow.nn.BatchNorm3d",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.BatchNorm3d.html",
        "api_signature": "oneflow.nn.BatchNorm3d(num_features, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)",
        "api_description": "Applies Batch Normalization over a 5D input (a mini-batch of 3D inputs\nwith additional channel dimension) as described in the paper\nBatch Normalization: Accelerating Deep Network Training by Reducing\nInternal Covariate Shift .\n\\[y = \\frac{x - \\mathrm{E}[x]}{ \\sqrt{\\mathrm{Var}[x] + \\epsilon}} * \\gamma + \\beta\\]\nThe mean and standard-deviation are calculated per-dimension over\nthe mini-batches and \\(\\gamma\\) and \\(\\beta\\) are learnable parameter vectors\nof size C (where C is the input size). By default, the elements of \\(\\gamma\\) are set\nto 1 and the elements of \\(\\beta\\) are set to 0. The standard-deviation is calculated\nvia the biased estimator, equivalent to oneflow.var(input, unbiased=False).\nAlso by default, during training this layer keeps running estimates of its\ncomputed mean and variance, which are then used for normalization during\nevaluation. The running estimates are kept with a default momentum\nof 0.1.\nIf track_running_stats is set to False, this layer then does not\nkeep running estimates, and batch statistics are instead used during\nevaluation time as well.",
        "return_value": "",
        "parameters": "\nnum_features – \\(C\\) from an expected input of size\n\\((N, C, D, H, W)\\)\neps – a value added to the denominator for numerical stability.\nDefault: 1e-5\nmomentum – the value used for the running_mean and running_var\ncomputation. Can be set to None for cumulative moving average\n(i.e. simple average). Default: 0.1\naffine – a boolean value that when set to True, this module has\nlearnable affine parameters. Default: True\ntrack_running_stats – a boolean value that when set to True, this\nmodule tracks the running mean and variance, and when set to False,\nthis module does not track such statistics, and initializes statistics\nbuffers running_mean and running_var as None.\nWhen these buffers are None, this module always uses batch statistics.\nin both training and eval modes. Default: True\n\n\n\n\n",
        "input_shape": "Input: \\((N, C, D, H, W)\\)\nOutput: \\((N, C, D, H, W)\\) (same shape as input)\n\n\n\n",
        "notes": "This momentum argument is different from one used in optimizer\nclasses and the conventional notion of momentum. Mathematically, the\nupdate rule for running statistics here is\n\\(\\hat{x}_\\text{new} = (1 - \\text{momentum}) \\times \\hat{x} + \\text{momentum} \\times     x_t\\),\nwhere \\(\\hat{x}\\) is the estimated statistic and \\(x_t\\) is the\nnew observed value.\nBecause the Batch Normalization is done over the C dimension, computing statistics\non (N, D, H, W) slices, it’s common terminology to call this Spatial Batch Normalization.",
        "code_example": ">>> import oneflow as flow\n>>> import numpy as np\n\n>>> x = flow.Tensor(np.random.randn(3, 2, 5, 8, 4))\n>>> m = flow.nn.BatchNorm3d(num_features=2, eps=1e-5, momentum=0.1)\n>>> y = m(x)\n>>> y.size()\noneflow.Size([3, 2, 5, 8, 4])\n\n\n\n"
    },
    {
        "api_name": "oneflow.nn.SyncBatchNorm",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.SyncBatchNorm.html",
        "api_signature": "oneflow.nn.SyncBatchNorm(num_features: int, eps: float = 1e-05, momentum: float = 0.1, affine: bool = True, track_running_stats: bool = True)",
        "api_description": "Applies Batch Normalization over a N-Dimensional input (a mini-batch of [N-2]D inputs\nwith additional channel dimension) as described in the paper\nBatch Normalization: Accelerating Deep Network Training by Reducing\nInternal Covariate Shift .\n\\[y = \\frac{x - \\mathrm{E}[x]}{ \\sqrt{\\mathrm{Var}[x] + \\epsilon}} * \\gamma + \\beta\\]\nThe mean and standard-deviation are calculated per-dimension over all\nmini-batches of the same process groups. \\(\\gamma\\) and \\(\\beta\\)\nare learnable parameter vectors of size C (where C is the input size).\nBy default, the elements of \\(\\gamma\\) are sampled from\n\\(\\mathcal{U}(0, 1)\\) and the elements of \\(\\beta\\) are set to 0.\nThe standard-deviation is calculated via the biased estimator, equivalent to\noneflow.var(input, unbiased=False).\nAlso by default, during training this layer keeps running estimates of its\ncomputed mean and variance, which are then used for normalization during\nevaluation. The running estimates are kept with a default momentum\nof 0.1.\nIf track_running_stats is set to False, this layer then does not\nkeep running estimates, and batch statistics are instead used during\nevaluation time as well.",
        "return_value": "",
        "parameters": "\nnum_features – \\(C\\) from an expected input of size\n\\((N, C, +)\\)\neps – a value added to the denominator for numerical stability.\nDefault: 1e-5\nmomentum – the value used for the running_mean and running_var\ncomputation. Can be set to None for cumulative moving average\n(i.e. simple average). Default: 0.1\naffine – a boolean value that when set to True, this module has\nlearnable affine parameters. Default: True\ntrack_running_stats – a boolean value that when set to True, this\nmodule tracks the running mean and variance, and when set to False,\nthis module does not track such statistics, and initializes statistics\nbuffers running_mean and running_var as None.\nWhen these buffers are None, this module always uses batch statistics.\nin both training and eval modes. Default: True\n\n\n\n\n",
        "input_shape": "Input: \\((N, C, +)\\)\nOutput: \\((N, C, +)\\) (same shape as input)\n\n\n\n\n",
        "notes": "This momentum argument is different from one used in optimizer\nclasses and the conventional notion of momentum. Mathematically, the\nupdate rule for running statistics here is\n\\(\\hat{x}_\\text{new} = (1 - \\text{momentum}) \\times \\hat{x} + \\text{momentum} \\times x_t\\),\nwhere \\(\\hat{x}\\) is the estimated statistic and \\(x_t\\) is the\nnew observed value.\nBecause the Batch Normalization is done for each channel in the C dimension, computing\nstatistics on (N, +) slices, it’s common terminology to call this Volumetric Batch\nNormalization or Spatio-temporal Batch Normalization.\nCurrently SyncBatchNorm only supports\nDistributedDataParallel (DDP) with single GPU per process. Use\noneflow.nn.SyncBatchNorm.convert_sync_batchnorm() to convert\nBatchNorm*D layer to SyncBatchNorm before wrapping\nNetwork with DDP.\nSynchronization of batchnorm statistics occurs only while training, i.e.\nsynchronization is disabled when model.eval() is set or if\nself.training is otherwise False.",
        "code_example": ""
    },
    {
        "api_name": "oneflow.nn.FusedBatchNorm1d",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.FusedBatchNorm1d.html",
        "api_signature": "oneflow.nn.FusedBatchNorm1d(num_features, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)",
        "api_description": "Applies Fused Batch Normalization over a 2D or 3D input, the formula is:\n\\[out = ReLU(BatchNorm(input) + addend)\\]\nThe formula of Batch Normalization is:\n\\[y = \\frac{x - \\mathrm{E}[x]}{\\sqrt{\\mathrm{Var}[x] + \\epsilon}} * \\gamma + \\beta\\]\nThe mean and standard-deviation are calculated per-dimension over\nthe mini-batches and \\(\\gamma\\) and \\(\\beta\\) are learnable parameter vectors\nof size C (where C is the input size). By default, the elements of \\(\\gamma\\) are set\nto 1 and the elements of \\(\\beta\\) are set to 0. The standard-deviation is calculated\nvia the biased estimator, equivalent to torch.var(input, unbiased=False).\nAlso by default, during training this layer keeps running estimates of its\ncomputed mean and variance, which are then used for normalization during\nevaluation. The running estimates are kept with a default momentum\nof 0.1.\nIf track_running_stats is set to False, this layer then does not\nkeep running estimates, and batch statistics are instead used during\nevaluation time as well.",
        "return_value": "",
        "parameters": "\nnum_features – \\(C\\) from an expected input of size\n\\((N, C, L)\\) or \\(L\\) from input of size \\((N, L)\\)\neps – a value added to the denominator for numerical stability.\nDefault: 1e-5\nmomentum – the value used for the running_mean and running_var\ncomputation. Can be set to None for cumulative moving average\n(i.e. simple average). Default: 0.1\naffine – a boolean value that when set to True, this module has\nlearnable affine parameters. Default: True\ntrack_running_stats – a boolean value that when set to True, this\nmodule tracks the running mean and variance, and when set to False,\nthis module does not track such statistics, and initializes statistics\nbuffers running_mean and running_var as None.\nWhen these buffers are None, this module always uses batch statistics.\nin both training and eval modes. Default: True\n\n\n\n\n",
        "input_shape": "Input: \\((N, C)\\) or \\((N, C, L)\\)\nOutput: \\((N, C)\\) or \\((N, C, L)\\) (same shape as input)\n\n\n\n",
        "notes": "This momentum argument is different from one used in optimizer\nclasses and the conventional notion of momentum. Mathematically, the\nupdate rule for running statistics here is\n\\(\\hat{x}_\\text{new} = (1 - \\text{momentum}) \\times \\hat{x} + \\text{momentum} \\times x_t\\),\nwhere \\(\\hat{x}\\) is the estimated statistic and \\(x_t\\) is the\nnew observed value.\nBecause the Batch Normalization is done over the C dimension, computing statistics\non (N, L) slices, it’s common terminology to call this Temporal Batch Normalization.",
        "code_example": ">>> import oneflow as flow\n>>> import numpy as np\n\n>>> x = flow.Tensor(np.random.randn(20, 100)).to(\"cuda\") # FusedBatchNorm support in GPU currently.\n>>> m = flow.nn.FusedBatchNorm1d(num_features=100, eps=1e-5, momentum=0.1).to(\"cuda\")\n>>> y = m(x, addend=None)\n\n\n\n"
    },
    {
        "api_name": "oneflow.nn.FusedBatchNorm2d",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.FusedBatchNorm2d.html",
        "api_signature": "oneflow.nn.FusedBatchNorm2d(num_features, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)",
        "api_description": "Applies Fused Batch Normalization over a 4D input, the formula is:\n\\[out = ReLU(BatchNorm(input) + addend)\\]\nThe formula of Batch Normalization is:\n\\[y = \\frac{x - \\mathrm{E}[x]}{\\sqrt{\\mathrm{Var}[x] + \\epsilon}} * \\gamma + \\beta\\]\nThe mean and standard-deviation are calculated per-dimension over\nthe mini-batches and \\(\\gamma\\) and \\(\\beta\\) are learnable parameter vectors\nof size C (where C is the input size). By default, the elements of \\(\\gamma\\) are set\nto 1 and the elements of \\(\\beta\\) are set to 0. The standard-deviation is calculated\nvia the biased estimator, equivalent to torch.var(input, unbiased=False).\nAlso by default, during training this layer keeps running estimates of its\ncomputed mean and variance, which are then used for normalization during\nevaluation. The running estimates are kept with a default momentum\nof 0.1.\nIf track_running_stats is set to False, this layer then does not\nkeep running estimates, and batch statistics are instead used during\nevaluation time as well.",
        "return_value": "",
        "parameters": "\nnum_features – \\(C\\) from an expected input of size\n\\((N, C, H, W)\\)\neps – a value added to the denominator for numerical stability.\nDefault: 1e-5\nmomentum – the value used for the running_mean and running_var\ncomputation. Can be set to None for cumulative moving average\n(i.e. simple average). Default: 0.1\naffine – a boolean value that when set to True, this module has\nlearnable affine parameters. Default: True\ntrack_running_stats – a boolean value that when set to True, this\nmodule tracks the running mean and variance, and when set to False,\nthis module does not track such statistics, and initializes statistics\nbuffers running_mean and running_var as None.\nWhen these buffers are None, this module always uses batch statistics.\nin both training and eval modes. Default: True\n\n\n\n\n",
        "input_shape": "Input: \\((N, C, H, W)\\)\nOutput: \\((N, C, H, W)\\) (same shape as input)\n\n\n\n",
        "notes": "This momentum argument is different from one used in optimizer\nclasses and the conventional notion of momentum. Mathematically, the\nupdate rule for running statistics here is\n\\(\\hat{x}_\\text{new} = (1 - \\text{momentum}) \\times \\hat{x} + \\text{momentum} \\times x_t\\),\nwhere \\(\\hat{x}\\) is the estimated statistic and \\(x_t\\) is the\nnew observed value.\nBecause the Batch Normalization is done over the C dimension, computing statistics\non (N, H, W) slices, it’s common terminology to call this Spatial Batch Normalization.",
        "code_example": ">>> import oneflow as flow\n>>> import numpy as np\n\n>>> x = flow.Tensor(np.random.randn(4, 2, 8, 3)).to(\"cuda\") # FusedBatchNorm support in GPU currently.\n>>> m = flow.nn.FusedBatchNorm2d(num_features=2, eps=1e-5, momentum=0.1).to(\"cuda\")\n>>> y = m(x, addend=None)\n\n\n\n"
    },
    {
        "api_name": "oneflow.nn.FusedBatchNorm3d",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.FusedBatchNorm3d.html",
        "api_signature": "oneflow.nn.FusedBatchNorm3d(num_features, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)",
        "api_description": "Applies Fused Batch Normalization over a 5D input, the formula is:\n\\[out = ReLU(BatchNorm(input) + addend)\\]\nThe formula of Batch Normalization is:\n\\[\\begin{split}y = \\\\frac{x - \\\\mathrm{E}[x]}{\\\\sqrt{\\\\mathrm{Var}[x] + \\\\epsilon}} * \\\\gamma + \\\\beta\\end{split}\\]\nThe mean and standard-deviation are calculated per-dimension over\nthe mini-batches and \\(\\gamma\\) and \\(\\beta\\) are learnable parameter vectors\nof size C (where C is the input size). By default, the elements of \\(\\gamma\\) are set\nto 1 and the elements of \\(\\beta\\) are set to 0. The standard-deviation is calculated\nvia the biased estimator, equivalent to torch.var(input, unbiased=False).\nAlso by default, during training this layer keeps running estimates of its\ncomputed mean and variance, which are then used for normalization during\nevaluation. The running estimates are kept with a default momentum\nof 0.1.\nIf track_running_stats is set to False, this layer then does not\nkeep running estimates, and batch statistics are instead used during\nevaluation time as well.",
        "return_value": "",
        "parameters": "\nnum_features – \\(C\\) from an expected input of size\n\\((N, C, D, H, W)\\)\neps – a value added to the denominator for numerical stability.\nDefault: 1e-5\nmomentum – the value used for the running_mean and running_var\ncomputation. Can be set to None for cumulative moving average\n(i.e. simple average). Default: 0.1\naffine – a boolean value that when set to True, this module has\nlearnable affine parameters. Default: True\ntrack_running_stats – a boolean value that when set to True, this\nmodule tracks the running mean and variance, and when set to False,\nthis module does not track such statistics, and initializes statistics\nbuffers running_mean and running_var as None.\nWhen these buffers are None, this module always uses batch statistics.\nin both training and eval modes. Default: True\n\n\n\n\n",
        "input_shape": "Input: \\((N, C, D, H, W)\\)\nOutput: \\((N, C, D, H, W)\\) (same shape as input)\n\n\n\n",
        "notes": "This momentum argument is different from one used in optimizer\nclasses and the conventional notion of momentum. Mathematically, the\nupdate rule for running statistics here is\n\\(\\hat{x}_\\text{new} = (1 - \\text{momentum}) \\times \\hat{x} + \\text{momentum} \\times     x_t\\),\nwhere \\(\\hat{x}\\) is the estimated statistic and \\(x_t\\) is the\nnew observed value.\nBecause the Batch Normalization is done over the C dimension, computing statistics\non (N, D, H, W) slices, it’s common terminology to call this Spatial Batch Normalization.",
        "code_example": ">>> import oneflow as flow\n>>> import numpy as np\n\n>>> x = flow.Tensor(np.random.randn(3, 2, 5, 8, 4)).to(\"cuda\") # FusedBatchNorm support in GPU currently.\n>>> m = flow.nn.FusedBatchNorm3d(num_features=2, eps=1e-5, momentum=0.1).to(\"cuda\")\n>>> y = m(x, addend=None)\n\n\n\n"
    },
    {
        "api_name": "oneflow.nn.GroupNorm",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.GroupNorm.html",
        "api_signature": "oneflow.nn.GroupNorm(num_groups: int, num_channels: int, eps: float = 1e-05, affine: bool = True)",
        "api_description": "Applies Group Normalization over a mini-batch of inputs as described in\nthe paper Group Normalization\n\\[y = \\frac{x - \\mathrm{E}[x]}{ \\sqrt{\\mathrm{Var}[x] + \\epsilon}} * \\gamma + \\beta\\]\nThe input channels are separated into num_groups groups, each containing\nnum_channels / num_groups channels. The mean and standard-deviation are calculated\nseparately over the each group. \\(\\gamma\\) and \\(\\beta\\) are learnable\nper-channel affine transform parameter vectors of size num_channels if\naffine is True.\nThe standard-deviation is calculated via the biased estimator, equivalent to\ntorch.var(input, unbiased=False).\nThis layer uses statistics computed from input data in both training and\nevaluation modes.\nThe documentation is referenced from:",
        "return_value": "",
        "parameters": "\nnum_groups (int) – number of groups to separate the channels into\nnum_channels (int) – number of channels expected in input\neps – a value added to the denominator for numerical stability. Default: 1e-5\naffine – a boolean value that when set to True, this module\nhas learnable per-channel affine parameters initialized to ones (for weights)\nand zeros (for biases). Default: True.\n\n\n\n\n",
        "input_shape": "Input: \\((N, C, *)\\) where \\(C=\\text{num_channels}\\)\nOutput: \\((N, C, *)\\) (same shape as input)\n\n\n\n",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> import numpy as np\n>>> input = flow.Tensor(np.random.randn(20, 6, 10, 10))\n>>> # Separate 6 channels into 3 groups\n>>> m = flow.nn.GroupNorm(3, 6)\n>>> # Separate 6 channels into 6 groups (equivalent with InstanceNorm)\n>>> m = flow.nn.GroupNorm(6, 6)\n>>> # Put all 6 channels into a single group (equivalent with LayerNorm)\n>>> m = flow.nn.GroupNorm(1, 6)\n>>> # Activating the module\n>>> output = m(input)\n\n\n\n"
    },
    {
        "api_name": "oneflow.nn.InstanceNorm1d",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.InstanceNorm1d.html",
        "api_signature": "oneflow.nn.InstanceNorm1d(num_features: int, eps: float = 1e-05, momentum: float = 0.1, affine: bool = False, track_running_stats: bool = False)",
        "api_description": "Applies Instance Normalization over a 3D input (a mini-batch of 1D\ninputs with optional additional channel dimension) as described in the paper\nInstance Normalization: The Missing Ingredient for Fast Stylization.\n\\[y = \\frac{x - \\mathrm{E}[x]}{ \\sqrt{\\mathrm{Var}[x] + \\epsilon}} * \\gamma + \\beta\\]\nThe mean and standard-deviation are calculated per-dimension separately\nfor each object in a mini-batch. \\(\\gamma\\) and \\(\\beta\\) are learnable parameter vectors\nof size C (where C is the input size) if affine is True.\nThe standard-deviation is calculated via the biased estimator, equivalent to\ntorch.var(input, unbiased=False).\nBy default, this layer uses instance statistics computed from input data in\nboth training and evaluation modes.\nIf track_running_stats is set to True, during training this\nlayer keeps running estimates of its computed mean and variance, which are\nthen used for normalization during evaluation. The running estimates are\nkept with a default momentum of 0.1.",
        "return_value": "",
        "parameters": "\nnum_features – \\(C\\) from an expected input of size\n\\((N, C, L)\\) or \\(L\\) from input of size \\((N, L)\\)\neps – a value added to the denominator for numerical stability. Default: 1e-5\nmomentum – the value used for the running_mean and running_var computation. Default: 0.1\naffine – a boolean value that when set to True, this module has\nlearnable affine parameters, initialized the same way as done for batch normalization.\nDefault: False.\ntrack_running_stats – a boolean value that when set to True, this\nmodule tracks the running mean and variance, and when set to False,\nthis module does not track such statistics and always uses batch\nstatistics in both training and eval modes. Default: False\n\n\n\n\n",
        "input_shape": "Input: \\((N, C, L)\\)\nOutput: \\((N, C, L)\\) (same shape as input)\n\n\n\n",
        "notes": "This momentum argument is different from one used in optimizer\nclasses and the conventional notion of momentum. Mathematically, the\nupdate rule for running statistics here is\n\\(\\hat{x}_\\text{new} = (1 - \\text{momentum}) \\times \\hat{x} + \\text{momentum} \\times x_t\\),\nwhere \\(\\hat{x}\\) is the estimated statistic and \\(x_t\\) is the\nnew observed value.\nInstanceNorm1d and LayerNorm are very similar, but\nhave some subtle differences. InstanceNorm1d is applied\non each channel of channeled data like multidimensional time series, but\nLayerNorm is usually applied on entire sample and often in NLP\ntasks. Additionally, LayerNorm applies elementwise affine\ntransform, while InstanceNorm1d usually don’t apply affine\ntransform.",
        "code_example": ">>> import oneflow as flow\n>>> import numpy as np\n\n>>> # Without Learnable Parameters\n>>> m = flow.nn.InstanceNorm1d(100)\n>>> # With Learnable Parameters\n>>> m = flow.nn.InstanceNorm1d(100, affine=True)\n>>> x = flow.Tensor(np.random.randn(20, 100, 40))\n>>> output = m(x)\n\n\n\n"
    },
    {
        "api_name": "oneflow.nn.InstanceNorm2d",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.InstanceNorm2d.html",
        "api_signature": "oneflow.nn.InstanceNorm2d(num_features: int, eps: float = 1e-05, momentum: float = 0.1, affine: bool = False, track_running_stats: bool = False)",
        "api_description": "Applies Instance Normalization over a 4D input (a mini-batch of 2D inputs\nwith additional channel dimension) as described in the paper\nInstance Normalization: The Missing Ingredient for Fast Stylization.\n\\[y = \\frac{x - \\mathrm{E}[x]}{ \\sqrt{\\mathrm{Var}[x] + \\epsilon}} * \\gamma + \\beta\\]\nThe mean and standard-deviation are calculated per-dimension separately\nfor each object in a mini-batch. \\(\\gamma\\) and \\(\\beta\\) are learnable parameter vectors\nof size C (where C is the input size) if affine is True.\nThe standard-deviation is calculated via the biased estimator, equivalent to\ntorch.var(input, unbiased=False).\nBy default, this layer uses instance statistics computed from input data in\nboth training and evaluation modes.\nIf track_running_stats is set to True, during training this\nlayer keeps running estimates of its computed mean and variance, which are\nthen used for normalization during evaluation. The running estimates are\nkept with a default momentum of 0.1.",
        "return_value": "",
        "parameters": "\nnum_features – \\(C\\) from an expected input of size\n\\((N, C, H, W)\\)\neps – a value added to the denominator for numerical stability. Default: 1e-5\nmomentum – the value used for the running_mean and running_var computation. Default: 0.1\naffine – a boolean value that when set to True, this module has\nlearnable affine parameters, initialized the same way as done for batch normalization.\nDefault: False.\ntrack_running_stats – a boolean value that when set to True, this\nmodule tracks the running mean and variance, and when set to False,\nthis module does not track such statistics and always uses batch\nstatistics in both training and eval modes. Default: False\n\n\n\n\n",
        "input_shape": "Input: \\((N, C, H, W)\\)\nOutput: \\((N, C, H, W)\\) (same shape as input)\n\n\n\n",
        "notes": "This momentum argument is different from one used in optimizer\nclasses and the conventional notion of momentum. Mathematically, the\nupdate rule for running statistics here is\n\\(\\hat{x}_\\text{new} = (1 - \\text{momentum}) \\times \\hat{x} + \\text{momentum} \\times x_t\\),\nwhere \\(\\hat{x}\\) is the estimated statistic and \\(x_t\\) is the\nnew observed value.\nInstanceNorm2d and LayerNorm are very similar, but\nhave some subtle differences. InstanceNorm2d is applied\non each channel of channeled data like RGB images, but\nLayerNorm is usually applied on entire sample and often in NLP\ntasks. Additionally, LayerNorm applies elementwise affine\ntransform, while InstanceNorm2d usually don’t apply affine\ntransform.",
        "code_example": ">>> import oneflow as flow\n>>> import numpy as np\n\n>>> # Without Learnable Parameters\n>>> m = flow.nn.InstanceNorm2d(100)\n>>> # With Learnable Parameters\n>>> m = flow.nn.InstanceNorm2d(100, affine=True)\n>>> x = flow.Tensor(np.random.randn(20, 100, 35, 45))\n>>> output = m(x)\n\n\n\n"
    },
    {
        "api_name": "oneflow.nn.InstanceNorm3d",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.InstanceNorm3d.html",
        "api_signature": "oneflow.nn.InstanceNorm3d(num_features: int, eps: float = 1e-05, momentum: float = 0.1, affine: bool = False, track_running_stats: bool = False)",
        "api_description": "Applies Instance Normalization over a 5D input (a mini-batch of 3D inputs\nwith additional channel dimension) as described in the paper\nInstance Normalization: The Missing Ingredient for Fast Stylization.\n\\[y = \\frac{x - \\mathrm{E}[x]}{ \\sqrt{\\mathrm{Var}[x] + \\epsilon}} * \\gamma + \\beta\\]\nThe mean and standard-deviation are calculated per-dimension separately\nfor each object in a mini-batch. \\(\\gamma\\) and \\(\\beta\\) are learnable parameter vectors\nof size C (where C is the input size) if affine is True.\nThe standard-deviation is calculated via the biased estimator, equivalent to\ntorch.var(input, unbiased=False).\nBy default, this layer uses instance statistics computed from input data in\nboth training and evaluation modes.\nIf track_running_stats is set to True, during training this\nlayer keeps running estimates of its computed mean and variance, which are\nthen used for normalization during evaluation. The running estimates are\nkept with a default momentum of 0.1.",
        "return_value": "",
        "parameters": "\nnum_features – \\(C\\) from an expected input of size\n\\((N, C, D, H, W)\\)\neps – a value added to the denominator for numerical stability. Default: 1e-5\nmomentum – the value used for the running_mean and running_var computation. Default: 0.1\naffine – a boolean value that when set to True, this module has\nlearnable affine parameters, initialized the same way as done for batch normalization.\nDefault: False.\ntrack_running_stats – a boolean value that when set to True, this\nmodule tracks the running mean and variance, and when set to False,\nthis module does not track such statistics and always uses batch\nstatistics in both training and eval modes. Default: False\n\n\n\n\n",
        "input_shape": "Input: \\((N, C, D, H, W)\\)\nOutput: \\((N, C, D, H, W)\\) (same shape as input)\n\n\n\n",
        "notes": "This momentum argument is different from one used in optimizer\nclasses and the conventional notion of momentum. Mathematically, the\nupdate rule for running statistics here is\n\\(\\hat{x}_\\text{new} = (1 - \\text{momentum}) \\times \\hat{x} + \\text{momentum} \\times x_t\\),\nwhere \\(\\hat{x}\\) is the estimated statistic and \\(x_t\\) is the\nnew observed value.\nInstanceNorm3d and LayerNorm are very similar, but\nhave some subtle differences. InstanceNorm3d is applied\non each channel of channeled data like 3D models with RGB color, but\nLayerNorm is usually applied on entire sample and often in NLP\ntasks. Additionally, LayerNorm applies elementwise affine\ntransform, while InstanceNorm3d usually don’t apply affine\ntransform.",
        "code_example": ">>> import oneflow as flow\n>>> import numpy as np\n>>> # Without Learnable Parameters\n>>> m = flow.nn.InstanceNorm3d(100)\n>>> # With Learnable Parameters\n>>> m = flow.nn.InstanceNorm3d(100, affine=True)\n>>> x = flow.Tensor(np.random.randn(20, 100, 35, 45, 10))\n>>> output = m(x)\n\n\n\n"
    },
    {
        "api_name": "oneflow.nn.LayerNorm",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.LayerNorm.html",
        "api_signature": "oneflow.nn.LayerNorm(normalized_shape: Union[int, Tuple[int], oneflow.Size], eps: float = 1e-05, elementwise_affine: bool = True)",
        "api_description": "Applies Layer Normalization over a mini-batch of inputs as described in\nthe paper Layer Normalization\n\\[y = \\frac{x - \\mathrm{E}[x]}{ \\sqrt{\\mathrm{Var}[x] + \\epsilon}} * \\gamma + \\beta\\]\nThe mean and standard-deviation are calculated separately over the last\ncertain number dimensions which have to be of the shape specified by\nnormalized_shape.\n\\(\\gamma\\) and \\(\\beta\\) are learnable affine transform parameters of\nnormalized_shape if elementwise_affine is True.\nThe standard-deviation is calculated via the biased estimator.",
        "return_value": "",
        "parameters": "\nnormalized_shape (int or list or oneflow.Size) – input shape from an expected input of size\n\n\\[[* \\times \\text{normalized_shape}[0] \\times \\text{normalized_shape}[1] \\times \\ldots \\times \\text{normalized_shape}[-1]]\\]\nIf a single integer is used, it is treated as a singleton list, and this module will\nnormalize over the last dimension which is expected to be of that specific size.\n\neps – a value added to the denominator for numerical stability. Default: 1e-5\nelementwise_affine – a boolean value that when set to True, this module\nhas learnable per-element affine parameters initialized to ones (for weights)\nand zeros (for biases). Default: True.\n\n\n\n\n",
        "input_shape": "Input: \\((N, *)\\)\nOutput: \\((N, *)\\) (same shape as input)\n\n\n\n",
        "notes": "Unlike Batch Normalization and Instance Normalization, which applies\nscalar scale and bias for each entire channel/plane with the\naffine option, Layer Normalization applies per-element scale and\nbias with elementwise_affine.\nThis layer uses statistics computed from input data in both training and\nevaluation modes.",
        "code_example": ">>> import numpy as np\n>>> import oneflow as flow\n\n>>> input_arr = np.array(\n...     [\n...         [\n...             [[-0.16046895, -1.03667831], [-0.34974465, 0.26505867]],\n...             [[-1.24111986, -0.53806001], [1.72426331, 0.43572459]],\n...         ],\n...         [\n...             [[-0.77390957, -0.42610624], [0.16398858, -1.35760343]],\n...             [[1.07541728, 0.11008703], [0.26361224, -0.48663723]],\n...         ],\n...     ],\n...     dtype=np.float32,\n... )\n\n>>> x = flow.Tensor(input_arr)\n>>> m = flow.nn.LayerNorm(2)\n>>> y = m(x).numpy()\n>>> y\narray([[[[ 0.99997395, -0.99997395],\n         [-0.999947  ,  0.999947  ]],\n\n        [[-0.99995965,  0.9999595 ],\n         [ 0.99998784, -0.99998784]]],\n\n\n       [[[-0.9998348 ,  0.99983466],\n         [ 0.9999914 , -0.9999914 ]],\n\n        [[ 0.9999785 , -0.9999785 ],\n         [ 0.9999646 , -0.9999646 ]]]], dtype=float32)\n\n\n\n"
    },
    {
        "api_name": "oneflow.nn.RMSLayerNorm",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.RMSLayerNorm.html",
        "api_signature": "oneflow.nn.RMSLayerNorm(hidden_size, eps=1e-06)",
        "api_description": "Construct a layernorm module in the T5 style. No bias and no subtraction of mean.\nT5 uses a layer_norm which only scales and doesn’t shift, which is also known as Root Mean\nSquare Layer Normalization https://arxiv.org/abs/1910.07467 thus varience is calculated\nw/o mean and there is no bias. Additionally we want to make sure that the accumulation for\nhalf-precision inputs is done in fp32.",
        "return_value": "",
        "parameters": "\nhidden_size (int) – number of features in the hidden state\neps – a value added to the denominator for numerical stability. Default: 1e-6\n\n\n\n\n",
        "input_shape": "Input: \\((N, *)\\)\nOutput: \\((N, *)\\) (same shape as input)\n\n\n\n",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n\n>>> x = flow.randn(2, 4, 3)\n>>> m = flow.nn.RMSLayerNorm(3)\n>>> y = m(x)\n>>> y.size()\noneflow.Size([2, 4, 3])\n\n\n\n"
    },
    {
        "api_name": "oneflow.nn.RMSNorm",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.RMSNorm.html",
        "api_signature": "oneflow.nn.RMSNorm(normalized_shape: Union[int, Tuple[int], oneflow.Size], eps: float = 1e-05, elementwise_affine: bool = True, device=None, dtype=None)",
        "api_description": "Applies Root Mean Square Layer Normalization over a mini-batch of inputs as described in\nthe paper Root Mean Square Layer Normalization\n\\[y = \\frac{x}{\\mathrm{RMS}[x]} \\mathrm{weight},\\text{ where }\\mathrm{RMS}[x] = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} x^{2}}\\]\nThere is no bias and no subtraction of mean with RMS Layer Normalization,\nand it only scales and doesn’t shift.\nThe root mean squre are calculated separately over the last\ncertain number dimensions which have to be of the shape specified by\nnormalized_shape.\n\\(\\weight\\) is learnable affine transform parameters of\nnormalized_shape if elementwise_affine is True.",
        "return_value": "",
        "parameters": "\nnormalized_shape (int or list or oneflow.Size) – input shape from an expected input of size\n\n\\[[* \\times \\text{normalized_shape}[0] \\times \\text{normalized_shape}[1] \\times \\ldots \\times \\text{normalized_shape}[-1]]\\]\nIf a single integer is used, it is treated as a singleton list, and this module will\nnormalize over the last dimension which is expected to be of that specific size.\n\neps – a value added to the denominator for numerical stability. Default: 1e-5\nelementwise_affine – a boolean value that when set to True, this module\nhas learnable per-element affine parameters initialized to ones (for weights).\nDefault: True.\n\n\n\n\n",
        "input_shape": "Input: \\((N, *)\\)\nOutput: \\((N, *)\\) (same shape as input)\n\n\n\n",
        "notes": "Like Layer Normalization, Root Mean Square Layer Normalization applies per-element scale\nwith elementwise_affine.\nThis layer uses statistics computed from input data in both training and\nevaluation modes.",
        "code_example": ">>> import numpy as np\n>>> import oneflow as flow\n\n>>> input_arr = np.array(\n...     [\n...         [\n...             [[-0.16046895, -1.03667831], [-0.34974465, 0.26505867]],\n...             [[-1.24111986, -0.53806001], [1.72426331, 0.43572459]],\n...         ],\n...         [\n...             [[-0.77390957, -0.42610624], [0.16398858, -1.35760343]],\n...             [[1.07541728, 0.11008703], [0.26361224, -0.48663723]],\n...         ],\n...     ],\n...     dtype=np.float32,\n... )\n\n>>> x = flow.Tensor(input_arr, device=\"cuda\")\n>>> m = flow.nn.RMSNorm(2).to(device=\"cuda\")\n>>> y = m(x).numpy()\n>>> y\narray([[[[-0.21632987, -1.3975569 ],\n         [-1.127044  ,  0.8541454 ]],\n\n        [[-1.2975204 , -0.5625112 ],\n         [ 1.3711083 ,  0.34648165]]],\n\n\n       [[[-1.2388322 , -0.6820876 ],\n         [ 0.16959298, -1.4040003 ]],\n\n        [[ 1.4068495 ,  0.14401469],\n         [ 0.6735778 , -1.2434478 ]]]], dtype=float32)\n\n\n\n"
    },
    {
        "api_name": "oneflow.nn.RNN",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.RNN.html",
        "api_signature": "oneflow.nn.RNN(*args, **kwargs)",
        "api_description": "Applies a multi-layer Elman RNN with tanhtanh or text{ReLU}ReLU non-linearity to an input sequence.\nFor each element in the input sequence, each layer computes the following function:\nfunction:\n\\[h_t = \\tanh(W_{ih} x_t + b_{ih} + W_{hh} h_{(t-1)} + b_{hh})\\]\nwhere \\(h_t\\) is the hidden state at time t, \\(x_t\\) is\nthe input at time t, and \\(h_{(t-1)}\\) is the hidden state of the\nprevious layer at time t-1 or the initial hidden state at time 0.\nIf nonlinearity is 'relu', then \\(\\text{ReLU}\\) is used instead of \\(\\tanh\\).",
        "return_value": "",
        "parameters": "\ninput_size – The number of expected features in the input x\nhidden_size – The number of features in the hidden state h\nnum_layers – Number of recurrent layers. E.g., setting num_layers=2\nwould mean stacking two RNNs together to form a stacked RNN,\nwith the second RNN taking in outputs of the first RNN and\ncomputing the final results. Default: 1\nnonlinearity – The non-linearity to use. Can be either 'tanh' or 'relu'. Default: 'tanh'\nbias – If False, then the layer does not use bias weights b_ih and b_hh.\nDefault: True\nbatch_first – If True, then the input and output tensors are provided\nas (batch, seq, feature) instead of (seq, batch, feature).\n",
        "input_shape": "",
        "notes": "Inputs/Outputs sections below for details.  Default: False\ndropout – If non-zero, introduces a Dropout layer on the outputs of each\nRNN layer except the last layer, with dropout probability equal to\ndropout. Default: 0\nbidirectional – If True, becomes a bidirectional RNN. Default: False\nInputs: input, h_0\ninput: tensor of shape \\((L, N, H_{in})\\) when batch_first=False or\n\\((N, L, H_{in})\\) when batch_first=True containing the features of\nthe input sequence.\nh_0: tensor of shape \\((D * \\text{num\\_layers}, N, H_{out})\\) containing the initial hidden\nstate for each element in the batch. Defaults to zeros if not provided.\nwhere:\n\\[\\begin{split}\\begin{aligned}\nN ={} & \\text{batch size} \\\\\nL ={} & \\text{sequence length} \\\\\nD ={} & 2 \\text{ if bidirectional=True otherwise } 1 \\\\\nH_{in} ={} & \\text{input_size} \\\\\nH_{out} ={} & \\text{hidden_size}\n\\end{aligned}\\end{split}\\]\nOutputs: output, h_n\noutput: tensor of shape \\((L, N, D * H_{out})\\) when batch_first=False or\n\\((N, L, D * H_{out})\\) when batch_first=True containing the output features\n(h_t) from the last layer of the RNN, for each t.\nh_n: tensor of shape \\((D * \\text{num\\_layers}, N, H_{out})\\) containing the final hidden state\nfor each element in the batch.\nweight_ih_l[k]\nthe learnable input-hidden weights of the k-th layer,\nof shape (hidden_size, input_size) for k = 0. Otherwise, the shape is\n(hidden_size, num_directions * hidden_size)\nweight_hh_l[k]\nthe learnable hidden-hidden weights of the k-th layer,\nof shape (hidden_size, hidden_size)\nbias_ih_l[k]\nthe learnable input-hidden bias of the k-th layer,\nof shape (hidden_size)\nbias_hh_l[k]\nthe learnable hidden-hidden bias of the k-th layer,\nof shape (hidden_size)\nAll the weights and biases are initialized from \\(\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})\\)\nwhere \\(k = \\frac{1}{\\text{hidden\\_size}}\\)\nFor bidirectional RNNs, forward and backward are directions 0 and 1 respectively.\nExample of splitting the output layers when batch_first=False:\noutput.view((seq_len, batch, num_directions, hidden_size)).",
        "code_example": ">>> import oneflow as flow\n>>> import numpy as np\n>>> rnn = flow.nn.RNN(10, 20, 2)\n>>> input = flow.tensor(np.random.randn(5, 3, 10), dtype=flow.float32)\n>>> h0 = flow.tensor(np.random.randn(2, 3, 20), dtype=flow.float32)\n>>> output, hn = rnn(input, h0)\n>>> output.size()\noneflow.Size([5, 3, 20])\n\n\n\n"
    },
    {
        "api_name": "oneflow.nn.LSTM",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.LSTM.html",
        "api_signature": "oneflow.nn.LSTM(*args, **kwargs)",
        "api_description": "Applies a multi-layer long short-term memory (LSTM) RNN to an input sequence.\nFor each element in the input sequence, each layer computes the following\nfunction:\n\\[\\begin{split}\\begin{array}{ll} \\\\\ni_t = \\sigma(W_{ii} x_t + b_{ii} + W_{hi} h_{t-1} + b_{hi}) \\\\\nf_t = \\sigma(W_{if} x_t + b_{if} + W_{hf} h_{t-1} + b_{hf}) \\\\\ng_t = \\tanh(W_{ig} x_t + b_{ig} + W_{hg} h_{t-1} + b_{hg}) \\\\\no_t = \\sigma(W_{io} x_t + b_{io} + W_{ho} h_{t-1} + b_{ho}) \\\\\nc_t = f_t \\odot c_{t-1} + i_t \\odot g_t \\\\\nh_t = o_t \\odot \\tanh(c_t) \\\\\n\\end{array}\\end{split}\\]\nwhere \\(h_t\\) is the hidden state at time t, \\(c_t\\) is the cell\nstate at time t, \\(x_t\\) is the input at time t, \\(h_{t-1}\\)\nis the hidden state of the layer at time t-1 or the initial hidden\nstate at time 0, and \\(i_t\\), \\(f_t\\), \\(g_t\\),\n\\(o_t\\) are the input, forget, cell, and output gates, respectively.\n\\(\\sigma\\) is the sigmoid function, and \\(\\odot\\) is the Hadamard product.\nIn a multilayer LSTM, the input \\(x^{(l)}_t\\) of the \\(l\\) -th layer\n(\\(l >= 2\\)) is the hidden state \\(h^{(l-1)}_t\\) of the previous layer multiplied by\ndropout \\(\\delta^{(l-1)}_t\\) where each \\(\\delta^{(l-1)}_t\\) is a Bernoulli random\nvariable which is \\(0\\) with probability dropout.\nIf proj_size > 0 is specified, LSTM with projections will be used. This changes\nthe LSTM cell in the following way. First, the dimension of \\(h_t\\) will be changed from\nhidden_size to proj_size (dimensions of \\(W_{hi}\\) will be changed accordingly).\nSecond, the output hidden state of each layer will be multiplied by a learnable projection\nmatrix: \\(h_t = W_{hr}h_t\\). Note that as a consequence of this, the output\nof LSTM network will be of different shape as well. See Inputs/Outputs sections below for exact\ndimensions of all variables. You can find more details in https://arxiv.org/abs/1402.1128.",
        "return_value": "",
        "parameters": "\ninput_size – The number of expected features in the input x\nhidden_size – The number of features in the hidden state h\nnum_layers – Number of recurrent layers. E.g., setting num_layers=2\nwould mean stacking two LSTMs together to form a stacked LSTM,\nwith the second LSTM taking in outputs of the first LSTM and\ncomputing the final results. Default: 1\nbias – If False, then the layer does not use bias weights b_ih and b_hh.\nDefault: True\nbatch_first – If True, then the input and output tensors are provided\nas (batch, seq, feature) instead of (seq, batch, feature).\n",
        "input_shape": "",
        "notes": "Inputs/Outputs sections below for details.  Default: False\ndropout – If non-zero, introduces a Dropout layer on the outputs of each\nLSTM layer except the last layer, with dropout probability equal to\ndropout. Default: 0\nbidirectional – If True, becomes a bidirectional LSTM. Default: False\nproj_size – If > 0, will use LSTM with projections of corresponding size. Default: 0\nInputs: input, (h_0, c_0)\ninput: tensor of shape \\((L, N, H_{in})\\) when batch_first=False or\n\\((N, L, H_{in})\\) when batch_first=True containing the features of\nthe input sequence.\nh_0: tensor of shape \\((D * \\text{num\\_layers}, N, H_{out})\\) containing the\ninitial hidden state for each element in the batch.\nDefaults to zeros if (h_0, c_0) is not provided.\nc_0: tensor of shape \\((D * \\text{num\\_layers}, N, H_{cell})\\) containing the\ninitial cell state for each element in the batch.\nDefaults to zeros if (h_0, c_0) is not provided.\nwhere:\n\\[\\begin{split}\\begin{aligned}\nN ={} & \\text{batch size} \\\\\nL ={} & \\text{sequence length} \\\\\nD ={} & 2 \\text{ if bidirectional=True otherwise } 1 \\\\\nH_{in} ={} & \\text{input\\_size} \\\\\nH_{cell} ={} & \\text{hidden\\_size} \\\\\nH_{out} ={} & \\text{proj\\_size if } \\text{proj\\_size}>0 \\text{ otherwise hidden\\_size} \\\\\n\\end{aligned}\\end{split}\\]\nOutputs: output, (h_n, c_n)\noutput: tensor of shape \\((L, N, D * H_{out})\\) when batch_first=False or\n\\((N, L, D * H_{out})\\) when batch_first=True containing the output features\n(h_t) from the last layer of the LSTM, for each t.\nh_n: tensor of shape \\((D * \\text{num\\_layers}, N, H_{out})\\) containing the\nfinal hidden state for each element in the batch.\nc_n: tensor of shape \\((D * \\text{num\\_layers}, N, H_{cell})\\) containing the\nfinal cell state for each element in the batch.\nweight_ih_l[k]\nthe learnable input-hidden weights of the \\(\\text{k}^{th}\\) layer\n(W_ii|W_if|W_ig|W_io), of shape (4*hidden_size, input_size) for k = 0.\nOtherwise, the shape is (4*hidden_size, num_directions * hidden_size)\nweight_hh_l[k]\nthe learnable hidden-hidden weights of the \\(\\text{k}^{th}\\) layer\n(W_hi|W_hf|W_hg|W_ho), of shape (4*hidden_size, hidden_size). If proj_size > 0\nwas specified, the shape will be (4*hidden_size, proj_size).\nbias_ih_l[k]\nthe learnable input-hidden bias of the \\(\\text{k}^{th}\\) layer\n(b_ii|b_if|b_ig|b_io), of shape (4*hidden_size)\nbias_hh_l[k]\nthe learnable hidden-hidden bias of the \\(\\text{k}^{th}\\) layer\n(b_hi|b_hf|b_hg|b_ho), of shape (4*hidden_size)\nweight_hr_l[k]\nthe learnable projection weights of the \\(\\text{k}^{th}\\) layer\nof shape (proj_size, hidden_size). Only present when proj_size > 0 was\nspecified.\nAll the weights and biases are initialized from \\(\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})\\)\nwhere \\(k = \\frac{1}{\\text{hidden\\_size}}\\)\nFor bidirectional LSTMs, forward and backward are directions 0 and 1 respectively.\nExample of splitting the output layers when batch_first=False:\noutput.view(seq_len, batch, num_directions, hidden_size).",
        "code_example": ">>> import oneflow as flow\n>>> import numpy as np\n>>> rnn = flow.nn.LSTM(10, 20, 2)\n>>> input = flow.tensor(np.random.randn(5, 3, 10), dtype=flow.float32)\n>>> h0 = flow.tensor(np.random.randn(2, 3, 20), dtype=flow.float32)\n>>> c0 = flow.tensor(np.random.randn(2, 3, 20), dtype=flow.float32)\n>>> output, (hn, cn) = rnn(input, (h0, c0))\n>>> output.size()\noneflow.Size([5, 3, 20])\n\n\n\n"
    },
    {
        "api_name": "oneflow.nn.GRU",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.GRU.html",
        "api_signature": "oneflow.nn.GRU(*args, **kwargs)",
        "api_description": "Applies a multi-layer gated recurrent unit (GRU) RNN to an input sequence.\nFor each element in the input sequence, each layer computes the following\nfunction:\n\\[\\begin{split}\\begin{array}{ll}\nr_t = \\sigma(W_{ir} x_t + b_{ir} + W_{hr} h_{(t-1)} + b_{hr}) \\\\\nz_t = \\sigma(W_{iz} x_t + b_{iz} + W_{hz} h_{(t-1)} + b_{hz}) \\\\\nn_t = \\\\tanh(W_{in} x_t + b_{in} + r_t * (W_{hn} h_{(t-1)}+ b_{hn})) \\\\\nh_t = (1 - z_t) * n_t + z_t * h_{(t-1)}\n\\end{array}\\end{split}\\]\nwhere \\(h_t\\) is the hidden state at time t, \\(x_t\\) is the input\nat time t, \\(h_{(t-1)}\\) is the hidden state of the layer\nat time t-1 or the initial hidden state at time 0, and \\(r_t\\),\n\\(z_t\\), \\(n_t\\) are the reset, update, and new gates, respectively.\n\\(\\sigma\\) is the sigmoid function, and \\(*\\) is the Hadamard product.\nIn a multilayer GRU, the input \\(x^{(l)}_t\\) of the \\(l\\) -th layer\n(\\(l >= 2\\)) is the hidden state \\(h^{(l-1)}_t\\) of the previous layer multiplied by\ndropout \\(\\delta^{(l-1)}_t\\) where each \\(\\delta^{(l-1)}_t\\) is a Bernoulli random\nvariable which is \\(0\\) with probability dropout.",
        "return_value": "",
        "parameters": "\nnum_layers – Number of recurrent layers. E.g., setting num_layers=2\nwould mean stacking two GRUs together to form a stacked GRU,\nwith the second GRU taking in outputs of the first GRU and\ncomputing the final results. Default: 1\nbias – If False, then the layer does not use bias weights b_ih and b_hh.\nDefault: True\nbatch_first – If True, then the input and output tensors are provided\nas (batch, seq, feature) instead of (seq, batch, feature).\n",
        "input_shape": "",
        "notes": "Inputs/Outputs sections below for details.  Default: False\ndropout – If non-zero, introduces a Dropout layer on the outputs of each\nGRU layer except the last layer, with dropout probability equal to\ndropout. Default: 0\nbidirectional – If True, becomes a bidirectional GRU. Default: False\nInputs: input, h_0\ninput: tensor of shape \\((L, N, H_{in})\\) when batch_first=False or\n\\((N, L, H_{in})\\) when batch_first=True containing the features of\nthe input sequence.\nh_0: tensor of shape \\((D * \\text{num\\_layers}, N, H_{out})\\) containing the initial hidden\nstate for each element in the batch. Defaults to zeros if not provided.\nwhere:\n\\[\\begin{split}\\begin{aligned}\nN ={} & \\text{batch size} \\\\\nL ={} & \\text{sequence length} \\\\\nD ={} & 2 \\text{ if bidirectional=True otherwise } 1 \\\\\nH_{in} ={} & \\text{input\\_size} \\\\\nH_{out} ={} & \\text{hidden\\_size}\n\\end{aligned}\\end{split}\\]\nOutputs: output, h_n\noutput: tensor of shape \\((L, N, D * H_{out})\\) when batch_first=False or\n\\((N, L, D * H_{out})\\) when batch_first=True containing the output features\n(h_t) from the last layer of the GRU, for each t. If a\nh_n: tensor of shape \\((D * \\text{num\\_layers}, N, H_{out})\\) containing the final hidden state\nfor each element in the batch.\nweight_ih_l[k]\nthe learnable input-hidden weights of the \\(\\text{k}^{th}\\) layer\n(W_ir|W_iz|W_in), of shape (3*hidden_size, input_size) for k = 0.\nOtherwise, the shape is (3*hidden_size, num_directions * hidden_size)\nweight_hh_l[k]\nthe learnable hidden-hidden weights of the \\(\\text{k}^{th}\\) layer\n(W_hr|W_hz|W_hn), of shape (3*hidden_size, hidden_size)\nbias_ih_l[k]\nthe learnable input-hidden bias of the \\(\\text{k}^{th}\\) layer\n(b_ir|b_iz|b_in), of shape (3*hidden_size)\nbias_hh_l[k]\nthe learnable hidden-hidden bias of the \\(\\text{k}^{th}\\) layer\n(b_hr|b_hz|b_hn), of shape (3*hidden_size)\nAll the weights and biases are initialized from \\(\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})\\)\nwhere \\(k = \\frac{1}{\\text{hidden\\_size}}\\)\nFor bidirectional GRUs, forward and backward are directions 0 and 1 respectively.\nExample of splitting the output layers when batch_first=False:\noutput.view(seq_len, batch, num_directions, hidden_size).",
        "code_example": ">>> import oneflow as flow\n>>> import numpy as np\n>>> rnn = flow.nn.GRU(10, 20, 2)\n>>> input = flow.tensor(np.random.randn(5, 3, 10), dtype=flow.float32)\n>>> h0 = flow.tensor(np.random.randn(2, 3, 20), dtype=flow.float32)\n>>> output, hn = rnn(input, h0)\n>>> output.size()\noneflow.Size([5, 3, 20])\n\n\n\n"
    },
    {
        "api_name": "oneflow.nn.RNNCell",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.RNNCell.html",
        "api_signature": "oneflow.nn.RNNCell(input_size: int, hidden_size: int, bias: bool = True, nonlinearity: str = 'tanh', device=None, dtype=None)",
        "api_description": "An Elman RNN cell with tanh or ReLU non-linearity.\n\\[h' = \\tanh(W_{ih} x + b_{ih}  +  W_{hh} h + b_{hh})\\]\nIf nonlinearity is ‘relu’, then ReLU is used in place of tanh.",
        "return_value": "",
        "parameters": "\ninput_size – The number of expected features in the input x\nhidden_size – The number of features in the hidden state h\nbias – If False, then the layer does not use bias weights b_ih and b_hh.\nDefault: True\nnonlinearity – The non-linearity to use. Can be either 'tanh' or 'relu'. Default: 'tanh'\n\n\n\n\nInputs: input, hidden\ninput: tensor containing input features\nhidden: tensor containing the initial hidden state\nDefaults to zero if not provided.\n\n\nOutputs: h’\nh’ of shape (batch, hidden_size): tensor containing the next hidden state\nfor each element in the batch\n\n\n",
        "input_shape": "input: \\((N, H_{in})\\) or \\((H_{in})\\) tensor containing input features where\n\\(H_{in}\\) = input_size.\nhidden: \\((N, H_{out})\\) or \\((H_{out})\\) tensor containing the initial hidden\nstate where \\(H_{out}\\) = hidden_size. Defaults to zero if not provided.\noutput: \\((N, H_{out})\\) or \\((H_{out})\\) tensor containing the next hidden state.\n\n\n\n\n\nweight_ih¶\nthe learnable input-hidden weights, of shape\n(hidden_size, input_size)\n\n\n\nweight_hh¶\nthe learnable hidden-hidden weights, of shape\n(hidden_size, hidden_size)\n\n\n\nbias_ih¶\nthe learnable input-hidden bias, of shape (hidden_size)\n\n\n\nbias_hh¶\nthe learnable hidden-hidden bias, of shape (hidden_size)\n\n\n",
        "notes": "All the weights and biases are initialized from \\(\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})\\)\nwhere \\(k = \\frac{1}{\\text{hidden\\_size}}\\)",
        "code_example": ">>> import oneflow as flow\n>>> import oneflow.nn as nn\n\n>>> rnn = nn.RNNCell(10, 20)\n>>> input = flow.randn(6, 3, 10)\n>>> hx = flow.randn(3, 20)\n>>> hx = rnn(input[0], hx)\n>>> hx.size()\noneflow.Size([3, 20])\n\n\n\n"
    },
    {
        "api_name": "oneflow.nn.LSTMCell",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.LSTMCell.html",
        "api_signature": "oneflow.nn.LSTMCell(input_size: int, hidden_size: int, bias: bool = True, device=None, dtype=None)",
        "api_description": "A long short-term memory (LSTM) cell.\n\\[\\begin{split}\\begin{array}{ll}\ni = \\sigma(W_{ii} x + b_{ii} + W_{hi} h + b_{hi}) \\\\\nf = \\sigma(W_{if} x + b_{if} + W_{hf} h + b_{hf}) \\\\\ng = \\tanh(W_{ig} x + b_{ig} + W_{hg} h + b_{hg}) \\\\\no = \\sigma(W_{io} x + b_{io} + W_{ho} h + b_{ho}) \\\\\nc' = f * c + i * g \\\\\nh' = o * \\tanh(c') \\\\\n\\end{array}\\end{split}\\]\nwhere \\(\\sigma\\) is the sigmoid function, and \\(*\\) is the Hadamard product.",
        "return_value": "",
        "parameters": "\ninput_size – The number of expected features in the input x\nhidden_size – The number of features in the hidden state h\nbias – If False, then the layer does not use bias weights b_ih and\nb_hh. Default: True\n\n\n\n\nInputs: input, (h_0, c_0)\ninput of shape (batch, input_size) or (input_size): tensor containing input features\nh_0 of shape (batch, hidden_size) or (hidden_size): tensor containing the initial hidden state\nc_0 of shape (batch, hidden_size) or (hidden_size): tensor containing the initial cell state\nIf (h_0, c_0) is not provided, both h_0 and c_0 default to zero.\n\n\n\nOutputs: (h_1, c_1)\nh_1 of shape (batch, hidden_size) or (hidden_size): tensor containing the next hidden state\nc_1 of shape (batch, hidden_size) or (hidden_size): tensor containing the next cell state\n\n\n\n\n\nweight_ih¶\nthe learnable input-hidden weights, of shape\n(4*hidden_size, input_size)\n\n\n\nweight_hh¶\nthe learnable hidden-hidden weights, of shape\n(4*hidden_size, hidden_size)\n\n\n\nbias_ih¶\nthe learnable input-hidden bias, of shape (4*hidden_size)\n\n\n\nbias_hh¶\nthe learnable hidden-hidden bias, of shape (4*hidden_size)\n\n\n",
        "input_shape": "",
        "notes": "All the weights and biases are initialized from \\(\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})\\)\nwhere \\(k = \\frac{1}{\\text{hidden\\_size}}\\)",
        "code_example": ">>> import oneflow as flow\n>>> import oneflow.nn as nn\n\n>>> rnn = nn.LSTMCell(10, 20) # (input_size, hidden_size)\n>>> input = flow.randn(2, 3, 10) # (time_steps, batch, input_size)\n>>> hx = flow.randn(3, 20) # (batch, hidden_size)\n>>> cx = flow.randn(3, 20)\n>>> hx, cx = rnn(input[0], (hx, cx))\n>>> hx.size()\noneflow.Size([3, 20])\n\n\n\n"
    },
    {
        "api_name": "oneflow.nn.GRUCell",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.GRUCell.html",
        "api_signature": "oneflow.nn.GRUCell(input_size: int, hidden_size: int, bias: bool = True, device=None, dtype=None)",
        "api_description": "A gated recurrent unit (GRU) cell\n\\[\\begin{split}\\begin{array}{ll}\nr = \\sigma(W_{ir} x + b_{ir} + W_{hr} h + b_{hr}) \\\\\nz = \\sigma(W_{iz} x + b_{iz} + W_{hz} h + b_{hz}) \\\\\nn = \\tanh(W_{in} x + b_{in} + r * (W_{hn} h + b_{hn})) \\\\\nh' = (1 - z) * n + z * h\n\\end{array}\\end{split}\\]\nwhere \\(\\sigma\\) is the sigmoid function, and \\(*\\) is the Hadamard product.",
        "return_value": "",
        "parameters": "\ninput_size – The number of expected features in the input x\nhidden_size – The number of features in the hidden state h\nbias – If False, then the layer does not use bias weights b_ih and\nb_hh. Default: True\n\n\n\n\nInputs: input, hidden\ninput : tensor containing input features\nhidden : tensor containing the initial hidden\nstate for each element in the batch.\nDefaults to zero if not provided.\n\n\nOutputs: h’\nh’ : tensor containing the next hidden state\nfor each element in the batch\n\n\n",
        "input_shape": "input: \\((N, H_{in})\\) or \\((H_{in})\\) tensor containing input features where\n\\(H_{in}\\) = input_size.\nhidden: \\((N, H_{out})\\) or \\((H_{out})\\) tensor containing the initial hidden\nstate where \\(H_{out}\\) = hidden_size. Defaults to zero if not provided.\noutput: \\((N, H_{out})\\) or \\((H_{out})\\) tensor containing the next hidden state.\n\n\n\n\n\nweight_ih¶\nthe learnable input-hidden weights, of shape\n(3*hidden_size, input_size)\n\n\n\nweight_hh¶\nthe learnable hidden-hidden weights, of shape\n(3*hidden_size, hidden_size)\n\n\n\nbias_ih¶\nthe learnable input-hidden bias, of shape (3*hidden_size)\n\n\n\nbias_hh¶\nthe learnable hidden-hidden bias, of shape (3*hidden_size)\n\n\n",
        "notes": "All the weights and biases are initialized from \\(\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})\\)\nwhere \\(k = \\frac{1}{\\text{hidden\\_size}}\\)",
        "code_example": ">>> import oneflow as flow\n>>> import oneflow.nn as nn\n\n>>> rnn = nn.GRUCell(10, 20)\n>>> input = flow.randn(6, 3, 10)\n>>> hx = flow.randn(3, 20)\n>>> hx = rnn(input[0], hx)\n>>> hx.size()\noneflow.Size([3, 20])\n\n\n\n"
    },
    {
        "api_name": "oneflow.nn.Identity",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.Identity.html",
        "api_signature": "oneflow.nn.Identity(*args, **kwargs)",
        "api_description": "A placeholder identity operator that is argument-insensitive.",
        "return_value": "",
        "parameters": "\nargs – any argument (unused)\nkwargs – any keyword argument (unused)\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": "import numpy as np\nimport oneflow as flow\n\nm = flow.nn.Identity()\ninput = flow.Tensor(np.random.rand(2, 3, 4, 5))\n\noutput = m(input)\n\n# output = input\n\n\n\n"
    },
    {
        "api_name": "oneflow.nn.Linear",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.Linear.html",
        "api_signature": "oneflow.nn.Linear(in_features: int, out_features: int, bias: bool = True, device=None, dtype=None)",
        "api_description": "Applies a linear transformation to the incoming data: \\(y = xA^T + b\\)",
        "return_value": "",
        "parameters": "\nin_features (-) – size of each input sample\nout_features (-) – size of each output sample\nbias (-) – If set to False, the layer will not learn an additive bias. Default: True\n\n\n\n\n",
        "input_shape": "Input: \\((N, *, H_{in})\\) where \\(*\\) means any number of\nadditional dimensions and \\(H_{in} = {in\\_features}\\)\nOutput: \\((N, *, H_{out})\\) where all but the last dimension\nare the same shape as the input and \\(H_{out} = {out\\_features}\\).\n\n\nAttr:\nweight: the learnable weights of the module of shape \\(({out\\_features}, {in\\_features})\\). The values are initialized from \\(\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})\\), where \\((k = 1 / {in\\_features})\\)\nbias: the learnable bias of the module of shape \\(({out\\_features})\\). If bias is True, the values are initialized from \\(\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})\\) where \\((k = 1 / {in\\_features})\\)\n\n\n\n",
        "notes": "",
        "code_example": ">>> import numpy as np\n>>> import oneflow as flow\n\n\n>>> m = flow.nn.Linear(20, 30, False)\n>>> input = flow.Tensor(np.random.randn(128, 20))\n>>> output = m(input)\n>>> output.size()\noneflow.Size([128, 30])\n\n\n\n"
    },
    {
        "api_name": "oneflow.nn.Dropout",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.Dropout.html",
        "api_signature": "oneflow.nn.Dropout(p: float = 0.5, inplace: bool = False, generator=None)",
        "api_description": "During training, randomly zeroes some of the elements of the input\ntensor with probability p using samples from a Bernoulli\ndistribution. Each channel will be zeroed out independently on every forward\ncall.\nThis has proven to be an effective technique for regularization and\npreventing the co-adaptation of neurons as described in the paper\n“Improving neural networks by preventing co-adaptation of feature\ndetectors”.\nFurthermore, the outputs are scaled by a factor of \\(\\frac{1}{1-p}\\) during\ntraining. This means that during evaluation the module simply computes an\nidentity function.\nAdditionally, we can pass an extra Tensor addend which shape is consistent with input Tensor.\nThe addend Tensor will be add in result after dropout, it is very useful in model’s residual connection structure.",
        "return_value": "",
        "parameters": "\np – probability of an element to be zeroed. Default: 0.5\ninplace – If set to True, will do this operation in-place. Default: False\ngenerator – A pseudorandom number generator for sampling\n\n\n\n\n",
        "input_shape": "Input: \\((*)\\). Input can be of any shape\nOutput: \\((*)\\). Output is of the same shape as input\n\n\n\n",
        "notes": "",
        "code_example": "example 1:\n>>> import numpy as np\n>>> import oneflow as flow\n\n>>> m = flow.nn.Dropout(p=0)\n>>> arr = np.array(\n...    [\n...        [-0.7797, 0.2264, 0.2458, 0.4163],\n...        [0.4299, 0.3626, -0.4892, 0.4141],\n...        [-1.4115, 1.2183, -0.5503, 0.6520],\n...    ]\n... )\n>>> x = flow.Tensor(arr)\n>>> y = m(x)\n>>> y \ntensor([[-0.7797,  0.2264,  0.2458,  0.4163],\n        [ 0.4299,  0.3626, -0.4892,  0.4141],\n        [-1.4115,  1.2183, -0.5503,  0.6520]], dtype=oneflow.float32)\n\n\nexample 2:\n>>> import numpy as np\n>>> import oneflow as flow\n\n>>> m = flow.nn.Dropout(p=0)\n>>> arr = np.array(\n...    [\n...        [-0.7797, 0.2264, 0.2458, 0.4163],\n...        [0.4299, 0.3626, -0.4892, 0.4141],\n...        [-1.4115, 1.2183, -0.5503, 0.6520],\n...    ]\n... )\n>>> x = flow.Tensor(arr)\n>>> addend = flow.ones((3, 4), dtype=flow.float32)\n>>> y = m(x, addend=addend)\n>>> y \ntensor([[ 0.2203,  1.2264,  1.2458,  1.4163],\n        [ 1.4299,  1.3626,  0.5108,  1.4141],\n        [-0.4115,  2.2183,  0.4497,  1.6520]], dtype=oneflow.float32)\n\n\n\n"
    },
    {
        "api_name": "oneflow.nn.Dropout1d",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.Dropout1d.html",
        "api_signature": "oneflow.nn.Dropout1d(p: float = 0.5, inplace: bool = False, generator=None)",
        "api_description": "Randomly zero out entire channels (a channel is a 1D feature map,\ne.g., the \\(j\\)-th channel of the \\(i\\)-th sample in the\nbatched input is a 1D tensor :math:`        ext{input}[i, j]`).\nEach channel will be zeroed out independently on every forward call with\nprobability p using samples from a Bernoulli distribution.\nUsually the input comes from nn.Conv1d modules.\nAs described in the paper\nEfficient Object Localization Using Convolutional Networks ,\nif adjacent pixels within feature maps are strongly correlated\n(as is normally the case in early convolution layers) then i.i.d. dropout\nwill not regularize the activations and will otherwise just result\nin an effective learning rate decrease.\nIn this case, oneflow.nn.Dropout1d() will help promote independence between\nfeature maps and should be used instead.",
        "return_value": "",
        "parameters": "\np (float, optional) – probability of an element to be zero-ed.\ninplace (bool, optional) – If set to True, will do this operation\nin-place\n\n\n\n\n",
        "input_shape": "Input: \\((N, C, L)\\) or \\((C, L)\\).\nOutput: \\((N, C, L)\\) or \\((C, L)\\) (same shape as input).\n\n\n\n",
        "notes": "",
        "code_example": ">>> import numpy as np\n>>> import oneflow as flow\n\n>>> m = flow.nn.Dropout1d(p=0)\n>>> arr = np.array(\n...    [\n...        [-0.7797, 0.2264, 0.2458, 0.4163],\n...        [0.4299, 0.3626, -0.4892, 0.4141],\n...        [-1.4115, 1.2183, -0.5503, 0.6520],\n...    ]\n... )\n>>> x = flow.Tensor(arr)\n>>> y = m(x)\n>>> y \ntensor([[-0.7797,  0.2264,  0.2458,  0.4163],\n        [ 0.4299,  0.3626, -0.4892,  0.4141],\n        [-1.4115,  1.2183, -0.5503,  0.6520]], dtype=oneflow.float32)\n\n\n\n"
    },
    {
        "api_name": "oneflow.nn.Dropout2d",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.Dropout2d.html",
        "api_signature": "oneflow.nn.Dropout2d(p: float = 0.5, inplace: bool = False, generator=None)",
        "api_description": "Randomly zero out entire channels (a channel is a 2D feature map,\ne.g., the \\(j\\)-th channel of the \\(i\\)-th sample in the\nbatched input is a 2D tensor :math:`        ext{input}[i, j]`).\nEach channel will be zeroed out independently on every forward call with\nprobability p using samples from a Bernoulli distribution.\nUsually the input comes from nn.Conv2d modules.\nAs described in the paper\nEfficient Object Localization Using Convolutional Networks ,\nif adjacent pixels within feature maps are strongly correlated\n(as is normally the case in early convolution layers) then i.i.d. dropout\nwill not regularize the activations and will otherwise just result\nin an effective learning rate decrease.\nIn this case, oneflow.nn.Dropout2d() will help promote independence between\nfeature maps and should be used instead.",
        "return_value": "",
        "parameters": "\np (float, optional) – probability of an element to be zero-ed.\ninplace (bool, optional) – If set to True, will do this operation\nin-place\n\n\n\n\n",
        "input_shape": "Input: \\((N, C, H, W)\\) or \\((C, H, W)\\).\nOutput: \\((N, C, H, W)\\) or \\((C, H, W)\\) (same shape as input).\n\n\n\n",
        "notes": "",
        "code_example": ">>> import numpy as np\n>>> import oneflow as flow\n\n>>> m = flow.nn.Dropout2d(p=0)\n>>> arr = np.array(\n...    [\n...        [-0.7797, 0.2264, 0.2458, 0.4163],\n...        [0.4299, 0.3626, -0.4892, 0.4141],\n...        [-1.4115, 1.2183, -0.5503, 0.6520],\n...    ]\n... )\n>>> x = flow.Tensor(arr)\n>>> y = m(x)\n>>> y \ntensor([[-0.7797,  0.2264,  0.2458,  0.4163],\n        [ 0.4299,  0.3626, -0.4892,  0.4141],\n        [-1.4115,  1.2183, -0.5503,  0.6520]], dtype=oneflow.float32)\n\n\n\n"
    },
    {
        "api_name": "oneflow.nn.Dropout3d",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.Dropout3d.html",
        "api_signature": "oneflow.nn.Dropout3d(p: float = 0.5, inplace: bool = False, generator=None)",
        "api_description": "Randomly zero out entire channels (a channel is a 3D feature map,\ne.g., the \\(j\\)-th channel of the \\(i\\)-th sample in the\nbatched input is a 3D tensor :math:`        ext{input}[i, j]`).\nEach channel will be zeroed out independently on every forward call with\nprobability p using samples from a Bernoulli distribution.\nUsually the input comes from nn.Conv3d modules.\nAs described in the paper\nEfficient Object Localization Using Convolutional Networks ,\nif adjacent pixels within feature maps are strongly correlated\n(as is normally the case in early convolution layers) then i.i.d. dropout\nwill not regularize the activations and will otherwise just result\nin an effective learning rate decrease.\nIn this case, oneflow.nn.Dropout3d() will help promote independence between\nfeature maps and should be used instead.",
        "return_value": "",
        "parameters": "\np (float, optional) – probability of an element to be zeroed.\ninplace (bool, optional) – If set to True, will do this operation\nin-place\n\n\n\n\n",
        "input_shape": "Input: \\((N, C, D, H, W)\\) or \\((C, D, H, W)\\).\nOutput: \\((N, C, D, H, W)\\) or \\((C, D, H, W)\\) (same shape as input).\n\n\n\n",
        "notes": "",
        "code_example": ">>> import numpy as np\n>>> import oneflow as flow\n\n>>> m = flow.nn.Dropout3d(p=0)\n>>> arr = np.array(\n...    [\n...        [-0.7797, 0.2264, 0.2458, 0.4163],\n...        [0.4299, 0.3626, -0.4892, 0.4141],\n...        [-1.4115, 1.2183, -0.5503, 0.6520],\n...    ]\n... )\n>>> x = flow.Tensor(arr)\n>>> y = m(x)\n>>> y \ntensor([[-0.7797,  0.2264,  0.2458,  0.4163],\n        [ 0.4299,  0.3626, -0.4892,  0.4141],\n        [-1.4115,  1.2183, -0.5503,  0.6520]], dtype=oneflow.float32)\n\n\n\n"
    },
    {
        "api_name": "oneflow.nn.Embedding",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.Embedding.html",
        "api_signature": "oneflow.nn.Embedding(num_embeddings: int, embedding_dim: int, padding_idx: Optional[int] = None, max_norm: Optional[float] = None, norm_type: float = 2.0, scale_grad_by_freq: bool = False, sparse: bool = False, _weight: Optional[oneflow.Tensor] = None, device=None, dtype=None)",
        "api_description": "A simple lookup table that stores embeddings of a fixed dictionary and size.\nThis module is often used to store word embeddings and retrieve them using indices.\nThe input to the module is a list of indices, and the output is the corresponding\nword embeddings.",
        "return_value": "",
        "parameters": "\nnum_embeddings (int) – size of the dictionary of embeddings\nembedding_dim (int) – the size of each embedding vector\npadding_idx (int, optional) – If specified, the entries at padding_idx do not contribute to the gradient;\ntherefore, the embedding vector at padding_idx is not updated during training,\ni.e. it remains as a fixed “pad”. For a newly constructed Embedding,\nthe embedding vector at padding_idx will default to all zeros,\nbut can be updated to another value to be used as the padding vector.\nmax_norm (float, optional) – If given, each embedding vector with norm larger than max_norm is renormalized to have\nnorm max_norm\nnorm_type (float, optional) – The p of the p-norm to compute for the max_norm option. Default 2.\nscale_grad_by_freq (boolean, optional) – If given, this will scale gradients by the inverse of\nfrequency of the words in the mini-batch. Default False\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import numpy as np\n>>> import oneflow as flow\n\n>>> indices = flow.tensor([[1, 2, 4, 5], [4, 3, 2, 9]], dtype=flow.int)\n>>> m = flow.nn.Embedding(10, 3)\n>>> y = m(indices)\n\n\n\n"
    },
    {
        "api_name": "oneflow.nn.CosineSimilarity",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.CosineSimilarity.html",
        "api_signature": "oneflow.nn.CosineSimilarity(dim: Optional[int] = 1, eps: Optional[float] = 1e-08)",
        "api_description": "",
        "return_value": "\n\\[\\text{similarity} = \\dfrac{x_1 \\cdot x_2}{\\max(\\Vert x_1 \\Vert _2 \\cdot \\Vert x_2 \\Vert _2, \\epsilon)}.\\]\nThe interface is consistent with PyTorch.\nThe documentation is referenced from: https://pytorch.org/docs/1.10/generated/torch.nn.CosineSimilarity.html#torch.nn.CosineSimilarity\n\n",
        "parameters": "\ndim (int, optional) – Dimension where cosine similarity is computed. Default: 1\neps (float, optional) – Small value to avoid division by zero.\nDefault: 1e-8\n\n\n\n\n",
        "input_shape": "Input1: \\((\\ast_1, D, \\ast_2)\\) where D is at position dim.\n\nInput2: \\((\\ast_1, D, \\ast_2)\\), same number of dimensions as x1, matching x1 size at dimension dim,and broadcastable with x1 at other dimensions.\n\n\n\nOutput: \\((\\ast_1, \\ast_2)\\)\n\n\n\n",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> from oneflow import nn\n>>> input1 = flow.randn(100, 128)\n>>> input2 = flow.randn(100, 128)\n>>> cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n>>> output = cos(input1, input2)\n\n\n\n"
    },
    {
        "api_name": "oneflow.nn.PairwiseDistance",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.PairwiseDistance.html",
        "api_signature": "oneflow.nn.PairwiseDistance(p: Optional[float] = 2.0, eps: Optional[float] = 1e-06, keepdim: Optional[bool] = False)",
        "api_description": "Computes the pairwise distance between vectors \\(v_1\\), \\(v_2\\) using the p-norm:\n\\[\\left \\| x \\right \\| _p = (\\sum_{i=1}^n \\left | x_i \\right |^p )^{\\frac{1}{p}}\\]",
        "return_value": "",
        "parameters": "\np (real) – the norm degree. Default: 2\neps (float, optional) – Small value to avoid division by zero. Default: 1e-6\nkeepdim (bool, optional) – Determines whether or not to keep the vector dimension. Default: False\n\n\n\n\n",
        "input_shape": "Input1: \\((N, D)\\) or \\((D)\\), where N = batch dimension and D = vector dimension\nInput2: \\((N, D)\\) or \\((D)\\), same shape as the input1\nOutput: \\((N)\\) or \\(()\\) based on input dimension. If keepdim is True, then \\((N, 1)\\) or \\((1)\\) based on input dimension.\n\n\n\n",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> pdist = flow.nn.PairwiseDistance(p=2)\n>>> x1 = flow.arange(12).reshape(3, 4)\n>>> x2 = flow.arange(12).reshape(3, 4)\n>>> pdist(x1, x2)\ntensor([2.0000e-06, 2.0000e-06, 2.0000e-06], dtype=oneflow.float32)\n>>> pdist(x1, x2).shape\noneflow.Size([3])\n\n\n\n"
    },
    {
        "api_name": "oneflow.nn.BCELoss",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.BCELoss.html",
        "api_signature": "oneflow.nn.BCELoss(weight: Optional[oneflow.Tensor] = None, reduction: str = 'mean')",
        "api_description": "This operator computes the binary cross entropy loss.\nThe equation is:\nif reduction = “none”:\n\\[out = -(Target_i*log(Input_i) + (1-Target_i)*log(1-Input_i))\\]\nif reduction = “mean”:\n\\[out = -\\frac{1}{n}\\sum_{i=1}^n(Target_i*log(Input_i) + (1-Target_i)*log(1-Input_i))\\]\nif reduction = “sum”:\n\\[out = -\\sum_{i=1}^n(Target_i*log(Input_i) + (1-Target_i)*log(1-Input_i))\\]",
        "return_value": "The result Tensor.\n\n",
        "parameters": "\nweight (oneflow.Tensor, optional) – The manual rescaling weight to the loss. Default to None, whose corresponding weight value is 1.\nreduction (str, optional) – The reduce type, it can be one of “none”, “mean”, “sum”. Defaults to “mean”.\n\n\n\n\nAttention\nThe input value must be in the range of (0, 1). Or the loss function may return nan value.\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> import numpy as np\n>>> input = flow.Tensor(np.array([[1.2, 0.2, -0.3], [0.7, 0.6, -2]]).astype(np.float32))\n>>> target = flow.Tensor(np.array([[0, 1, 0], [1, 0, 1]]).astype(np.float32))\n>>> weight = flow.Tensor(np.array([[2, 2, 2], [2, 2, 2]]).astype(np.float32))\n>>> activation = flow.nn.Sigmoid()\n>>> sigmoid_input = activation(input)\n>>> m = flow.nn.BCELoss(weight, reduction=\"none\")\n>>> out = m(sigmoid_input, target)\n>>> out\ntensor([[2.9266, 1.1963, 1.1087],\n        [0.8064, 2.0750, 4.2539]], dtype=oneflow.float32)\n>>> m_sum = flow.nn.BCELoss(weight, reduction=\"sum\")\n>>> out = m_sum(sigmoid_input, target)\n>>> out\ntensor(12.3668, dtype=oneflow.float32)\n>>> m_mean = flow.nn.BCELoss(weight, reduction=\"mean\")\n>>> out = m_mean(sigmoid_input, target)\n>>> out\ntensor(2.0611, dtype=oneflow.float32)\n>>> m_none = flow.nn.BCELoss()\n>>> out = m_none(sigmoid_input, target)\n>>> out\ntensor(1.0306, dtype=oneflow.float32)\n\n\n\n"
    },
    {
        "api_name": "oneflow.nn.BCEWithLogitsLoss",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.BCEWithLogitsLoss.html",
        "api_signature": "oneflow.nn.BCEWithLogitsLoss(weight: Optional[oneflow.Tensor] = None, reduction: str = 'mean', pos_weight: Optional[oneflow.Tensor] = None)",
        "api_description": "This operator combines the Sigmoid and BCELoss together. For numerical stability,\nwe apply some math tricks instead of using Sigmoid layer with BCELoss.\nThe equation is:\nif reduction = \"none\":\n\\[out = -weight*[Pos\\_weight*y*log\\sigma({x}) + (1-y)*log(1-\\sigma(x))]\\]\nif reduction = \"mean\":\n\\[out = -\\frac{weight}{n}\\sum_{i=1}^n[Pos\\_weight*y*log\\sigma({x}) + (1-y)*log(1-\\sigma(x))]\\]\nif reduction = \"sum\":\n\\[out = -weight*\\sum_{i=1}^n[Pos\\_weight*y*log\\sigma({x}) + (1-y)*log(1-\\sigma(x))]\\]",
        "return_value": "",
        "parameters": "\nweight (Tensor, optional) – The manual rescaling weight to the loss. Default: None\nsize_average (bool, optional) – Deprecated (see reduction). Default: True\nreduce (bool, optional) – Deprecated (see reduction). Default: True\nreduction (str, optional) – The reduce type, it can be one of \"none\", \"mean\", \"sum\".\n'none': no reduction will be applied, 'mean': the sum of the output will be divided\nby the number of elements in the output, 'sum': the output will be summed. Default: \"mean\"\npos_weight (Tensor, optional) – The manual rescaling weight to the positive examples.\nDefault: None\n\n\n\n\n",
        "input_shape": "Input: \\((N,*)\\) where * means, any number of additional dimensions\nTarget: \\((N,*)\\), same shape as the input\nOutput: scalar. If reduction is \"none\", then \\((N,*)\\), same shape as input.\n\n\n\n",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> input = flow.tensor([[1.2, 0.2, -0.3], [0.7, 0.6, -2], [0.7, 0.6, -2]], dtype=flow.float32)\n>>> target = flow.tensor([[0, 1, 0], [1, 0, 1], [1, 0, 1]], dtype=flow.float32)\n>>> weight = flow.tensor([[2, 2, 2], [2, 2, 2], [2, 2, 2]], dtype=flow.float32)\n>>> pos_weight = flow.tensor([1.2, 1.3, 1.4], dtype=flow.float32)\n\n>>> m = flow.nn.BCEWithLogitsLoss(weight=weight, pos_weight=pos_weight, reduction=\"none\")\n>>> out = m(input, target)\n>>> out\ntensor([[2.9266, 1.5552, 1.1087],\n        [0.9676, 2.0750, 5.9554],\n        [0.9676, 2.0750, 5.9554]], dtype=oneflow.float32)\n\n>>> m = flow.nn.BCEWithLogitsLoss(weight=weight, pos_weight=pos_weight, reduction=\"mean\")\n>>> out = m(input, target)\n>>> out\ntensor(2.6207, dtype=oneflow.float32)\n\n>>> m = flow.nn.BCEWithLogitsLoss(weight=weight, pos_weight=pos_weight, reduction=\"sum\")\n>>> out = m(input, target)\n>>> out\ntensor(23.5865, dtype=oneflow.float32)\n\n\n\n"
    },
    {
        "api_name": "oneflow.nn.CTCLoss",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.CTCLoss.html",
        "api_signature": "oneflow.nn.CTCLoss(blank: int = 0, reduction: str = 'mean', zero_infinity: bool = False)",
        "api_description": "The Connectionist Temporal Classification loss.\nThe documentation is referenced from:\nCalculates loss between a continuous (unsegmented) time series and a target sequence. CTCLoss sums over the\nprobability of possible alignments of input to target, producing a loss value which is differentiable\nwith respect to each input node. The alignment of input to target is assumed to be “many-to-one”, which\nlimits the length of the target sequence such that it must be \\(\\leq\\) the input length.",
        "return_value": "",
        "parameters": "\nblank (int, optional) – blank label. Default \\(0\\).\nreduction (string, optional) – Specifies the reduction to apply to the output:\n'none' | 'mean' | 'sum'. 'none': no reduction will be applied,\n'mean': the output losses will be divided by the target lengths and\nthen the mean over the batch is taken. Default: 'mean'\nzero_infinity (bool, optional) – Whether to zero infinite losses and the associated gradients.\nDefault: False\nInfinite losses mainly occur when the inputs are too short\nto be aligned to the targets.\n\n\n\n\n",
        "input_shape": "Log_probs: Tensor of size \\((T, N, C)\\),\nwhere \\(T = \\text{input length}\\),\n\\(N = \\text{batch size}\\), and\n\\(C = \\text{number of classes (including blank)}\\).\nTargets: Tensor of size \\((N, S)\\) or\n\\((\\operatorname{sum}(\\text{target_lengths}))\\),\nwhere \\(N = \\text{batch size}\\) and\n\\(S = \\text{max target length, if shape is } (N, S)\\).\nIt represent the target sequences. Each element in the target\nsequence is a class index. And the target index cannot be blank (default=0).\nIn the \\((N, S)\\) form, targets are padded to the\nlength of the longest sequence, and stacked.\nIn the \\((\\operatorname{sum}(\\text{target_lengths}))\\) form,\nthe targets are assumed to be un-padded and\nconcatenated within 1 dimension.\nInput_lengths: Tuple or tensor of size \\((N)\\),\nwhere \\(N = \\text{batch size}\\). It represent the lengths of the\ninputs (must each be \\(\\leq T\\)). And the lengths are specified\nfor each sequence to achieve masking under the assumption that sequences\nare padded to equal lengths.\nTarget_lengths: Tuple or tensor of size \\((N)\\),\nwhere \\(N = \\text{batch size}\\). It represent lengths of the targets.\nLengths are specified for each sequence to achieve masking under the\nassumption that sequences are padded to equal lengths. If target shape is\n\\((N,S)\\), target_lengths are effectively the stop index\n\\(s_n\\) for each target sequence, such that target_n = targets[n,0:s_n] for\neach target in a batch. Lengths must each be \\(\\leq S\\)\nIf the targets are given as a 1d tensor that is the concatenation of individual\ntargets, the target_lengths must add up to the total length of the tensor.\n\n\nReference:A. Graves et al.: Connectionist Temporal Classification:\nLabelling Unsegmented Sequence Data with Recurrent Neural Networks:\nhttps://www.cs.toronto.edu/~graves/icml_2006.pdf\n\n\n",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n\n>>> log_probs = flow.tensor(\n...    [\n...        [[-1.1031, -0.7998, -1.5200], [-0.9808, -1.1363, -1.1908]],\n...        [[-1.2258, -1.0665, -1.0153], [-1.1135, -1.2331, -0.9671]],\n...        [[-1.3348, -0.6611, -1.5118], [-0.9823, -1.2355, -1.0941]],\n...        [[-1.3850, -1.3273, -0.7247], [-0.8235, -1.4783, -1.0994]],\n...        [[-0.9049, -0.8867, -1.6962], [-1.4938, -1.3630, -0.6547]],\n...    ], dtype=flow.float32)\n>>> targets = flow.tensor([[1, 2, 2], [1, 2, 2]], dtype=flow.int32)\n>>> input_lengths = flow.tensor([5, 5], dtype=flow.int32)\n>>> target_lengths = flow.tensor([3, 3], dtype=flow.int32)\n>>> loss_mean = flow.nn.CTCLoss()\n>>> out = loss_mean(log_probs, targets, input_lengths, target_lengths)\n>>> out\ntensor(1.1376, dtype=oneflow.float32)\n>>> loss_sum = flow.nn.CTCLoss(blank=0, reduction=\"sum\")\n>>> out = loss_sum(log_probs, targets, input_lengths, target_lengths)\n>>> out\ntensor(6.8257, dtype=oneflow.float32)\n\n\n\n"
    },
    {
        "api_name": "oneflow.nn.CombinedMarginLoss",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.CombinedMarginLoss.html",
        "api_signature": "oneflow.nn.CombinedMarginLoss(m1: float = 1.0, m2: float = 0.0, m3: float = 0.0)",
        "api_description": "The operation implements “margin_softmax” in InsightFace:\nhttps://github.com/deepinsight/insightface/blob/master/recognition/arcface_mxnet/train.py\nThe implementation of margin_softmax in InsightFace is composed of multiple operators.\nWe fuse them for speed up.\nApplies the function:\n\\[\\begin{split}{\\rm CombinedMarginLoss}(x_i, label) =\n\\left\\{\\begin{matrix} \\cos(m_1\\cdot\\arccos x_i+m_2) - m_3 & {\\rm if} \\ i == label \\\\\nx_i & {\\rm otherwise} \\end{matrix}\\right.\\end{split}\\]",
        "return_value": "A Tensor\n\n",
        "parameters": "\nx (oneflow.Tensor) – A Tensor\nlabel (oneflow.Tensor) – label with integer data type\nm1 (float) – loss m1 parameter\nm2 (float) – loss m2 parameter\nm3 (float) – loss m3 parameter\n\n\n\n\n",
        "input_shape": "",
        "notes": "Here are some special cases:\nwhen \\(m_1=1, m_2\\neq 0, m_3=0\\), CombineMarginLoss has the same parameter as ArcFace .\nwhen \\(m_1=1, m_2=0, m_3\\neq 0\\), CombineMarginLoss has the same parameter as CosFace (a.k.a AM-Softmax) .\nwhen \\(m_1\\gt 1, m_2=m_3=0\\), CombineMarginLoss has the same parameter as A-Softmax.",
        "code_example": ">>> import numpy as np\n>>> import oneflow as flow\n>>> np_x = np.array([[-0.7027179, 0.0230609], [-0.02721931, -0.16056311], [-0.4565852, -0.64471215]])\n>>> np_label = np.array([0, 1, 1])\n>>> x = flow.tensor(np_x, dtype=flow.float32)\n>>> label = flow.tensor(np_label, dtype=flow.int32)\n>>> loss_func = flow.nn.CombinedMarginLoss(0.3, 0.5, 0.4)\n>>> out = loss_func(x, label)\n>>> out\ntensor([[-0.0423,  0.0231],\n        [-0.0272,  0.1237],\n        [-0.4566, -0.0204]], dtype=oneflow.float32)\n\n\n\n"
    },
    {
        "api_name": "oneflow.nn.CrossEntropyLoss",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.CrossEntropyLoss.html",
        "api_signature": "oneflow.nn.CrossEntropyLoss(weight: Optional[oneflow.Tensor] = None, ignore_index: int = - 100, reduction: str = 'mean', label_smoothing: float = 0.0)",
        "api_description": "The documentation is referenced from:\nThis criterion combines LogSoftmax and NLLLoss in one single class.\nIt is useful when training a classification problem with C classes.\nIf provided, the optional argument weight should be a 1D Tensor assigning weight to each of the classes.\nThis is particularly useful when you have an unbalanced training set.\nThe input is expected to contain raw, unnormalized scores for each class.\ninput has to be a Tensor of size either \\((minibatch, C)\\) or\n\\((minibatch, C, d_1, d_2, ..., d_K)\\)\nwith \\(K \\geq 1\\) for the K-dimensional case (described later).\nThe target that this criterion expects should contain either:\nClass indices in the range \\([0, C)\\) where \\(C\\) is the number of classes; if\nignore_index is specified, this loss also accepts this class index (this index\nmay not necessarily be in the class range). The unreduced (i.e. with reduction\nset to 'none') loss for this case can be described as:\n\\[\\ell(x, y) = L = \\{l_1,\\dots,l_N\\}^\\top, \\quad\nl_n = - w_{y_n} \\log \\frac{\\exp(x_{n,y_n})}{\\sum_{c=1}^C \\exp(x_{n,c})}\n\\cdot \\mathbb{1}\\{y_n \\not= \\text{ignore_index}\\}\\]\nwhere \\(x\\) is the input, \\(y\\) is the target, \\(w\\) is the weight,\n\\(C\\) is the number of classes, and \\(N\\) spans the minibatch dimension as well as\n\\(d_1, ..., d_k\\) for the K-dimensional case. If\nreduction is not 'none' (default 'mean'), then\n\\[\\begin{split}\\ell(x, y) = \\begin{cases}\n\\sum_{n=1}^N \\frac{1}{\\sum_{n=1}^N w_{y_n} \\cdot \\mathbb{1}\\{y_n \\not= \\text{ignore_index}\\}} l_n, &\n\\text{if reduction} = \\text{'mean';}\\\\\n\\sum_{n=1}^N l_n,  &\n\\text{if reduction} = \\text{'sum'.}\n\\end{cases}\\end{split}\\]",
        "return_value": "",
        "parameters": "\nweight (oneflow.Tensor, optional) – a manual rescaling weight given to each class.\nIf given, has to be a Tensor of size C\nignore_index (int, optional) – Specifies a target value that is ignored and does not\ncontribute to the input gradient. When reduction is mean, the loss is averaged\nover non-ignored targets. Note that ignore_index is only applicable when the target\ncontains class indices.\nreduction (string, optional) – Specifies the reduction to apply to the output:\n'none' | 'mean' | 'sum'. 'none': no reduction will\nbe applied, 'mean': the weighted mean of the output is taken,\n'sum': the output will be summed. Default: 'mean'\nlabel_smoothing (float, optinoal) – A float in [0.0, 1.0]. Specifies the amount\nof smoothing when computing the loss, where 0.0 means no smoothing.\nThe targets become a mixture of the original ground truth and a uniform\ndistribution as described in Rethinking the Inception Architecture for Computer Vision.\nDefault: \\(0.0\\).\n\n\n\n\n",
        "input_shape": "Input: Shape :\\((N, C)\\) or \\((N, C, d_1, d_2, ..., d_K)\\) with \\(K \\geq 1\\)\nin the case of K-dimensional loss.\nTarget: If containing class indices, shape \\((N)\\) or \\((N, d_1, d_2, ..., d_K)\\) with\n\\(K \\geq 1\\) in the case of K-dimensional loss where each value should be between \\([0, C)\\).\nIf containing class probabilities, same shape as the input and each value should be between \\([0, 1]\\).\nOutput: If reduction is ‘none’, same shape as the target. Otherwise, scalar.\n\nwhere:\n\n\\[\\begin{split}\\begin{aligned}\n    C ={} & \\text{number of classes} \\\\\n    N ={} & \\text{batch size} \\\\\n\\end{aligned}\\end{split}\\]\n\n\n",
        "notes": "NLLLoss.\nProbabilities for each class; useful when labels beyond a single class per minibatch item\nare required, such as for blended labels, label smoothing, etc. The unreduced (i.e. with\nreduction set to 'none') loss for this case can be described as:\n\\[\\ell(x, y) = L = \\{l_1,\\dots,l_N\\}^\\top, \\quad\nl_n = - \\sum_{c=1}^C w_c \\log \\frac{\\exp(x_{n,c})}{\\sum_{i=1}^C \\exp(x_{n,i})} y_{n,c}\\]\nwhere \\(x\\) is the input, \\(y\\) is the target, \\(w\\) is the weight,\n\\(C\\) is the number of classes, and \\(N\\) spans the minibatch dimension as well as\n\\(d_1, ..., d_k\\) for the K-dimensional case. If\nreduction is not 'none' (default 'mean'), then\n\\[\\begin{split}\\ell(x, y) = \\begin{cases}\n\\frac{\\sum_{n=1}^N l_n}{N}, &\n\\text{if reduction} = \\text{'mean';}\\\\\n\\sum_{n=1}^N l_n,  &\n\\text{if reduction} = \\text{'sum'.}\n\\end{cases}\\end{split}\\]",
        "code_example": ">>> import oneflow as flow\n>>> import numpy as np\n\n>>> input = flow.tensor(\n...    [[-0.1664078, -1.7256707, -0.14690138],\n...        [-0.21474946, 0.53737473, 0.99684894],\n...        [-1.135804, -0.50371903, 0.7645404]], dtype=flow.float32)\n>>> target = flow.tensor(np.array([0, 1, 2]), dtype=flow.int32)\n>>> out = flow.nn.CrossEntropyLoss(reduction=\"none\")(input, target)\n>>> out\ntensor([0.8020, 1.1167, 0.3583], dtype=oneflow.float32)\n>>> out_sum = flow.nn.CrossEntropyLoss(reduction=\"sum\")(input, target)\n>>> out_sum\ntensor(2.2769, dtype=oneflow.float32)\n>>> out_mean = flow.nn.CrossEntropyLoss(reduction=\"mean\")(input, target)\n>>> out_mean\ntensor(0.7590, dtype=oneflow.float32)\n>>> out_ignore_0 = flow.nn.CrossEntropyLoss(reduction=\"none\", ignore_index=0)(input, target)\n>>> out_ignore_0\ntensor([0.0000, 1.1167, 0.3583], dtype=oneflow.float32)\n>>> out_label_smoothing = flow.nn.CrossEntropyLoss(reduction=\"none\", label_smoothing=0.5)(input, target)\n>>> out_label_smoothing\ntensor([1.0586, 1.1654, 0.8864], dtype=oneflow.float32)\n>>> probs = flow.tensor([[ 0.99495536,  0.28255007, -0.2775054 ],\n...    [ 0.42397153,  0.01075112,  0.56527734],\n...    [ 0.72356546, -0.1304398 ,  0.4068744 ]], dtype=flow.float32)\n>>> out = flow.nn.CrossEntropyLoss()(input, probs)\n>>> out\ntensor(1.3305, dtype=oneflow.float32)\n\n\n\n"
    },
    {
        "api_name": "oneflow.nn.KLDivLoss",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.KLDivLoss.html",
        "api_signature": "oneflow.nn.KLDivLoss(reduction: str = 'mean', log_target: bool = False)",
        "api_description": "The Kullback-Leibler divergence loss measure\nKullback-Leibler divergence is a useful distance measure for continuous\ndistributions and is often useful when performing direct regression over\nthe space of (discretely sampled) continuous output distributions.\nAs with NLLLoss, the input given is expected to contain\nlog-probabilities and is not restricted to a 2D Tensor.\nThe targets are interpreted as probabilities by default, but could be considered\nas log-probabilities with log_target set to True.\nThis criterion expects a target Tensor of the same size as the\ninput Tensor.\nThe unreduced (i.e. with reduction set to 'none') loss can be described as:\n\\[l(x,y) = L = \\{ l_1,\\dots,l_N \\}, \\quad\nl_n = y_n \\cdot \\left( \\log y_n - x_n \\right)\\]\nwhere the index \\(N\\) spans all dimensions of input and \\(L\\) has the same\nshape as input. If reduction is not 'none' (default 'mean'), then:\n\\[\\begin{split}\\ell(x, y) = \\begin{cases}\n\\operatorname{mean}(L), & \\text{if reduction} = \\text{`mean';} \\\\\n\\operatorname{sum}(L),  & \\text{if reduction} = \\text{`sum'.}\n\\end{cases}\\end{split}\\]\nIn default reduction mode 'mean', the losses are averaged for each minibatch over observations\nas well as over dimensions. 'batchmean' mode gives the correct KL divergence where losses\nare averaged over batch dimension only. 'mean' mode’s behavior will be changed to the same as\n'batchmean' in the next major release.\nThe documentation is referenced from:",
        "return_value": "",
        "parameters": "\nreduction (string, optional) – Specifies the reduction to apply to the output:\n'none' | 'batchmean' | 'sum' | 'mean'.\n'none': no reduction will be applied.\n'batchmean': the sum of the output will be divided by batchsize.\n'sum': the output will be summed.\n'mean': the output will be divided by the number of elements in the output.\nDefault: 'mean'\nlog_target (bool, optional) – Specifies whether target is passed in the log space.\nDefault: False\n\n\n\n\n",
        "input_shape": "Input: \\((N, *)\\) where \\(*\\) means, any number of additional\ndimensions\nTarget: \\((N, *)\\), same shape as the input\nOutput: scalar by default. If :attr:reduction is 'none', then \\((N, *)\\),\nthe same shape as the input\n\n\n\n",
        "notes": "reduction = 'mean' doesn’t return the true kl divergence value, please use\nreduction = 'batchmean' which aligns with KL math definition.\nIn the next major release, 'mean' will be changed to be the same as 'batchmean'.",
        "code_example": ">>> import oneflow as flow\n>>> import numpy as np\n>>> input = flow.tensor([-0.9021705, 0.08798598, 1.04686249], dtype=flow.float32)\n>>> target = flow.tensor([1.22386942, -0.89729659, 0.01615712], dtype=flow.float32)\n>>> m = flow.nn.KLDivLoss(reduction=\"none\", log_target=False)\n>>> out = m(input, target)\n>>> out\ntensor([ 1.3514,  0.0000, -0.0836], dtype=oneflow.float32)\n>>> m = flow.nn.KLDivLoss(reduction=\"mean\", log_target=False)\n>>> out = m(input, target)\n>>> out\ntensor(0.4226, dtype=oneflow.float32)\n>>> m = flow.nn.KLDivLoss(reduction=\"sum\", log_target=True)\n>>> out = m(input, target)\n>>> out\ntensor(5.7801, dtype=oneflow.float32)\n\n\n\n"
    },
    {
        "api_name": "oneflow.nn.L1Loss",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.L1Loss.html",
        "api_signature": "oneflow.nn.L1Loss(reduction: str = 'mean')",
        "api_description": "This operator computes the L1 Loss between each element in input and target.\nThe equation is:\nif reduction = “none”:\n\\[output = |Target - Input|\\]\nif reduction = “mean”:\n\\[output = \\frac{1}{n}\\sum_{i=1}^n|Target_i - Input_i|\\]\nif reduction = “sum”:\n\\[output = \\sum_{i=1}^n|Target_i - Input_i|\\]",
        "return_value": "The result Tensor.\n\n",
        "parameters": "\ninput (oneflow.Tensor) – the input Tensor.\ntarget (oneflow.Tensor) – The target Tensor.\nreduction (str) – The reduce type, it can be one of “none”, “mean”, “sum”. Defaults to “mean”.\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> import numpy as np\n>>> input = flow.tensor([[1, 1, 1], [2, 2, 2], [7, 7, 7]], dtype = flow.float32)\n>>> target = flow.tensor([[4, 4, 4], [4, 4, 4], [4, 4, 4]], dtype = flow.float32)\n>>> m = flow.nn.L1Loss(reduction=\"none\")\n>>> out = m(input, target)\n>>> out\ntensor([[3., 3., 3.],\n        [2., 2., 2.],\n        [3., 3., 3.]], dtype=oneflow.float32)\n>>> m_mean = flow.nn.L1Loss(reduction=\"mean\")\n>>> out = m_mean(input, target)\n>>> out\ntensor(2.6667, dtype=oneflow.float32)\n>>> m_mean = flow.nn.L1Loss(reduction=\"sum\")\n>>> out = m_mean(input, target)\n>>> out\ntensor(24., dtype=oneflow.float32)\n\n\n\n"
    },
    {
        "api_name": "oneflow.nn.MSELoss",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.MSELoss.html",
        "api_signature": "oneflow.nn.MSELoss(reduction: str = 'mean')",
        "api_description": "Creates a criterion that measures the mean squared error (squared L2 norm) between\neach element in the input \\(x\\) and target \\(y\\).\nThe unreduced (i.e. with reduction set to 'none') loss can be described as:\n\\[\\ell(x, y) = L = \\{l_1,\\dots,l_N\\}^\\top, \\quad\nl_n = \\left( x_n - y_n \\right)^2,\\]\nwhere \\(N\\) is the batch size. If reduction is not 'none'\n(default 'mean'), then:\n\\[\\begin{split}\\ell(x, y) =\n\\begin{cases}\n\\operatorname{mean}(L), &  \\text{if reduction} = \\text{`mean';}\\\\\n\\operatorname{sum}(L),  &  \\text{if reduction} = \\text{`sum'.}\n\\end{cases}\\end{split}\\]\n\\(x\\) and \\(y\\) are tensors of arbitrary shapes with a total\nof \\(n\\) elements each.\nThe mean operation still operates over all the elements, and divides by \\(n\\).\nThe division by \\(n\\) can be avoided if one sets reduction = 'sum'.\nThe documentation is referenced from:",
        "return_value": "",
        "parameters": "reduction (string, optional) – Specifies the reduction to apply to the output:\n'none' | 'mean' | 'sum'. 'none': no reduction will be applied,\n'mean': the sum of the output will be divided by the number of\nelements in the output, 'sum': the output will be summed. Default: 'mean'\n\n\n\n",
        "input_shape": "Input: \\((N, *)\\) where \\(*\\) means, any number of additional\ndimensions\nTarget: \\((N, *)\\), same shape as the input\n\n\n\n",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> import numpy as np\n>>> input = flow.tensor(\n... [[-0.02557137, 0.03101675, 1.37493674],\n... [0.25599439, -1.08372561, -0.21006816]], dtype=flow.float32)\n>>> target = flow.tensor(\n... [[-1.53105064, -0.68137555, 0.5931354],\n... [-0.49158347, 0.93673637, 0.1324141]], dtype=flow.float32)\n>>> m = flow.nn.MSELoss(reduction=\"none\")\n>>> out = m(input, target)\n>>> out\ntensor([[2.2665, 0.5075, 0.6112],\n        [0.5589, 4.0823, 0.1173]], dtype=oneflow.float32)\n>>> m = flow.nn.MSELoss(reduction=\"mean\")\n>>> out = m(input, target)\n>>> out\ntensor(1.3573, dtype=oneflow.float32)\n>>> m = flow.nn.MSELoss(reduction=\"sum\")\n>>> out = m(input, target)\n>>> out\ntensor(8.1436, dtype=oneflow.float32)\n\n\n\n"
    },
    {
        "api_name": "oneflow.nn.MarginRankingLoss",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.MarginRankingLoss.html",
        "api_signature": "oneflow.nn.MarginRankingLoss(margin: float = 0.0, reduction: str = 'mean')",
        "api_description": "Creates a criterion that measures the loss given\ninputs \\(x1\\), \\(x2\\), two 1D mini-batch Tensors,\nand a label 1D mini-batch tensor \\(y\\) (containing 1 or -1).\nIf \\(y = 1\\) then it assumed the first input should be ranked higher\n(have a larger value) than the second input, and vice-versa for \\(y = -1\\).\nThe loss function for each sample in the mini-batch is:\n\\[\\text{loss}(x1, x2, y) = \\max(0, -y * (x1 - x2) + \\text{margin})\\]",
        "return_value": "",
        "parameters": "\nmargin (float, optional) – Has a default value of \\(0\\).\nreduction (string, optional) – Specifies the reduction to apply to the output:\n'none' | 'mean' | 'sum'. 'none': no reduction will be applied,\n'mean': the sum of the output will be divided by the number of\nelements in the output, 'sum': the output will be summed. Default: 'mean'\n\n\n\n\n",
        "input_shape": "x1 : \\((N, D)\\) where N is the batch size and D is the size of a sample.\nx2 : \\((N, D)\\) where N is the batch size and D is the size of a sample.\nTarget: \\((N)\\)\nOutput: scalar. If reduction is 'none', then \\((N)\\).\n\n\n\n",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> import numpy as np\n>>> x1 = flow.tensor(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]), dtype=flow.float32)\n>>> x2 = flow.tensor(np.array([[2, 2, 2], [2, 2, 2], [2, 2, 2]]), dtype=flow.float32)\n>>> target = flow.tensor(np.array([[1, -1, 1],[-1, 1, -1], [1, 1, 1]]), dtype=flow.float32)\n>>> m = flow.nn.MarginRankingLoss(margin =1.0, reduction=\"none\")\n>>> out = m(x1, x2, target)\n>>> out\ntensor([[2., 1., 0.],\n        [3., 0., 5.],\n        [0., 0., 0.]], dtype=oneflow.float32)\n\n>>> m = flow.nn.MarginRankingLoss(margin = 0.3, reduction=\"sum\")\n>>> out = m(x1, x2, target)\n>>> out\ntensor(8.2000, dtype=oneflow.float32)\n\n>>> m = flow.nn.MarginRankingLoss(margin = 10, reduction=\"mean\")\n>>> out = m(x1, x2, target)\n>>> out\ntensor(8.3333, dtype=oneflow.float32)\n\n\n\n"
    },
    {
        "api_name": "oneflow.nn.NLLLoss",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.NLLLoss.html",
        "api_signature": "oneflow.nn.NLLLoss(weight: Optional[oneflow.Tensor] = None, ignore_index: int = - 100, reduction: str = 'mean')",
        "api_description": "The negative log likelihood loss. It is useful to train a classification\nproblem with C classes.\nThe input given through a forward call is expected to contain\nlog-probabilities of each class. input has to be a Tensor of size either\n\\((minibatch, C)\\) or \\((minibatch, C, d_1, d_2, ..., d_K)\\)\nwith \\(K \\geq 1\\) for the K-dimensional case (described later).\nObtaining log-probabilities in a neural network is easily achieved by\nadding a  LogSoftmax  layer in the last layer of your network.\nYou may use CrossEntropyLoss instead, if you prefer not to add an extra\nlayer.\nThe target that this loss expects should be a class index in the range \\([0, C-1]\\)\nwhere C = number of classes;\nThe unreduced (i.e. with reduction set to 'none') loss can be described as:\n\\[\\ell(x, y) = L = \\{l_1,\\dots,l_N\\}^\\top, \\quad\nl_n = - w_{y_n} x_{n,y_n}, \\quad\nw_{c} = \\mathbb{1},\\]\nwhere \\(x\\) is the input, \\(y\\) is the target, \\(w\\) is the weight, and\n\\(N\\) is the batch size. If reduction is not 'none'\n(default 'mean'), then\n\\[\\begin{split}\\ell(x, y) = \\begin{cases}\n\\sum_{n=1}^N \\frac{1}{N} l_n, &\n\\text{if reduction} = \\text{`mean';}\\\\\n\\sum_{n=1}^N l_n,  &\n\\text{if reduction} = \\text{`sum'.}\n\\end{cases}\\end{split}\\]\nCan also be used for higher dimension inputs, such as 2D images, by providing\nan input of size \\((minibatch, C, d_1, d_2, ..., d_K)\\) with \\(K \\geq 1\\),\nwhere \\(K\\) is the number of dimensions, and a target of appropriate shape\n(see below). In the case of images, it computes NLL loss per-pixel.",
        "return_value": "",
        "parameters": "reduction (string, optional) – Specifies the reduction to apply to the output:\n'none' | 'mean' | 'sum'. 'none': no reduction will\nbe applied, 'mean': the weighted mean of the output is taken,\n'sum': the output will be summed. Default: 'mean'\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> import numpy as np\n\n>>> input = flow.tensor(\n... [[-0.1664078, -1.7256707, -0.14690138],\n... [-0.21474946, 0.53737473, 0.99684894],\n... [-1.135804, -0.50371903, 0.7645404]], dtype=flow.float32)\n>>> target = flow.tensor(np.array([0, 1, 2]), dtype=flow.int32)\n>>> m = flow.nn.NLLLoss(reduction=\"none\")\n>>> out = m(input, target)\n>>> out\ntensor([ 0.1664, -0.5374, -0.7645], dtype=oneflow.float32)\n\n>>> m = flow.nn.NLLLoss(reduction=\"sum\")\n>>> out = m(input, target)\n>>> out\ntensor(-1.1355, dtype=oneflow.float32)\n\n>>> m = flow.nn.NLLLoss(reduction=\"mean\")\n>>> out = m(input, target)\n>>> out\ntensor(-0.3785, dtype=oneflow.float32)\n\n\n\n"
    },
    {
        "api_name": "oneflow.nn.SmoothL1Loss",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.SmoothL1Loss.html",
        "api_signature": "oneflow.nn.SmoothL1Loss(reduction: str = 'mean', beta: float = 1.0)",
        "api_description": "Creates a criterion that uses a squared term if the absolute\nelement-wise error falls below beta and an L1 term otherwise.\nThe documentation is referenced from:\nIt is less sensitive to outliers than torch.nn.MSELoss and in some cases\nprevents exploding gradients (e.g. see the paper Fast R-CNN by Ross Girshick)..\nFor a batch of size \\(N\\), the unreduced loss can be described as:\n\\[\\ell(x, y) = L = \\{l_1, ..., l_N\\}^T\\]\nwith\n\\[\\begin{split}l_n = \\begin{cases}\n0.5 (x_n - y_n)^2 / beta, & \\text{if } |x_n - y_n| < beta \\\\\n|x_n - y_n| - 0.5 * beta, & \\text{otherwise }\n\\end{cases}\\end{split}\\]\nIf reduction is not none, then:\n\\[\\begin{split}\\ell(x, y) =\n\\begin{cases}\n\\operatorname{mean}(L), &  \\text{if reduction} = \\text{`mean';}\\\\\n\\operatorname{sum}(L),  &  \\text{if reduction} = \\text{`sum'.}\n\\end{cases}\\end{split}\\]",
        "return_value": "",
        "parameters": "\nsize_average (bool, optional) – Deprecated (see reduction). By default,\nthe losses are averaged over each loss element in the batch. Note that for\nsome losses, there are multiple elements per sample. If the field size_average\nis set to False, the losses are instead summed for each minibatch. Ignored\nwhen reduce is False. Default: True\nreduce (bool, optional) – Deprecated (see reduction). By default, the\nlosses are averaged or summed over observations for each minibatch depending\non size_average. When reduce is False, returns a loss per\nbatch element instead and ignores size_average. Default: True\nreduction (string, optional) – Specifies the reduction to apply to the output:\n'none' | 'mean' | 'sum'. 'none': no reduction will be applied,\n'mean': the sum of the output will be divided by the number of\nelements in the output, 'sum': the output will be summed. Note: size_average\nand reduce are in the process of being deprecated, and in the meantime,\nspecifying either of those two args will override reduction. Default: 'mean'\nbeta (float, optional) – Specifies the threshold at which to change between L1 and L2 loss.\nThe value must be non-negative. Default: 1.0\n\n\n\n\n",
        "input_shape": "Input: \\((N, *)\\) where \\(*\\) means any number of additional dimensions\nTarget: \\((N, *)\\); same shape as the input\nOutput: scalar. If reduction is 'none', then \\((N, *)\\); same shape as the input\n\n\n\n",
        "notes": "Smooth L1 loss can be seen as exactly L1Loss, but with the \\(|x - y| < beta\\)\nportion replaced with a quadratic function such that its slope is 1 at \\(|x - y| = beta\\).\nThe quadratic segment smooths the L1 loss near \\(|x - y| = 0\\).\nSmooth L1 loss is closely related to HuberLoss, being\nequivalent to \\(huber(x, y) / beta\\) (note that Smooth L1’s beta hyper-parameter is\nalso known as delta for Huber). This leads to the following differences:\nAs beta -> 0, Smooth L1 loss converges to L1Loss, while HuberLoss\nconverges to a constant 0 loss.\nAs beta -> \\(+\\infty\\), Smooth L1 loss converges to a constant 0 loss, while\nHuberLoss converges to MSELoss.\nFor Smooth L1 loss, as beta varies, the L1 segment of the loss has a constant slope of 1.\nFor HuberLoss, the slope of the L1 segment is beta.",
        "code_example": ">>> import oneflow as flow\n>>> import numpy as np\n\n>>> x = flow.tensor(np.array([0.1, 0.4, 0.3, 0.5, 0.9]).astype(np.float32), dtype=flow.float32)\n>>> y = flow.tensor(np.array([0.3, 0.9, 2.5, 0.4, 0.3]).astype(np.float32), dtype=flow.float32)\n>>> m = flow.nn.SmoothL1Loss(reduction=\"none\")\n>>> out = m(x, y)\n>>> out\ntensor([0.0200, 0.1250, 1.7000, 0.0050, 0.1800], dtype=oneflow.float32)\n\n>>> m = flow.nn.SmoothL1Loss(reduction=\"mean\")\n>>> out = m(x, y)\n>>> out\ntensor(0.4060, dtype=oneflow.float32)\n\n>>> m = flow.nn.SmoothL1Loss(reduction=\"sum\")\n>>> out = m(x, y)\n>>> out\ntensor(2.0300, dtype=oneflow.float32)\n\n\n\n"
    },
    {
        "api_name": "oneflow.nn.TripletMarginLoss",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.TripletMarginLoss.html",
        "api_signature": "oneflow.nn.TripletMarginLoss(margin: float = 1.0, p: float = 2.0, eps: float = 1e-06, swap: bool = False, size_average=None, reduce=None, reduction: str = 'mean')",
        "api_description": "Creates a criterion that measures the triplet loss given an input\ntensors \\(x1\\), \\(x2\\), \\(x3\\) and a margin with a value greater than \\(0\\).\nThis is used for measuring a relative similarity between samples. A triplet\nis composed by a, p and n (i.e., anchor, positive examples and negative\nexamples respectively). The shapes of all input tensors should be\n\\((N, D)\\).\nThe distance swap is described in detail in the paper Learning shallow\nconvolutional feature descriptors with triplet losses by\nV. Balntas, E. Riba et al.\nThe loss function for each sample in the mini-batch is:\n\\[L(a, p, n) = \\max \\{d(a_i, p_i) - d(a_i, n_i) + {\\rm margin}, 0\\}\\]\nwhere\n\\[d(x_i, y_i) = \\left\\lVert {\\bf x}_i - {\\bf y}_i \\right\\rVert_p\\]",
        "return_value": "",
        "parameters": "\nmargin (float, optional) – Default: \\(1\\).\np (float, optional) – The norm degree for pairwise distance. Default: \\(2.0\\).\nswap (bool, optional) – The distance swap is described in detail in the paper\nLearning shallow convolutional feature descriptors with triplet losses by\nV. Balntas, E. Riba et al. Default: False.\nreduction (string, optional) – Specifies the reduction to apply to the output:\n'none' | 'mean' | 'sum'. 'none': no reduction will be applied,\n'mean': the sum of the output will be divided by the number of\nelements in the output, 'sum': the output will be summed. Note: size_average\nand reduce are in the process of being deprecated, and in the meantime,\nspecifying either of those two args will override reduction. Default: 'mean'\n\n\n\n\n",
        "input_shape": "Input: \\((N, D)\\) where \\(D\\) is the vector dimension.\nOutput: A Tensor of shape \\((N)\\) if reduction is 'none', or a scalar\notherwise.\n\n\n\n",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> import numpy as np\n>>> triplet_loss = flow.nn.TripletMarginLoss(margin=1.0, p=2)\n>>> anchor = np.array([[1, -1, 1],[-1, 1, -1], [1, 1, 1]])\n>>> positive = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n>>> negative = np.array([[2, 2, 2], [2, 2, 2], [2, 2, 2]])\n>>> output = triplet_loss(flow.Tensor(anchor), flow.Tensor(positive), flow.Tensor(negative))\n>>> output\ntensor(6.2971, dtype=oneflow.float32)\n\n\n\n"
    },
    {
        "api_name": "oneflow.nn.PixelShuffle",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.PixelShuffle.html",
        "api_signature": null,
        "api_description": "alias of oneflow.nn.modules.pixelshuffle.PixelShufflev2",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.nn.Upsample",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.Upsample.html",
        "api_signature": "oneflow.nn.Upsample(size: Optional[Union[int, Tuple[int, …]]] = None, scale_factor: Optional[Union[float, Tuple[float, …]]] = None, mode: str = 'nearest', align_corners: Optional[bool] = None)",
        "api_description": "Upsamples a given multi-channel 1D (temporal), 2D (spatial) or 3D (volumetric) data.\nThe input data is assumed to be of the form\nminibatch x channels x [optional depth] x [optional height] x width.\nHence, for spatial inputs, we expect a 4D Tensor and for volumetric inputs, we expect a 5D Tensor.\nThe algorithms available for upsampling are nearest neighbor and linear,\nbilinear, bicubic and trilinear for 3D, 4D and 5D input Tensor,\nrespectively.\nOne can either give a scale_factor or the target output size to\ncalculate the output size. (You cannot give both, as it is ambiguous)",
        "return_value": "",
        "parameters": "\nsize (int or Tuple[int] or Tuple[int, int] or Tuple[int, int, int], optional) – output spatial sizes\nscale_factor (float or Tuple[float] or Tuple[float, float] or Tuple[float, float, float], optional) – multiplier for spatial size. Has to match input size if it is a tuple.\nmode (str, optional) – the upsampling algorithm: one of 'nearest',\n'linear', 'bilinear', 'bicubic' and 'trilinear'.\nDefault: 'nearest'\nalign_corners (bool, optional) – if True, the corner pixels of the input\nand output tensors are aligned, and thus preserving the values at\nthose pixels. This only has effect when mode is\n'linear', 'bilinear', or 'trilinear'. Default: False\n\n\n\n\n",
        "input_shape": "Input: \\((N, C, W_{in})\\), \\((N, C, H_{in}, W_{in})\\) or \\((N, C, D_{in}, H_{in}, W_{in})\\)\nOutput: \\((N, C, W_{out})\\), \\((N, C, H_{out}, W_{out})\\)\nor \\((N, C, D_{out}, H_{out}, W_{out})\\), where\n\n\n\n\n\\[D_{out} = \\left\\lfloor D_{in} \\times \\text{scale_factor} \\right\\rfloor\\]\n\n\\[H_{out} = \\left\\lfloor H_{in} \\times \\text{scale_factor} \\right\\rfloor\\]\n\n\\[W_{out} = \\left\\lfloor W_{in} \\times \\text{scale_factor} \\right\\rfloor\\]\n\nWarning\nWith align_corners = True, the linearly interpolating modes\n(linear, bilinear, bicubic, and trilinear) don’t proportionally\nalign the output and input pixels, and thus the output values can depend\non the input size. This was the default behavior for these modes up to\nversion 0.3.1. Since then, the default behavior is\nalign_corners = False. See below for concrete examples on how this\naffects the outputs.\n\n\n",
        "notes": "If you want downsampling/general resizing, you should use interpolate().",
        "code_example": ">>> import numpy as np\n>>> import oneflow as flow\n\n>>> input = flow.tensor(np.arange(1, 5).reshape((1, 1, 2, 2)), dtype=flow.float32)\n>>> input = input.to(\"cuda\")\n>>> m = flow.nn.Upsample(scale_factor=2.0, mode=\"nearest\")\n>>> output = m(input)\n>>> output \ntensor([[[[1., 1., 2., 2.],\n          ...\n          [3., 3., 4., 4.]]]], device='cuda:0', dtype=oneflow.float32)\n\n\n\n"
    },
    {
        "api_name": "oneflow.nn.UpsamplingBilinear2d",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.UpsamplingBilinear2d.html",
        "api_signature": "oneflow.nn.UpsamplingBilinear2d(size: Optional[Tuple[int, int]] = None, scale_factor: Optional[Tuple[float, float]] = None)",
        "api_description": "Applies a 2D bilinear upsampling to an input signal composed of several input\nchannels.\nTo specify the scale, it takes either the size or the scale_factor\nas it’s constructor argument.\nWhen size is given, it is the output size of the image (h, w).",
        "return_value": "",
        "parameters": "\nsize (int or Tuple[int, int], optional) – output spatial sizes\nscale_factor (float or Tuple[float, float], optional) – multiplier for\nspatial size.\n\n\n\n\nWarning\nThis class is deprecated in favor of interpolate(). It is\nequivalent to nn.functional.interpolate(..., mode='bilinear', align_corners=True).\n\n\n",
        "input_shape": "Input: \\((N, C, H_{in}, W_{in})\\)\nOutput: \\((N, C, H_{out}, W_{out})\\) where\n\n\n\n\n\\[H_{out} = \\left\\lfloor H_{in} \\times \\text{scale_factor} \\right\\rfloor\\]\n\n\\[W_{out} = \\left\\lfloor W_{in} \\times \\text{scale_factor} \\right\\rfloor\\]\n",
        "notes": "",
        "code_example": ">>> import numpy as np\n>>> import oneflow as flow\n\n>>> input = flow.tensor(np.arange(1, 5).reshape((1, 1, 2, 2)), dtype=flow.float32)\n>>> input = input.to(\"cuda\")\n>>> m = flow.nn.UpsamplingBilinear2d(scale_factor=2.0)\n>>> output = m(input)\n>>> output \ntensor([[[[1.0000, 1.3333, 1.6667, 2.0000],\n          ...\n          [3.0000, 3.3333, 3.6667, 4.0000]]]], device='cuda:0',\n       dtype=oneflow.float32)\n\n\n\n"
    },
    {
        "api_name": "oneflow.nn.UpsamplingNearest2d",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.UpsamplingNearest2d.html",
        "api_signature": "oneflow.nn.UpsamplingNearest2d(size: Optional[Tuple[int, int]] = None, scale_factor: Optional[Tuple[float, float]] = None)",
        "api_description": "Applies a 2D nearest neighbor upsampling to an input signal composed of several input\nchannels.\nTo specify the scale, it takes either the size or the scale_factor\nas it’s constructor argument.\nWhen size is given, it is the output size of the image (h, w).",
        "return_value": "",
        "parameters": "\nsize (int or Tuple[int, int], optional) – output spatial sizes\nscale_factor (float or Tuple[float, float], optional) – multiplier for\nspatial size.\n\n\n\n\nWarning\nThis class is deprecated in favor of interpolate().\n\n\n",
        "input_shape": "Input: \\((N, C, H_{in}, W_{in})\\)\nOutput: \\((N, C, H_{out}, W_{out})\\) where\n\n\n\n\n\\[H_{out} = \\left\\lfloor H_{in} \\times \\text{scale_factor} \\right\\rfloor\\]\n\n\\[W_{out} = \\left\\lfloor W_{in} \\times \\text{scale_factor} \\right\\rfloor\\]\n",
        "notes": "",
        "code_example": ">>> import numpy as np\n>>> import oneflow as flow\n\n>>> input = flow.tensor(np.arange(1, 5).reshape((1, 1, 2, 2)), dtype=flow.float32)\n>>> input = input.to(\"cuda\")\n>>> m = flow.nn.UpsamplingNearest2d(scale_factor=2.0)\n>>> output = m(input)\n>>> output \ntensor([[[[1., 1., 2., 2.],\n          ...\n          [3., 3., 4., 4.]]]], device='cuda:0', dtype=oneflow.float32)\n\n\n\n"
    },
    {
        "api_name": "oneflow.nn.parallel.DistributedDataParallel",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.parallel.DistributedDataParallel.html",
        "api_signature": "oneflow.nn.parallel.DistributedDataParallel(module: oneflow.nn.modules.module.Module, *, broadcast_buffers: bool = True, broadcast_parameters: bool = True, bucket_size: int = 10, use_bucket: bool = True)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.nn.COCOReader",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.COCOReader.html",
        "api_signature": "oneflow.nn.COCOReader(annotation_file: str, image_dir: str, batch_size: int, shuffle: bool = True, random_seed: Optional[int] = None, group_by_aspect_ratio: bool = True, remove_images_without_annotations: bool = True, stride_partition: bool = True, device: Optional[Union[str, oneflow._oneflow_internal.device]] = None, placement: Optional[oneflow._oneflow_internal.placement] = None, sbp: Optional[Union[oneflow._oneflow_internal.sbp.sbp, List[oneflow._oneflow_internal.sbp.sbp]]] = None)",
        "api_description": "__init__(annotation_file: str, image_dir: str, batch_size: int, shuffle: bool = True, random_seed: Optional[int] = None, group_by_aspect_ratio: bool = True, remove_images_without_annotations: bool = True, stride_partition: bool = True, device: Optional[Union[str, oneflow._oneflow_internal.device]] = None, placement: Optional[oneflow._oneflow_internal.placement] = None, sbp: Optional[Union[oneflow._oneflow_internal.sbp.sbp, List[oneflow._oneflow_internal.sbp.sbp]]] = None)¶\nCalls super().__setattr__(‘a’, a) instead of the typical self.a = a\nto avoid Module.__setattr__ overhead. Module’s __setattr__ has special\nhandling for parameters, submodules, and buffers but simply calls into\nsuper().__setattr__ for all other attributes.\nMethods\n__call__(*args, **kwargs)\nCall self as a function.\n__delattr__(name)\nImplement delattr(self, name).\n__dir__()\nDefault dir() implementation.\n__eq__(value, /)\nReturn self==value.\n__format__(format_spec, /)\nDefault object formatter.\n__ge__(value, /)\nReturn self>=value.\n__getattr__(name)\n__getattribute__(name, /)\nReturn getattr(self, name).\n__getstate__()\n__gt__(value, /)\nReturn self>value.\n__hash__()\nReturn hash(self).\n__init__(annotation_file, image_dir, batch_size)\nCalls super().__setattr__(‘a’, a) instead of the typical self.a = a to avoid Module.__setattr__ overhead.\n__init_subclass__\nThis method is called when a class is subclassed.\n__le__(value, /)\nReturn self<=value.\n__lt__(value, /)\nReturn self<value.\n__ne__(value, /)\nReturn self!=value.\n__new__(**kwargs)\nCreate and return a new object.\n__reduce__()\nHelper for pickle.\n__reduce_ex__(protocol, /)\nHelper for pickle.\n__repr__()\nReturn repr(self).\n__setattr__(name, value)\nImplement setattr(self, name, value).\n__setstate__(state)\n__sizeof__()\nSize of object in memory, in bytes.\n__str__()\nReturn str(self).\n__subclasshook__\nAbstract classes can override this to customize issubclass().\n_apply(fn)\n_get_backward_hooks()",
        "return_value": "\n_get_name()\n\n\n_load_from_state_dict(state_dict, prefix, …)\n\n\n_maybe_warn_non_full_backward_hook(args, …)\n\n\n_named_members(get_members_fn[, prefix, recurse])\n\n\n_register_load_state_dict_pre_hook(hook[, …])\nThese hooks will be called with arguments: state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs, before loading state_dict into self.\n\n_register_state_dict_hook(hook)\nThese hooks will be called with arguments: self, state_dict, prefix, local_metadata, after the state_dict of self is set.\n\n_save_to_state_dict(destination, prefix, …)\n\n\n_shallow_repr()\n\n\n_to_memory_format(memory_format)\nCasts the parameters and buffers in this module to another memory format.\n\nadd_module(name, module)\nAdds a child module to the current module.\n\napply(fn)\nApplies fn recursively to every submodule (as returned by .children()) as well as self.\n\nbuffers([recurse])\n\nchildren()\n\ncpu()\nMoves all model parameters and buffers to the CPU.\n\ncuda([device])\nMoves all model parameters and buffers to the GPU.\n\ndouble()\nCasts all floating point parameters and buffers to double datatype.\n\neval()\nSets the module in evaluation mode.\n\nextra_repr()\nSet the extra representation of the module\n\nfloat()\nCasts all floating point parameters and buffers to float datatype.\n\nforward()\n\n\nget_parameter(target)\nReturn the parameter refenreced by target.\n\nget_submodule(target)\nGet submodule accroding to the name of submodule.\n\nhalf()\nCasts all floating point parameters and buffers to half datatype.\n\nload_state_dict(state_dict[, strict])\nCopies parameters and buffers from state_dict into this module and its descendants.\n\nmake_contiguous_params_group()\nGet contiguous parameters group after creating the whole module.\n\nmodules()\n\nnamed_buffers([prefix, recurse])\n\nnamed_children()\n\nnamed_modules([memo, prefix])\n\nnamed_parameters([prefix, recurse])\n\nparameters([recurse])\n\nregister_backward_hook(hook)\nRegisters a backward hook on the module.\n\nregister_buffer(name, tensor[, persistent])\nAdds a buffer to the module.\n\nregister_forward_hook(hook)\nRegisters a forward hook on the module.\n\nregister_forward_pre_hook(hook)\nRegisters a forward pre-hook on the module.\n\nregister_full_backward_hook(hook)\nRegisters a backward hook on the module.\n\nregister_parameter(name, param)\nAdds a parameter to the module.\n\nregister_state_dict_pre_hook(hook)\nThese hooks will be called with arguments: self, prefix, and keep_vars before calling state_dict on self.\n\nrequires_grad_([requires_grad])\nChange if autograd should record operations on parameters in this module.\n\nstate_dict([destination, prefix, keep_vars])\n\nto(*args, **kwargs)\nMoves and/or casts the parameters and buffers.\n\nto_consistent(*args, **kwargs)\nThis interface is no longer available, please use oneflow.nn.Module.to_global() instead.\n\nto_empty(*, device)\nMoves the parameters and buffers to the specified device without copying storage.\n\nto_global([placement, sbp])\nConvert the parameters and buffers to global.\n\nto_local()\n\n\nto_memory_format(memory_format)\n\n\ntrain([mode])\nSets the module in training mode.\n\nzero_grad([set_to_none])\nSets gradients of all model parameters to zero.\n\n\n\nAttributes\n\n\n\n\n\n\n_grad_t\nalias of Union[Tuple[oneflow.Tensor, …], oneflow.Tensor]\n\n\n\n\n",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.nn.CoinFlip",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.CoinFlip.html",
        "api_signature": "oneflow.nn.CoinFlip(batch_size=1, random_seed=None, probability=0.5, device=None, placement=None, sbp=None)",
        "api_description": "Generates random boolean values following a bernoulli distribution.\nThe probability of generating a value 1 (true) is determined by the probability argument.\nThe shape of the generated data can be either specified explicitly with a shape argument,\nor chosen to match the shape of the input, if provided. If none are present, a single value per\nsample is generated.\nThe documentation is referenced from:\nhttps://docs.nvidia.com/deeplearning/dali/user-guide/docs/supported_ops_legacy.html#nvidia.dali.ops.CoinFlip.",
        "return_value": "\n_get_name()\n\n\n_load_from_state_dict(state_dict, prefix, …)\n\n\n_maybe_warn_non_full_backward_hook(args, …)\n\n\n_named_members(get_members_fn[, prefix, recurse])\n\n\n_register_load_state_dict_pre_hook(hook[, …])\nThese hooks will be called with arguments: state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs, before loading state_dict into self.\n\n_register_state_dict_hook(hook)\nThese hooks will be called with arguments: self, state_dict, prefix, local_metadata, after the state_dict of self is set.\n\n_save_to_state_dict(destination, prefix, …)\n\n\n_shallow_repr()\n\n\n_to_memory_format(memory_format)\nCasts the parameters and buffers in this module to another memory format.\n\nadd_module(name, module)\nAdds a child module to the current module.\n\napply(fn)\nApplies fn recursively to every submodule (as returned by .children()) as well as self.\n\nbuffers([recurse])\n\nchildren()\n\ncpu()\nMoves all model parameters and buffers to the CPU.\n\ncuda([device])\nMoves all model parameters and buffers to the GPU.\n\ndouble()\nCasts all floating point parameters and buffers to double datatype.\n\neval()\nSets the module in evaluation mode.\n\nextra_repr()\nSet the extra representation of the module\n\nfloat()\nCasts all floating point parameters and buffers to float datatype.\n\nforward()\n\n\nget_parameter(target)\nReturn the parameter refenreced by target.\n\nget_submodule(target)\nGet submodule accroding to the name of submodule.\n\nhalf()\nCasts all floating point parameters and buffers to half datatype.\n\nload_state_dict(state_dict[, strict])\nCopies parameters and buffers from state_dict into this module and its descendants.\n\nmake_contiguous_params_group()\nGet contiguous parameters group after creating the whole module.\n\nmodules()\n\nnamed_buffers([prefix, recurse])\n\nnamed_children()\n\nnamed_modules([memo, prefix])\n\nnamed_parameters([prefix, recurse])\n\nparameters([recurse])\n\nregister_backward_hook(hook)\nRegisters a backward hook on the module.\n\nregister_buffer(name, tensor[, persistent])\nAdds a buffer to the module.\n\nregister_forward_hook(hook)\nRegisters a forward hook on the module.\n\nregister_forward_pre_hook(hook)\nRegisters a forward pre-hook on the module.\n\nregister_full_backward_hook(hook)\nRegisters a backward hook on the module.\n\nregister_parameter(name, param)\nAdds a parameter to the module.\n\nregister_state_dict_pre_hook(hook)\nThese hooks will be called with arguments: self, prefix, and keep_vars before calling state_dict on self.\n\nrequires_grad_([requires_grad])\nChange if autograd should record operations on parameters in this module.\n\nstate_dict([destination, prefix, keep_vars])\n\nto(*args, **kwargs)\nMoves and/or casts the parameters and buffers.\n\nto_consistent(*args, **kwargs)\nThis interface is no longer available, please use oneflow.nn.Module.to_global() instead.\n\nto_empty(*, device)\nMoves the parameters and buffers to the specified device without copying storage.\n\nto_global([placement, sbp])\nConvert the parameters and buffers to global.\n\nto_local()\n\n\nto_memory_format(memory_format)\n\n\ntrain([mode])\nSets the module in training mode.\n\nzero_grad([set_to_none])\nSets gradients of all model parameters to zero.\n\n\n\nAttributes\n\n\n\n\n\n\n_grad_t\nalias of Union[Tuple[oneflow.Tensor, …], oneflow.Tensor]\n\n\n\n\n",
        "parameters": "\nbatch_size (int, optional) – Maximum batch size of the pipeline. Negative values for this parameter\nare invalid - the default value may only be used with serialized pipeline (the value stored in\nserialized pipeline is used instead). In most cases, the actual batch size of the pipeline will be\nequal to the maximum one. Default: 1\nrandom_seed (int, optional) – Random seed. Default: None\nprobability (float, optional) – Probability of value 1. Default: 0.5\ndevice (oneflow.device, optional) – Desired device of returned tensor. Default: if None, uses the\ncurrent device for the default tensor type.\nplacement (oneflow.placement, optional) – Desired placement of returned global tensor.\nDefault: if None, the returned tensor is local one using the argument device.\nsbp (oneflow.sbp.sbp or tuple of oneflow.sbp.sbp, optional) – Desired sbp descriptor of returned\nglobal tensor. Default: if None, the returned tensor is local one using the argument device.\n\n\n\n\n\n__init__(batch_size: int = 1, random_seed: Optional[int] = None, probability: float = 0.5, device: Optional[Union[str, oneflow._oneflow_internal.device]] = None, placement: Optional[oneflow._oneflow_internal.placement] = None, sbp: Optional[Union[oneflow._oneflow_internal.sbp.sbp, List[oneflow._oneflow_internal.sbp.sbp]]] = None)¶\nCalls super().__setattr__(‘a’, a) instead of the typical self.a = a\nto avoid Module.__setattr__ overhead. Module’s __setattr__ has special\nhandling for parameters, submodules, and buffers but simply calls into\nsuper().__setattr__ for all other attributes.\n\nMethods\n\n\n\n\n\n\n__call__(*args, **kwargs)\nCall self as a function.\n\n__delattr__(name)\nImplement delattr(self, name).\n\n__dir__()\nDefault dir() implementation.\n\n__eq__(value, /)\nReturn self==value.\n\n__format__(format_spec, /)\nDefault object formatter.\n\n__ge__(value, /)\nReturn self>=value.\n\n__getattr__(name)\n\n\n__getattribute__(name, /)\nReturn getattr(self, name).\n\n__getstate__()\n\n\n__gt__(value, /)\nReturn self>value.\n\n__hash__()\nReturn hash(self).\n\n__init__([batch_size, random_seed, …])\nCalls super().__setattr__(‘a’, a) instead of the typical self.a = a to avoid Module.__setattr__ overhead.\n\n__init_subclass__\nThis method is called when a class is subclassed.\n\n__le__(value, /)\nReturn self<=value.\n\n__lt__(value, /)\nReturn self<value.\n\n__ne__(value, /)\nReturn self!=value.\n\n__new__(**kwargs)\nCreate and return a new object.\n\n__reduce__()\nHelper for pickle.\n\n__reduce_ex__(protocol, /)\nHelper for pickle.\n\n__repr__()\nReturn repr(self).\n\n__setattr__(name, value)\nImplement setattr(self, name, value).\n\n__setstate__(state)\n\n\n__sizeof__()\nSize of object in memory, in bytes.\n\n__str__()\nReturn str(self).\n\n__subclasshook__\nAbstract classes can override this to customize issubclass().\n\n_apply(fn)\n\n\n_get_backward_hooks()\n",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.nn.CropMirrorNormalize",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.CropMirrorNormalize.html",
        "api_signature": "oneflow.nn.CropMirrorNormalize(color_space='BGR', output_layout='NCHW', crop_h=0, crop_w=0, crop_pos_y=0.5, crop_pos_x=0.5, mean=[0.0], std=[1.0], output_dtype=oneflow.float)",
        "api_description": "Performs fused cropping, normalization, format conversion\n(NHWC to NCHW) if desired, and type casting.\nNormalization takes the input images and produces the output by using the following formula:\n\\[output = (input - mean) / std\\]",
        "return_value": "\n_get_name()\n\n\n_load_from_state_dict(state_dict, prefix, …)\n\n\n_maybe_warn_non_full_backward_hook(args, …)\n\n\n_named_members(get_members_fn[, prefix, recurse])\n\n\n_register_load_state_dict_pre_hook(hook[, …])\nThese hooks will be called with arguments: state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs, before loading state_dict into self.\n\n_register_state_dict_hook(hook)\nThese hooks will be called with arguments: self, state_dict, prefix, local_metadata, after the state_dict of self is set.\n\n_save_to_state_dict(destination, prefix, …)\n\n\n_shallow_repr()\n\n\n_to_memory_format(memory_format)\nCasts the parameters and buffers in this module to another memory format.\n\nadd_module(name, module)\nAdds a child module to the current module.\n\napply(fn)\nApplies fn recursively to every submodule (as returned by .children()) as well as self.\n\nbuffers([recurse])\n\nchildren()\n\ncpu()\nMoves all model parameters and buffers to the CPU.\n\ncuda([device])\nMoves all model parameters and buffers to the GPU.\n\ndouble()\nCasts all floating point parameters and buffers to double datatype.\n\neval()\nSets the module in evaluation mode.\n\nextra_repr()\nSet the extra representation of the module\n\nfloat()\nCasts all floating point parameters and buffers to float datatype.\n\nforward(input[, mirror])\n\n\nget_parameter(target)\nReturn the parameter refenreced by target.\n\nget_submodule(target)\nGet submodule accroding to the name of submodule.\n\nhalf()\nCasts all floating point parameters and buffers to half datatype.\n\nload_state_dict(state_dict[, strict])\nCopies parameters and buffers from state_dict into this module and its descendants.\n\nmake_contiguous_params_group()\nGet contiguous parameters group after creating the whole module.\n\nmodules()\n\nnamed_buffers([prefix, recurse])\n\nnamed_children()\n\nnamed_modules([memo, prefix])\n\nnamed_parameters([prefix, recurse])\n\nparameters([recurse])\n\nregister_backward_hook(hook)\nRegisters a backward hook on the module.\n\nregister_buffer(name, tensor[, persistent])\nAdds a buffer to the module.\n\nregister_forward_hook(hook)\nRegisters a forward hook on the module.\n\nregister_forward_pre_hook(hook)\nRegisters a forward pre-hook on the module.\n\nregister_full_backward_hook(hook)\nRegisters a backward hook on the module.\n\nregister_parameter(name, param)\nAdds a parameter to the module.\n\nregister_state_dict_pre_hook(hook)\nThese hooks will be called with arguments: self, prefix, and keep_vars before calling state_dict on self.\n\nrequires_grad_([requires_grad])\nChange if autograd should record operations on parameters in this module.\n\nstate_dict([destination, prefix, keep_vars])\n\nto(*args, **kwargs)\nMoves and/or casts the parameters and buffers.\n\nto_consistent(*args, **kwargs)\nThis interface is no longer available, please use oneflow.nn.Module.to_global() instead.\n\nto_empty(*, device)\nMoves the parameters and buffers to the specified device without copying storage.\n\nto_global([placement, sbp])\nConvert the parameters and buffers to global.\n\nto_local()\n\n\nto_memory_format(memory_format)\n\n\ntrain([mode])\nSets the module in training mode.\n\nzero_grad([set_to_none])\nSets gradients of all model parameters to zero.\n\n\n\nAttributes\n\n\n\n\n\n\n_grad_t\nalias of Union[Tuple[oneflow.Tensor, …], oneflow.Tensor]\n\n\n\n\n",
        "parameters": "\ncolor_space (str, optional) – The color space of the input image. Default: “BGR”\noutput_layout (str, optional) – Tensor data layout for the output. Default: “NCHW”\ncrop_h (int, optional) – Cropping the window height (in pixels). Default: 0\ncrop_w (int, optional) – Cropping window width (in pixels). Default: 0\ncrop_pos_y (float, optional) – Normalized (0.0 - 1.0) vertical position of the start of the cropping\nwindow (typically, the upper left corner). Default: 0.5\ncrop_pos_x (float, optional) – Normalized (0.0 - 1.0) horizontal position of the cropping window\n(upper left corner). Default: 0.5\nmean (float or list of float, optional) – Mean pixel values for image normalization. Default: [0.0],\nstd (float or list of float, optional) – Standard deviation values for image normalization.\nDefault: [1.0]\noutput_dtype (oneflow.dtype, optional) – Output data type. Default: oneflow.float\n\n\n\n\n\n__init__(color_space: str = 'BGR', output_layout: str = 'NCHW', crop_h: int = 0, crop_w: int = 0, crop_pos_y: float = 0.5, crop_pos_x: float = 0.5, mean: Sequence[float] = [0.0], std: Sequence[float] = [1.0], output_dtype: oneflow._oneflow_internal.dtype = oneflow.float32)¶\nCalls super().__setattr__(‘a’, a) instead of the typical self.a = a\nto avoid Module.__setattr__ overhead. Module’s __setattr__ has special\nhandling for parameters, submodules, and buffers but simply calls into\nsuper().__setattr__ for all other attributes.\n\nMethods\n\n\n\n\n\n\n__call__(*args, **kwargs)\nCall self as a function.\n\n__delattr__(name)\nImplement delattr(self, name).\n\n__dir__()\nDefault dir() implementation.\n\n__eq__(value, /)\nReturn self==value.\n\n__format__(format_spec, /)\nDefault object formatter.\n\n__ge__(value, /)\nReturn self>=value.\n\n__getattr__(name)\n\n\n__getattribute__(name, /)\nReturn getattr(self, name).\n\n__getstate__()\n\n\n__gt__(value, /)\nReturn self>value.\n\n__hash__()\nReturn hash(self).\n\n__init__([color_space, output_layout, …])\nCalls super().__setattr__(‘a’, a) instead of the typical self.a = a to avoid Module.__setattr__ overhead.\n\n__init_subclass__\nThis method is called when a class is subclassed.\n\n__le__(value, /)\nReturn self<=value.\n\n__lt__(value, /)\nReturn self<value.\n\n__ne__(value, /)\nReturn self!=value.\n\n__new__(**kwargs)\nCreate and return a new object.\n\n__reduce__()\nHelper for pickle.\n\n__reduce_ex__(protocol, /)\nHelper for pickle.\n\n__repr__()\nReturn repr(self).\n\n__setattr__(name, value)\nImplement setattr(self, name, value).\n\n__setstate__(state)\n\n\n__sizeof__()\nSize of object in memory, in bytes.\n\n__str__()\nReturn str(self).\n\n__subclasshook__\nAbstract classes can override this to customize issubclass().\n\n_apply(fn)\n\n\n_get_backward_hooks()\n",
        "input_shape": "",
        "notes": "If no cropping arguments are specified, only mirroring and normalization will occur.\nThis operator allows sequence inputs and supports volumetric data.\nThe documentation is referenced from:\nhttps://docs.nvidia.com/deeplearning/dali/user-guide/docs/supported_ops_legacy.html#nvidia.dali.ops.CropMirrorNormalize.",
        "code_example": ""
    },
    {
        "api_name": "oneflow.nn.OFRecordBytesDecoder",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.OFRecordBytesDecoder.html",
        "api_signature": "oneflow.nn.OFRecordBytesDecoder(blob_name: str, name: Optional[str] = None)",
        "api_description": "This operator reads an tensor as bytes. The output might need\nfurther decoding process like cv2.imdecode() for images and decode(“utf-8”)\nfor characters,depending on the downstream task.",
        "return_value": "The result Tensor encoded with bytes.\n\n\n\n_get_name()\n\n\n_load_from_state_dict(state_dict, prefix, …)\n\n\n_maybe_warn_non_full_backward_hook(args, …)\n\n\n_named_members(get_members_fn[, prefix, recurse])\n\n\n_register_load_state_dict_pre_hook(hook[, …])\nThese hooks will be called with arguments: state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs, before loading state_dict into self.\n\n_register_state_dict_hook(hook)\nThese hooks will be called with arguments: self, state_dict, prefix, local_metadata, after the state_dict of self is set.\n\n_save_to_state_dict(destination, prefix, …)\n\n\n_shallow_repr()\n\n\n_to_memory_format(memory_format)\nCasts the parameters and buffers in this module to another memory format.\n\nadd_module(name, module)\nAdds a child module to the current module.\n\napply(fn)\nApplies fn recursively to every submodule (as returned by .children()) as well as self.\n\nbuffers([recurse])\n\nchildren()\n\ncpu()\nMoves all model parameters and buffers to the CPU.\n\ncuda([device])\nMoves all model parameters and buffers to the GPU.\n\ndouble()\nCasts all floating point parameters and buffers to double datatype.\n\neval()\nSets the module in evaluation mode.\n\nextra_repr()\nSet the extra representation of the module\n\nfloat()\nCasts all floating point parameters and buffers to float datatype.\n\nforward(input)\n\n\nget_parameter(target)\nReturn the parameter refenreced by target.\n\nget_submodule(target)\nGet submodule accroding to the name of submodule.\n\nhalf()\nCasts all floating point parameters and buffers to half datatype.\n\nload_state_dict(state_dict[, strict])\nCopies parameters and buffers from state_dict into this module and its descendants.\n\nmake_contiguous_params_group()\nGet contiguous parameters group after creating the whole module.\n\nmodules()\n\nnamed_buffers([prefix, recurse])\n\nnamed_children()\n\nnamed_modules([memo, prefix])\n\nnamed_parameters([prefix, recurse])\n\nparameters([recurse])\n\nregister_backward_hook(hook)\nRegisters a backward hook on the module.\n\nregister_buffer(name, tensor[, persistent])\nAdds a buffer to the module.\n\nregister_forward_hook(hook)\nRegisters a forward hook on the module.\n\nregister_forward_pre_hook(hook)\nRegisters a forward pre-hook on the module.\n\nregister_full_backward_hook(hook)\nRegisters a backward hook on the module.\n\nregister_parameter(name, param)\nAdds a parameter to the module.\n\nregister_state_dict_pre_hook(hook)\nThese hooks will be called with arguments: self, prefix, and keep_vars before calling state_dict on self.\n\nrequires_grad_([requires_grad])\nChange if autograd should record operations on parameters in this module.\n\nstate_dict([destination, prefix, keep_vars])\n\nto(*args, **kwargs)\nMoves and/or casts the parameters and buffers.\n\nto_consistent(*args, **kwargs)\nThis interface is no longer available, please use oneflow.nn.Module.to_global() instead.\n\nto_empty(*, device)\nMoves the parameters and buffers to the specified device without copying storage.\n\nto_global([placement, sbp])\nConvert the parameters and buffers to global.\n\nto_local()\n\n\nto_memory_format(memory_format)\n\n\ntrain([mode])\nSets the module in training mode.\n\nzero_grad([set_to_none])\nSets gradients of all model parameters to zero.\n\n\n\nAttributes\n\n\n\n\n\n\n_grad_t\nalias of Union[Tuple[oneflow.Tensor, …], oneflow.Tensor]\n\n\n\n\n",
        "parameters": "\nblob_name – The name of the target feature in OFRecord.\nname – The name for this component in the graph.\ninput – the Tensor which might be provided by an OFRecordReader.\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import numpy as np\n>>> import oneflow as flow\n\n>>> def example():\n...      batch_size = 16\n...      record_reader = flow.nn.OFRecordReader(\n...         \"dataset/\",\n...         batch_size=batch_size,\n...         part_name_suffix_length=5,\n...      )\n...      val_record = record_reader()\n\n...      bytesdecoder_img = flow.nn.OFRecordBytesDecoder(\"encoded\")\n\n...      image_bytes_batch = bytesdecoder_img(val_record)\n\n...      image_bytes = image_bytes_batch.numpy()[0]\n...      return image_bytes\n... example()  \narray([255 216 255 ...  79 255 217], dtype=uint8)\n\n\n\n\n__init__(blob_name: str, name: Optional[str] = None)¶\nCalls super().__setattr__(‘a’, a) instead of the typical self.a = a\nto avoid Module.__setattr__ overhead. Module’s __setattr__ has special\nhandling for parameters, submodules, and buffers but simply calls into\nsuper().__setattr__ for all other attributes.\n\nMethods\n\n\n\n\n\n\n__call__(*args, **kwargs)\nCall self as a function.\n\n__delattr__(name)\nImplement delattr(self, name).\n\n__dir__()\nDefault dir() implementation.\n\n__eq__(value, /)\nReturn self==value.\n\n__format__(format_spec, /)\nDefault object formatter.\n\n__ge__(value, /)\nReturn self>=value.\n\n__getattr__(name)\n\n\n__getattribute__(name, /)\nReturn getattr(self, name).\n\n__getstate__()\n\n\n__gt__(value, /)\nReturn self>value.\n\n__hash__()\nReturn hash(self).\n\n__init__(blob_name[, name])\nCalls super().__setattr__(‘a’, a) instead of the typical self.a = a to avoid Module.__setattr__ overhead.\n\n__init_subclass__\nThis method is called when a class is subclassed.\n\n__le__(value, /)\nReturn self<=value.\n\n__lt__(value, /)\nReturn self<value.\n\n__ne__(value, /)\nReturn self!=value.\n\n__new__(**kwargs)\nCreate and return a new object.\n\n__reduce__()\nHelper for pickle.\n\n__reduce_ex__(protocol, /)\nHelper for pickle.\n\n__repr__()\nReturn repr(self).\n\n__setattr__(name, value)\nImplement setattr(self, name, value).\n\n__setstate__(state)\n\n\n__sizeof__()\nSize of object in memory, in bytes.\n\n__str__()\nReturn str(self).\n\n__subclasshook__\nAbstract classes can override this to customize issubclass().\n\n_apply(fn)\n\n\n_get_backward_hooks()\n"
    },
    {
        "api_name": "oneflow.nn.OFRecordImageDecoder",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.OFRecordImageDecoder.html",
        "api_signature": "oneflow.nn.OFRecordImageDecoder(blob_name: str, color_space: str = 'BGR')",
        "api_description": "__init__(blob_name: str, color_space: str = 'BGR')¶\nCalls super().__setattr__(‘a’, a) instead of the typical self.a = a\nto avoid Module.__setattr__ overhead. Module’s __setattr__ has special\nhandling for parameters, submodules, and buffers but simply calls into\nsuper().__setattr__ for all other attributes.\nMethods\n__call__(*args, **kwargs)\nCall self as a function.\n__delattr__(name)\nImplement delattr(self, name).\n__dir__()\nDefault dir() implementation.\n__eq__(value, /)\nReturn self==value.\n__format__(format_spec, /)\nDefault object formatter.\n__ge__(value, /)\nReturn self>=value.\n__getattr__(name)\n__getattribute__(name, /)\nReturn getattr(self, name).\n__getstate__()\n__gt__(value, /)\nReturn self>value.\n__hash__()\nReturn hash(self).\n__init__(blob_name[, color_space])\nCalls super().__setattr__(‘a’, a) instead of the typical self.a = a to avoid Module.__setattr__ overhead.\n__init_subclass__\nThis method is called when a class is subclassed.\n__le__(value, /)\nReturn self<=value.\n__lt__(value, /)\nReturn self<value.\n__ne__(value, /)\nReturn self!=value.\n__new__(**kwargs)\nCreate and return a new object.\n__reduce__()\nHelper for pickle.\n__reduce_ex__(protocol, /)\nHelper for pickle.\n__repr__()\nReturn repr(self).\n__setattr__(name, value)\nImplement setattr(self, name, value).\n__setstate__(state)\n__sizeof__()\nSize of object in memory, in bytes.\n__str__()\nReturn str(self).\n__subclasshook__\nAbstract classes can override this to customize issubclass().\n_apply(fn)\n_get_backward_hooks()",
        "return_value": "\n_get_name()\n\n\n_load_from_state_dict(state_dict, prefix, …)\n\n\n_maybe_warn_non_full_backward_hook(args, …)\n\n\n_named_members(get_members_fn[, prefix, recurse])\n\n\n_register_load_state_dict_pre_hook(hook[, …])\nThese hooks will be called with arguments: state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs, before loading state_dict into self.\n\n_register_state_dict_hook(hook)\nThese hooks will be called with arguments: self, state_dict, prefix, local_metadata, after the state_dict of self is set.\n\n_save_to_state_dict(destination, prefix, …)\n\n\n_shallow_repr()\n\n\n_to_memory_format(memory_format)\nCasts the parameters and buffers in this module to another memory format.\n\nadd_module(name, module)\nAdds a child module to the current module.\n\napply(fn)\nApplies fn recursively to every submodule (as returned by .children()) as well as self.\n\nbuffers([recurse])\n\nchildren()\n\ncpu()\nMoves all model parameters and buffers to the CPU.\n\ncuda([device])\nMoves all model parameters and buffers to the GPU.\n\ndouble()\nCasts all floating point parameters and buffers to double datatype.\n\neval()\nSets the module in evaluation mode.\n\nextra_repr()\nSet the extra representation of the module\n\nfloat()\nCasts all floating point parameters and buffers to float datatype.\n\nforward(input)\n\n\nget_parameter(target)\nReturn the parameter refenreced by target.\n\nget_submodule(target)\nGet submodule accroding to the name of submodule.\n\nhalf()\nCasts all floating point parameters and buffers to half datatype.\n\nload_state_dict(state_dict[, strict])\nCopies parameters and buffers from state_dict into this module and its descendants.\n\nmake_contiguous_params_group()\nGet contiguous parameters group after creating the whole module.\n\nmodules()\n\nnamed_buffers([prefix, recurse])\n\nnamed_children()\n\nnamed_modules([memo, prefix])\n\nnamed_parameters([prefix, recurse])\n\nparameters([recurse])\n\nregister_backward_hook(hook)\nRegisters a backward hook on the module.\n\nregister_buffer(name, tensor[, persistent])\nAdds a buffer to the module.\n\nregister_forward_hook(hook)\nRegisters a forward hook on the module.\n\nregister_forward_pre_hook(hook)\nRegisters a forward pre-hook on the module.\n\nregister_full_backward_hook(hook)\nRegisters a backward hook on the module.\n\nregister_parameter(name, param)\nAdds a parameter to the module.\n\nregister_state_dict_pre_hook(hook)\nThese hooks will be called with arguments: self, prefix, and keep_vars before calling state_dict on self.\n\nrequires_grad_([requires_grad])\nChange if autograd should record operations on parameters in this module.\n\nstate_dict([destination, prefix, keep_vars])\n\nto(*args, **kwargs)\nMoves and/or casts the parameters and buffers.\n\nto_consistent(*args, **kwargs)\nThis interface is no longer available, please use oneflow.nn.Module.to_global() instead.\n\nto_empty(*, device)\nMoves the parameters and buffers to the specified device without copying storage.\n\nto_global([placement, sbp])\nConvert the parameters and buffers to global.\n\nto_local()\n\n\nto_memory_format(memory_format)\n\n\ntrain([mode])\nSets the module in training mode.\n\nzero_grad([set_to_none])\nSets gradients of all model parameters to zero.\n\n\n\nAttributes\n\n\n\n\n\n\n_grad_t\nalias of Union[Tuple[oneflow.Tensor, …], oneflow.Tensor]\n\n\n\n\n",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.nn.OFRecordImageDecoderRandomCrop",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.OFRecordImageDecoderRandomCrop.html",
        "api_signature": "oneflow.nn.OFRecordImageDecoderRandomCrop(blob_name: str, color_space: str = 'BGR', num_attempts: int = 10, random_seed: Optional[int] = None, random_area: Sequence[float] = [0.08, 1.0], random_aspect_ratio: Sequence[float] = [0.75, 1.333333])",
        "api_description": "__init__(blob_name: str, color_space: str = 'BGR', num_attempts: int = 10, random_seed: Optional[int] = None, random_area: Sequence[float] = [0.08, 1.0], random_aspect_ratio: Sequence[float] = [0.75, 1.333333])¶\nCalls super().__setattr__(‘a’, a) instead of the typical self.a = a\nto avoid Module.__setattr__ overhead. Module’s __setattr__ has special\nhandling for parameters, submodules, and buffers but simply calls into\nsuper().__setattr__ for all other attributes.\nMethods\n__call__(*args, **kwargs)\nCall self as a function.\n__delattr__(name)\nImplement delattr(self, name).\n__dir__()\nDefault dir() implementation.\n__eq__(value, /)\nReturn self==value.\n__format__(format_spec, /)\nDefault object formatter.\n__ge__(value, /)\nReturn self>=value.\n__getattr__(name)\n__getattribute__(name, /)\nReturn getattr(self, name).\n__getstate__()\n__gt__(value, /)\nReturn self>value.\n__hash__()\nReturn hash(self).\n__init__(blob_name[, color_space, …])\nCalls super().__setattr__(‘a’, a) instead of the typical self.a = a to avoid Module.__setattr__ overhead.\n__init_subclass__\nThis method is called when a class is subclassed.\n__le__(value, /)\nReturn self<=value.\n__lt__(value, /)\nReturn self<value.\n__ne__(value, /)\nReturn self!=value.\n__new__(**kwargs)\nCreate and return a new object.\n__reduce__()\nHelper for pickle.\n__reduce_ex__(protocol, /)\nHelper for pickle.\n__repr__()\nReturn repr(self).\n__setattr__(name, value)\nImplement setattr(self, name, value).\n__setstate__(state)\n__sizeof__()\nSize of object in memory, in bytes.\n__str__()\nReturn str(self).\n__subclasshook__\nAbstract classes can override this to customize issubclass().\n_apply(fn)\n_get_backward_hooks()",
        "return_value": "\n_get_name()\n\n\n_load_from_state_dict(state_dict, prefix, …)\n\n\n_maybe_warn_non_full_backward_hook(args, …)\n\n\n_named_members(get_members_fn[, prefix, recurse])\n\n\n_register_load_state_dict_pre_hook(hook[, …])\nThese hooks will be called with arguments: state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs, before loading state_dict into self.\n\n_register_state_dict_hook(hook)\nThese hooks will be called with arguments: self, state_dict, prefix, local_metadata, after the state_dict of self is set.\n\n_save_to_state_dict(destination, prefix, …)\n\n\n_shallow_repr()\n\n\n_to_memory_format(memory_format)\nCasts the parameters and buffers in this module to another memory format.\n\nadd_module(name, module)\nAdds a child module to the current module.\n\napply(fn)\nApplies fn recursively to every submodule (as returned by .children()) as well as self.\n\nbuffers([recurse])\n\nchildren()\n\ncpu()\nMoves all model parameters and buffers to the CPU.\n\ncuda([device])\nMoves all model parameters and buffers to the GPU.\n\ndouble()\nCasts all floating point parameters and buffers to double datatype.\n\neval()\nSets the module in evaluation mode.\n\nextra_repr()\nSet the extra representation of the module\n\nfloat()\nCasts all floating point parameters and buffers to float datatype.\n\nforward(input)\n\n\nget_parameter(target)\nReturn the parameter refenreced by target.\n\nget_submodule(target)\nGet submodule accroding to the name of submodule.\n\nhalf()\nCasts all floating point parameters and buffers to half datatype.\n\nload_state_dict(state_dict[, strict])\nCopies parameters and buffers from state_dict into this module and its descendants.\n\nmake_contiguous_params_group()\nGet contiguous parameters group after creating the whole module.\n\nmodules()\n\nnamed_buffers([prefix, recurse])\n\nnamed_children()\n\nnamed_modules([memo, prefix])\n\nnamed_parameters([prefix, recurse])\n\nparameters([recurse])\n\nregister_backward_hook(hook)\nRegisters a backward hook on the module.\n\nregister_buffer(name, tensor[, persistent])\nAdds a buffer to the module.\n\nregister_forward_hook(hook)\nRegisters a forward hook on the module.\n\nregister_forward_pre_hook(hook)\nRegisters a forward pre-hook on the module.\n\nregister_full_backward_hook(hook)\nRegisters a backward hook on the module.\n\nregister_parameter(name, param)\nAdds a parameter to the module.\n\nregister_state_dict_pre_hook(hook)\nThese hooks will be called with arguments: self, prefix, and keep_vars before calling state_dict on self.\n\nrequires_grad_([requires_grad])\nChange if autograd should record operations on parameters in this module.\n\nstate_dict([destination, prefix, keep_vars])\n\nto(*args, **kwargs)\nMoves and/or casts the parameters and buffers.\n\nto_consistent(*args, **kwargs)\nThis interface is no longer available, please use oneflow.nn.Module.to_global() instead.\n\nto_empty(*, device)\nMoves the parameters and buffers to the specified device without copying storage.\n\nto_global([placement, sbp])\nConvert the parameters and buffers to global.\n\nto_local()\n\n\nto_memory_format(memory_format)\n\n\ntrain([mode])\nSets the module in training mode.\n\nzero_grad([set_to_none])\nSets gradients of all model parameters to zero.\n\n\n\nAttributes\n\n\n\n\n\n\n_grad_t\nalias of Union[Tuple[oneflow.Tensor, …], oneflow.Tensor]\n\n\n\n\n",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.nn.OFRecordRawDecoder",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.OFRecordRawDecoder.html",
        "api_signature": "oneflow.nn.OFRecordRawDecoder(blob_name: str, shape: Sequence[int], dtype: oneflow._oneflow_internal.dtype, dim1_varying_length: bool = False, truncate: bool = False, auto_zero_padding: bool = False, name: Optional[str] = None)",
        "api_description": "__init__(blob_name: str, shape: Sequence[int], dtype: oneflow._oneflow_internal.dtype, dim1_varying_length: bool = False, truncate: bool = False, auto_zero_padding: bool = False, name: Optional[str] = None)¶\nCalls super().__setattr__(‘a’, a) instead of the typical self.a = a\nto avoid Module.__setattr__ overhead. Module’s __setattr__ has special\nhandling for parameters, submodules, and buffers but simply calls into\nsuper().__setattr__ for all other attributes.\nMethods\n__call__(*args, **kwargs)\nCall self as a function.\n__delattr__(name)\nImplement delattr(self, name).\n__dir__()\nDefault dir() implementation.\n__eq__(value, /)\nReturn self==value.\n__format__(format_spec, /)\nDefault object formatter.\n__ge__(value, /)\nReturn self>=value.\n__getattr__(name)\n__getattribute__(name, /)\nReturn getattr(self, name).\n__getstate__()\n__gt__(value, /)\nReturn self>value.\n__hash__()\nReturn hash(self).\n__init__(blob_name, shape, dtype[, …])\nCalls super().__setattr__(‘a’, a) instead of the typical self.a = a to avoid Module.__setattr__ overhead.\n__init_subclass__\nThis method is called when a class is subclassed.\n__le__(value, /)\nReturn self<=value.\n__lt__(value, /)\nReturn self<value.\n__ne__(value, /)\nReturn self!=value.\n__new__(**kwargs)\nCreate and return a new object.\n__reduce__()\nHelper for pickle.\n__reduce_ex__(protocol, /)\nHelper for pickle.\n__repr__()\nReturn repr(self).\n__setattr__(name, value)\nImplement setattr(self, name, value).\n__setstate__(state)\n__sizeof__()\nSize of object in memory, in bytes.\n__str__()\nReturn str(self).\n__subclasshook__\nAbstract classes can override this to customize issubclass().\n_apply(fn)\n_get_backward_hooks()",
        "return_value": "\n_get_name()\n\n\n_load_from_state_dict(state_dict, prefix, …)\n\n\n_maybe_warn_non_full_backward_hook(args, …)\n\n\n_named_members(get_members_fn[, prefix, recurse])\n\n\n_register_load_state_dict_pre_hook(hook[, …])\nThese hooks will be called with arguments: state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs, before loading state_dict into self.\n\n_register_state_dict_hook(hook)\nThese hooks will be called with arguments: self, state_dict, prefix, local_metadata, after the state_dict of self is set.\n\n_save_to_state_dict(destination, prefix, …)\n\n\n_shallow_repr()\n\n\n_to_memory_format(memory_format)\nCasts the parameters and buffers in this module to another memory format.\n\nadd_module(name, module)\nAdds a child module to the current module.\n\napply(fn)\nApplies fn recursively to every submodule (as returned by .children()) as well as self.\n\nbuffers([recurse])\n\nchildren()\n\ncpu()\nMoves all model parameters and buffers to the CPU.\n\ncuda([device])\nMoves all model parameters and buffers to the GPU.\n\ndouble()\nCasts all floating point parameters and buffers to double datatype.\n\neval()\nSets the module in evaluation mode.\n\nextra_repr()\nSet the extra representation of the module\n\nfloat()\nCasts all floating point parameters and buffers to float datatype.\n\nforward(input)\n\n\nget_parameter(target)\nReturn the parameter refenreced by target.\n\nget_submodule(target)\nGet submodule accroding to the name of submodule.\n\nhalf()\nCasts all floating point parameters and buffers to half datatype.\n\nload_state_dict(state_dict[, strict])\nCopies parameters and buffers from state_dict into this module and its descendants.\n\nmake_contiguous_params_group()\nGet contiguous parameters group after creating the whole module.\n\nmodules()\n\nnamed_buffers([prefix, recurse])\n\nnamed_children()\n\nnamed_modules([memo, prefix])\n\nnamed_parameters([prefix, recurse])\n\nparameters([recurse])\n\nregister_backward_hook(hook)\nRegisters a backward hook on the module.\n\nregister_buffer(name, tensor[, persistent])\nAdds a buffer to the module.\n\nregister_forward_hook(hook)\nRegisters a forward hook on the module.\n\nregister_forward_pre_hook(hook)\nRegisters a forward pre-hook on the module.\n\nregister_full_backward_hook(hook)\nRegisters a backward hook on the module.\n\nregister_parameter(name, param)\nAdds a parameter to the module.\n\nregister_state_dict_pre_hook(hook)\nThese hooks will be called with arguments: self, prefix, and keep_vars before calling state_dict on self.\n\nrequires_grad_([requires_grad])\nChange if autograd should record operations on parameters in this module.\n\nstate_dict([destination, prefix, keep_vars])\n\nto(*args, **kwargs)\nMoves and/or casts the parameters and buffers.\n\nto_consistent(*args, **kwargs)\nThis interface is no longer available, please use oneflow.nn.Module.to_global() instead.\n\nto_empty(*, device)\nMoves the parameters and buffers to the specified device without copying storage.\n\nto_global([placement, sbp])\nConvert the parameters and buffers to global.\n\nto_local()\n\n\nto_memory_format(memory_format)\n\n\ntrain([mode])\nSets the module in training mode.\n\nzero_grad([set_to_none])\nSets gradients of all model parameters to zero.\n\n\n\nAttributes\n\n\n\n\n\n\n_grad_t\nalias of Union[Tuple[oneflow.Tensor, …], oneflow.Tensor]\n\n\n\n\n",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.nn.OFRecordReader",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.OFRecordReader.html",
        "api_signature": "oneflow.nn.OFRecordReader(ofrecord_dir: str, batch_size: int = 1, data_part_num: int = 1, part_name_prefix: str = 'part-', part_name_suffix_length: int = - 1, random_shuffle: bool = False, shuffle_buffer_size: int = 1024, shuffle_after_epoch: bool = False, random_seed: int = - 1, device: Optional[Union[str, oneflow._oneflow_internal.device]] = None, placement: Optional[oneflow._oneflow_internal.placement] = None, sbp: Optional[Union[oneflow._oneflow_internal.sbp.sbp, List[oneflow._oneflow_internal.sbp.sbp]]] = None, name: Optional[str] = None)",
        "api_description": "__init__(ofrecord_dir: str, batch_size: int = 1, data_part_num: int = 1, part_name_prefix: str = 'part-', part_name_suffix_length: int = - 1, random_shuffle: bool = False, shuffle_buffer_size: int = 1024, shuffle_after_epoch: bool = False, random_seed: int = - 1, device: Optional[Union[str, oneflow._oneflow_internal.device]] = None, placement: Optional[oneflow._oneflow_internal.placement] = None, sbp: Optional[Union[oneflow._oneflow_internal.sbp.sbp, List[oneflow._oneflow_internal.sbp.sbp]]] = None, name: Optional[str] = None)¶\nCalls super().__setattr__(‘a’, a) instead of the typical self.a = a\nto avoid Module.__setattr__ overhead. Module’s __setattr__ has special\nhandling for parameters, submodules, and buffers but simply calls into\nsuper().__setattr__ for all other attributes.\nMethods\n__call__(*args, **kwargs)\nCall self as a function.\n__delattr__(name)\nImplement delattr(self, name).\n__dir__()\nDefault dir() implementation.\n__eq__(value, /)\nReturn self==value.\n__format__(format_spec, /)\nDefault object formatter.\n__ge__(value, /)\nReturn self>=value.\n__getattr__(name)\n__getattribute__(name, /)\nReturn getattr(self, name).\n__getstate__()\n__gt__(value, /)\nReturn self>value.\n__hash__()\nReturn hash(self).\n__init__(ofrecord_dir[, batch_size, …])\nCalls super().__setattr__(‘a’, a) instead of the typical self.a = a to avoid Module.__setattr__ overhead.\n__init_subclass__\nThis method is called when a class is subclassed.\n__le__(value, /)\nReturn self<=value.\n__lt__(value, /)\nReturn self<value.\n__ne__(value, /)\nReturn self!=value.\n__new__(**kwargs)\nCreate and return a new object.\n__reduce__()\nHelper for pickle.\n__reduce_ex__(protocol, /)\nHelper for pickle.\n__repr__()\nReturn repr(self).\n__setattr__(name, value)\nImplement setattr(self, name, value).\n__setstate__(state)\n__sizeof__()\nSize of object in memory, in bytes.\n__str__()\nReturn str(self).\n__subclasshook__\nAbstract classes can override this to customize issubclass().\n_apply(fn)\n_get_backward_hooks()",
        "return_value": "\n_get_name()\n\n\n_load_from_state_dict(state_dict, prefix, …)\n\n\n_maybe_warn_non_full_backward_hook(args, …)\n\n\n_named_members(get_members_fn[, prefix, recurse])\n\n\n_register_load_state_dict_pre_hook(hook[, …])\nThese hooks will be called with arguments: state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs, before loading state_dict into self.\n\n_register_state_dict_hook(hook)\nThese hooks will be called with arguments: self, state_dict, prefix, local_metadata, after the state_dict of self is set.\n\n_save_to_state_dict(destination, prefix, …)\n\n\n_shallow_repr()\n\n\n_to_memory_format(memory_format)\nCasts the parameters and buffers in this module to another memory format.\n\nadd_module(name, module)\nAdds a child module to the current module.\n\napply(fn)\nApplies fn recursively to every submodule (as returned by .children()) as well as self.\n\nbuffers([recurse])\n\nchildren()\n\ncpu()\nMoves all model parameters and buffers to the CPU.\n\ncuda([device])\nMoves all model parameters and buffers to the GPU.\n\ndouble()\nCasts all floating point parameters and buffers to double datatype.\n\neval()\nSets the module in evaluation mode.\n\nextra_repr()\nSet the extra representation of the module\n\nfloat()\nCasts all floating point parameters and buffers to float datatype.\n\nforward()\n\n\nget_parameter(target)\nReturn the parameter refenreced by target.\n\nget_submodule(target)\nGet submodule accroding to the name of submodule.\n\nhalf()\nCasts all floating point parameters and buffers to half datatype.\n\nload_state_dict(state_dict[, strict])\nCopies parameters and buffers from state_dict into this module and its descendants.\n\nmake_contiguous_params_group()\nGet contiguous parameters group after creating the whole module.\n\nmodules()\n\nnamed_buffers([prefix, recurse])\n\nnamed_children()\n\nnamed_modules([memo, prefix])\n\nnamed_parameters([prefix, recurse])\n\nparameters([recurse])\n\nregister_backward_hook(hook)\nRegisters a backward hook on the module.\n\nregister_buffer(name, tensor[, persistent])\nAdds a buffer to the module.\n\nregister_forward_hook(hook)\nRegisters a forward hook on the module.\n\nregister_forward_pre_hook(hook)\nRegisters a forward pre-hook on the module.\n\nregister_full_backward_hook(hook)\nRegisters a backward hook on the module.\n\nregister_parameter(name, param)\nAdds a parameter to the module.\n\nregister_state_dict_pre_hook(hook)\nThese hooks will be called with arguments: self, prefix, and keep_vars before calling state_dict on self.\n\nrequires_grad_([requires_grad])\nChange if autograd should record operations on parameters in this module.\n\nstate_dict([destination, prefix, keep_vars])\n\nto(*args, **kwargs)\nMoves and/or casts the parameters and buffers.\n\nto_consistent(*args, **kwargs)\nThis interface is no longer available, please use oneflow.nn.Module.to_global() instead.\n\nto_empty(*, device)\nMoves the parameters and buffers to the specified device without copying storage.\n\nto_global([placement, sbp])\nConvert the parameters and buffers to global.\n\nto_local()\n\n\nto_memory_format(memory_format)\n\n\ntrain([mode])\nSets the module in training mode.\n\nzero_grad([set_to_none])\nSets gradients of all model parameters to zero.\n\n\n\nAttributes\n\n\n\n\n\n\n_grad_t\nalias of Union[Tuple[oneflow.Tensor, …], oneflow.Tensor]\n\n\n\n\n",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.nn.MinMaxObserver",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.MinMaxObserver.html",
        "api_signature": "oneflow.nn.MinMaxObserver(quantization_formula: str = 'google', quantization_bit: int = 8, quantization_scheme: str = 'symmetric', per_layer_quantization: bool = True)",
        "api_description": "Compute the quantization parameters of the input tensor.\nFirst compute the max and min values of input tensor:\n\\[ \\begin{align}\\begin{aligned}& max\\_value = max(input)\\\\& min\\_value = min(input)\\end{aligned}\\end{align} \\]\nThen compute the scale and zero_point with the following equations:\nif quantization_scheme == “symmetric”:\n\\[ \\begin{align}\\begin{aligned}& denom = 2^{quantization\\_to\\_bit - 1} - 1\\\\& scale = max(|max\\_value|,|min\\_value|) / denom\\\\& zero\\_point = 0\\end{aligned}\\end{align} \\]\nelif quantization_scheme == “affine”:\n\\[ \\begin{align}\\begin{aligned}& denom = 2^{quantization\\_to\\_bit} - 1\\\\& scale = (max\\_value - min\\_value) / denom\\\\& zero\\_point = -min\\_value / scale\\end{aligned}\\end{align} \\]\nIf per_layer_quantization is False, then the shape of scale and zero_point will be (input.shape[0],).",
        "return_value": "The scale and zero_point of input tensor.\n\n\n_get_name()\n\n\n_load_from_state_dict(state_dict, prefix, …)\n\n\n_maybe_warn_non_full_backward_hook(args, …)\n\n\n_named_members(get_members_fn[, prefix, recurse])\n\n\n_register_load_state_dict_pre_hook(hook[, …])\nThese hooks will be called with arguments: state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs, before loading state_dict into self.\n\n_register_state_dict_hook(hook)\nThese hooks will be called with arguments: self, state_dict, prefix, local_metadata, after the state_dict of self is set.\n\n_save_to_state_dict(destination, prefix, …)\n\n\n_shallow_repr()\n\n\n_to_memory_format(memory_format)\nCasts the parameters and buffers in this module to another memory format.\n\nadd_module(name, module)\nAdds a child module to the current module.\n\napply(fn)\nApplies fn recursively to every submodule (as returned by .children()) as well as self.\n\nbuffers([recurse])\n\nchildren()\n\ncpu()\nMoves all model parameters and buffers to the CPU.\n\ncuda([device])\nMoves all model parameters and buffers to the GPU.\n\ndouble()\nCasts all floating point parameters and buffers to double datatype.\n\neval()\nSets the module in evaluation mode.\n\nextra_repr()\nSet the extra representation of the module\n\nfloat()\nCasts all floating point parameters and buffers to float datatype.\n\nforward(input)\n\n\nget_parameter(target)\nReturn the parameter refenreced by target.\n\nget_submodule(target)\nGet submodule accroding to the name of submodule.\n\nhalf()\nCasts all floating point parameters and buffers to half datatype.\n\nload_state_dict(state_dict[, strict])\nCopies parameters and buffers from state_dict into this module and its descendants.\n\nmake_contiguous_params_group()\nGet contiguous parameters group after creating the whole module.\n\nmodules()\n\nnamed_buffers([prefix, recurse])\n\nnamed_children()\n\nnamed_modules([memo, prefix])\n\nnamed_parameters([prefix, recurse])\n\nparameters([recurse])\n\nregister_backward_hook(hook)\nRegisters a backward hook on the module.\n\nregister_buffer(name, tensor[, persistent])\nAdds a buffer to the module.\n\nregister_forward_hook(hook)\nRegisters a forward hook on the module.\n\nregister_forward_pre_hook(hook)\nRegisters a forward pre-hook on the module.\n\nregister_full_backward_hook(hook)\nRegisters a backward hook on the module.\n\nregister_parameter(name, param)\nAdds a parameter to the module.\n\nregister_state_dict_pre_hook(hook)\nThese hooks will be called with arguments: self, prefix, and keep_vars before calling state_dict on self.\n\nrequires_grad_([requires_grad])\nChange if autograd should record operations on parameters in this module.\n\nstate_dict([destination, prefix, keep_vars])\n\nto(*args, **kwargs)\nMoves and/or casts the parameters and buffers.\n\nto_consistent(*args, **kwargs)\nThis interface is no longer available, please use oneflow.nn.Module.to_global() instead.\n\nto_empty(*, device)\nMoves the parameters and buffers to the specified device without copying storage.\n\nto_global([placement, sbp])\nConvert the parameters and buffers to global.\n\nto_local()\n\n\nto_memory_format(memory_format)\n\n\ntrain([mode])\nSets the module in training mode.\n\nzero_grad([set_to_none])\nSets gradients of all model parameters to zero.\n\n\n\nAttributes\n\n\n\n\n\n\n_grad_t\nalias of Union[Tuple[oneflow.Tensor, …], oneflow.Tensor]\n\n\n\n\n",
        "parameters": "\ninput (oneflow.Tensor) – the input value(s), in oneflow.float32.\nquantization_formula (str) – Support “google” or “cambricon”.\nquantization_bit (int) – Quantize input to uintX / intX, X can be in range [2, 8]. Defaults to 8.\nquantization_scheme (str) – “symmetric” or “affine”, quantize to signed / unsigned integer. Defaults to “symmetric”.\nper_layer_quantization (bool) – True or False, means per-layer / per-channel quantization. Defaults to True.\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import numpy as np\n>>> import oneflow as flow\n\n>>> weight = (np.random.random((2, 3, 4, 5)) - 0.5).astype(np.float32)\n\n>>> input_tensor = flow.tensor(\n...    weight, dtype=flow.float32\n... )\n\n>>> quantization_bit = 8\n>>> quantization_scheme = \"symmetric\"\n>>> quantization_formula = \"google\"\n>>> per_layer_quantization = True\n\n>>> min_max_observer = flow.nn.MinMaxObserver(quantization_formula=quantization_formula, quantization_bit=quantization_bit,\n... quantization_scheme=quantization_scheme, per_layer_quantization=per_layer_quantization)\n\n>>> scale, zero_point = min_max_observer(\n...    input_tensor, )\n\n\n\n\n__init__(quantization_formula: str = 'google', quantization_bit: int = 8, quantization_scheme: str = 'symmetric', per_layer_quantization: bool = True) → None¶\nCalls super().__setattr__(‘a’, a) instead of the typical self.a = a\nto avoid Module.__setattr__ overhead. Module’s __setattr__ has special\nhandling for parameters, submodules, and buffers but simply calls into\nsuper().__setattr__ for all other attributes.\n\nMethods\n\n\n\n\n\n\n__call__(*args, **kwargs)\nCall self as a function.\n\n__delattr__(name)\nImplement delattr(self, name).\n\n__dir__()\nDefault dir() implementation.\n\n__eq__(value, /)\nReturn self==value.\n\n__format__(format_spec, /)\nDefault object formatter.\n\n__ge__(value, /)\nReturn self>=value.\n\n__getattr__(name)\n\n\n__getattribute__(name, /)\nReturn getattr(self, name).\n\n__getstate__()\n\n\n__gt__(value, /)\nReturn self>value.\n\n__hash__()\nReturn hash(self).\n\n__init__([quantization_formula, …])\nCalls super().__setattr__(‘a’, a) instead of the typical self.a = a to avoid Module.__setattr__ overhead.\n\n__init_subclass__\nThis method is called when a class is subclassed.\n\n__le__(value, /)\nReturn self<=value.\n\n__lt__(value, /)\nReturn self<value.\n\n__ne__(value, /)\nReturn self!=value.\n\n__new__(**kwargs)\nCreate and return a new object.\n\n__reduce__()\nHelper for pickle.\n\n__reduce_ex__(protocol, /)\nHelper for pickle.\n\n__repr__()\nReturn repr(self).\n\n__setattr__(name, value)\nImplement setattr(self, name, value).\n\n__setstate__(state)\n\n\n__sizeof__()\nSize of object in memory, in bytes.\n\n__str__()\nReturn str(self).\n\n__subclasshook__\nAbstract classes can override this to customize issubclass().\n\n_apply(fn)\n\n\n_get_backward_hooks()\n"
    },
    {
        "api_name": "oneflow.nn.MovingAverageMinMaxObserver",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.MovingAverageMinMaxObserver.html",
        "api_signature": "oneflow.nn.MovingAverageMinMaxObserver(stop_update_after_iters: int = 1, quantization_formula: str = 'google', quantization_bit: int = 8, quantization_scheme: str = 'symmetric', momentum: float = 0.95)",
        "api_description": "Compute the quantization parameters based on the moving average of the input tensor’s min and max values.\nFirst compute the moving_max and moving_min value of input tensor:\nif quantization_scheme == “symmetric”:\n\\[ \\begin{align}\\begin{aligned}& moving\\_max = moving\\_max * momentum + |max(input)| * (1 - momentum)\\\\& moving\\_min = moving\\_max\\end{aligned}\\end{align} \\]\nelif quantization_scheme == “affine”:\n\\[ \\begin{align}\\begin{aligned}& moving\\_max = moving\\_max * momentum + max(input) * (1 - momentum)\\\\& moving\\_min = moving\\_min * momentum + min(input) * (1 - momentum)\\end{aligned}\\end{align} \\]\nThe moving average of min and max values are initialized as the first batch of input Blob’s min and max.\nThen compute the scale and zero_point with the following equations:\nif quantization_scheme == “symmetric”:\n\\[ \\begin{align}\\begin{aligned}& denom = 2^{quantization\\_to\\_bit - 1} - 1\\\\& scale = moving\\_max / denom\\\\& zero\\_point = 0\\end{aligned}\\end{align} \\]\nelif quantization_scheme == “affine”:\n\\[ \\begin{align}\\begin{aligned}& denom = 2^{quantization\\_to\\_bit} - 1\\\\& scale = (moving\\_max - moving\\_min) / denom\\\\& zero\\_point = -moving\\_min / scale\\end{aligned}\\end{align} \\]",
        "return_value": "The scale and zero_point of input tensor.\n\n\n_get_name()\n\n\n_load_from_state_dict(state_dict, prefix, …)\n\n\n_maybe_warn_non_full_backward_hook(args, …)\n\n\n_named_members(get_members_fn[, prefix, recurse])\n\n\n_register_load_state_dict_pre_hook(hook[, …])\nThese hooks will be called with arguments: state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs, before loading state_dict into self.\n\n_register_state_dict_hook(hook)\nThese hooks will be called with arguments: self, state_dict, prefix, local_metadata, after the state_dict of self is set.\n\n_save_to_state_dict(destination, prefix, …)\n\n\n_shallow_repr()\n\n\n_to_memory_format(memory_format)\nCasts the parameters and buffers in this module to another memory format.\n\nadd_module(name, module)\nAdds a child module to the current module.\n\napply(fn)\nApplies fn recursively to every submodule (as returned by .children()) as well as self.\n\nbuffers([recurse])\n\nchildren()\n\ncpu()\nMoves all model parameters and buffers to the CPU.\n\ncuda([device])\nMoves all model parameters and buffers to the GPU.\n\ndouble()\nCasts all floating point parameters and buffers to double datatype.\n\neval()\nSets the module in evaluation mode.\n\nextra_repr()\nSet the extra representation of the module\n\nfloat()\nCasts all floating point parameters and buffers to float datatype.\n\nforward(input, current_train_step)\n\n\nget_parameter(target)\nReturn the parameter refenreced by target.\n\nget_submodule(target)\nGet submodule accroding to the name of submodule.\n\nhalf()\nCasts all floating point parameters and buffers to half datatype.\n\nload_state_dict(state_dict[, strict])\nCopies parameters and buffers from state_dict into this module and its descendants.\n\nmake_contiguous_params_group()\nGet contiguous parameters group after creating the whole module.\n\nmodules()\n\nnamed_buffers([prefix, recurse])\n\nnamed_children()\n\nnamed_modules([memo, prefix])\n\nnamed_parameters([prefix, recurse])\n\nparameters([recurse])\n\nregister_backward_hook(hook)\nRegisters a backward hook on the module.\n\nregister_buffer(name, tensor[, persistent])\nAdds a buffer to the module.\n\nregister_forward_hook(hook)\nRegisters a forward hook on the module.\n\nregister_forward_pre_hook(hook)\nRegisters a forward pre-hook on the module.\n\nregister_full_backward_hook(hook)\nRegisters a backward hook on the module.\n\nregister_parameter(name, param)\nAdds a parameter to the module.\n\nregister_state_dict_pre_hook(hook)\nThese hooks will be called with arguments: self, prefix, and keep_vars before calling state_dict on self.\n\nrequires_grad_([requires_grad])\nChange if autograd should record operations on parameters in this module.\n\nreset_running_stats()\n\n\nstate_dict([destination, prefix, keep_vars])\n\nto(*args, **kwargs)\nMoves and/or casts the parameters and buffers.\n\nto_consistent(*args, **kwargs)\nThis interface is no longer available, please use oneflow.nn.Module.to_global() instead.\n\nto_empty(*, device)\nMoves the parameters and buffers to the specified device without copying storage.\n\nto_global([placement, sbp])\nConvert the parameters and buffers to global.\n\nto_local()\n\n\nto_memory_format(memory_format)\n\n\ntrain([mode])\nSets the module in training mode.\n\nzero_grad([set_to_none])\nSets gradients of all model parameters to zero.\n\n\n\nAttributes\n\n\n\n\n\n\n_grad_t\nalias of Union[Tuple[oneflow.Tensor, …], oneflow.Tensor]\n\n\n\n\n",
        "parameters": "\ninput (oneflow.Tensor) – the input value(s), in oneflow.float32.\ncurrent_train_step_tensor (oneflow.Tensor) – record train step for quantionzation aware training.\nstop_update_after_iters (int) – stop record train step for quantionzation aware training when train iter greater than stop_update_after_iters.\nquantization_formula (str) – Support “google” or “cambricon”.\nquantization_bit (int) – Quantize input to uintX / intX, X can be in range [2, 8]. Defaults to 8.\nquantization_scheme (str) – “symmetric” or “affine”, quantize to signed / unsigned integer. Defaults to “symmetric”.\nmomentum (float) – Smoothing parameter for exponential moving average operation. Defaults to 0.95.\n\n\n",
        "input_shape": "",
        "notes": "current_train_step can be directly assigned to an optimizer(eg.SGD) step.",
        "code_example": ">>> import numpy as np\n>>> import oneflow as flow\n\n>>> weight = (np.random.random((2, 3, 4, 5)) - 0.5).astype(np.float32)\n\n>>> input_tensor = flow.tensor(\n...    weight, dtype=flow.float32\n... )\n\n>>> current_train_step_tensor = flow.tensor(\n...   np.zeros((1,)).astype(np.float32),\n...    dtype=flow.int64,\n... )\n\n>>> momentum = 0.95\n>>> quantization_bit = 8\n>>> quantization_scheme = \"symmetric\"\n>>> quantization_formula = \"google\"\n\n>>> moving_average_min_max_observer = flow.nn.MovingAverageMinMaxObserver(stop_update_after_iters=1,\n...                                                                       quantization_formula=quantization_formula, quantization_bit=quantization_bit,\n...                                                                       quantization_scheme=quantization_scheme, momentum=momentum,\n...                                                                       )\n\n>>> (scale, zero_point) = moving_average_min_max_observer(\n...    input_tensor,\n...    current_train_step_tensor,\n... )\n\n\n\n\n__init__(stop_update_after_iters: int = 1, quantization_formula: str = 'google', quantization_bit: int = 8, quantization_scheme: str = 'symmetric', momentum: float = 0.95) → None¶\nCalls super().__setattr__(‘a’, a) instead of the typical self.a = a\nto avoid Module.__setattr__ overhead. Module’s __setattr__ has special\nhandling for parameters, submodules, and buffers but simply calls into\nsuper().__setattr__ for all other attributes.\n\nMethods\n\n\n\n\n\n\n__call__(*args, **kwargs)\nCall self as a function.\n\n__delattr__(name)\nImplement delattr(self, name).\n\n__dir__()\nDefault dir() implementation.\n\n__eq__(value, /)\nReturn self==value.\n\n__format__(format_spec, /)\nDefault object formatter.\n\n__ge__(value, /)\nReturn self>=value.\n\n__getattr__(name)\n\n\n__getattribute__(name, /)\nReturn getattr(self, name).\n\n__getstate__()\n\n\n__gt__(value, /)\nReturn self>value.\n\n__hash__()\nReturn hash(self).\n\n__init__([stop_update_after_iters, …])\nCalls super().__setattr__(‘a’, a) instead of the typical self.a = a to avoid Module.__setattr__ overhead.\n\n__init_subclass__\nThis method is called when a class is subclassed.\n\n__le__(value, /)\nReturn self<=value.\n\n__lt__(value, /)\nReturn self<value.\n\n__ne__(value, /)\nReturn self!=value.\n\n__new__(**kwargs)\nCreate and return a new object.\n\n__reduce__()\nHelper for pickle.\n\n__reduce_ex__(protocol, /)\nHelper for pickle.\n\n__repr__()\nReturn repr(self).\n\n__setattr__(name, value)\nImplement setattr(self, name, value).\n\n__setstate__(state)\n\n\n__sizeof__()\nSize of object in memory, in bytes.\n\n__str__()\nReturn str(self).\n\n__subclasshook__\nAbstract classes can override this to customize issubclass().\n\n_apply(fn)\n\n\n_get_backward_hooks()\n"
    },
    {
        "api_name": "oneflow.nn.FakeQuantization",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.FakeQuantization.html",
        "api_signature": "oneflow.nn.FakeQuantization(quantization_formula: str = 'google', quantization_bit: int = 8, quantization_scheme: str = 'symmetric')",
        "api_description": "Simulate the quantize and dequantize operations in training time.\nThe output will be computed as:\nif quantization_scheme == “symmetric”:\n\\[ \\begin{align}\\begin{aligned}& quant\\_max = 2^{quantization\\_to\\_bit - 1} - 1\\\\& quant\\_min = -quant\\_max\\\\& clamp(round(x / scale), quant\\_min, quant\\_max) * scale\\end{aligned}\\end{align} \\]\nelif quantization_scheme == “affine”:\n\\[ \\begin{align}\\begin{aligned}& quant\\_max = 2^{quantization\\_to\\_bit} - 1\\\\& quant\\_min = 0\\\\& (clamp(round(x / scale + zero\\_point), quant\\_min, quant\\_max) - zero\\_point) * scale\\end{aligned}\\end{align} \\]",
        "return_value": "Input tensor after quantize and dequantize operations.\n\n\n_get_name()\n\n\n_load_from_state_dict(state_dict, prefix, …)\n\n\n_maybe_warn_non_full_backward_hook(args, …)\n\n\n_named_members(get_members_fn[, prefix, recurse])\n\n\n_register_load_state_dict_pre_hook(hook[, …])\nThese hooks will be called with arguments: state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs, before loading state_dict into self.\n\n_register_state_dict_hook(hook)\nThese hooks will be called with arguments: self, state_dict, prefix, local_metadata, after the state_dict of self is set.\n\n_save_to_state_dict(destination, prefix, …)\n\n\n_shallow_repr()\n\n\n_to_memory_format(memory_format)\nCasts the parameters and buffers in this module to another memory format.\n\nadd_module(name, module)\nAdds a child module to the current module.\n\napply(fn)\nApplies fn recursively to every submodule (as returned by .children()) as well as self.\n\nbuffers([recurse])\n\nchildren()\n\ncpu()\nMoves all model parameters and buffers to the CPU.\n\ncuda([device])\nMoves all model parameters and buffers to the GPU.\n\ndouble()\nCasts all floating point parameters and buffers to double datatype.\n\neval()\nSets the module in evaluation mode.\n\nextra_repr()\nSet the extra representation of the module\n\nfloat()\nCasts all floating point parameters and buffers to float datatype.\n\nforward(input, scale, zero_point)\n\n\nget_parameter(target)\nReturn the parameter refenreced by target.\n\nget_submodule(target)\nGet submodule accroding to the name of submodule.\n\nhalf()\nCasts all floating point parameters and buffers to half datatype.\n\nload_state_dict(state_dict[, strict])\nCopies parameters and buffers from state_dict into this module and its descendants.\n\nmake_contiguous_params_group()\nGet contiguous parameters group after creating the whole module.\n\nmodules()\n\nnamed_buffers([prefix, recurse])\n\nnamed_children()\n\nnamed_modules([memo, prefix])\n\nnamed_parameters([prefix, recurse])\n\nparameters([recurse])\n\nregister_backward_hook(hook)\nRegisters a backward hook on the module.\n\nregister_buffer(name, tensor[, persistent])\nAdds a buffer to the module.\n\nregister_forward_hook(hook)\nRegisters a forward hook on the module.\n\nregister_forward_pre_hook(hook)\nRegisters a forward pre-hook on the module.\n\nregister_full_backward_hook(hook)\nRegisters a backward hook on the module.\n\nregister_parameter(name, param)\nAdds a parameter to the module.\n\nregister_state_dict_pre_hook(hook)\nThese hooks will be called with arguments: self, prefix, and keep_vars before calling state_dict on self.\n\nrequires_grad_([requires_grad])\nChange if autograd should record operations on parameters in this module.\n\nstate_dict([destination, prefix, keep_vars])\n\nto(*args, **kwargs)\nMoves and/or casts the parameters and buffers.\n\nto_consistent(*args, **kwargs)\nThis interface is no longer available, please use oneflow.nn.Module.to_global() instead.\n\nto_empty(*, device)\nMoves the parameters and buffers to the specified device without copying storage.\n\nto_global([placement, sbp])\nConvert the parameters and buffers to global.\n\nto_local()\n\n\nto_memory_format(memory_format)\n\n\ntrain([mode])\nSets the module in training mode.\n\nzero_grad([set_to_none])\nSets gradients of all model parameters to zero.\n\n\n\nAttributes\n\n\n\n\n\n\n_grad_t\nalias of Union[Tuple[oneflow.Tensor, …], oneflow.Tensor]\n\n\n\n\n",
        "parameters": "\ninput (oneflow.Tensor) – the input value(s), in oneflow.float32.\nscale (oneflow.Tensor) – quantization scale.\nzero_point (oneflow.Tensor) – quantization zero_point.\nquantization_bit (int) – Quantize input to uintX / intX, X can be in range [2, 8]. Defaults to 8.\nquantization_scheme (str) – “symmetric” or “affine”, quantize to signed / unsigned integer. Defaults to “symmetric”.\nquantization_formula (str) – Support “google” or “cambricon”.\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import numpy as np\n>>> import oneflow as flow\n\n>>> weight = (np.random.random((2, 3, 4, 5)) - 0.5).astype(np.float32)\n\n>>> input_tensor = flow.tensor(\n...    weight, dtype=flow.float32\n... )\n\n>>> quantization_bit = 8\n>>> quantization_scheme = \"symmetric\"\n>>> quantization_formula = \"google\"\n>>> per_layer_quantization = True\n\n>>> min_max_observer = flow.nn.MinMaxObserver(quantization_formula=quantization_formula, quantization_bit=quantization_bit,\n... quantization_scheme=quantization_scheme, per_layer_quantization=per_layer_quantization)\n>>> fake_quantization = flow.nn.FakeQuantization(quantization_formula=quantization_formula, quantization_bit=quantization_bit,\n... quantization_scheme=quantization_scheme)\n\n>>> scale, zero_point = min_max_observer(\n...    input_tensor,\n... )\n\n>>> output_tensor = fake_quantization(\n...    input_tensor,\n...    scale,\n...    zero_point,\n... )\n\n\n\n\n__init__(quantization_formula: str = 'google', quantization_bit: int = 8, quantization_scheme: str = 'symmetric') → None¶\nCalls super().__setattr__(‘a’, a) instead of the typical self.a = a\nto avoid Module.__setattr__ overhead. Module’s __setattr__ has special\nhandling for parameters, submodules, and buffers but simply calls into\nsuper().__setattr__ for all other attributes.\n\nMethods\n\n\n\n\n\n\n__call__(*args, **kwargs)\nCall self as a function.\n\n__delattr__(name)\nImplement delattr(self, name).\n\n__dir__()\nDefault dir() implementation.\n\n__eq__(value, /)\nReturn self==value.\n\n__format__(format_spec, /)\nDefault object formatter.\n\n__ge__(value, /)\nReturn self>=value.\n\n__getattr__(name)\n\n\n__getattribute__(name, /)\nReturn getattr(self, name).\n\n__getstate__()\n\n\n__gt__(value, /)\nReturn self>value.\n\n__hash__()\nReturn hash(self).\n\n__init__([quantization_formula, …])\nCalls super().__setattr__(‘a’, a) instead of the typical self.a = a to avoid Module.__setattr__ overhead.\n\n__init_subclass__\nThis method is called when a class is subclassed.\n\n__le__(value, /)\nReturn self<=value.\n\n__lt__(value, /)\nReturn self<value.\n\n__ne__(value, /)\nReturn self!=value.\n\n__new__(**kwargs)\nCreate and return a new object.\n\n__reduce__()\nHelper for pickle.\n\n__reduce_ex__(protocol, /)\nHelper for pickle.\n\n__repr__()\nReturn repr(self).\n\n__setattr__(name, value)\nImplement setattr(self, name, value).\n\n__setstate__(state)\n\n\n__sizeof__()\nSize of object in memory, in bytes.\n\n__str__()\nReturn str(self).\n\n__subclasshook__\nAbstract classes can override this to customize issubclass().\n\n_apply(fn)\n\n\n_get_backward_hooks()\n"
    },
    {
        "api_name": "oneflow.nn.QatConv1d",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.QatConv1d.html",
        "api_signature": "oneflow.nn.QatConv1d(in_channels: int, out_channels: int, kernel_size: Union[int, Tuple[int]], stride: Union[int, Tuple[int]] = 1, padding: Union[str, int, Tuple[int]] = 0, dilation: Union[int, Tuple[int]] = 1, groups: int = 1, bias: bool = True, padding_mode: str = 'zeros', quantization_formula: str = 'google', quantization_bit: int = 8, quantization_scheme: str = 'symmetric', weight_quant_per_layer: bool = True, input_quant_momentum: float = 0.95)",
        "api_description": "A Conv1d module attached with nn.MinMaxObserver, nn.MovingAverageMinMaxObserver and nn.FakeQuantization modules for weight and input,\nused for quantization aware training.\nThe parameters of QatConv1d are the same as Conv1d with some extra parameters for fake quantization,\nsee MinMaxObserver, MovingAverageMinMaxObserver and FakeQuantization for more details.",
        "return_value": "\n_get_name()\n\n\n_load_from_state_dict(state_dict, prefix, …)\n\n\n_maybe_warn_non_full_backward_hook(args, …)\n\n\n_named_members(get_members_fn[, prefix, recurse])\n\n\n_register_load_state_dict_pre_hook(hook[, …])\nThese hooks will be called with arguments: state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs, before loading state_dict into self.\n\n_register_state_dict_hook(hook)\nThese hooks will be called with arguments: self, state_dict, prefix, local_metadata, after the state_dict of self is set.\n\n_save_to_state_dict(destination, prefix, …)\n\n\n_shallow_repr()\n\n\n_to_memory_format(memory_format)\nCasts the parameters and buffers in this module to another memory format.\n\nadd_module(name, module)\nAdds a child module to the current module.\n\napply(fn)\nApplies fn recursively to every submodule (as returned by .children()) as well as self.\n\nbuffers([recurse])\n\nchildren()\n\ncpu()\nMoves all model parameters and buffers to the CPU.\n\ncuda([device])\nMoves all model parameters and buffers to the GPU.\n\ndouble()\nCasts all floating point parameters and buffers to double datatype.\n\neval()\nSets the module in evaluation mode.\n\nextra_repr()\nSet the extra representation of the module\n\nfloat()\nCasts all floating point parameters and buffers to float datatype.\n\nforward(x)\n\n\nget_parameter(target)\nReturn the parameter refenreced by target.\n\nget_submodule(target)\nGet submodule accroding to the name of submodule.\n\nhalf()\nCasts all floating point parameters and buffers to half datatype.\n\nload_state_dict(state_dict[, strict])\nCopies parameters and buffers from state_dict into this module and its descendants.\n\nmake_contiguous_params_group()\nGet contiguous parameters group after creating the whole module.\n\nmodules()\n\nnamed_buffers([prefix, recurse])\n\nnamed_children()\n\nnamed_modules([memo, prefix])\n\nnamed_parameters([prefix, recurse])\n\nparameters([recurse])\n\nregister_backward_hook(hook)\nRegisters a backward hook on the module.\n\nregister_buffer(name, tensor[, persistent])\nAdds a buffer to the module.\n\nregister_forward_hook(hook)\nRegisters a forward hook on the module.\n\nregister_forward_pre_hook(hook)\nRegisters a forward pre-hook on the module.\n\nregister_full_backward_hook(hook)\nRegisters a backward hook on the module.\n\nregister_parameter(name, param)\nAdds a parameter to the module.\n\nregister_state_dict_pre_hook(hook)\nThese hooks will be called with arguments: self, prefix, and keep_vars before calling state_dict on self.\n\nrequires_grad_([requires_grad])\nChange if autograd should record operations on parameters in this module.\n\nreset_parameters()\n\n\nstate_dict([destination, prefix, keep_vars])\n\nto(*args, **kwargs)\nMoves and/or casts the parameters and buffers.\n\nto_consistent(*args, **kwargs)\nThis interface is no longer available, please use oneflow.nn.Module.to_global() instead.\n\nto_empty(*, device)\nMoves the parameters and buffers to the specified device without copying storage.\n\nto_global([placement, sbp])\nConvert the parameters and buffers to global.\n\nto_local()\n\n\nto_memory_format(memory_format)\n\n\ntrain([mode])\nSets the module in training mode.\n\nzero_grad([set_to_none])\nSets gradients of all model parameters to zero.\n\n\n\nAttributes\n\n\n\n\n\n\n_grad_t\nalias of Union[Tuple[oneflow.Tensor, …], oneflow.Tensor]\n\n\n\n\n",
        "parameters": "\nin_channels (int) – Number of channels in the input image\nout_channels (int) – Number of channels produced by the convolution\nkernel_size (int or tuple) – Size of the convolving kernel\nstride (int or tuple, optional) – Stride of the convolution. Default: 1\npadding (int, tuple or str, optional) – Padding added to both sides of\nthe input. Default: 0\ndilation (int or tuple, optional) – Spacing between kernel\nelements. Default: 1\ngroups (int, optional) – Number of blocked connections from input\nchannels to output channels. Default: 1\nbias (bool, optional) – If True, adds a learnable bias to the\noutput. Default: True\npadding_mode (string, optional) – 'zeros'. Default: 'zeros'\nquantization_formula (str) – Support “google” or “cambricon”.\nquantization_bit (int) – Quantize input to uintX / intX, X can be in range [2, 8]. Defaults to 8.\nquantization_scheme (str) – “symmetric” or “affine”, quantize to signed / unsigned integer. Defaults to “symmetric”.\nweight_quant_per_layer (bool) – True or False, means per-layer / per-channel for weight quantization. Defaults to True.\ninput_quant_momentum (float) – Smoothing parameter for exponential moving average operation for input quantization. Defaults to 0.95.\n\n\n\n\n",
        "input_shape": "Input: \\((N, C_{in}, L_{in})\\)\nOutput: \\((N, C_{out}, L_{out})\\) where\n\n\\[\\begin{split}L_{out} = \\\\left\\\\lfloor\\\\frac{L_{in} + 2 \\\\times \\\\text{padding} - \\\\text{dilation}\n          \\\\times (\\\\text{kernel\\\\_size} - 1) - 1}{\\\\text{stride}} + 1\\\\right\\\\rfloor\\end{split}\\]\n\n\n\n\n\n\nweight¶\nthe learnable weights of the module of shape\n\\((\\\\text{out\\\\_channels},\n\\\\frac{\\\\text{in\\\\_channels}}{\\\\text{groups}}, \\\\text{kernel\\\\_size})\\).\nThe values of these weights are sampled from\n\\(\\\\mathcal{U}(-\\\\sqrt{k}, \\\\sqrt{k})\\) where\n\\(k = \\\\frac{groups}{C_\\\\text{in} * \\\\text{kernel\\\\_size}}\\)\n\nType\nTensor\n\n\n\n\n\nbias¶\nthe learnable bias of the module of shape\n(out_channels). If bias is True, then the values of these weights are\nsampled from \\(\\\\mathcal{U}(-\\\\sqrt{k}, \\\\sqrt{k})\\) where\n\\(k = \\\\frac{groups}{C_\\\\text{in} * \\\\text{kernel\\\\_size}}\\)\n\nType\nTensor\n\n\n\n",
        "notes": "",
        "code_example": ">>> import numpy as np\n>>> import oneflow as flow\n>>> import oneflow.nn as nn\n\n>>> arr = np.random.randn(20, 16, 50)\n>>> input = flow.Tensor(arr)\n>>> m = nn.QatConv1d(16, 33, 3, stride=2, quantization_formula=\"google\", quantization_bit=8, quantization_scheme=\"symmetric\")\n>>> output = m(input)\n\n\n\n\n__init__(in_channels: int, out_channels: int, kernel_size: Union[int, Tuple[int]], stride: Union[int, Tuple[int]] = 1, padding: Union[str, int, Tuple[int]] = 0, dilation: Union[int, Tuple[int]] = 1, groups: int = 1, bias: bool = True, padding_mode: str = 'zeros', quantization_formula: str = 'google', quantization_bit: int = 8, quantization_scheme: str = 'symmetric', weight_quant_per_layer: bool = True, input_quant_momentum: float = 0.95)¶\nCalls super().__setattr__(‘a’, a) instead of the typical self.a = a\nto avoid Module.__setattr__ overhead. Module’s __setattr__ has special\nhandling for parameters, submodules, and buffers but simply calls into\nsuper().__setattr__ for all other attributes.\n\nMethods\n\n\n\n\n\n\n__call__(*args, **kwargs)\nCall self as a function.\n\n__delattr__(name)\nImplement delattr(self, name).\n\n__dir__()\nDefault dir() implementation.\n\n__eq__(value, /)\nReturn self==value.\n\n__format__(format_spec, /)\nDefault object formatter.\n\n__ge__(value, /)\nReturn self>=value.\n\n__getattr__(name)\n\n\n__getattribute__(name, /)\nReturn getattr(self, name).\n\n__getstate__()\n\n\n__gt__(value, /)\nReturn self>value.\n\n__hash__()\nReturn hash(self).\n\n__init__(in_channels, out_channels, kernel_size)\nCalls super().__setattr__(‘a’, a) instead of the typical self.a = a to avoid Module.__setattr__ overhead.\n\n__init_subclass__\nThis method is called when a class is subclassed.\n\n__le__(value, /)\nReturn self<=value.\n\n__lt__(value, /)\nReturn self<value.\n\n__ne__(value, /)\nReturn self!=value.\n\n__new__(**kwargs)\nCreate and return a new object.\n\n__reduce__()\nHelper for pickle.\n\n__reduce_ex__(protocol, /)\nHelper for pickle.\n\n__repr__()\nReturn repr(self).\n\n__setattr__(name, value)\nImplement setattr(self, name, value).\n\n__setstate__(state)\n\n\n__sizeof__()\nSize of object in memory, in bytes.\n\n__str__()\nReturn str(self).\n\n__subclasshook__\nAbstract classes can override this to customize issubclass().\n\n_apply(fn)\n\n\n_conv_forward(x, weight, bias)\n\n\n_get_backward_hooks()\n"
    },
    {
        "api_name": "oneflow.nn.QatConv2d",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.QatConv2d.html",
        "api_signature": "oneflow.nn.QatConv2d(in_channels: int, out_channels: int, kernel_size: Union[int, Tuple[int, int]], stride: Union[int, Tuple[int, int]] = 1, padding: Union[str, int, Tuple[int, int]] = 0, dilation: Union[int, Tuple[int, int]] = 1, groups: int = 1, bias: bool = True, padding_mode: str = 'zeros', quantization_formula: str = 'google', quantization_bit: int = 8, quantization_scheme: str = 'symmetric', weight_quant_per_layer: bool = True, input_quant_momentum: float = 0.95)",
        "api_description": "A Conv2d module attached with nn.MinMaxObserver, nn.MovingAverageMinMaxObserver and nn.FakeQuantization modules for weight and input,\nused for quantization aware training.\nThe parameters of QatConv2d are the same as Conv2d with some extra parameters for fake quantization,\nsee MinMaxObserver, MovingAverageMinMaxObserver and FakeQuantization for more details.",
        "return_value": "\n_get_name()\n\n\n_load_from_state_dict(state_dict, prefix, …)\n\n\n_maybe_warn_non_full_backward_hook(args, …)\n\n\n_named_members(get_members_fn[, prefix, recurse])\n\n\n_register_load_state_dict_pre_hook(hook[, …])\nThese hooks will be called with arguments: state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs, before loading state_dict into self.\n\n_register_state_dict_hook(hook)\nThese hooks will be called with arguments: self, state_dict, prefix, local_metadata, after the state_dict of self is set.\n\n_save_to_state_dict(destination, prefix, …)\n\n\n_shallow_repr()\n\n\n_to_memory_format(memory_format)\nCasts the parameters and buffers in this module to another memory format.\n\nadd_module(name, module)\nAdds a child module to the current module.\n\napply(fn)\nApplies fn recursively to every submodule (as returned by .children()) as well as self.\n\nbuffers([recurse])\n\nchildren()\n\ncpu()\nMoves all model parameters and buffers to the CPU.\n\ncuda([device])\nMoves all model parameters and buffers to the GPU.\n\ndouble()\nCasts all floating point parameters and buffers to double datatype.\n\neval()\nSets the module in evaluation mode.\n\nextra_repr()\nSet the extra representation of the module\n\nfloat()\nCasts all floating point parameters and buffers to float datatype.\n\nforward(x)\n\n\nget_parameter(target)\nReturn the parameter refenreced by target.\n\nget_submodule(target)\nGet submodule accroding to the name of submodule.\n\nhalf()\nCasts all floating point parameters and buffers to half datatype.\n\nload_state_dict(state_dict[, strict])\nCopies parameters and buffers from state_dict into this module and its descendants.\n\nmake_contiguous_params_group()\nGet contiguous parameters group after creating the whole module.\n\nmodules()\n\nnamed_buffers([prefix, recurse])\n\nnamed_children()\n\nnamed_modules([memo, prefix])\n\nnamed_parameters([prefix, recurse])\n\nparameters([recurse])\n\nregister_backward_hook(hook)\nRegisters a backward hook on the module.\n\nregister_buffer(name, tensor[, persistent])\nAdds a buffer to the module.\n\nregister_forward_hook(hook)\nRegisters a forward hook on the module.\n\nregister_forward_pre_hook(hook)\nRegisters a forward pre-hook on the module.\n\nregister_full_backward_hook(hook)\nRegisters a backward hook on the module.\n\nregister_parameter(name, param)\nAdds a parameter to the module.\n\nregister_state_dict_pre_hook(hook)\nThese hooks will be called with arguments: self, prefix, and keep_vars before calling state_dict on self.\n\nrequires_grad_([requires_grad])\nChange if autograd should record operations on parameters in this module.\n\nreset_parameters()\n\n\nstate_dict([destination, prefix, keep_vars])\n\nto(*args, **kwargs)\nMoves and/or casts the parameters and buffers.\n\nto_consistent(*args, **kwargs)\nThis interface is no longer available, please use oneflow.nn.Module.to_global() instead.\n\nto_empty(*, device)\nMoves the parameters and buffers to the specified device without copying storage.\n\nto_global([placement, sbp])\nConvert the parameters and buffers to global.\n\nto_local()\n\n\nto_memory_format(memory_format)\n\n\ntrain([mode])\nSets the module in training mode.\n\nzero_grad([set_to_none])\nSets gradients of all model parameters to zero.\n\n\n\nAttributes\n\n\n\n\n\n\n_grad_t\nalias of Union[Tuple[oneflow.Tensor, …], oneflow.Tensor]\n\n\n\n\n",
        "parameters": "\nin_channels (int) – Number of channels in the input image\nout_channels (int) – Number of channels produced by the convolution\nkernel_size (int or tuple) – Size of the convolving kernel\nstride (int or tuple, optional) – Stride of the convolution. Default: 1\npadding (int or tuple, optional) – Zero-padding added to both sides of\nthe input. Default: 0\ndilation (int or tuple, optional) – Spacing between kernel elements. Default: 1\ngroups (int, optional) – Number of blocked connections from input\nchannels to output channels. Default: 1\nbias (bool, optional) – If True, adds a learnable bias to the\noutput. Default: True\npadding_mode (string, optional) – 'zeros'. Default: 'zeros'\nquantization_formula (str) – Support “google” or “cambricon”.\nquantization_bit (int) – Quantize input to uintX / intX, X can be in range [2, 8]. Defaults to 8.\nquantization_scheme (str) – “symmetric” or “affine”, quantize to signed / unsigned integer. Defaults to “symmetric”.\nweight_quant_per_layer (bool) – True or False, means per-layer / per-channel for weight quantization. Defaults to True.\ninput_quant_momentum (float) – Smoothing parameter for exponential moving average operation for input quantization. Defaults to 0.95.\n\n\n\n\n",
        "input_shape": "Input: \\((N, C_{in}, H_{in}, W_{in})\\)\nOutput: \\((N, C_{out}, H_{out}, W_{out})\\) where\n\n\\[\\begin{split}H_{out} = \\\\left\\\\lfloor\\\\frac{H_{in}  + 2 \\\\times \\\\text{padding}[0] - \\\\text{dilation}[0]\n          \\\\times (\\\\text{kernel_size}[0] - 1) - 1}{\\\\text{stride}[0]} + 1\\\\right\\\\rfloor\\end{split}\\]\n\n\\[\\begin{split}W_{out} = \\\\left\\\\lfloor\\\\frac{W_{in}  + 2 \\\\times \\\\text{padding}[1] - \\\\text{dilation}[1]\n          \\\\times (\\\\text{kernel_size}[1] - 1) - 1}{\\\\text{stride}[1]} + 1\\\\right\\\\rfloor\\end{split}\\]\n\n\n\nAttr:\n\nweight (Tensor): the learnable weights of the module of shape\\((\\\\text{out_channels}, \\\\frac{\\\\text{in_channels}}{\\\\text{groups}},\\)\n\\(\\\\text{kernel_size[0]}, \\\\text{kernel_size[1]})\\).\nThe values of these weights are sampled from\n\\(\\\\mathcal{U}(-\\\\sqrt{k}, \\\\sqrt{k})\\) where\n\\(k = \\\\frac{groups}{C_\\\\text{in} * \\\\prod_{i=0}^{1}\\\\text{kernel_size}[i]}\\)\n\n\n\n\nbias (Tensor):   the learnable bias of the module of shape(out_channels). If bias is True,\nthen the values of these weights are\nsampled from \\(\\\\mathcal{U}(-\\\\sqrt{k}, \\\\sqrt{k})\\) where\n\\(k = \\\\frac{groups}{C_\\\\text{in} * \\\\prod_{i=0}^{1}\\\\text{kernel_size}[i]}\\)\n\n\n\n\n\n\n",
        "notes": "",
        "code_example": ">>> import numpy as np\n>>> import oneflow as flow\n>>> import oneflow.nn as nn\n\n>>> arr = np.random.randn(20, 16, 50, 100)\n>>> input = flow.Tensor(arr)\n>>> m = nn.QatConv2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2), dilation=(3, 1), quantization_formula=\"google\", quantization_bit=8, quantization_scheme=\"symmetric\")\n>>> output = m(input)\n\n\n\n\n__init__(in_channels: int, out_channels: int, kernel_size: Union[int, Tuple[int, int]], stride: Union[int, Tuple[int, int]] = 1, padding: Union[str, int, Tuple[int, int]] = 0, dilation: Union[int, Tuple[int, int]] = 1, groups: int = 1, bias: bool = True, padding_mode: str = 'zeros', quantization_formula: str = 'google', quantization_bit: int = 8, quantization_scheme: str = 'symmetric', weight_quant_per_layer: bool = True, input_quant_momentum: float = 0.95)¶\nCalls super().__setattr__(‘a’, a) instead of the typical self.a = a\nto avoid Module.__setattr__ overhead. Module’s __setattr__ has special\nhandling for parameters, submodules, and buffers but simply calls into\nsuper().__setattr__ for all other attributes.\n\nMethods\n\n\n\n\n\n\n__call__(*args, **kwargs)\nCall self as a function.\n\n__delattr__(name)\nImplement delattr(self, name).\n\n__dir__()\nDefault dir() implementation.\n\n__eq__(value, /)\nReturn self==value.\n\n__format__(format_spec, /)\nDefault object formatter.\n\n__ge__(value, /)\nReturn self>=value.\n\n__getattr__(name)\n\n\n__getattribute__(name, /)\nReturn getattr(self, name).\n\n__getstate__()\n\n\n__gt__(value, /)\nReturn self>value.\n\n__hash__()\nReturn hash(self).\n\n__init__(in_channels, out_channels, kernel_size)\nCalls super().__setattr__(‘a’, a) instead of the typical self.a = a to avoid Module.__setattr__ overhead.\n\n__init_subclass__\nThis method is called when a class is subclassed.\n\n__le__(value, /)\nReturn self<=value.\n\n__lt__(value, /)\nReturn self<value.\n\n__ne__(value, /)\nReturn self!=value.\n\n__new__(**kwargs)\nCreate and return a new object.\n\n__reduce__()\nHelper for pickle.\n\n__reduce_ex__(protocol, /)\nHelper for pickle.\n\n__repr__()\nReturn repr(self).\n\n__setattr__(name, value)\nImplement setattr(self, name, value).\n\n__setstate__(state)\n\n\n__sizeof__()\nSize of object in memory, in bytes.\n\n__str__()\nReturn str(self).\n\n__subclasshook__\nAbstract classes can override this to customize issubclass().\n\n_apply(fn)\n\n\n_conv_forward(x, weight, bias)\n\n\n_get_backward_hooks()\n"
    },
    {
        "api_name": "oneflow.nn.QatConv3d",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.QatConv3d.html",
        "api_signature": "oneflow.nn.QatConv3d(in_channels: int, out_channels: int, kernel_size: Union[int, Tuple[int, int, int]], stride: Union[int, Tuple[int, int, int]] = 1, padding: Union[str, int, Tuple[int, int, int]] = 0, dilation: Union[int, Tuple[int, int, int]] = 1, groups: int = 1, bias: bool = True, padding_mode: str = 'zeros', quantization_formula: str = 'google', quantization_bit: int = 8, quantization_scheme: str = 'symmetric', weight_quant_per_layer: bool = True, input_quant_momentum: float = 0.95)",
        "api_description": "A Conv3d module attached with nn.MinMaxObserver, nn.MovingAverageMinMaxObserver and nn.FakeQuantization modules for weight and input,\nused for quantization aware training.\nThe parameters of QatConv3d are the same as Conv3d with some extra parameters for fake quantization,\nsee MinMaxObserver, MovingAverageMinMaxObserver and FakeQuantization for more details.",
        "return_value": "\n_get_name()\n\n\n_load_from_state_dict(state_dict, prefix, …)\n\n\n_maybe_warn_non_full_backward_hook(args, …)\n\n\n_named_members(get_members_fn[, prefix, recurse])\n\n\n_register_load_state_dict_pre_hook(hook[, …])\nThese hooks will be called with arguments: state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs, before loading state_dict into self.\n\n_register_state_dict_hook(hook)\nThese hooks will be called with arguments: self, state_dict, prefix, local_metadata, after the state_dict of self is set.\n\n_save_to_state_dict(destination, prefix, …)\n\n\n_shallow_repr()\n\n\n_to_memory_format(memory_format)\nCasts the parameters and buffers in this module to another memory format.\n\nadd_module(name, module)\nAdds a child module to the current module.\n\napply(fn)\nApplies fn recursively to every submodule (as returned by .children()) as well as self.\n\nbuffers([recurse])\n\nchildren()\n\ncpu()\nMoves all model parameters and buffers to the CPU.\n\ncuda([device])\nMoves all model parameters and buffers to the GPU.\n\ndouble()\nCasts all floating point parameters and buffers to double datatype.\n\neval()\nSets the module in evaluation mode.\n\nextra_repr()\nSet the extra representation of the module\n\nfloat()\nCasts all floating point parameters and buffers to float datatype.\n\nforward(x)\n\n\nget_parameter(target)\nReturn the parameter refenreced by target.\n\nget_submodule(target)\nGet submodule accroding to the name of submodule.\n\nhalf()\nCasts all floating point parameters and buffers to half datatype.\n\nload_state_dict(state_dict[, strict])\nCopies parameters and buffers from state_dict into this module and its descendants.\n\nmake_contiguous_params_group()\nGet contiguous parameters group after creating the whole module.\n\nmodules()\n\nnamed_buffers([prefix, recurse])\n\nnamed_children()\n\nnamed_modules([memo, prefix])\n\nnamed_parameters([prefix, recurse])\n\nparameters([recurse])\n\nregister_backward_hook(hook)\nRegisters a backward hook on the module.\n\nregister_buffer(name, tensor[, persistent])\nAdds a buffer to the module.\n\nregister_forward_hook(hook)\nRegisters a forward hook on the module.\n\nregister_forward_pre_hook(hook)\nRegisters a forward pre-hook on the module.\n\nregister_full_backward_hook(hook)\nRegisters a backward hook on the module.\n\nregister_parameter(name, param)\nAdds a parameter to the module.\n\nregister_state_dict_pre_hook(hook)\nThese hooks will be called with arguments: self, prefix, and keep_vars before calling state_dict on self.\n\nrequires_grad_([requires_grad])\nChange if autograd should record operations on parameters in this module.\n\nreset_parameters()\n\n\nstate_dict([destination, prefix, keep_vars])\n\nto(*args, **kwargs)\nMoves and/or casts the parameters and buffers.\n\nto_consistent(*args, **kwargs)\nThis interface is no longer available, please use oneflow.nn.Module.to_global() instead.\n\nto_empty(*, device)\nMoves the parameters and buffers to the specified device without copying storage.\n\nto_global([placement, sbp])\nConvert the parameters and buffers to global.\n\nto_local()\n\n\nto_memory_format(memory_format)\n\n\ntrain([mode])\nSets the module in training mode.\n\nzero_grad([set_to_none])\nSets gradients of all model parameters to zero.\n\n\n\nAttributes\n\n\n\n\n\n\n_grad_t\nalias of Union[Tuple[oneflow.Tensor, …], oneflow.Tensor]\n\n\n\n\n",
        "parameters": "\nin_channels (int) – Number of channels in the input image\nout_channels (int) – Number of channels produced by the convolution\nkernel_size (int or tuple) – Size of the convolving kernel\nstride (int or tuple, optional) – Stride of the convolution. Default: 1\npadding (int, tuple or str, optional) – Padding added to all six sides of\nthe input. Default: 0\ndilation (int or tuple, optional) – Spacing between kernel elements. Default: 1\ngroups (int, optional) – Number of blocked connections from input channels to output channels. Default: 1\nbias (bool, optional) – If True, adds a learnable bias to the output. Default: True\npadding_mode (string, optional) – 'zeros'. Default: 'zeros'\nquantization_formula (str) – Support “google” or “cambricon”.\nquantization_bit (int) – Quantize input to uintX / intX, X can be in range [2, 8]. Defaults to 8.\nquantization_scheme (str) – “symmetric” or “affine”, quantize to signed / unsigned integer. Defaults to “symmetric”.\nweight_quant_per_layer (bool) – True or False, means per-layer / per-channel for weight quantization. Defaults to True.\ninput_quant_momentum (float) – Smoothing parameter for exponential moving average operation for input quantization. Defaults to 0.95.\n\n\n\n\n",
        "input_shape": "Input: \\((N, C_{in}, D_{in}, H_{in}, W_{in})\\)\nOutput: \\((N, C_{out}, D_{out}, H_{out}, W_{out})\\) where\n\n\\[D_{out} = \\left\\lfloor\\frac{D_{in} + 2 \\times \\text{padding}[0] - \\text{dilation}[0]\n      \\times (\\text{kernel\\_size}[0] - 1) - 1}{\\text{stride}[0]} + 1\\right\\rfloor\\]\n\n\\[H_{out} = \\left\\lfloor\\frac{H_{in} + 2 \\times \\text{padding}[1] - \\text{dilation}[1]\n      \\times (\\text{kernel\\_size}[1] - 1) - 1}{\\text{stride}[1]} + 1\\right\\rfloor\\]\n\n\\[W_{out} = \\left\\lfloor\\frac{W_{in} + 2 \\times \\text{padding}[2] - \\text{dilation}[2]\n      \\times (\\text{kernel\\_size}[2] - 1) - 1}{\\text{stride}[2]} + 1\\right\\rfloor\\]\n\n\n\n\n\n\nweight¶\nthe learnable weights of the module of shape\n\\((\\text{out\\_channels}, \\frac{\\text{in\\_channels}}{\\text{groups}},\\)\n\\(\\text{kernel\\_size[0]}, \\text{kernel\\_size[1]}, \\text{kernel\\_size[2]})\\).\nThe values of these weights are sampled from\n\\(\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})\\) where\n\\(k = \\frac{groups}{C_\\text{in} * \\prod_{i=0}^{2}\\text{kernel\\_size}[i]}\\)\n\nType\nTensor\n\n\n\n\n\nbias¶\nthe learnable bias of the module of shape (out_channels). If bias is True,\nthen the values of these weights are\nsampled from \\(\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})\\) where\n\\(k = \\frac{groups}{C_\\text{in} * \\prod_{i=0}^{2}\\text{kernel\\_size}[i]}\\)\n\nType\nTensor\n\n\n\n",
        "notes": "",
        "code_example": ">>> import numpy as np\n>>> import oneflow as flow\n>>> import oneflow.nn as nn\n\n>>> arr = np.random.randn(1, 2, 5, 5, 5)\n>>> input = flow.Tensor(arr)\n>>> m = nn.QatConv3d(2, 4, kernel_size=3, stride=1, quantization_formula=\"google\", quantization_bit=8, quantization_scheme=\"symmetric\")\n>>> output = m(input)\n\n\n\n\n__init__(in_channels: int, out_channels: int, kernel_size: Union[int, Tuple[int, int, int]], stride: Union[int, Tuple[int, int, int]] = 1, padding: Union[str, int, Tuple[int, int, int]] = 0, dilation: Union[int, Tuple[int, int, int]] = 1, groups: int = 1, bias: bool = True, padding_mode: str = 'zeros', quantization_formula: str = 'google', quantization_bit: int = 8, quantization_scheme: str = 'symmetric', weight_quant_per_layer: bool = True, input_quant_momentum: float = 0.95)¶\nCalls super().__setattr__(‘a’, a) instead of the typical self.a = a\nto avoid Module.__setattr__ overhead. Module’s __setattr__ has special\nhandling for parameters, submodules, and buffers but simply calls into\nsuper().__setattr__ for all other attributes.\n\nMethods\n\n\n\n\n\n\n__call__(*args, **kwargs)\nCall self as a function.\n\n__delattr__(name)\nImplement delattr(self, name).\n\n__dir__()\nDefault dir() implementation.\n\n__eq__(value, /)\nReturn self==value.\n\n__format__(format_spec, /)\nDefault object formatter.\n\n__ge__(value, /)\nReturn self>=value.\n\n__getattr__(name)\n\n\n__getattribute__(name, /)\nReturn getattr(self, name).\n\n__getstate__()\n\n\n__gt__(value, /)\nReturn self>value.\n\n__hash__()\nReturn hash(self).\n\n__init__(in_channels, out_channels, kernel_size)\nCalls super().__setattr__(‘a’, a) instead of the typical self.a = a to avoid Module.__setattr__ overhead.\n\n__init_subclass__\nThis method is called when a class is subclassed.\n\n__le__(value, /)\nReturn self<=value.\n\n__lt__(value, /)\nReturn self<value.\n\n__ne__(value, /)\nReturn self!=value.\n\n__new__(**kwargs)\nCreate and return a new object.\n\n__reduce__()\nHelper for pickle.\n\n__reduce_ex__(protocol, /)\nHelper for pickle.\n\n__repr__()\nReturn repr(self).\n\n__setattr__(name, value)\nImplement setattr(self, name, value).\n\n__setstate__(state)\n\n\n__sizeof__()\nSize of object in memory, in bytes.\n\n__str__()\nReturn str(self).\n\n__subclasshook__\nAbstract classes can override this to customize issubclass().\n\n_apply(fn)\n\n\n_conv_forward(x, weight, bias)\n\n\n_get_backward_hooks()\n"
    },
    {
        "api_name": "oneflow.nn.utils.clip_grad_norm_",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.utils.clip_grad_norm_.html",
        "api_signature": "oneflow.nn.utils.clip_grad_norm_(parameters: Union[oneflow.Tensor, Iterable[oneflow.Tensor]], max_norm: float, norm_type: float = 2.0, fused: bool = False, error_if_nonfinite: bool = False)",
        "api_description": "Clips gradient norm of an iterable of parameters.\nThe norm is computed over all gradients together, as if they were\nconcatenated into a single vector.",
        "return_value": "",
        "parameters": "\nparameters (Iterable[Tensor] or Tensor) – an iterable of Tensors or a\nsingle Tensor that will have gradients normalized\nmax_norm (float or int) – max norm of the gradients\nnorm_type (float or int) – type of the used p-norm. Can be 'inf' for\ninfinity norm.\nerror_if_nonfinite (bool) – if True, an error is thrown if the total\nnorm of the gradients from :attr:parameters is nan,\ninf, or -inf. Default: False (will switch to True in the future)\n\n\nTotal norm of the parameters (viewed as a single vector).\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> import numpy as np\n>>> x1 = flow.tensor(np.array([[2, 3, 4], [1.5, 2.6, 3.7]]).astype(np.float32), requires_grad=True)\n>>> m1 = flow.nn.ReLU()\n>>> out1 = m1(x1)\n>>> out1 = out1.sum()\n>>> out1.backward()\n>>> norm1 = flow.nn.utils.clip_grad_norm_(x1, 0.6, 1.0)\n>>> norm1\ntensor(6., dtype=oneflow.float32)\n>>> x1.grad\ntensor([[0.1000, 0.1000, 0.1000],\n        [0.1000, 0.1000, 0.1000]], dtype=oneflow.float32)\n>>> x2 = flow.tensor(np.array([[-2, -3, -4], [2.5, 0, 3.2]]).astype(np.float32), requires_grad=True)\n>>> out2 = flow.atan(x2)\n>>> out2 = out2.sum()\n>>> out2.backward()\n>>> norm2 = flow.nn.utils.clip_grad_norm_(x2, 0.5)\n>>> norm2\ntensor(1.0394, dtype=oneflow.float32)\n>>> x2.grad\ntensor([[0.0962, 0.0481, 0.0283],\n        [0.0663, 0.4810, 0.0428]], dtype=oneflow.float32)\n\n\n\n"
    },
    {
        "api_name": "oneflow.nn.utils.clip_grad_value_",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.utils.clip_grad_value_.html",
        "api_signature": "oneflow.nn.utils.clip_grad_value_(parameters: Union[oneflow.Tensor, Iterable[oneflow.Tensor]], clip_value: float)",
        "api_description": "Clips gradient of an iterable of parameters at specified value.\nGradients are modified in-place.",
        "return_value": "",
        "parameters": "\nparameters (Iterable[Tensor] or Tensor) – an iterable of Tensors or a\nsingle Tensor that will have gradients normalized\nclip_value (float or int) – maximum allowed value of the gradients.\nThe gradients are clipped in the range\n\\(\\left[\\text{-clip\\_value}, \\text{clip\\_value}\\right]\\)\n\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.nn.utils.weight_norm",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.utils.weight_norm.html",
        "api_signature": "oneflow.nn.utils.weight_norm(module: T_module, name: str = 'weight', dim: int = 0)",
        "api_description": "Applies weight normalization to a parameter in the given module.\n\\[\\mathbf{w}=g \\frac{\\mathbf{v}}{\\|\\mathbf{v}\\|}\\]\nWeight normalization is a reparameterization that decouples the magnitude\nof a weight tensor from its direction. This replaces the parameter specified\nby name (e.g. 'weight') with two parameters: one specifying the magnitude\n(e.g. 'weight_g') and one specifying the direction (e.g. 'weight_v').\nWeight normalization is implemented via a hook that recomputes the weight\ntensor from the magnitude and direction before every forward()\ncall.\nBy default, with dim=0, the norm is computed independently per output\nchannel/plane. To compute a norm over the entire weight tensor, use\ndim=None.\nSee https://arxiv.org/abs/1602.07868",
        "return_value": "The original module with the weight norm hook\n\n\n",
        "parameters": "\nmodule (Module) – containing module\nname (str, optional) – name of weight parameter\ndim (int, optional) – dimension over which to compute the norm\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> m = flow.nn.utils.weight_norm(flow.nn.Linear(20, 40), name='weight')\n>>> m\nLinear(in_features=20, out_features=40, bias=True)\n>>> m.weight_g.size()\noneflow.Size([40, 1])\n>>> m.weight_v.size()\noneflow.Size([40, 20])\n\n\n\n"
    },
    {
        "api_name": "oneflow.nn.utils.remove_weight_norm",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.utils.remove_weight_norm.html",
        "api_signature": "oneflow.nn.utils.remove_weight_norm(module: T_module, name: str = 'weight')",
        "api_description": "Removes the weight normalization reparameterization from a module.",
        "return_value": "",
        "parameters": "\nmodule (Module) – containing module\nname (str, optional) – name of weight parameter\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> m = flow.nn.utils.weight_norm(flow.nn.Linear(20, 40))\n>>> flow.nn.utils.remove_weight_norm(m)\nLinear(in_features=20, out_features=40, bias=True)\n\n\n\n"
    },
    {
        "api_name": "oneflow.nn.utils.rnn.PackedSequence",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.utils.rnn.PackedSequence.html",
        "api_signature": "oneflow.nn.utils.rnn.PackedSequence(data: oneflow.Tensor, batch_sizes: Optional[oneflow.Tensor] = None, sorted_indices: Optional[oneflow.Tensor] = None, unsorted_indices: Optional[oneflow.Tensor] = None)",
        "api_description": "Holds the data and list of batch_sizes of a packed sequence.\nAll RNN modules accept packed sequences as inputs.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "Instances of this class should never be created manually. They are meant\nto be instantiated by functions like pack_padded_sequence().\nBatch sizes represent the number elements at each sequence step in\nthe batch, not the varying sequence lengths passed to\npack_padded_sequence().  For instance, given data abc and x\nthe PackedSequence would contain data axbc with\nbatch_sizes=[2,1,1].\ndata¶\nTensor containing packed sequence\nType\nTensor\nbatch_sizes¶\nTensor of integers holding\ninformation about the batch size at each sequence step\nType\nTensor\nsorted_indices¶\nTensor of integers holding how this\nPackedSequence is constructed from sequences.\nType\nTensor, optional\nunsorted_indices¶\nTensor of integers holding how this\nto recover the original sequences with correct order.\nType\nTensor, optional\ndata can be on arbitrary device and of arbitrary dtype.\nsorted_indices and unsorted_indices must be oneflow.int64\ntensors on the same device as data.\nHowever, batch_sizes should always be a CPU oneflow.int64 tensor.\nThis invariant is maintained throughout PackedSequence class,\n(i.e., they only pass in tensors conforming to this constraint).",
        "code_example": ""
    },
    {
        "api_name": "oneflow.nn.utils.rnn.pack_padded_sequence",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.utils.rnn.pack_padded_sequence.html",
        "api_signature": "oneflow.nn.utils.rnn.pack_padded_sequence(input: oneflow.Tensor, lengths: oneflow.Tensor, batch_first: bool = False, enforce_sorted: bool = True)",
        "api_description": "Packs a Tensor containing padded sequences of variable length.\ninput can be of size T x B x * where T is the length of the\nlongest sequence (equal to lengths[0]), B is the batch size, and\n* is any number of dimensions (including 0). If batch_first is\nTrue, B x T x * input is expected.\nFor unsorted sequences, use enforce_sorted = False. If enforce_sorted is\nTrue, the sequences should be sorted by length in a decreasing order, i.e.\ninput[:,0] should be the longest sequence, and input[:,B-1] the shortest\none. enforce_sorted = True is only necessary for ONNX export.",
        "return_value": "a PackedSequence object\n\n\n\n",
        "parameters": "\ninput (Tensor) – padded batch of variable length sequences.\nlengths (Tensor or list(int)) – list of sequence lengths of each batch\nelement (must be on the CPU if provided as a tensor).\nbatch_first (bool, optional) – if True, the input is expected in B x T x *\nformat.\nenforce_sorted (bool, optional) – if True, the input is expected to\ncontain sequences sorted by length in a decreasing order. If\nFalse, the input will get sorted unconditionally. Default: True.\n\n\n",
        "input_shape": "",
        "notes": "This function accepts any input that has at least two dimensions. You\ncan apply it to pack the labels, and use the output of the RNN with\nthem to compute the loss directly. A Tensor can be retrieved from\na PackedSequence object by accessing its .data attribute.",
        "code_example": ""
    },
    {
        "api_name": "oneflow.nn.utils.rnn.pad_packed_sequence",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.utils.rnn.pad_packed_sequence.html",
        "api_signature": "oneflow.nn.utils.rnn.pad_packed_sequence(sequence: oneflow.nn.utils.rnn.PackedSequence, batch_first: bool = False, padding_value: float = 0.0, total_length: Optional[int] = None)",
        "api_description": "Pads a packed batch of variable length sequences.\nIt is an inverse operation to pack_padded_sequence().\nThe returned Tensor’s data will be of size T x B x *, where T is the length\nof the longest sequence and B is the batch size. If batch_first is True,\nthe data will be transposed into B x T x * format.",
        "return_value": "Tuple of Tensor containing the padded sequence, and a Tensor\ncontaining the list of lengths of each sequence in the batch.\nBatch elements will be re-ordered as they were ordered originally when\nthe batch was passed to pack_padded_sequence or pack_sequence.\n\n\n",
        "parameters": "\nsequence (PackedSequence) – batch to pad\nbatch_first (bool, optional) – if True, the output will be in B x T x *\nformat.\npadding_value (float, optional) – values for padded elements.\ntotal_length (int, optional) – if not None, the output will be padded to\nhave length total_length. This method will throw ValueError\nif total_length is less than the max sequence length in\nsequence.\n\n\n",
        "input_shape": "",
        "notes": "total_length is useful to implement the\npack sequence -> recurrent network -> unpack sequence pattern in a\nModule wrapped in DataParallel.",
        "code_example": ">>> from oneflow.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n>>> import oneflow as flow\n\n>>> seq = flow.tensor([[4,5,6], [1,2,0], [3,0,0]])\n>>> lens = [3, 2, 1]\n>>> packed = pack_padded_sequence(seq, lens, batch_first=True, enforce_sorted=True)\n>>> packed.data\ntensor([4, 1, 3, 5, 2, 6], dtype=oneflow.int64)\n>>> packed.batch_sizes\ntensor([3, 2, 1], dtype=oneflow.int64)\n>>> seq_unpacked, lens_unpacked = pad_packed_sequence(packed, batch_first=True)\n>>> seq_unpacked\ntensor([[4, 5, 6],\n        [1, 2, 0],\n        [3, 0, 0]], dtype=oneflow.int64)\n>>> lens_unpacked\ntensor([3., 2., 1.], dtype=oneflow.float32)\n\n\n\n"
    },
    {
        "api_name": "oneflow.nn.utils.rnn.pad_sequence",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.utils.rnn.pad_sequence.html",
        "api_signature": "oneflow.nn.utils.rnn.pad_sequence(sequences: Union[oneflow.Tensor, List[oneflow.Tensor]], batch_first: bool = False, padding_value: float = 0.0)",
        "api_description": "Pad a list of variable length Tensors with padding_value\npad_sequence stacks a list of Tensors along a new dimension,\nand pads them to equal length. For example, if the input is list of\nsequences with size L x * and if batch_first is False, and T x B x *\notherwise.\nB is batch size. It is equal to the number of elements in sequences.\nT is length of the longest sequence.\nL is length of the sequence.\n* is any number of trailing dimensions, including none.",
        "return_value": "Tensor of size T x B x * if batch_first is False.\nTensor of size B x T x * otherwise\n\n\n",
        "parameters": "\nsequences (list[Tensor]) – list of variable length sequences.\nbatch_first (bool, optional) – output will be in B x T x * if True, or in\nT x B x * otherwise. Default: False.\npadding_value (float, optional) – value for padded elements. Default: 0.\n\n\n",
        "input_shape": "",
        "notes": "This function returns a Tensor of size T x B x * or B x T x *\nwhere T is the length of the longest sequence. This function assumes\ntrailing dimensions and type of all the Tensors in sequences are same.",
        "code_example": ">>> from oneflow.nn.utils.rnn import pad_sequence\n>>> import oneflow as flow\n\n>>> a = flow.ones(25, 300)\n>>> b = flow.ones(22, 300)\n>>> c = flow.ones(15, 300)\n>>> out = pad_sequence([a, b, c])\n>>> out.size()\noneflow.Size([25, 3, 300])\n\n\n\n"
    },
    {
        "api_name": "oneflow.nn.utils.rnn.pack_sequence",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.utils.rnn.pack_sequence.html",
        "api_signature": "oneflow.nn.utils.rnn.pack_sequence(sequences: List[oneflow.Tensor], enforce_sorted: bool = True)",
        "api_description": "Packs a list of variable length Tensors\nConsecutive call of the next functions: pad_sequence, pack_padded_sequence.\nsequences should be a list of Tensors of size L x *, where L is\nthe length of a sequence and * is any number of trailing dimensions,\nincluding zero.\nFor unsorted sequences, use enforce_sorted = False. If enforce_sorted\nis True, the sequences should be sorted in the order of decreasing length.\nenforce_sorted = True is only necessary for ONNX export.",
        "return_value": "a PackedSequence object\n\n\n",
        "parameters": "\nsequences (list[Tensor]) – A list of sequences of decreasing length.\nenforce_sorted (bool, optional) – if True, checks that the input\ncontains sequences sorted by length in a decreasing order. If\nFalse, this condition is not checked. Default: True.\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> from oneflow.nn.utils.rnn import pack_sequence\n>>> import oneflow as flow\n\n>>> a = flow.tensor([1,2,3])\n>>> b = flow.tensor([4,5])\n>>> c = flow.tensor([6])\n>>> packed = pack_sequence([a, b, c])\n>>> packed.data\ntensor([1, 4, 6, 2, 5, 3], dtype=oneflow.int64)\n>>> packed.batch_sizes\ntensor([3, 2, 1], dtype=oneflow.int64)\n\n\n\n"
    },
    {
        "api_name": "oneflow.nn.Flatten",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.Flatten.html",
        "api_signature": "oneflow.nn.Flatten(start_dim: int = 1, end_dim: int = - 1)",
        "api_description": "Flattens a contiguous range of dims into a tensor. For use with: nn.Sequential.",
        "return_value": "",
        "parameters": "\nstart_dim – first dim to flatten (default = 1).\nend_dim – last dim to flatten (default = -1).\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> input = flow.Tensor(32, 1, 5, 5)\n>>> m = flow.nn.Flatten()\n>>> output = m(input)\n>>> output.shape\noneflow.Size([32, 25])\n\n\n\n"
    },
    {
        "api_name": "oneflow.nn.FakeQuantization",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.FakeQuantization.html",
        "api_signature": "oneflow.nn.FakeQuantization(quantization_formula: str = 'google', quantization_bit: int = 8, quantization_scheme: str = 'symmetric')",
        "api_description": "Simulate the quantize and dequantize operations in training time.\nThe output will be computed as:\nif quantization_scheme == “symmetric”:\n\\[ \\begin{align}\\begin{aligned}& quant\\_max = 2^{quantization\\_to\\_bit - 1} - 1\\\\& quant\\_min = -quant\\_max\\\\& clamp(round(x / scale), quant\\_min, quant\\_max) * scale\\end{aligned}\\end{align} \\]\nelif quantization_scheme == “affine”:\n\\[ \\begin{align}\\begin{aligned}& quant\\_max = 2^{quantization\\_to\\_bit} - 1\\\\& quant\\_min = 0\\\\& (clamp(round(x / scale + zero\\_point), quant\\_min, quant\\_max) - zero\\_point) * scale\\end{aligned}\\end{align} \\]",
        "return_value": "Input tensor after quantize and dequantize operations.\n\n\n_get_name()\n\n\n_load_from_state_dict(state_dict, prefix, …)\n\n\n_maybe_warn_non_full_backward_hook(args, …)\n\n\n_named_members(get_members_fn[, prefix, recurse])\n\n\n_register_load_state_dict_pre_hook(hook[, …])\nThese hooks will be called with arguments: state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs, before loading state_dict into self.\n\n_register_state_dict_hook(hook)\nThese hooks will be called with arguments: self, state_dict, prefix, local_metadata, after the state_dict of self is set.\n\n_save_to_state_dict(destination, prefix, …)\n\n\n_shallow_repr()\n\n\n_to_memory_format(memory_format)\nCasts the parameters and buffers in this module to another memory format.\n\nadd_module(name, module)\nAdds a child module to the current module.\n\napply(fn)\nApplies fn recursively to every submodule (as returned by .children()) as well as self.\n\nbuffers([recurse])\n\nchildren()\n\ncpu()\nMoves all model parameters and buffers to the CPU.\n\ncuda([device])\nMoves all model parameters and buffers to the GPU.\n\ndouble()\nCasts all floating point parameters and buffers to double datatype.\n\neval()\nSets the module in evaluation mode.\n\nextra_repr()\nSet the extra representation of the module\n\nfloat()\nCasts all floating point parameters and buffers to float datatype.\n\nforward(input, scale, zero_point)\n\n\nget_parameter(target)\nReturn the parameter refenreced by target.\n\nget_submodule(target)\nGet submodule accroding to the name of submodule.\n\nhalf()\nCasts all floating point parameters and buffers to half datatype.\n\nload_state_dict(state_dict[, strict])\nCopies parameters and buffers from state_dict into this module and its descendants.\n\nmake_contiguous_params_group()\nGet contiguous parameters group after creating the whole module.\n\nmodules()\n\nnamed_buffers([prefix, recurse])\n\nnamed_children()\n\nnamed_modules([memo, prefix])\n\nnamed_parameters([prefix, recurse])\n\nparameters([recurse])\n\nregister_backward_hook(hook)\nRegisters a backward hook on the module.\n\nregister_buffer(name, tensor[, persistent])\nAdds a buffer to the module.\n\nregister_forward_hook(hook)\nRegisters a forward hook on the module.\n\nregister_forward_pre_hook(hook)\nRegisters a forward pre-hook on the module.\n\nregister_full_backward_hook(hook)\nRegisters a backward hook on the module.\n\nregister_parameter(name, param)\nAdds a parameter to the module.\n\nregister_state_dict_pre_hook(hook)\nThese hooks will be called with arguments: self, prefix, and keep_vars before calling state_dict on self.\n\nrequires_grad_([requires_grad])\nChange if autograd should record operations on parameters in this module.\n\nstate_dict([destination, prefix, keep_vars])\n\nto(*args, **kwargs)\nMoves and/or casts the parameters and buffers.\n\nto_consistent(*args, **kwargs)\nThis interface is no longer available, please use oneflow.nn.Module.to_global() instead.\n\nto_empty(*, device)\nMoves the parameters and buffers to the specified device without copying storage.\n\nto_global([placement, sbp])\nConvert the parameters and buffers to global.\n\nto_local()\n\n\nto_memory_format(memory_format)\n\n\ntrain([mode])\nSets the module in training mode.\n\nzero_grad([set_to_none])\nSets gradients of all model parameters to zero.\n\n\n\nAttributes\n\n\n\n\n\n\n_grad_t\nalias of Union[Tuple[oneflow.Tensor, …], oneflow.Tensor]\n\n\n\n\n",
        "parameters": "\ninput (oneflow.Tensor) – the input value(s), in oneflow.float32.\nscale (oneflow.Tensor) – quantization scale.\nzero_point (oneflow.Tensor) – quantization zero_point.\nquantization_bit (int) – Quantize input to uintX / intX, X can be in range [2, 8]. Defaults to 8.\nquantization_scheme (str) – “symmetric” or “affine”, quantize to signed / unsigned integer. Defaults to “symmetric”.\nquantization_formula (str) – Support “google” or “cambricon”.\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import numpy as np\n>>> import oneflow as flow\n\n>>> weight = (np.random.random((2, 3, 4, 5)) - 0.5).astype(np.float32)\n\n>>> input_tensor = flow.tensor(\n...    weight, dtype=flow.float32\n... )\n\n>>> quantization_bit = 8\n>>> quantization_scheme = \"symmetric\"\n>>> quantization_formula = \"google\"\n>>> per_layer_quantization = True\n\n>>> min_max_observer = flow.nn.MinMaxObserver(quantization_formula=quantization_formula, quantization_bit=quantization_bit,\n... quantization_scheme=quantization_scheme, per_layer_quantization=per_layer_quantization)\n>>> fake_quantization = flow.nn.FakeQuantization(quantization_formula=quantization_formula, quantization_bit=quantization_bit,\n... quantization_scheme=quantization_scheme)\n\n>>> scale, zero_point = min_max_observer(\n...    input_tensor,\n... )\n\n>>> output_tensor = fake_quantization(\n...    input_tensor,\n...    scale,\n...    zero_point,\n... )\n\n\n\n\n__init__(quantization_formula: str = 'google', quantization_bit: int = 8, quantization_scheme: str = 'symmetric') → None¶\nCalls super().__setattr__(‘a’, a) instead of the typical self.a = a\nto avoid Module.__setattr__ overhead. Module’s __setattr__ has special\nhandling for parameters, submodules, and buffers but simply calls into\nsuper().__setattr__ for all other attributes.\n\nMethods\n\n\n\n\n\n\n__call__(*args, **kwargs)\nCall self as a function.\n\n__delattr__(name)\nImplement delattr(self, name).\n\n__dir__()\nDefault dir() implementation.\n\n__eq__(value, /)\nReturn self==value.\n\n__format__(format_spec, /)\nDefault object formatter.\n\n__ge__(value, /)\nReturn self>=value.\n\n__getattr__(name)\n\n\n__getattribute__(name, /)\nReturn getattr(self, name).\n\n__getstate__()\n\n\n__gt__(value, /)\nReturn self>value.\n\n__hash__()\nReturn hash(self).\n\n__init__([quantization_formula, …])\nCalls super().__setattr__(‘a’, a) instead of the typical self.a = a to avoid Module.__setattr__ overhead.\n\n__init_subclass__\nThis method is called when a class is subclassed.\n\n__le__(value, /)\nReturn self<=value.\n\n__lt__(value, /)\nReturn self<value.\n\n__ne__(value, /)\nReturn self!=value.\n\n__new__(**kwargs)\nCreate and return a new object.\n\n__reduce__()\nHelper for pickle.\n\n__reduce_ex__(protocol, /)\nHelper for pickle.\n\n__repr__()\nReturn repr(self).\n\n__setattr__(name, value)\nImplement setattr(self, name, value).\n\n__setstate__(state)\n\n\n__sizeof__()\nSize of object in memory, in bytes.\n\n__str__()\nReturn str(self).\n\n__subclasshook__\nAbstract classes can override this to customize issubclass().\n\n_apply(fn)\n\n\n_get_backward_hooks()\n"
    },
    {
        "api_name": "oneflow.nn.MinMaxObserver",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.MinMaxObserver.html",
        "api_signature": "oneflow.nn.MinMaxObserver(quantization_formula: str = 'google', quantization_bit: int = 8, quantization_scheme: str = 'symmetric', per_layer_quantization: bool = True)",
        "api_description": "Compute the quantization parameters of the input tensor.\nFirst compute the max and min values of input tensor:\n\\[ \\begin{align}\\begin{aligned}& max\\_value = max(input)\\\\& min\\_value = min(input)\\end{aligned}\\end{align} \\]\nThen compute the scale and zero_point with the following equations:\nif quantization_scheme == “symmetric”:\n\\[ \\begin{align}\\begin{aligned}& denom = 2^{quantization\\_to\\_bit - 1} - 1\\\\& scale = max(|max\\_value|,|min\\_value|) / denom\\\\& zero\\_point = 0\\end{aligned}\\end{align} \\]\nelif quantization_scheme == “affine”:\n\\[ \\begin{align}\\begin{aligned}& denom = 2^{quantization\\_to\\_bit} - 1\\\\& scale = (max\\_value - min\\_value) / denom\\\\& zero\\_point = -min\\_value / scale\\end{aligned}\\end{align} \\]\nIf per_layer_quantization is False, then the shape of scale and zero_point will be (input.shape[0],).",
        "return_value": "The scale and zero_point of input tensor.\n\n\n_get_name()\n\n\n_load_from_state_dict(state_dict, prefix, …)\n\n\n_maybe_warn_non_full_backward_hook(args, …)\n\n\n_named_members(get_members_fn[, prefix, recurse])\n\n\n_register_load_state_dict_pre_hook(hook[, …])\nThese hooks will be called with arguments: state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs, before loading state_dict into self.\n\n_register_state_dict_hook(hook)\nThese hooks will be called with arguments: self, state_dict, prefix, local_metadata, after the state_dict of self is set.\n\n_save_to_state_dict(destination, prefix, …)\n\n\n_shallow_repr()\n\n\n_to_memory_format(memory_format)\nCasts the parameters and buffers in this module to another memory format.\n\nadd_module(name, module)\nAdds a child module to the current module.\n\napply(fn)\nApplies fn recursively to every submodule (as returned by .children()) as well as self.\n\nbuffers([recurse])\n\nchildren()\n\ncpu()\nMoves all model parameters and buffers to the CPU.\n\ncuda([device])\nMoves all model parameters and buffers to the GPU.\n\ndouble()\nCasts all floating point parameters and buffers to double datatype.\n\neval()\nSets the module in evaluation mode.\n\nextra_repr()\nSet the extra representation of the module\n\nfloat()\nCasts all floating point parameters and buffers to float datatype.\n\nforward(input)\n\n\nget_parameter(target)\nReturn the parameter refenreced by target.\n\nget_submodule(target)\nGet submodule accroding to the name of submodule.\n\nhalf()\nCasts all floating point parameters and buffers to half datatype.\n\nload_state_dict(state_dict[, strict])\nCopies parameters and buffers from state_dict into this module and its descendants.\n\nmake_contiguous_params_group()\nGet contiguous parameters group after creating the whole module.\n\nmodules()\n\nnamed_buffers([prefix, recurse])\n\nnamed_children()\n\nnamed_modules([memo, prefix])\n\nnamed_parameters([prefix, recurse])\n\nparameters([recurse])\n\nregister_backward_hook(hook)\nRegisters a backward hook on the module.\n\nregister_buffer(name, tensor[, persistent])\nAdds a buffer to the module.\n\nregister_forward_hook(hook)\nRegisters a forward hook on the module.\n\nregister_forward_pre_hook(hook)\nRegisters a forward pre-hook on the module.\n\nregister_full_backward_hook(hook)\nRegisters a backward hook on the module.\n\nregister_parameter(name, param)\nAdds a parameter to the module.\n\nregister_state_dict_pre_hook(hook)\nThese hooks will be called with arguments: self, prefix, and keep_vars before calling state_dict on self.\n\nrequires_grad_([requires_grad])\nChange if autograd should record operations on parameters in this module.\n\nstate_dict([destination, prefix, keep_vars])\n\nto(*args, **kwargs)\nMoves and/or casts the parameters and buffers.\n\nto_consistent(*args, **kwargs)\nThis interface is no longer available, please use oneflow.nn.Module.to_global() instead.\n\nto_empty(*, device)\nMoves the parameters and buffers to the specified device without copying storage.\n\nto_global([placement, sbp])\nConvert the parameters and buffers to global.\n\nto_local()\n\n\nto_memory_format(memory_format)\n\n\ntrain([mode])\nSets the module in training mode.\n\nzero_grad([set_to_none])\nSets gradients of all model parameters to zero.\n\n\n\nAttributes\n\n\n\n\n\n\n_grad_t\nalias of Union[Tuple[oneflow.Tensor, …], oneflow.Tensor]\n\n\n\n\n",
        "parameters": "\ninput (oneflow.Tensor) – the input value(s), in oneflow.float32.\nquantization_formula (str) – Support “google” or “cambricon”.\nquantization_bit (int) – Quantize input to uintX / intX, X can be in range [2, 8]. Defaults to 8.\nquantization_scheme (str) – “symmetric” or “affine”, quantize to signed / unsigned integer. Defaults to “symmetric”.\nper_layer_quantization (bool) – True or False, means per-layer / per-channel quantization. Defaults to True.\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import numpy as np\n>>> import oneflow as flow\n\n>>> weight = (np.random.random((2, 3, 4, 5)) - 0.5).astype(np.float32)\n\n>>> input_tensor = flow.tensor(\n...    weight, dtype=flow.float32\n... )\n\n>>> quantization_bit = 8\n>>> quantization_scheme = \"symmetric\"\n>>> quantization_formula = \"google\"\n>>> per_layer_quantization = True\n\n>>> min_max_observer = flow.nn.MinMaxObserver(quantization_formula=quantization_formula, quantization_bit=quantization_bit,\n... quantization_scheme=quantization_scheme, per_layer_quantization=per_layer_quantization)\n\n>>> scale, zero_point = min_max_observer(\n...    input_tensor, )\n\n\n\n\n__init__(quantization_formula: str = 'google', quantization_bit: int = 8, quantization_scheme: str = 'symmetric', per_layer_quantization: bool = True) → None¶\nCalls super().__setattr__(‘a’, a) instead of the typical self.a = a\nto avoid Module.__setattr__ overhead. Module’s __setattr__ has special\nhandling for parameters, submodules, and buffers but simply calls into\nsuper().__setattr__ for all other attributes.\n\nMethods\n\n\n\n\n\n\n__call__(*args, **kwargs)\nCall self as a function.\n\n__delattr__(name)\nImplement delattr(self, name).\n\n__dir__()\nDefault dir() implementation.\n\n__eq__(value, /)\nReturn self==value.\n\n__format__(format_spec, /)\nDefault object formatter.\n\n__ge__(value, /)\nReturn self>=value.\n\n__getattr__(name)\n\n\n__getattribute__(name, /)\nReturn getattr(self, name).\n\n__getstate__()\n\n\n__gt__(value, /)\nReturn self>value.\n\n__hash__()\nReturn hash(self).\n\n__init__([quantization_formula, …])\nCalls super().__setattr__(‘a’, a) instead of the typical self.a = a to avoid Module.__setattr__ overhead.\n\n__init_subclass__\nThis method is called when a class is subclassed.\n\n__le__(value, /)\nReturn self<=value.\n\n__lt__(value, /)\nReturn self<value.\n\n__ne__(value, /)\nReturn self!=value.\n\n__new__(**kwargs)\nCreate and return a new object.\n\n__reduce__()\nHelper for pickle.\n\n__reduce_ex__(protocol, /)\nHelper for pickle.\n\n__repr__()\nReturn repr(self).\n\n__setattr__(name, value)\nImplement setattr(self, name, value).\n\n__setstate__(state)\n\n\n__sizeof__()\nSize of object in memory, in bytes.\n\n__str__()\nReturn str(self).\n\n__subclasshook__\nAbstract classes can override this to customize issubclass().\n\n_apply(fn)\n\n\n_get_backward_hooks()\n"
    },
    {
        "api_name": "oneflow.nn.MovingAverageMinMaxObserver",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.MovingAverageMinMaxObserver.html",
        "api_signature": "oneflow.nn.MovingAverageMinMaxObserver(stop_update_after_iters: int = 1, quantization_formula: str = 'google', quantization_bit: int = 8, quantization_scheme: str = 'symmetric', momentum: float = 0.95)",
        "api_description": "Compute the quantization parameters based on the moving average of the input tensor’s min and max values.\nFirst compute the moving_max and moving_min value of input tensor:\nif quantization_scheme == “symmetric”:\n\\[ \\begin{align}\\begin{aligned}& moving\\_max = moving\\_max * momentum + |max(input)| * (1 - momentum)\\\\& moving\\_min = moving\\_max\\end{aligned}\\end{align} \\]\nelif quantization_scheme == “affine”:\n\\[ \\begin{align}\\begin{aligned}& moving\\_max = moving\\_max * momentum + max(input) * (1 - momentum)\\\\& moving\\_min = moving\\_min * momentum + min(input) * (1 - momentum)\\end{aligned}\\end{align} \\]\nThe moving average of min and max values are initialized as the first batch of input Blob’s min and max.\nThen compute the scale and zero_point with the following equations:\nif quantization_scheme == “symmetric”:\n\\[ \\begin{align}\\begin{aligned}& denom = 2^{quantization\\_to\\_bit - 1} - 1\\\\& scale = moving\\_max / denom\\\\& zero\\_point = 0\\end{aligned}\\end{align} \\]\nelif quantization_scheme == “affine”:\n\\[ \\begin{align}\\begin{aligned}& denom = 2^{quantization\\_to\\_bit} - 1\\\\& scale = (moving\\_max - moving\\_min) / denom\\\\& zero\\_point = -moving\\_min / scale\\end{aligned}\\end{align} \\]",
        "return_value": "The scale and zero_point of input tensor.\n\n\n_get_name()\n\n\n_load_from_state_dict(state_dict, prefix, …)\n\n\n_maybe_warn_non_full_backward_hook(args, …)\n\n\n_named_members(get_members_fn[, prefix, recurse])\n\n\n_register_load_state_dict_pre_hook(hook[, …])\nThese hooks will be called with arguments: state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs, before loading state_dict into self.\n\n_register_state_dict_hook(hook)\nThese hooks will be called with arguments: self, state_dict, prefix, local_metadata, after the state_dict of self is set.\n\n_save_to_state_dict(destination, prefix, …)\n\n\n_shallow_repr()\n\n\n_to_memory_format(memory_format)\nCasts the parameters and buffers in this module to another memory format.\n\nadd_module(name, module)\nAdds a child module to the current module.\n\napply(fn)\nApplies fn recursively to every submodule (as returned by .children()) as well as self.\n\nbuffers([recurse])\n\nchildren()\n\ncpu()\nMoves all model parameters and buffers to the CPU.\n\ncuda([device])\nMoves all model parameters and buffers to the GPU.\n\ndouble()\nCasts all floating point parameters and buffers to double datatype.\n\neval()\nSets the module in evaluation mode.\n\nextra_repr()\nSet the extra representation of the module\n\nfloat()\nCasts all floating point parameters and buffers to float datatype.\n\nforward(input, current_train_step)\n\n\nget_parameter(target)\nReturn the parameter refenreced by target.\n\nget_submodule(target)\nGet submodule accroding to the name of submodule.\n\nhalf()\nCasts all floating point parameters and buffers to half datatype.\n\nload_state_dict(state_dict[, strict])\nCopies parameters and buffers from state_dict into this module and its descendants.\n\nmake_contiguous_params_group()\nGet contiguous parameters group after creating the whole module.\n\nmodules()\n\nnamed_buffers([prefix, recurse])\n\nnamed_children()\n\nnamed_modules([memo, prefix])\n\nnamed_parameters([prefix, recurse])\n\nparameters([recurse])\n\nregister_backward_hook(hook)\nRegisters a backward hook on the module.\n\nregister_buffer(name, tensor[, persistent])\nAdds a buffer to the module.\n\nregister_forward_hook(hook)\nRegisters a forward hook on the module.\n\nregister_forward_pre_hook(hook)\nRegisters a forward pre-hook on the module.\n\nregister_full_backward_hook(hook)\nRegisters a backward hook on the module.\n\nregister_parameter(name, param)\nAdds a parameter to the module.\n\nregister_state_dict_pre_hook(hook)\nThese hooks will be called with arguments: self, prefix, and keep_vars before calling state_dict on self.\n\nrequires_grad_([requires_grad])\nChange if autograd should record operations on parameters in this module.\n\nreset_running_stats()\n\n\nstate_dict([destination, prefix, keep_vars])\n\nto(*args, **kwargs)\nMoves and/or casts the parameters and buffers.\n\nto_consistent(*args, **kwargs)\nThis interface is no longer available, please use oneflow.nn.Module.to_global() instead.\n\nto_empty(*, device)\nMoves the parameters and buffers to the specified device without copying storage.\n\nto_global([placement, sbp])\nConvert the parameters and buffers to global.\n\nto_local()\n\n\nto_memory_format(memory_format)\n\n\ntrain([mode])\nSets the module in training mode.\n\nzero_grad([set_to_none])\nSets gradients of all model parameters to zero.\n\n\n\nAttributes\n\n\n\n\n\n\n_grad_t\nalias of Union[Tuple[oneflow.Tensor, …], oneflow.Tensor]\n\n\n\n\n",
        "parameters": "\ninput (oneflow.Tensor) – the input value(s), in oneflow.float32.\ncurrent_train_step_tensor (oneflow.Tensor) – record train step for quantionzation aware training.\nstop_update_after_iters (int) – stop record train step for quantionzation aware training when train iter greater than stop_update_after_iters.\nquantization_formula (str) – Support “google” or “cambricon”.\nquantization_bit (int) – Quantize input to uintX / intX, X can be in range [2, 8]. Defaults to 8.\nquantization_scheme (str) – “symmetric” or “affine”, quantize to signed / unsigned integer. Defaults to “symmetric”.\nmomentum (float) – Smoothing parameter for exponential moving average operation. Defaults to 0.95.\n\n\n",
        "input_shape": "",
        "notes": "current_train_step can be directly assigned to an optimizer(eg.SGD) step.",
        "code_example": ">>> import numpy as np\n>>> import oneflow as flow\n\n>>> weight = (np.random.random((2, 3, 4, 5)) - 0.5).astype(np.float32)\n\n>>> input_tensor = flow.tensor(\n...    weight, dtype=flow.float32\n... )\n\n>>> current_train_step_tensor = flow.tensor(\n...   np.zeros((1,)).astype(np.float32),\n...    dtype=flow.int64,\n... )\n\n>>> momentum = 0.95\n>>> quantization_bit = 8\n>>> quantization_scheme = \"symmetric\"\n>>> quantization_formula = \"google\"\n\n>>> moving_average_min_max_observer = flow.nn.MovingAverageMinMaxObserver(stop_update_after_iters=1,\n...                                                                       quantization_formula=quantization_formula, quantization_bit=quantization_bit,\n...                                                                       quantization_scheme=quantization_scheme, momentum=momentum,\n...                                                                       )\n\n>>> (scale, zero_point) = moving_average_min_max_observer(\n...    input_tensor,\n...    current_train_step_tensor,\n... )\n\n\n\n\n__init__(stop_update_after_iters: int = 1, quantization_formula: str = 'google', quantization_bit: int = 8, quantization_scheme: str = 'symmetric', momentum: float = 0.95) → None¶\nCalls super().__setattr__(‘a’, a) instead of the typical self.a = a\nto avoid Module.__setattr__ overhead. Module’s __setattr__ has special\nhandling for parameters, submodules, and buffers but simply calls into\nsuper().__setattr__ for all other attributes.\n\nMethods\n\n\n\n\n\n\n__call__(*args, **kwargs)\nCall self as a function.\n\n__delattr__(name)\nImplement delattr(self, name).\n\n__dir__()\nDefault dir() implementation.\n\n__eq__(value, /)\nReturn self==value.\n\n__format__(format_spec, /)\nDefault object formatter.\n\n__ge__(value, /)\nReturn self>=value.\n\n__getattr__(name)\n\n\n__getattribute__(name, /)\nReturn getattr(self, name).\n\n__getstate__()\n\n\n__gt__(value, /)\nReturn self>value.\n\n__hash__()\nReturn hash(self).\n\n__init__([stop_update_after_iters, …])\nCalls super().__setattr__(‘a’, a) instead of the typical self.a = a to avoid Module.__setattr__ overhead.\n\n__init_subclass__\nThis method is called when a class is subclassed.\n\n__le__(value, /)\nReturn self<=value.\n\n__lt__(value, /)\nReturn self<value.\n\n__ne__(value, /)\nReturn self!=value.\n\n__new__(**kwargs)\nCreate and return a new object.\n\n__reduce__()\nHelper for pickle.\n\n__reduce_ex__(protocol, /)\nHelper for pickle.\n\n__repr__()\nReturn repr(self).\n\n__setattr__(name, value)\nImplement setattr(self, name, value).\n\n__setstate__(state)\n\n\n__sizeof__()\nSize of object in memory, in bytes.\n\n__str__()\nReturn str(self).\n\n__subclasshook__\nAbstract classes can override this to customize issubclass().\n\n_apply(fn)\n\n\n_get_backward_hooks()\n"
    },
    {
        "api_name": "oneflow.nn.Quantization",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.Quantization.html",
        "api_signature": "oneflow.nn.Quantization(quantization_formula: str = 'google', quantization_bit: int = 8, quantization_scheme: str = 'symmetric')",
        "api_description": "Simulate the quantize operation in inference time.\nThe output will be computed as:\nif quantization_scheme == “symmetric”:\n\\[ \\begin{align}\\begin{aligned}& quant\\_max = 2^{quantization\\_to\\_bit - 1} - 1\\\\& quant\\_min = -quant\\_max\\\\& clamp(round(x / scale), quant\\_min, quant\\_max)\\end{aligned}\\end{align} \\]\nelif quantization_scheme == “affine”:\n\\[ \\begin{align}\\begin{aligned}& quant\\_max = 2^{quantization\\_to\\_bit} - 1\\\\& quant\\_min = 0\\\\& (clamp(round(x / scale + zero\\_point), quant\\_min, quant\\_max) - zero\\_point)\\end{aligned}\\end{align} \\]",
        "return_value": "Input tensor after quantize operation.\n\n\n_get_name()\n\n\n_load_from_state_dict(state_dict, prefix, …)\n\n\n_maybe_warn_non_full_backward_hook(args, …)\n\n\n_named_members(get_members_fn[, prefix, recurse])\n\n\n_register_load_state_dict_pre_hook(hook[, …])\nThese hooks will be called with arguments: state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs, before loading state_dict into self.\n\n_register_state_dict_hook(hook)\nThese hooks will be called with arguments: self, state_dict, prefix, local_metadata, after the state_dict of self is set.\n\n_save_to_state_dict(destination, prefix, …)\n\n\n_shallow_repr()\n\n\n_to_memory_format(memory_format)\nCasts the parameters and buffers in this module to another memory format.\n\nadd_module(name, module)\nAdds a child module to the current module.\n\napply(fn)\nApplies fn recursively to every submodule (as returned by .children()) as well as self.\n\nbuffers([recurse])\n\nchildren()\n\ncpu()\nMoves all model parameters and buffers to the CPU.\n\ncuda([device])\nMoves all model parameters and buffers to the GPU.\n\ndouble()\nCasts all floating point parameters and buffers to double datatype.\n\neval()\nSets the module in evaluation mode.\n\nextra_repr()\nSet the extra representation of the module\n\nfloat()\nCasts all floating point parameters and buffers to float datatype.\n\nforward(input, scale, zero_point)\n\n\nget_parameter(target)\nReturn the parameter refenreced by target.\n\nget_submodule(target)\nGet submodule accroding to the name of submodule.\n\nhalf()\nCasts all floating point parameters and buffers to half datatype.\n\nload_state_dict(state_dict[, strict])\nCopies parameters and buffers from state_dict into this module and its descendants.\n\nmake_contiguous_params_group()\nGet contiguous parameters group after creating the whole module.\n\nmodules()\n\nnamed_buffers([prefix, recurse])\n\nnamed_children()\n\nnamed_modules([memo, prefix])\n\nnamed_parameters([prefix, recurse])\n\nparameters([recurse])\n\nregister_backward_hook(hook)\nRegisters a backward hook on the module.\n\nregister_buffer(name, tensor[, persistent])\nAdds a buffer to the module.\n\nregister_forward_hook(hook)\nRegisters a forward hook on the module.\n\nregister_forward_pre_hook(hook)\nRegisters a forward pre-hook on the module.\n\nregister_full_backward_hook(hook)\nRegisters a backward hook on the module.\n\nregister_parameter(name, param)\nAdds a parameter to the module.\n\nregister_state_dict_pre_hook(hook)\nThese hooks will be called with arguments: self, prefix, and keep_vars before calling state_dict on self.\n\nrequires_grad_([requires_grad])\nChange if autograd should record operations on parameters in this module.\n\nstate_dict([destination, prefix, keep_vars])\n\nto(*args, **kwargs)\nMoves and/or casts the parameters and buffers.\n\nto_consistent(*args, **kwargs)\nThis interface is no longer available, please use oneflow.nn.Module.to_global() instead.\n\nto_empty(*, device)\nMoves the parameters and buffers to the specified device without copying storage.\n\nto_global([placement, sbp])\nConvert the parameters and buffers to global.\n\nto_local()\n\n\nto_memory_format(memory_format)\n\n\ntrain([mode])\nSets the module in training mode.\n\nzero_grad([set_to_none])\nSets gradients of all model parameters to zero.\n\n\n\nAttributes\n\n\n\n\n\n\n_grad_t\nalias of Union[Tuple[oneflow.Tensor, …], oneflow.Tensor]\n\n\n\n\n",
        "parameters": "\nquantization_bit (int) – Quantize input to uintX / intX, X can be in range [2, 8]. Defaults to 8.\nquantization_scheme (str) – “symmetric” or “affine”, quantize to signed / unsigned integer. Defaults to “symmetric”.\nquantization_formula (str) – Support “google” or “cambricon”.\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import numpy as np\n>>> import oneflow as flow\n\n>>> weight = (np.random.random((2, 3, 4, 5)) - 0.5).astype(np.float32)\n\n>>> input_tensor = flow.tensor(\n...    weight, dtype=flow.float32\n... )\n\n>>> quantization_bit = 8\n>>> quantization_scheme = \"symmetric\"\n>>> quantization_formula = \"google\"\n>>> per_layer_quantization = True\n\n>>> min_max_observer = flow.nn.MinMaxObserver(quantization_formula=quantization_formula, quantization_bit=quantization_bit,\n... quantization_scheme=quantization_scheme, per_layer_quantization=per_layer_quantization)\n>>> quantization = flow.nn.Quantization(quantization_formula=quantization_formula, quantization_bit=quantization_bit,\n... quantization_scheme=quantization_scheme)\n\n>>> scale, zero_point = min_max_observer(\n...    input_tensor,\n... )\n\n>>> output_tensor = quantization(\n...    input_tensor,\n...    scale,\n...    zero_point,\n... )\n\n\n\n\n__init__(quantization_formula: str = 'google', quantization_bit: int = 8, quantization_scheme: str = 'symmetric') → None¶\nCalls super().__setattr__(‘a’, a) instead of the typical self.a = a\nto avoid Module.__setattr__ overhead. Module’s __setattr__ has special\nhandling for parameters, submodules, and buffers but simply calls into\nsuper().__setattr__ for all other attributes.\n\nMethods\n\n\n\n\n\n\n__call__(*args, **kwargs)\nCall self as a function.\n\n__delattr__(name)\nImplement delattr(self, name).\n\n__dir__()\nDefault dir() implementation.\n\n__eq__(value, /)\nReturn self==value.\n\n__format__(format_spec, /)\nDefault object formatter.\n\n__ge__(value, /)\nReturn self>=value.\n\n__getattr__(name)\n\n\n__getattribute__(name, /)\nReturn getattr(self, name).\n\n__getstate__()\n\n\n__gt__(value, /)\nReturn self>value.\n\n__hash__()\nReturn hash(self).\n\n__init__([quantization_formula, …])\nCalls super().__setattr__(‘a’, a) instead of the typical self.a = a to avoid Module.__setattr__ overhead.\n\n__init_subclass__\nThis method is called when a class is subclassed.\n\n__le__(value, /)\nReturn self<=value.\n\n__lt__(value, /)\nReturn self<value.\n\n__ne__(value, /)\nReturn self!=value.\n\n__new__(**kwargs)\nCreate and return a new object.\n\n__reduce__()\nHelper for pickle.\n\n__reduce_ex__(protocol, /)\nHelper for pickle.\n\n__repr__()\nReturn repr(self).\n\n__setattr__(name, value)\nImplement setattr(self, name, value).\n\n__setstate__(state)\n\n\n__sizeof__()\nSize of object in memory, in bytes.\n\n__str__()\nReturn str(self).\n\n__subclasshook__\nAbstract classes can override this to customize issubclass().\n\n_apply(fn)\n\n\n_get_backward_hooks()\n"
    },
    {
        "api_name": "oneflow.nn.functional.conv1d",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.functional.conv1d.html",
        "api_signature": "oneflow.nn.functional.conv1d(input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1)",
        "api_description": "Applies a 1D convolution over an input signal composed of several input\nplanes.\nSee Conv1d for details and output shape.",
        "return_value": "",
        "parameters": "\ninput – input tensor of shape \\((\\text{minibatch} , \\text{in_channels} , iW)\\)\nweight – filters of shape \\((\\text{out_channels} , \\frac{\\text{in_channels}}{\\text{groups}} , iW)\\)\nbias – optional bias of shape \\((\\text{out_channels})\\). Default: None.\nstride – the stride of the convolving kernel. Can be a single number or a\ntuple (sW,). Default: 1\npadding – implicit paddings on both sides of the input. Can be a\nsingle number or a tuple (padW,). Default: 0\ndilation – the spacing between kernel elements. Can be a single number or\na tuple (dW,). Default: 1\ngroups – split input into groups, \\(\\text{in_channels}\\) should be divisible by the\nnumber of groups. Default: 1\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> import oneflow.nn.functional as F\n\n>>> inputs = flow.randn(33, 16, 30)\n>>> filters = flow.randn(20, 16, 5)\n>>> outputs = F.conv1d(inputs, filters)\n\n\n\n"
    },
    {
        "api_name": "oneflow.nn.functional.conv2d",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.functional.conv2d.html",
        "api_signature": "oneflow.nn.functional.conv2d(input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1)",
        "api_description": "Applies a 2D convolution over an input image composed of several input\nplanes.\nSee Conv2d for details and output shape.",
        "return_value": "",
        "parameters": "\ninput – input tensor of shape \\((\\text{minibatch} , \\text{in_channels} , iH , iW)\\)\nweight – filters of shape \\((\\text{out_channels} , \\frac{\\text{in_channels}}{\\text{groups}} , kH , kW)\\)\nbias – optional bias of shape \\((\\text{out_channels})\\). Default: None.\nstride – the stride of the convolving kernel. Can be a single number or a\ntuple (sH, sW). Default: 1\npadding – implicit paddings on both sides of the input. Can be a\nsingle number or a tuple (padH, padW). Default: 0\ndilation – the spacing between kernel elements. Can be a single number or\na tuple (dH, dW). Default: 1\ngroups – split input into groups, \\(\\text{in_channels}\\) should be divisible by the\nnumber of groups. Default: 1\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> import oneflow.nn.functional as F\n\n>>> inputs = flow.randn(8, 4, 3, 3)\n>>> filters = flow.randn(1, 4, 5, 5)\n>>> outputs = F.conv2d(inputs, filters, padding=1)\n\n\n\n"
    },
    {
        "api_name": "oneflow.nn.functional.conv3d",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.functional.conv3d.html",
        "api_signature": "oneflow.nn.functional.conv3d(input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1)",
        "api_description": "Applies a 3D convolution over an input image composed of several input\nplanes.\nSee Conv3d for details and output shape.",
        "return_value": "",
        "parameters": "\ninput – input tensor of shape\n\\((\\text{minibatch} , \\text{in_channels} , iD , iH , iW)\\)\nweight – filters of shape\n\\((\\text{out_channels} , \\frac{\\text{in_channels}}{\\text{groups}} , kD , kH , kW)\\)\nbias – optional bias of shape \\((\\text{out_channels})\\). Default: None.\nstride – the stride of the convolving kernel. Can be a single number or a\ntuple (sD, sH, sW). Default: 1\npadding – implicit paddings on both sides of the input. Can be a\nsingle number or a tuple (padD, padH, padW). Default: 0\ndilation – the spacing between kernel elements. Can be a single number or\na tuple (dD, dH, dW). Default: 1\ngroups – split input into groups, \\(\\text{in_channels}\\) should be\ndivisible by the number of groups. Default: 1\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> import oneflow.nn.functional as F\n\n>>> inputs = flow.randn(20, 16, 50, 10, 20)\n>>> filters = flow.randn(33, 16, 3, 3, 3)\n>>> outputs = F.conv3d(inputs, filters)\n\n\n\n"
    },
    {
        "api_name": "oneflow.nn.functional.conv_transpose1d",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.functional.conv_transpose1d.html",
        "api_signature": "oneflow.nn.functional.conv_transpose1d(input, weight, bias=None, stride=1, padding=0, output_padding=0, groups=1, dilation=1)",
        "api_description": "Applies a 1D transposed convolution operator over an input signal composed of several input planes, sometimes also called “deconvolution”.\nSee ConvTranspose1d for details and output shape.",
        "return_value": "",
        "parameters": "\ninput – input tensor of shape \\((\\text{minibatch} , \\text{in_channels} , iW)\\)\nweight – filters of shape \\((\\text{in_channels} , \\frac{\\text{out_channels}}{\\text{groups}} , kW)\\)\nbias – optional bias of shape \\((\\text{out_channels})\\). Default: None.\nstride – the stride of the convolving kernel. Can be a single number or a\ntuple (sW,). Default: 1\npadding – dilation * (kernel_size - 1) - padding zero-padding will be added to both sides of each dimension in the input. Can be a single number or a tuple (padW,). Default: 0\noutput_padding – additional size added to one side of each dimension in the output shape. Can be a single number or a tuple (out_padW). Default: 0\ngroups – split input into groups, \\(\\text{in_channels}\\) should be divisible by the\nnumber of groups. Default: 1\ndilation – the spacing between kernel elements. Can be a single number or\na tuple (dW,). Default: 1\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> import oneflow.nn.functional as F\n\n>>> inputs = flow.randn(20, 16, 50)\n>>> weights = flow.randn(16, 33, 5)\n>>> outputs = F.conv_transpose1d(inputs, weights)\n\n\n\n"
    },
    {
        "api_name": "oneflow.nn.functional.conv_transpose2d",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.functional.conv_transpose2d.html",
        "api_signature": "oneflow.nn.functional.conv_transpose2d(input, weight, bias=None, stride=1, padding=0, output_padding=0, groups=1, dilation=1)",
        "api_description": "Applies a 2D transposed convolution operator over an input image composed of several input planes, sometimes also called “deconvolution”.\nSee ConvTranspose2d for details and output shape.",
        "return_value": "",
        "parameters": "\ninput – input tensor of shape \\((\\text{minibatch} , \\text{in_channels} , iH , iW)\\)\nweight – filters of shape \\((\\text{in_channels} , \\frac{\\text{out_channels}}{\\text{groups}} , kH , kW)\\)\nbias – optional bias of shape \\((\\text{out_channels})\\). Default: None.\nstride – the stride of the convolving kernel. Can be a single number or a\ntuple (sH, sW). Default: 1\npadding – dilation * (kernel_size - 1) - padding zero-padding will be added to both sides of each dimension in the input. Can be a single number or a tuple (padH, padW). Default: 0\noutput_padding – additional size added to one side of each dimension in the output shape. Can be a single number or a tuple (out_padH, out_padW). Default: 0\ngroups – split input into groups, \\(\\text{in_channels}\\) should be divisible by the\nnumber of groups. Default: 1\ndilation – the spacing between kernel elements. Can be a single number or\na tuple (dH, dW). Default: 1\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> import oneflow.nn.functional as F\n\n>>> inputs = flow.randn(1, 4, 5, 5)\n>>> weights = flow.randn(4, 8, 3, 3)\n>>> outputs = F.conv_transpose2d(inputs, weights, padding=1)\n\n\n\n"
    },
    {
        "api_name": "oneflow.nn.functional.conv_transpose3d",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.functional.conv_transpose3d.html",
        "api_signature": "oneflow.nn.functional.conv_transpose3d(input, weight, bias=None, stride=1, padding=0, output_padding=0, groups=1, dilation=1)",
        "api_description": "Applies a 3D transposed convolution operator over an input image composed of several input planes, sometimes also called “deconvolution”.\nSee ConvTranspose3d for details and output shape.",
        "return_value": "",
        "parameters": "\ninput – input tensor of shape\n\\((\\text{minibatch} , \\text{in_channels} , iT , iH , iW)\\)\nweight – filters of shape\n\\((\\text{in_channels} , \\frac{\\text{out_channels}}{\\text{groups}} , kT , kH , kW)\\)\nbias – optional bias of shape \\((\\text{out_channels})\\). Default: None.\nstride – the stride of the convolving kernel. Can be a single number or a\ntuple (sD, sH, sW). Default: 1\npadding – dilation * (kernel_size - 1) - padding zero-padding will be added to both sides of each dimension in the input. Can be a single number or a tuple (padT, padH, padW). Default: 0\noutput_padding – additional size added to one side of each dimension in the output shape. Can be a single number or a tuple (out_padT, out_padH, out_padW). Default: 0\ngroups – split input into groups, \\(\\text{in_channels}\\) should be\ndivisible by the number of groups. Default: 1\ndilation – the spacing between kernel elements. Can be a single number or\na tuple (dT, dH, dW). Default: 1\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> import oneflow.nn.functional as F\n\n>>> inputs = flow.randn(20, 16, 50, 10, 20)\n>>> weights = flow.randn(16, 33, 3, 3, 3)\n>>> outputs = F.conv_transpose3d(inputs, weights)\n\n\n\n"
    },
    {
        "api_name": "oneflow.nn.functional.fold",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.functional.fold.html",
        "api_signature": "oneflow.nn.functional.fold(input, output_size, kernel_size, dilation=1, padding=0, stride=1)",
        "api_description": "Combines an array of sliding local blocks into a large containing tensor.\nWarning\nCurrently, only 3-D input tensors (batched image-like tensors) are supported, and only unbatched (3D)\nor batched (4D) image-like output tensors are supported.\nSee oneflow.nn.Fold for details.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.nn.functional.unfold",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.functional.unfold.html",
        "api_signature": "oneflow.nn.functional.unfold(input, kernel_size, dilation=1, padding=0, stride=1)",
        "api_description": "Extracts sliding local blocks from a batched input tensor.\nWarning\nCurrently, only 4-D input tensors (batched image-like tensors) are supported.\nWarning\nMore than one element of the unfolded tensor may refer to a single\nmemory location. As a result, in-place operations (especially ones that\nare vectorized) may result in incorrect behavior. If you need to write\nto the tensor, please clone it first.\nSee oneflow.nn.Unfold for details.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.nn.functional.batch_norm",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.functional.batch_norm.html",
        "api_signature": "oneflow.nn.functional.batch_norm(input: oneflow.Tensor, running_mean: Optional[oneflow.Tensor], running_var: Optional[oneflow.Tensor], weight: Optional[oneflow.Tensor] = None, bias: Optional[oneflow.Tensor] = None, training: bool = False, momentum: float = 0.1, eps: float = 1e-05)",
        "api_description": "Applies Batch Normalization for each channel across a batch of data.\nSee BatchNorm1d, BatchNorm2d,\nBatchNorm3d for details.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.nn.functional.avg_pool1d",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.functional.avg_pool1d.html",
        "api_signature": "oneflow.nn.functional.avg_pool1d(input, kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True)",
        "api_description": "Applies a 1D average pooling over an input signal composed of several input planes.\nSee AvgPool1d for details and output shape.",
        "return_value": "",
        "parameters": "\ninput – input tensor of shape \\((\\text{minibatch} , \\text{in_channels} , iW)\\)\nkernel_size – the size of the window. Can be a single number or a tuple (kW,)\nstride – the stride of the window. Can be a single number or a tuple (sW,). Default: kernel_size\npadding – implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nceil_mode – when True, will use ceil instead of floor to compute the output shape. Default: False\ncount_include_pad – when True, will include the zero-padding in the averaging calculation. Default: True\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.nn.functional.avg_pool2d",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.functional.avg_pool2d.html",
        "api_signature": "oneflow.nn.functional.avg_pool2d(input, kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True, divisor_override=0)",
        "api_description": "Applies 2D average-pooling operation in \\(kH \\times kW\\) regions by step size \\(sH \\times sW\\) steps. The number of output features is equal to the number of input planes.\nSee AvgPool2d for details and output shape.",
        "return_value": "",
        "parameters": "\ninput – input tensor \\((\\text{minibatch} , \\text{in_channels} , iH , iW)\\)\nkernel_size – size of the pooling region. Can be a single number or a tuple (kH, kW)\nstride – stride of the pooling operation. Can be a single number or a tuple (sH, sW). Default: kernel_size\npadding – implicit zero paddings on both sides of the input. Can be a single number or a tuple (padH, padW). Default: 0\nceil_mode – when True, will use ceil instead of floor in the formula to compute the output shape. Default: False\ncount_include_pad – when True, will include the zero-padding in the averaging calculation. Default: True\ndivisor_override – if specified, it will be used as divisor, otherwise size of the pooling region will be used. Default: 0\n\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.nn.functional.avg_pool3d",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.functional.avg_pool3d.html",
        "api_signature": "oneflow.nn.functional.avg_pool3d(input, kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True, divisor_override=0)",
        "api_description": "Applies 3D average-pooling operation in \\(kT \\times kH \\times kW\\) regions by step size \\(sT \\times sH \\times sW\\) steps. The number of output features is equal to \\(\\lfloor\\frac{\\text{input planes}}{sT}\\rfloor\\).\nSee AvgPool3d for details and output shape.",
        "return_value": "",
        "parameters": "\ninput – input tensor \\((\\text{minibatch} , \\text{in_channels} , iT \\times iH , iW)\\)\nkernel_size – size of the pooling region. Can be a single number or a tuple (kT, kH, kW)\nstride – stride of the pooling operation. Can be a single number or a tuple (sT, sH, sW). Default: kernel_size\npadding – implicit zero paddings on both sides of the input. Can be a single number or a tuple (padT, padH, padW), Default: 0\nceil_mode – when True, will use ceil instead of floor in the formula to compute the output shape\ncount_include_pad – when True, will include the zero-padding in the averaging calculation\ndivisor_override – if specified, it will be used as divisor, otherwise size of the pooling region will be used. Default: 0\n\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.nn.functional.max_pool1d",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.functional.max_pool1d.html",
        "api_signature": "oneflow.nn.functional.max_pool1d(input, kernel_size, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False, data_format='channels_first')",
        "api_description": "Applies a 1D max pooling over an input signal composed of several input\nplanes.",
        "return_value": "",
        "parameters": "\ninput – input tensor of shape \\((\\text{minibatch} , \\text{in_channels} , iW)\\), minibatch dim optional.\nkernel_size – the size of the window. Can be a single number or a tuple (kW,)\nstride – the stride of the window. Can be a single number or a tuple (sW,). Default: kernel_size\npadding – Implicit negative infinity padding to be added on both sides, must be >= 0 and <= kernel_size / 2.\ndilation – The stride between elements within a sliding window, must be > 0.\nreturn_indices – If True, will return the argmax along with the max values.Useful for oneflow.nn.functional.max_unpool1d later.\nceil_mode – If True, will use ceil instead of floor to compute the output shape. This ensures that every element in the input tensor is covered by a sliding window.\n\n\n\n\n",
        "input_shape": "",
        "notes": "The order of ceil_mode and return_indices is different from\nwhat seen in MaxPool1d, and will change in a future release.\nSee MaxPool1d for details.",
        "code_example": ""
    },
    {
        "api_name": "oneflow.nn.functional.max_pool2d",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.functional.max_pool2d.html",
        "api_signature": "oneflow.nn.functional.max_pool2d(input, kernel_size, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False, data_format='channels_first')",
        "api_description": "Applies a 2D max pooling over an input signal composed of several input\nplanes.",
        "return_value": "",
        "parameters": "\ninput – input tensor \\((\\text{minibatch} , \\text{in_channels} , iH , iW)\\), minibatch dim optional.\nkernel_size – size of the pooling region. Can be a single number or a tuple (kH, kW)\nstride – stride of the pooling operation. Can be a single number or a tuple (sH, sW). Default: kernel_size\npadding – Implicit negative infinity padding to be added on both sides, must be >= 0 and <= kernel_size / 2.\ndilation – The stride between elements within a sliding window, must be > 0.\nreturn_indices – If True, will return the argmax along with the max values.Useful for oneflow.nn.functional.max_unpool2d later.\nceil_mode – If True, will use ceil instead of floor to compute the output shape. This ensures that every element in the input tensor is covered by a sliding window.\n\n\n\n\n",
        "input_shape": "",
        "notes": "The order of ceil_mode and return_indices is different from\nwhat seen in MaxPool2d, and will change in a future release.\nSee MaxPool2d for details.",
        "code_example": ""
    },
    {
        "api_name": "oneflow.nn.functional.max_pool3d",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.functional.max_pool3d.html",
        "api_signature": "oneflow.nn.functional.max_pool3d(input, kernel_size, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False, data_format='channels_first')",
        "api_description": "Applies a 3D max pooling over an input signal composed of several input\nplanes.",
        "return_value": "",
        "parameters": "\ninput – input tensor \\((\\text{minibatch} , \\text{in_channels} , iD, iH , iW)\\), minibatch dim optional.\nkernel_size – size of the pooling region. Can be a single number or a tuple (kT, kH, kW)\nstride – stride of the pooling operation. Can be a single number or a tuple (sT, sH, sW). Default: kernel_size\npadding – Implicit negative infinity padding to be added on both sides, must be >= 0 and <= kernel_size / 2.\ndilation – The stride between elements within a sliding window, must be > 0.\nreturn_indices – If True, will return the argmax along with the max values.Useful for max_unpool3d later.\nceil_mode – If True, will use ceil instead of floor to compute the output shape. This ensures that every element in the input tensor is covered by a sliding window.\n\n\n\n\n",
        "input_shape": "",
        "notes": "The order of ceil_mode and return_indices is different from\nwhat seen in MaxPool3d, and will change in a future release.\nSee MaxPool3d for details.",
        "code_example": ""
    },
    {
        "api_name": "oneflow.nn.functional.max_unpool1d",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.functional.max_unpool1d.html",
        "api_signature": "oneflow.nn.functional.max_unpool1d(input, indices, kernel_size, stride=None, padding=0, output_size=None)",
        "api_description": "Computes a partial inverse of MaxPool1d.\nSee MaxUnpool1d for details.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.nn.functional.max_unpool2d",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.functional.max_unpool2d.html",
        "api_signature": "oneflow.nn.functional.max_unpool2d(input, indices, kernel_size, stride=None, padding=0, output_size=None)",
        "api_description": "Computes a partial inverse of MaxPool2d.\nSee MaxUnpool2d for details.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.nn.functional.max_unpool3d",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.functional.max_unpool3d.html",
        "api_signature": "oneflow.nn.functional.max_unpool3d(input, indices, kernel_size, stride=None, padding=0, output_size=None)",
        "api_description": "Computes a partial inverse of MaxPool3d.\nSee MaxUnpool3d for details.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.nn.functional.adaptive_avg_pool1d",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.functional.adaptive_avg_pool1d.html",
        "api_signature": "oneflow.nn.functional.adaptive_avg_pool1d(input, output_size)",
        "api_description": "Applies a 1D adaptive average pooling over an input signal composed of\nseveral input planes.\nSee AdaptiveAvgPool1d for details and output shape.",
        "return_value": "",
        "parameters": "\ninput – the input tensor\noutput_size – the target output size (single integer)\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> import numpy as np\n\n>>> arr = np.array([[[ 0.0558, -0.6875, -1.6544, -0.6226,  0.1018,  0.0502, -1.2538, 0.1491]]])\n>>> input = flow.tensor(arr, dtype=flow.float32)\n>>> flow.nn.functional.adaptive_avg_pool1d(input, output_size=[4])\ntensor([[[-0.3158, -1.1385,  0.0760, -0.5524]]], dtype=oneflow.float32)\n\n\n\n"
    },
    {
        "api_name": "oneflow.nn.functional.adaptive_avg_pool2d",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.functional.adaptive_avg_pool2d.html",
        "api_signature": "oneflow.nn.functional.adaptive_avg_pool2d(input, output_size)",
        "api_description": "Applies a 2D adaptive average pooling over an input signal composed of several input planes.\nSee AdaptiveAvgPool2d for details and output shape.",
        "return_value": "",
        "parameters": "\ninput – the input tensor\noutput_size – the target output size (single integer or double-integer tuple)\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> import numpy as np\n\n>>> arr = np.array([[[[ 0.1004,  0.0488, -1.0515,  0.9466],[ 0.4538,  0.2361,  1.3437,  0.398 ],[ 0.0558, -0.6875, -1.6544, -0.6226],[ 0.1018,  0.0502, -1.2538,  0.1491]]]])\n>>> input = flow.tensor(arr, dtype=flow.float32)\n>>> outputs = flow.nn.functional.adaptive_avg_pool2d(input, (2, 2))\n\n\n\n"
    },
    {
        "api_name": "oneflow.nn.functional.adaptive_avg_pool3d",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.functional.adaptive_avg_pool3d.html",
        "api_signature": "oneflow.nn.functional.adaptive_avg_pool3d(input, output_size)",
        "api_description": "Applies a 3D adaptive average pooling over an input signal composed of several input planes.\nSee AdaptiveAvgPool3d for details and output shape.",
        "return_value": "",
        "parameters": "\ninput – the input tensor\noutput_size – the target output size (single integer or triple-integer tuple)\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> import numpy as np\n\n>>> input = flow.tensor(np.random.randn(1, 1, 4, 4, 4), dtype=flow.float32)\n>>> output = flow.nn.functional.adaptive_avg_pool3d(input, (2, 2, 2))\n\n\n\n"
    },
    {
        "api_name": "oneflow.nn.functional.adaptive_max_pool1d",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.functional.adaptive_max_pool1d.html",
        "api_signature": "oneflow.nn.functional.adaptive_max_pool1d(input, output_size, return_indices: bool = False)",
        "api_description": "Applies a 1D adaptive max pooling over an input signal composed of\nseveral input planes.\nThe documentation is referenced from:\nSee AdaptiveMaxPool1d for details and output shape.",
        "return_value": "",
        "parameters": "\noutput_size – the target output size (single integer)\nreturn_indices – whether to return pooling indices. Default: False\n\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.nn.functional.adaptive_max_pool2d",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.functional.adaptive_max_pool2d.html",
        "api_signature": "oneflow.nn.functional.adaptive_max_pool2d(input, output_size, return_indices: bool = False, data_format='channels_first')",
        "api_description": "Applies a 2D adaptive max pooling over an input signal composed of\nseveral input planes.\nThe documentation is referenced from:\nSee AdaptiveMaxPool2d for details and output shape.",
        "return_value": "",
        "parameters": "\noutput_size – the target output size (single integer or\ndouble-integer tuple)\nreturn_indices – whether to return pooling indices. Default: False\n\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.nn.functional.adaptive_max_pool3d",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.functional.adaptive_max_pool3d.html",
        "api_signature": "oneflow.nn.functional.adaptive_max_pool3d(input, output_size, return_indices: bool = False)",
        "api_description": "Applies a 3D adaptive max pooling over an input signal composed of\nseveral input planes.\nThe documentation is referenced from:\nSee AdaptiveMaxPool3d for details and output shape.",
        "return_value": "",
        "parameters": "\noutput_size – the target output size (single integer or\ntriple-integer tuple)\nreturn_indices – whether to return pooling indices. Default: False\n\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.nn.functional.threshold",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.functional.threshold.html",
        "api_signature": "oneflow.nn.functional.threshold(input: Tensor, threshold: float, value: float)",
        "api_description": "Thresholds each element of the input Tensor.\nSee Threshold for more details.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.nn.functional.relu",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.functional.relu.html",
        "api_signature": "oneflow.nn.functional.relu()",
        "api_description": "Applies the rectified linear unit function element-wise. See ReLU for more details.",
        "return_value": "",
        "parameters": "inplace – If set to True, will do this operation in-place. Default: False\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> import numpy as np\n\n>>> ndarr = np.asarray([1, -2, 3])\n>>> input = flow.Tensor(ndarr)\n>>> output = flow.relu(input)\n>>> output\ntensor([1., 0., 3.], dtype=oneflow.float32)\n\n\n\n"
    },
    {
        "api_name": "oneflow.nn.functional.hardtanh",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.functional.hardtanh.html",
        "api_signature": "oneflow.nn.functional.hardtanh(input, min_val=- 1.0, max_val=1.0)",
        "api_description": "Applies the HardTanh function element-wise. See Hardtanh for more\ndetails.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.nn.functional.hardswish",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.functional.hardswish.html",
        "api_signature": "oneflow.nn.functional.hardswish(x: Tensor)",
        "api_description": "Applies the hardswish function, element-wise, as described in the paper:\nSearching for MobileNetV3.\n\\[ext{Hardswish}(x) = \begin{cases}\n0 &         ext{if~} x \\le -3, \\\nx &         ext{if~} x \\ge +3, \\\nx \\cdot (x + 3) /6 &        ext{otherwise}\n\\end{cases}\\]\nSee Hardswish for more details.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.nn.functional.relu6",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.functional.relu6.html",
        "api_signature": "oneflow.nn.functional.relu6(input, inplace=False)",
        "api_description": "Applies the element-wise function \\(\\text{ReLU6}(x) = \\min(\\max(0,x), 6)\\).\nSee ReLU6 for more details.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.nn.functional.elu",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.functional.elu.html",
        "api_signature": "oneflow.nn.functional.elu(x: Tensor, alpha: Float)",
        "api_description": "Applies element-wise,:math:` ext{ELU}(x) = max(0,x) + min(0, \u0007lpha * (exp(x) - 1))`.\nSee ELU for more details.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import numpy as np\n>>> import oneflow as flow\n\n>>> x = np.array([-0.5, 0, 0.5]).astype(np.float32)\n>>> input = flow.tensor(x)\n>>> out = flow.nn.functional.elu(input, alpha=1.0)\n>>> out\ntensor([-0.3935,  0.0000,  0.5000], dtype=oneflow.float32)\n\n\n\n"
    },
    {
        "api_name": "oneflow.nn.functional.selu",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.functional.selu.html",
        "api_signature": "oneflow.nn.functional.selu(x: Tensor)",
        "api_description": "Applies element-wise function\n\\[ext{SELU}(x) = scale * (\\max(0,x) + \\min(0, \u0007lpha * (\\exp(x) - 1)))`, with :math:`\u0007lpha=1.6732632423543772848170429916717` and  :math:`scale=1.0507009873554804934193349852946`.\\]\nSee SELU for more details.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import numpy as np\n>>> import oneflow as flow\n\n>>> x = np.array([1, 2, 3]).astype(np.float32)\n>>> input = flow.tensor(x)\n>>> out = flow.nn.functional.selu(input)\n>>> out\ntensor([1.0507, 2.1014, 3.1521], dtype=oneflow.float32)\n\n\n\n"
    },
    {
        "api_name": "oneflow.nn.functional.celu",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.functional.celu.html",
        "api_signature": "oneflow.nn.functional.celu(x: Tensor, alpha: Float = 1.0, inplace: bool = False)",
        "api_description": "Applies the element-wise function:\n\\[\\text{CELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x/\\alpha) - 1))\\]\nSee CELU for more details.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import numpy as np\n>>> import oneflow as flow\n\n>>> x = np.array([-0.5, 0, 0.5]).astype(np.float32)\n>>> input = flow.tensor(x)\n>>> out = flow.nn.functional.celu(input, alpha=0.5)\n>>> out\ntensor([-0.3161,  0.0000,  0.5000], dtype=oneflow.float32)\n\n\n\n"
    },
    {
        "api_name": "oneflow.nn.functional.leaky_relu",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.functional.leaky_relu.html",
        "api_signature": "oneflow.nn.functional.leaky_relu(x: Tensor, alpha: Float)",
        "api_description": "Applies element-wise,\n:math:`     ext{LeakyReLU}(x) = max(0, x) +        ext{negative_slope} * min(0, x)`\nSee LeakyReLU for more details.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.nn.functional.square_relu",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.functional.square_relu.html",
        "api_signature": "oneflow.nn.functional.square_relu(x: Tensor)",
        "api_description": "Applies the relu^2 activation introduced in https://arxiv.org/abs/2109.08668v2\n\\[\\begin{split}\\\\text{ReLU}(x) = \\\\max(0, x) * \\\\max(0, x)\\end{split}\\]",
        "return_value": "A Tensor has same shape as the input.\n\n",
        "parameters": "input (oneflow.Tensor) – Input Tensor\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.nn.functional.prelu",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.functional.prelu.html",
        "api_signature": "oneflow.nn.functional.prelu(x: Tensor, alpha: Tensor)",
        "api_description": "Applies the element-wise function:\n\\[prelu(x) = max(0,x) + alpha * min(0,x)\\]",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import numpy as np\n>>> import oneflow as flow\n\n>>> x = flow.tensor(np.asarray([[[[1, -2], [3, 4]]]]), dtype=flow.float32)\n>>> alpha = flow.nn.Parameter(flow.tensor([1], dtype=flow.float32).fill_(0.25))\n>>> flow.nn.functional.prelu(x, alpha)\ntensor([[[[ 1.0000, -0.5000],\n          [ 3.0000,  4.0000]]]], dtype=oneflow.float32,\n       grad_fn=<preluBackward>)\n\n\nSee\nPReLU for more details.\n\n"
    },
    {
        "api_name": "oneflow.nn.functional.glu",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.functional.glu.html",
        "api_signature": "oneflow.nn.functional.glu(input: Tensor, dim: int)",
        "api_description": "The equation is:\n\\[GLU(input) = GLU(a, b) = a \\otimes sigmoid(b)\\]",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "where input is split in half along dim to form a and b, ⊗ is the element-wise product between matrices.",
        "code_example": ">>> import oneflow as flow\n>>> import oneflow.nn as nn\n>>> x = flow.tensor([[1, 2, 3, 4], [5, 6, 7, 8]], dtype=flow.float32)\n>>> y = nn.functional.glu(x)\n>>> y\ntensor([[0.9526, 1.9640],\n        [4.9954, 5.9980]], dtype=oneflow.float32)\n\n\nSee\nGLU for more details.\n\n"
    },
    {
        "api_name": "oneflow.nn.functional.gelu",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.functional.gelu.html",
        "api_signature": "oneflow.nn.functional.gelu(x: Tensor)",
        "api_description": "Applies the Gaussian Error Linear Units function:\n\\[\\begin{split}\\\\text{GELU}(x) = x * \\Phi(x)\\end{split}\\]\nwhere \\(\\Phi(x)\\) is the Cumulative Distribution Function for Gaussian Distribution.\nWhen the approximate argument is ‘tanh’, Gelu is estimated with:\n\\[\\begin{split}\\\\text{GELU}(x) = 0.5 * x * (1 + \\\\text{Tanh}(\\sqrt(2 / \\pi) * (x + 0.044715 * x^3)))\\end{split}\\]",
        "return_value": "A Tensor has same shape as the input.\n\n",
        "parameters": "\ninput (oneflow.Tensor) – Input Tensor\napproximate (string, optional) – the gelu approximation algorithm to use:\n'none' | 'tanh'. Default: 'none'\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import numpy as np\n>>> import oneflow as flow\n\n>>> x = np.array([-0.5, 0, 0.5]).astype(np.float32)\n>>> input = flow.tensor(x)\n\n>>> out = flow.gelu(input)\n>>> out\ntensor([-0.1543,  0.0000,  0.3457], dtype=oneflow.float32)\n\n\nSee\nGELU for more details.\n\n"
    },
    {
        "api_name": "oneflow.nn.functional.quick_gelu",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.functional.quick_gelu.html",
        "api_signature": "oneflow.nn.functional.quick_gelu(x: Tensor)",
        "api_description": "Applies GELU approximation that is fast but somewhat inaccurate. See: https://github.com/hendrycks/GELUs\n\\[\\begin{split}\\\\text{QuickGELU}(x) = x * \\\\sigma(1.702x) = x * \\\\frac{1}{1 + \\\\exp(-1.702x)}\\end{split}\\]",
        "return_value": "A Tensor has same shape as the input.\n\n",
        "parameters": "input (oneflow.Tensor) – Input Tensor\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.nn.functional.logsigmoid",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.functional.logsigmoid.html",
        "api_signature": "oneflow.nn.functional.logsigmoid(x: Tensor)",
        "api_description": "Applies the element-wise function:\n\\[\\text{logsigmoid}(x) = \\log\\left(\\frac{ 1 }{ 1 + \\exp(-x)}\\right)\\]",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import numpy as np\n>>> import oneflow as flow\n\n>>> x = np.array([-0.5, 0, 0.5]).astype(np.float32)\n>>> input = flow.tensor(x)\n\n>>> out = flow.nn.functional.logsigmoid(input)\n>>> out\ntensor([-0.9741, -0.6931, -0.4741], dtype=oneflow.float32)\n\n\nSee LogSigmoid for more details.\n\n"
    },
    {
        "api_name": "oneflow.nn.functional.hardshrink",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.functional.hardshrink.html",
        "api_signature": "oneflow.nn.functional.hardshrink(input: Tensor, lambd: float = 0.5, inplace: bool = False)",
        "api_description": "Applies the hard shrinkage function in an element-wise manner.\nSee Hardshrink for more details.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.nn.functional.softsign",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.functional.softsign.html",
        "api_signature": "oneflow.nn.functional.softsign(x: Tensor)",
        "api_description": "The formula is:\n\\[softsign(x) = \\frac{x}{1 + |x|}\\]",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import numpy as np\n>>> import oneflow as flow\n\n>>> x = np.array([1, 2, 3]).astype(np.float32)\n>>> input = flow.tensor(x)\n>>> out = flow.nn.functional.softsign(input)\n>>> out\ntensor([0.5000, 0.6667, 0.7500], dtype=oneflow.float32)\n\n\nSee Softsign for more details.\n\n"
    },
    {
        "api_name": "oneflow.nn.functional.softplus",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.functional.softplus.html",
        "api_signature": "oneflow.nn.functional.softplus(x: Tensor, beta: double = 1, threshold: double = 20)",
        "api_description": "Applies the element-wise function:\n\\[\\text{Softplus}(x) = \\frac{1}{\\beta} * \\log(1 + \\exp(\\beta * x))\\]\nFor numerical stability the implementation reverts to the linear function\nwhen \\(input \\times \\beta > threshold\\).\nSee Softplus for more details.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.nn.functional.softmax",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.functional.softmax.html",
        "api_signature": "oneflow.nn.functional.softmax(input: oneflow.Tensor, dim: Optional[int] = None, dtype=None)",
        "api_description": "Applies a softmax function.\nSoftmax is defined as:\n\\(\\text{Softmax}(x_{i}) = \\frac{\\exp(x_i)}{\\sum_j \\exp(x_j)}\\)\nIt is applied to all slices along dim, and will re-scale them so that the elements\nlie in the range [0, 1] and sum to 1.\nSee Softmax for more details.",
        "return_value": "",
        "parameters": "\ninput (Tensor) – input\ndim (int) – A dimension along which softmax will be computed.\ndtype (oneflow.dtype, optional) – the desired data type of returned tensor.\nIf specified, the input tensor is casted to dtype before the operation\nis performed. This is useful for preventing data type overflows. Default: None.\n\n\n\n\n",
        "input_shape": "",
        "notes": "This function doesn’t work directly with NLLLoss,\nwhich expects the Log to be computed between the Softmax and itself.\nUse log_softmax instead (it’s faster and has better numerical properties).",
        "code_example": ""
    },
    {
        "api_name": "oneflow.nn.functional.softshrink",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.functional.softshrink.html",
        "api_signature": "oneflow.nn.functional.softshrink(input: Tensor, lambd: float = 0.5, inplace: bool = False)",
        "api_description": "Applies the soft shrinkage function in an element-wise manner.\nSee Softshrink for more details.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.nn.functional.log_softmax",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.functional.log_softmax.html",
        "api_signature": "oneflow.nn.functional.log_softmax(x: Tensor, dim: int)",
        "api_description": "LogSoftmax is defined as:\n\\[\\text{LogSoftmax}(x_{i}) = \\log\\left(\\frac{\\exp(x_i) }{ \\sum_j \\exp(x_j)} \\right) = x_i - \\log({ \\sum_j \\exp(x_j)})\\]\nSee LogSoftmax for more details.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.nn.functional.gumbel_softmax",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.functional.gumbel_softmax.html",
        "api_signature": "oneflow.nn.functional.gumbel_softmax(x: Tensor, dim: int, tau: float = 1.0, hard: bool = False)",
        "api_description": "Solve the problem that the output values of argmax do not reflect the probability distribution of the model’s output.\nCompensates for the fact that the argmax cannot participate in gradient back-propagation.\nGumbel is defined as:\n\\[Gumbel_i = -log(-log(U_i)),\\ U_i \\sim U(0,1)\\]\nAdd Noise ~ Gumbel:\n\\[In = (In + Noise) / tau\\]\nCalculate Softmax value:\n\\[gumbel\\_softmax(In)=\\frac{e^{In_i/tau}}{\\sum_{j=1}^n{e^{In_j/tau}}},i=1,2,3...n\\]",
        "return_value": "",
        "parameters": "\nx (oneflow.Tensor) – the input Tensor.\ndim (int, Tuple[int]) – the dimension to softmax.\ntau (double) – the input tensor of Softmax should obey the Gumbel(x, tau).\nhard (bool) – if hard=True, the output tensor will be one-hot.\n\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.nn.functional.tanh",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.functional.tanh.html",
        "api_signature": "oneflow.nn.functional.tanh(x: Tensor)",
        "api_description": "The equation is:\n\\[out = \\frac{e^x-e^{-x}}{e^x+e^{-x}}\\]\nSee Tanh for more details.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.nn.functional.sigmoid",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.functional.sigmoid.html",
        "api_signature": "oneflow.nn.functional.sigmoid(input)",
        "api_description": "Applies the element-wise function \\(\\text{Sigmoid}(x) = \\frac{1}{1 + \\exp(-x)}\\)\nSee Sigmoid for more details.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import numpy as np\n>>> import oneflow as flow\n\n>>> x = np.array([0.81733328, 0.43621480, 0.10351428])\n>>> input = flow.tensor(x, dtype=flow.float32)\n>>> out = flow.nn.functional.sigmoid(input)\n>>> out\ntensor([0.6937, 0.6074, 0.5259], dtype=oneflow.float32)\n\n\n\n"
    },
    {
        "api_name": "oneflow.nn.functional.hardsigmoid",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.functional.hardsigmoid.html",
        "api_signature": "oneflow.nn.functional.hardsigmoid(x: Tensor)",
        "api_description": "Applies the element-wise function\n\\[ext{Hardsigmoid}(x) = \begin{cases}\n0 &         ext{if~} x \\le -3, \\\n1 &         ext{if~} x \\ge +3, \\\nx / 6 + 1 / 2 &     ext{otherwise}\n\\end{cases}\\]\nSee Hardsigmoid for more details.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.nn.functional.silu",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.functional.silu.html",
        "api_signature": "oneflow.nn.functional.silu(x: Tensor)",
        "api_description": "The formula is:\n\\[ext{silu}(x) = x * sigmoid(x)\\]",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import numpy as np\n>>> import oneflow as flow\n\n>>> x = np.array([1, 2, 3]).astype(np.float32)\n>>> input = flow.tensor(x)\n>>> out = flow.silu(input)\n>>> out\ntensor([0.7311, 1.7616, 2.8577], dtype=oneflow.float32)\n\n\nSee SiLU for more details.\n\n"
    },
    {
        "api_name": "oneflow.nn.functional.mish",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.functional.mish.html",
        "api_signature": "oneflow.nn.functional.mish(x: Tensor)",
        "api_description": "Applies the element-wise function:\n\\[ext{mish}(x) = x *      ext{tanh}(      ext{softplus}(x))\\]",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import numpy as np\n>>> import oneflow as flow\n\n>>> x = np.array([1, 2, 3]).astype(np.float32)\n>>> input = flow.tensor(x)\n\n>>> out = flow.mish(input)\n>>> out\ntensor([0.8651, 1.9440, 2.9865], dtype=oneflow.float32)\n\n\nSee Mish for more details.\n\n"
    },
    {
        "api_name": "oneflow.nn.functional.layer_norm",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.functional.layer_norm.html",
        "api_signature": "oneflow.nn.functional.layer_norm(input, normalized_shape, weight=None, bias=None, eps=1e-05)",
        "api_description": "Applies Layer Normalization for last certain number of dimensions.\nSee LayerNorm for details.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.nn.functional.normalize",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.functional.normalize.html",
        "api_signature": "oneflow.nn.functional.normalize(input: Tensor, p: float = 2.0, dim: int = 0, epsilon: float = 1e-12)",
        "api_description": "Performs \\(L_p\\) normalization of inputs over specified dimension\nFor a tensor input of sizes \\((n_0, ..., n_{dim}, ..., n_k)\\), each\n\\(n_{dim}\\) -element vector \\(v\\) along dimension dim is transformed as:\n\\[v = \\frac{v}{\\max(\\lVert v \\rVert_p, \\epsilon)}.\\]\nWith the default arguments it uses the Euclidean norm over vectors along dimension \\(1\\) for normalization.\nBut note that the gradient calculation of the input tensor has different results on different frameworks\nwhen input.shape[dim] = 1.",
        "return_value": "",
        "parameters": "\ninput (oneflow.Tensor) – input tensor of any shape\np (float) – the exponent value in the norm formulation. Default: 2\ndim (int) – the dimension to reduce. Default: 1\neps (float) – small value to avoid division by zero. Default: 1e-12\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> x = flow.tensor([[1, 2], [3, 4]], dtype=flow.float32)\n>>> out = flow.nn.functional.normalize(x, 2, 0)\n>>> out\ntensor([[0.3162, 0.4472],\n        [0.9487, 0.8944]], dtype=oneflow.float32)\n>>> out = flow.nn.functional.normalize(x, 2, 1)\n>>> out\ntensor([[0.4472, 0.8944],\n        [0.6000, 0.8000]], dtype=oneflow.float32)\n\n\n\n"
    },
    {
        "api_name": "oneflow.nn.functional.linear",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.functional.linear.html",
        "api_signature": "oneflow.nn.functional.linear(input, weight, bias=None)",
        "api_description": "Applies a linear transformation to the incoming data: \\(y = xA^T + b\\).",
        "return_value": "",
        "parameters": "",
        "input_shape": "\n\nInput: \\((N, *, in\\_features)\\) N is the batch size, * means any number of\nadditional dimensions\nWeight: \\((out\\_features, in\\_features)\\)\nBias: \\((out\\_features)\\)\nOutput: \\((N, *, out\\_features)\\)\n\n\n",
        "notes": "",
        "code_example": ">>> import numpy as np\n>>> import oneflow as flow\n\n>>> input = flow.tensor(np.random.randn(128, 20))\n>>> weight = flow.tensor(np.random.randn(30, 20))\n>>> output = flow.nn.functional.linear(input, weight)\n>>> output.size()\noneflow.Size([128, 30])\n\n\n\n"
    },
    {
        "api_name": "oneflow.nn.functional.dropout",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.functional.dropout.html",
        "api_signature": "oneflow.nn.functional.dropout(x: Tensor, p: float = 0.5, training: bool = True, generator: Generator = None, *, addend: Tensor)",
        "api_description": "During training, randomly zeroes some of the elements of the input\ntensor with probability p using samples from a Bernoulli\ndistribution.\nThe documentation is referenced from:",
        "return_value": "",
        "parameters": "\nx (Tensor) – A Tensor which will be applyed dropout.\np (float) – probability of an element to be zeroed. Default: 0.5\ntraining (bool) – If is True it will apply dropout. Default: True\ngenerator (Generator, optional) – A pseudorandom number generator for sampling\naddend (Tensor, optional) – A Tensor add in result after dropout, it can be used in model’s residual connection structure. Default: None\n\n\n\n\n",
        "input_shape": "Input: \\((*)\\). Input can be of any shape\nOutput: \\((*)\\). Output is of the same shape as input\n\n\n\n",
        "notes": "",
        "code_example": "Example 1:\n>>> import numpy as np\n>>> import oneflow as flow\n\n\n>>> arr = np.array(\n...    [\n...        [-0.7797, 0.2264, 0.2458, 0.4163],\n...        [0.4299, 0.3626, -0.4892, 0.4141],\n...        [-1.4115, 1.2183, -0.5503, 0.6520],\n...    ]\n... )\n>>> x = flow.tensor(arr, dtype=flow.float32)\n>>> y = flow.nn.functional.dropout(x, p=0)\n\n>>> arr = np.array(\n...    [\n...        [-0.7797, 0.2264, 0.2458, 0.4163],\n...        [0.4299, 0.3626, -0.4892, 0.4141],\n...        [-1.4115, 1.2183, -0.5503, 0.6520],\n...    ]\n... )\n>>> x = flow.tensor(arr, dtype=flow.float32)\n>>> generator = flow.Generator()\n>>> y = flow.nn.functional.dropout(x, p=0.5, generator=generator)\n\n\nExample 2:\n>>> import numpy as np\n>>> import oneflow as flow\n\n\n>>> arr = np.array(\n...    [\n...        [-0.7797, 0.2264, 0.2458, 0.4163],\n...        [0.4299, 0.3626, -0.4892, 0.4141],\n...        [-1.4115, 1.2183, -0.5503, 0.6520],\n...    ]\n... )\n>>> x = flow.tensor(arr, dtype=flow.float32)\n>>> addend = flow.ones((3, 4), dtype=flow.float32)\n>>> y = flow.nn.functional.dropout(x, p=0, addend=addend)\n>>> y \ntensor([[ 0.2203,  1.2264,  1.2458,  1.4163],\n        [ 1.4299,  1.3626,  0.5108,  1.4141],\n        [-0.4115,  2.2183,  0.4497,  1.6520]], dtype=oneflow.float32)\n\n\nSee Dropout for details.\n\n"
    },
    {
        "api_name": "oneflow.nn.functional.dropout1d",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.functional.dropout1d.html",
        "api_signature": "oneflow.nn.functional.dropout1d(x: Tensor, p: float = 0.5, training: bool = True)",
        "api_description": "The documentation is referenced from:\nRandomly zero out entire channels (a channel is a 1D feature map,\ne.g., the \\(j\\)-th channel of the \\(i\\)-th sample in the\nbatched input is a 1D tensor \\(\\text{input}[i, j]\\)) of the input tensor).\nEach channel will be zeroed out independently on every forward call with\nprobability p using samples from a Bernoulli distribution.\nSee Dropout1d for details.",
        "return_value": "",
        "parameters": "\np – probability of a channel to be zeroed. Default: 0.5\ntraining – apply dropout if is True. Default: True\n\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.nn.functional.dropout2d",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.functional.dropout2d.html",
        "api_signature": "oneflow.nn.functional.dropout2d()",
        "api_description": "dropout1d(x: Tensor, p: float = 0.5, training: bool = True) -> Tensor\nThe documentation is referenced from:\nRandomly zero out entire channels (a channel is a 2D feature map,\ne.g., the \\(j\\)-th channel of the \\(i\\)-th sample in the\nbatched input is a 2D tensor \\(\\text{input}[i, j]\\)) of the input tensor).\nEach channel will be zeroed out independently on every forward call with\nprobability p using samples from a Bernoulli distribution.\nSee Dropout2d for details.",
        "return_value": "",
        "parameters": "\np – probability of a channel to be zeroed. Default: 0.5\ntraining – apply dropout if is True. Default: True\n\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.nn.functional.dropout3d",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.functional.dropout3d.html",
        "api_signature": "oneflow.nn.functional.dropout3d()",
        "api_description": "dropout1d(x: Tensor, p: float = 0.5, training: bool = True) -> Tensor\nThe documentation is referenced from:\nRandomly zero out entire channels (a channel is a 3D feature map,\ne.g., the \\(j\\)-th channel of the \\(i\\)-th sample in the\nbatched input is a 3D tensor \\(\\text{input}[i, j]\\)) of the input tensor).\nEach channel will be zeroed out independently on every forward call with\nprobability p using samples from a Bernoulli distribution.\nSee Dropout3d for details.",
        "return_value": "",
        "parameters": "\np – probability of a channel to be zeroed. Default: 0.5\ntraining – apply dropout if is True. Default: True\n\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.nn.functional.embedding",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.functional.embedding.html",
        "api_signature": "oneflow.nn.functional.embedding(input, weight, padding_idx=None, max_norm=None, norm_type=2.0, scale_grad_by_freq=False, sparse=False)",
        "api_description": "A simple lookup table that looks up embeddings in a fixed dictionary and size.\nThis module is often used to retrieve word embeddings using indices.\nThe input to the module is a list of indices, and the embedding matrix,\nand the output is the corresponding word embeddings.\nSee oneflow.nn.Embedding for more details.",
        "return_value": "",
        "parameters": "\ninput (oneflow.LongTensor) – Tensor containing indices into the embedding matrix\nweight (Tensor) – The embedding matrix with number of rows equal to the maximum possible index + 1,\nand number of columns equal to the embedding size\npadding_idx (int, optional) – If specified, the entries at padding_idx do not contribute to the gradient;\ntherefore, the embedding vector at padding_idx is not updated during training,\ni.e. it remains as a fixed “pad”.\nmax_norm (float, optional) – If given, each embedding vector with norm larger than max_norm is renormalized to have\nnorm max_norm\nnorm_type (float, optional) – The p of the p-norm to compute for the max_norm option. Default 2.\nscale_grad_by_freq (boolean, optional) – If given, this will scale gradients by the inverse of\nfrequency of the words in the mini-batch. Default False\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> import oneflow.nn.functional as F\n\n>>> # a batch of 2 samples of 4 indices each\n>>> input = flow.tensor([[1,2,4,5],[4,3,2,9]])\n>>> # an embedding matrix containing 10 tensors of size 3\n>>> embedding_matrix = flow.rand(10, 3)\n>>> output = F.embedding(input, embedding_matrix)\n>>> output.shape\noneflow.Size([2, 4, 3])\n>>> # example with padding_idx\n>>> input = flow.tensor([[0,2,0,5]])\n>>> output = F.embedding(input, embedding_matrix, padding_idx=0)\n>>> output.shape\noneflow.Size([1, 4, 3])\n\n\n\n"
    },
    {
        "api_name": "oneflow.nn.functional.one_hot",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.functional.one_hot.html",
        "api_signature": "oneflow.nn.functional.one_hot(input, num_classes=- 1, on_value=1, off_value=0)",
        "api_description": "This operator generates a onehot Tensor from input Tensor.\nIf input Tensor’s rank is N, the corresponding onehot Tensor’s rank is N+1.",
        "return_value": "oneflow.Tensor.\n\n\n",
        "parameters": "\ninput (Tensor) – The input Tensor.\nnum_classes (int) – The length of onehot Tensor.\non_value (Union[int, float], optional) – The fill value when x[i] == i. Defaults to 1.\noff_value (Union[int, float], optional) – The fill value when x[i] != i. Defaults to 0.\n\n\n\n\n",
        "input_shape": "",
        "notes": "The data type of input tensor should be int32 or int64.",
        "code_example": ">>> import oneflow as flow\n>>> import numpy as np\n\n>>> input=flow.tensor(np.array([0, 3, 1, 2]).astype(np.int64), dtype=flow.int64)\n>>> out = flow.nn.functional.one_hot(input, num_classes=5)\n>>> out\ntensor([[1, 0, 0, 0, 0],\n        [0, 0, 0, 1, 0],\n        [0, 1, 0, 0, 0],\n        [0, 0, 1, 0, 0]], dtype=oneflow.int64)\n\n\n\n"
    },
    {
        "api_name": "oneflow.nn.functional.cosine_similarity",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.functional.cosine_similarity.html",
        "api_signature": "oneflow.nn.functional.cosine_similarity(x1: Tensor, x2: Tensor, dim: int = 1, eps: float = 1e-08)",
        "api_description": "",
        "return_value": "to a common shape. dim refers to the dimension in this common shape. Dimension dim of the output is\nsqueezed (see oneflow.squeeze()), resulting in the\noutput tensor having 1 fewer dimension.\nThe documentation is referenced from: https://pytorch.org/docs/1.10/generated/torch.nn.functional.cosine_similarity.html\n\n\\[\\text{similarity} = \\dfrac{x_1 \\cdot x_2}{\\max(\\Vert x_1 \\Vert _2 \\cdot \\Vert x_2 \\Vert _2, \\epsilon)}\\]\n\n",
        "parameters": "\nx1 (Tensor) – First input.\nx2 (Tensor) – Second input.\ndim (int, optional) – Dimension along which cosine similarity is computed. Default: 1\neps (float, optional) – Small value to avoid division by zero.\nDefault: 1e-8\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> import oneflow.nn.functional as F\n>>> input1 = flow.randn(100, 128)\n>>> input2 = flow.randn(100, 128)\n>>> output = F.cosine_similarity(input1, input2)\n\n\n\n"
    },
    {
        "api_name": "oneflow.nn.functional.pairwise_distance",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.functional.pairwise_distance.html",
        "api_signature": "oneflow.nn.functional.pairwise_distance(x1: Tensor, x2: Tensor, dim: float = 2.0, eps: float = 1e-06, keepdim: bool = False)",
        "api_description": "Computes the pairwise distance between vectors \\(v_1\\), \\(v_2\\) using the p-norm:\n\\[\\left \\| x \\right \\| _p = (\\sum_{i=1}^n \\left | x_i \\right |^p )^{\\frac{1}{p}}\\]",
        "return_value": "",
        "parameters": "\nx1 (Tensor) – First input.\nx2 (Tensor) – Second input.\np (real) – the norm degree. Default: 2\neps (float, optional) – Small value to avoid division by zero. Default: 1e-6\nkeepdim (bool, optional) – Determines whether or not to keep the vector dimension. Default: False\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> x1 = flow.arange(12).reshape(3, 4)\n>>> x2 = flow.arange(12).reshape(3, 4)\n>>> output = flow.nn.functional.pairwise_distance(x1, x2, p=2)\n>>> output\ntensor([2.0000e-06, 2.0000e-06, 2.0000e-06], dtype=oneflow.float32)\n>>> output.shape\noneflow.Size([3])\n\n\n\n"
    },
    {
        "api_name": "oneflow.nn.functional.sparse_softmax_cross_entropy",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.functional.sparse_softmax_cross_entropy.html",
        "api_signature": "oneflow.nn.functional.sparse_softmax_cross_entropy(labels, logits)",
        "api_description": "The interface is consistent with TensorFlow.\nThe documentation is referenced from:\nhttps://www.tensorflow.org/api_docs/python/tf/nn/sparse_softmax_cross_entropy_with_logits\nComputes sparse softmax cross entropy between logits and labels.\nMeasures the probability error in discrete classification tasks in which the\nclasses are mutually exclusive (each entry is in exactly one class).  For\nexample, each CIFAR-10 image is labeled with one and only one label: an image\ncan be a dog or a truck, but not both.\nA common use case is to have logits of shape\n[batch_size, num_classes] and have labels of shape\n[batch_size], but higher dimensions are supported, in which\ncase the dim-th dimension is assumed to be of size num_classes.\nlogits must have the dtype of float16, float32, or float64, and\nlabels must have the dtype of int32 or int64.",
        "return_value": "A Tensor of the same shape as labels and of the same type as logits\nwith the softmax cross entropy loss.\n\n",
        "parameters": "\nlabels (Tensor) – shape with [d_0, d_1, …, d_{r-1}] (where r is rank of\nlabels and output) and dtype int32 or int64. Each entry in labels\nmust be an index in [0, num_classes).\nlogits (Tensor) – Per-label activations (typically a linear output) of shape\n[d_0, d_1, …, d_{r-1}, num_classes] and dtype float16, float32, or\nfloat64. These activation energies are interpreted as unnormalized log\nprobabilities.\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.nn.functional.cross_entropy",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.functional.cross_entropy.html",
        "api_signature": "oneflow.nn.functional.cross_entropy(input, target, weight=None, ignore_index=- 100, reduction='mean', label_smoothing=0.0)",
        "api_description": "See CrossEntropyLoss for details.",
        "return_value": "",
        "parameters": "\ninput (Tensor) – \\((N, C)\\) where C = number of classes or \\((N, C, H, W)\\)\nin case of 2D Loss, or \\((N, C, d_1, d_2, ..., d_K)\\) where \\(K \\geq 1\\)\nin the case of K-dimensional loss. input is expected to contain unnormalized scores\n(often referred to as logits).\ntarget (Tensor) – If containing class indices, shape \\((N)\\) where each value is\n\\(0 \\leq \\text{targets}[i] \\leq C-1\\), or \\((N, d_1, d_2, ..., d_K)\\) with\n\\(K \\geq 1\\) in the case of K-dimensional loss. If containing class probabilities,\nsame shape as the input.\nweight (Tensor, optional) – a manual rescaling weight given to each\nclass. If given, has to be a Tensor of size C\nignore_index (int, optional) – Specifies a target value that is ignored\nand does not contribute to the input gradient. When size_average is\nTrue, the loss is averaged over non-ignored targets. Note that\nignore_index is only applicable when the target contains class indices.\nDefault: -100\nreduction (string, optional) – Specifies the reduction to apply to the output:\n'none' | 'mean' | 'sum'. 'none': no reduction will be applied,\n'mean': the sum of the output will be divided by the number of\nelements in the output, 'sum': the output will be summed. Note: size_average\nand reduce are in the process of being deprecated, and in the meantime,\nspecifying either of those two args will override reduction. Default: 'mean'\nlabel_smoothing (float, optinoal) – A float in [0.0, 1.0]. Specifies the amount\nof smoothing when computing the loss, where 0.0 means no smoothing.\nThe targets become a mixture of the original ground truth and a uniform\ndistribution as described in Rethinking the Inception Architecture for Computer Vision.\nDefault: \\(0.0\\).\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> import oneflow.nn.functional as F\n>>> input = flow.randn(3, 5, requires_grad=True)\n>>> target = flow.ones(3, dtype=flow.int64)\n>>> loss = F.cross_entropy(input, target)\n>>> loss.backward()\n\n\n\n"
    },
    {
        "api_name": "oneflow.nn.functional.ctc_loss",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.functional.ctc_loss.html",
        "api_signature": "oneflow.nn.functional.ctc_loss(log_probs: oneflow.Tensor, targets: oneflow.Tensor, input_lengths: oneflow.Tensor, target_lengths: oneflow.Tensor, blank=0, reduction='mean', zero_infinity=False)",
        "api_description": "The Connectionist Temporal Classification loss.\nThe documentation is referenced from:\nSee CTCLoss for details.",
        "return_value": "",
        "parameters": "\nlog_probs – The logarithmized probabilities of the outputs.\ntargets – Targets cannot be blank. In the second form, the targets are assumed to be concatenated.\ninput_lengths – Lengths of the inputs.\ntarget_lengths – Lengths of the targets.\nblank – Black label, default 0.\nreduction – Specifies the reduction to apply to the output:  'none' | 'mean' | 'sum' . Default 'Mean'.\nzero_infinity – Whether to zero infinite losses and the associated gradients. Default False.\n\n\n\nExample\n>>> import oneflow as flow\n>>> import oneflow.nn as nn\n>>> import oneflow.nn.functional as F\n>>> log_probs = flow.tensor(\n...     [\n...         [[-1.1031, -0.7998, -1.5200], [-0.9808, -1.1363, -1.1908]],\n...         [[-1.2258, -1.0665, -1.0153], [-1.1135, -1.2331, -0.9671]],\n...         [[-1.3348, -0.6611, -1.5118], [-0.9823, -1.2355, -1.0941]],\n...         [[-1.3850, -1.3273, -0.7247], [-0.8235, -1.4783, -1.0994]],\n...         [[-0.9049, -0.8867, -1.6962], [-1.4938, -1.3630, -0.6547]],\n...     ],\n...     dtype=flow.float32,\n...     requires_grad=True,\n...     )\n>>> targets = flow.tensor([[1, 2, 2], [1, 2, 2]], dtype=flow.int32, device=\"cuda\")\n>>> input_lengths = flow.tensor([5, 5], dtype=flow.int32)\n>>> target_lengths = flow.tensor([3, 3], dtype=flow.int32)\n>>> out = F.ctc_loss(log_probs, targets, input_lengths, target_lengths)\n>>> out\ntensor(1.1376, dtype=oneflow.float32, grad_fn=<scalar_mulBackward>)\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.nn.functional.l1_loss",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.functional.l1_loss.html",
        "api_signature": "oneflow.nn.functional.l1_loss(input, target, reduction='mean')",
        "api_description": "This operator computes the L1 loss between each element in input and target.\nsee L1Loss for details.",
        "return_value": "",
        "parameters": "\ninput (Tensor) – The input Tensor.\ntarget (Tensor) – The target Tensor.\nreduction (string, optional) – The reduce type, it can be one of “none”, “mean”, “sum”. Defaults to “mean”.\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.nn.functional.mse_loss",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.functional.mse_loss.html",
        "api_signature": "oneflow.nn.functional.mse_loss(input, target, reduction='mean')",
        "api_description": "This operator computes the mean squared error (squared L2 norm)\nloss between each element in input and target.\nsee MSELoss for details.",
        "return_value": "",
        "parameters": "\ninput (Tensor) – The input Tensor.\ntarget (Tensor) – The target Tensor.\nreduction (string, optional) – Specifies the reduction to apply to the output:\n'none' | 'mean' | 'sum'. 'none': no reduction will be applied,\n'mean': the sum of the output will be divided by the number of\nelements in the output, 'sum': the output will be summed. Default: 'mean'\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.nn.functional.smooth_l1_loss",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.functional.smooth_l1_loss.html",
        "api_signature": "oneflow.nn.functional.smooth_l1_loss(input: Tensor, target: Tensor, size_average: bool = True, reduce: bool = True, reduction: str = 'mean', beta: float = 1.0)",
        "api_description": "Function that uses a squared term if the absolute\nelement-wise error falls below beta and an L1 term otherwise.\nSee SmoothL1Loss for details.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.nn.functional.triplet_margin_loss",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.functional.triplet_margin_loss.html",
        "api_signature": "oneflow.nn.functional.triplet_margin_loss()",
        "api_description": "Creates a criterion that measures the triplet loss given an input\ntensors \\(x1\\), \\(x2\\), \\(x3\\) and a margin with a value greater than \\(0\\).\nThis is used for measuring a relative similarity between samples. A triplet\nis composed by a, p and n (i.e., anchor, positive examples and negative\nexamples respectively). The shapes of all input tensors should be\n\\((N, D)\\).\nThe distance swap is described in detail in the paper Learning shallow\nconvolutional feature descriptors with triplet losses by\nV. Balntas, E. Riba et al.\nThe loss function for each sample in the mini-batch is:\n\\[L(a, p, n) = \\max \\{d(a_i, p_i) - d(a_i, n_i) + {\\rm margin}, 0\\}\\]\nwhere\n\\[d(x_i, y_i) = \\left\\lVert {\\bf x}_i - {\\bf y}_i \\right\\rVert_p\\]",
        "return_value": "",
        "parameters": "\nmargin (float, optional) – Default: \\(1\\).\np (float, optional) – The norm degree for pairwise distance. Default: \\(2.0\\).\nswap (bool, optional) – The distance swap is described in detail in the paper\nLearning shallow convolutional feature descriptors with triplet losses by\nV. Balntas, E. Riba et al. Default: False.\nreduction (string, optional) – Specifies the reduction to apply to the output:\n'none' | 'mean' | 'sum'. 'none': no reduction will be applied,\n'mean': the sum of the output will be divided by the number of\nelements in the output, 'sum': the output will be summed. Note: size_average\nand reduce are in the process of being deprecated, and in the meantime,\nspecifying either of those two args will override reduction. Default: 'mean'\n\n\n\n\n",
        "input_shape": "Input: \\((N, D)\\) where \\(D\\) is the vector dimension.\nOutput: A Tensor of shape \\((N)\\) if reduction is 'none', or a scalar\notherwise.\n\n\n\n",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> import numpy as np\n>>> triplet_loss = flow.nn.TripletMarginLoss(margin=1.0, p=2)\n>>> anchor = np.array([[1, -1, 1],[-1, 1, -1], [1, 1, 1]])\n>>> positive = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n>>> negative = np.array([[2, 2, 2], [2, 2, 2], [2, 2, 2]])\n>>> output = triplet_loss(flow.Tensor(anchor), flow.Tensor(positive), flow.Tensor(negative))\n>>> output\ntensor(6.2971, dtype=oneflow.float32)\n\n\n\n"
    },
    {
        "api_name": "oneflow.nn.functional.binary_cross_entropy",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.functional.binary_cross_entropy.html",
        "api_signature": "oneflow.nn.functional.binary_cross_entropy(input, target, weight=None, reduction='mean')",
        "api_description": "Function that measures the Binary Cross Entropy between the target and input probabilities.\nSee BCELoss for details.",
        "return_value": "",
        "parameters": "\ninput – Tensor of arbitrary shape as probabilities.\ntarget – Tensor of the same shape as input with values between 0 and 1.\nweight (Tensor, optional) – a manual rescaling weight\nif provided it’s repeated to match input tensor shape\nreduction (string, optional) – Specifies the reduction to apply to the output:\n'none' | 'mean' | 'sum'. 'none': no reduction will be applied,\n'mean': the sum of the output will be divided by the number of\nelements in the output, 'sum': the output will be summed. Note: size_average\nand reduce are in the process of being deprecated, and in the meantime,\nspecifying either of those two args will override reduction. Default: 'mean'\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.nn.functional.binary_cross_entropy_with_logits",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.functional.binary_cross_entropy_with_logits.html",
        "api_signature": "oneflow.nn.functional.binary_cross_entropy_with_logits(input, target, weight=None, reduction='mean', pos_weight=None)",
        "api_description": "Function that measures Binary Cross Entropy between target and input logits.\nSee BCEWithLogitsLoss for details.",
        "return_value": "",
        "parameters": "\ninput – Tensor of arbitrary shape as unnormalized scores (often referred to as logits).\ntarget – Tensor of the same shape as input with values between 0 and 1\nweight (Tensor, optional) – a manual rescaling weight\nif provided it’s repeated to match input tensor shape\nreduction (string, optional) – Specifies the reduction to apply to the output:\n'none' | 'mean' | 'sum'. 'none': no reduction will be applied,\n'mean': the sum of the output will be divided by the number of\nelements in the output, 'sum': the output will be summed. Note: size_average\nand reduce are in the process of being deprecated, and in the meantime,\nspecifying either of those two args will override reduction. Default: 'mean'\npos_weight (Tensor, optional) – a weight of positive examples.\nMust be a vector with length equal to the number of classes.\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.nn.functional.deform_conv2d",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.functional.deform_conv2d.html",
        "api_signature": "oneflow.nn.functional.deform_conv2d(input: oneflow.Tensor, offset: oneflow.Tensor, weight: oneflow.Tensor, bias: Optional[oneflow.Tensor] = None, stride: Tuple[int, int] = (1, 1)",
        "api_description": "Performs Deformable Convolution v2, described in\nDeformable ConvNets v2: More Deformable, Better Results if mask is not None and\nPerforms Deformable Convolution, described in\nDeformable Convolutional Networks if mask is None.",
        "return_value": "result of convolution\n\n",
        "parameters": "\ninput (Tensor[batch_size, in_channels, in_height, in_width]) – input tensor\noffset (Tensor[batch_size, 2 * offset_groups * kernel_height * kernel_width, out_height, out_width]) – offsets to be applied for each position in the convolution kernel.\nweight (Tensor[out_channels, in_channels // groups, kernel_height, kernel_width]) – convolution weights,\nsplit into groups of size (in_channels // groups)\nbias (Tensor[out_channels]) – optional bias of shape (out_channels,). Default: None\nstride (int or Tuple[int, int]) – distance between convolution centers. Default: 1\npadding (int or Tuple[int, int]) – height/width of padding of zeroes around\neach image. Default: 0\ndilation (int or Tuple[int, int]) – the spacing between kernel elements. Default: 1\nmask (Tensor[batch_size, offset_groups * kernel_height * kernel_width, out_height, out_width]) – masks to be applied for each position in the convolution kernel. Default: None\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.nn.functional.pad",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.functional.pad.html",
        "api_signature": "oneflow.nn.functional.pad(input: oneflow.Tensor, pad: List[int], mode: str = 'constant', value: float = 0.0)",
        "api_description": "Pads tensor.\nThe documentation is referenced from:\nPadding size:The padding size by which to pad some dimensions of input\nare described starting from the last dimension and moving forward.\n\\(\\left\\lfloor\\frac{\\text{len(pad)}}{2}\\right\\rfloor\\) dimensions\nof input will be padded.",
        "return_value": "",
        "parameters": "\ninput (Tensor) – N-dimensional tensor\npad (tuple) – m-elements tuple, where\n\\(\\frac{m}{2} \\leq\\) input dimensions and \\(m\\) is even.\nmode – 'constant', 'reflect', 'replicate' or 'circular'.\nDefault: 'constant'\nvalue – fill value for 'constant' padding. Default: 0\n\n\n\n",
        "input_shape": "",
        "notes": "When using the CUDA backend, this operation may induce nondeterministic\nbehaviour in its backward pass that is not easily switched off.",
        "code_example": "pad has the form\n\\((\\text{padding_left}, \\text{padding_right})\\);\nto pad the last 2 dimensions of the input tensor, then use\n\\((\\text{padding_left}, \\text{padding_right},\\)\n\\(\\text{padding_top}, \\text{padding_bottom})\\);\nto pad the last 3 dimensions, use\n\\((\\text{padding_left}, \\text{padding_right},\\)\n\\(\\text{padding_top}, \\text{padding_bottom}\\)\n\\(\\text{padding_front}, \\text{padding_back})\\).\n\nPadding mode:See oneflow.nn.ConstantPad2d, oneflow.nn.ReflectionPad2d, and\noneflow.nn.ReplicationPad2d for concrete examples on how each of the\npadding modes works. Constant padding is implemented for arbitrary dimensions.\nReplicate and reflection padding is implemented for padding the last 3\ndimensions of 5D input tensor, or the last 2 dimensions of 4D input\ntensor, or the last dimension of 3D input tensor.\n\n\n\n"
    },
    {
        "api_name": "oneflow.nn.functional.interpolate",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.functional.interpolate.html",
        "api_signature": "oneflow.nn.functional.interpolate(input, size=None, scale_factor=None, mode='nearest', align_corners=None, recompute_scale_factor=None)",
        "api_description": "Down/up samples the input to either the given size or the given\nscale_factor\nThe algorithm used for interpolation is determined by mode.\nCurrently temporal, spatial and volumetric sampling are supported, i.e.\nexpected inputs are 3-D, 4-D or 5-D in shape.\nThe input dimensions are interpreted in the form:\nmini-batch x channels x [optional depth] x [optional height] x width.\nThe modes available for resizing are: nearest, linear (3D-only),\nbilinear, bicubic (4D-only), trilinear (5D-only), area",
        "return_value": "",
        "parameters": "\ninput (Tensor) – the input tensor\nsize (int or Tuple[int] or Tuple[int, int] or Tuple[int, int, int]) – output spatial size.\nscale_factor (float or Tuple[float]) – multiplier for spatial size. Has to match input size if it is a tuple.\nmode (str) – algorithm used for upsampling:\n'nearest' | 'linear' | 'bilinear' | 'bicubic' |\n'trilinear' | 'area'. Default: 'nearest'\nalign_corners (bool, optional) – Geometrically, we consider the pixels of the\ninput and output as squares rather than points.\nIf set to True, the input and output tensors are aligned by the\ncenter points of their corner pixels, preserving the values at the corner pixels.\nIf set to False, the input and output tensors are aligned by the corner\npoints of their corner pixels, and the interpolation uses edge value padding\nfor out-of-boundary values, making this operation independent of input size\nwhen scale_factor is kept the same. This only has an effect when mode\nis 'linear', 'bilinear', 'bicubic' or 'trilinear'.\nDefault: False\nrecompute_scale_factor (bool, optional) – recompute the scale_factor for use in the\ninterpolation calculation.  When scale_factor is passed as a parameter, it is used\nto compute the output_size.  If recompute_scale_factor is False or not specified,\nthe passed-in scale_factor will be used in the interpolation computation.\nOtherwise, a new scale_factor will be computed based on the output and input sizes for\nuse in the interpolation computation (i.e. the computation will be identical to if the computed\noutput_size were passed-in explicitly).  Note that when scale_factor is floating-point,\nthe recomputed scale_factor may differ from the one passed in due to rounding and precision\nissues.\n\n\n\n\n",
        "input_shape": "",
        "notes": "With mode='bicubic', it’s possible to cause overshoot, in other words it can produce\nnegative values or values greater than 255 for images.\nExplicitly call result.clamp(min=0, max=255) if you want to reduce the overshoot\nwhen displaying the image.\nWarning\nWith align_corners = True, the linearly interpolating modes\n(linear, bilinear, and trilinear) don’t proportionally align the\noutput and input pixels, and thus the output values can depend on the\ninput size. This was the default behavior for these modes up to version\n0.3.1. Since then, the default behavior is align_corners = False.\nSee Upsample for concrete examples on how this\naffects the outputs.\nWarning\nWhen scale_factor is specified, if recompute_scale_factor=True,\nscale_factor is used to compute the output_size which will then\nbe used to infer new scales for the interpolation.",
        "code_example": ">>> import oneflow as flow\n>>> import numpy as np\n\n>>> input = flow.tensor(np.arange(1, 5).reshape((1, 1, 4)), dtype=flow.float32)\n>>> output = flow.nn.functional.interpolate(input, scale_factor=2.0, mode=\"linear\")\n>>> output\ntensor([[[1.0000, 1.2500, 1.7500, 2.2500, 2.7500, 3.2500, 3.7500, 4.0000]]],\n       dtype=oneflow.float32)\n\n\n\n"
    },
    {
        "api_name": "oneflow.nn.functional.upsample",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.functional.upsample.html",
        "api_signature": null,
        "api_description": "alias of oneflow.nn.modules.upsampling.Upsample",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.nn.functional.grid_sample",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.functional.grid_sample.html",
        "api_signature": "oneflow.nn.functional.grid_sample(input, grid, mode: str = 'bilinear', padding_mode: str = 'zeros', align_corners: bool = False)",
        "api_description": "The documentation is referenced from:\nGiven an input and a flow-field grid, computes the\noutput using input values and pixel locations from grid.\nCurrently, only spatial (4-D) and volumetric (5-D) input are\nsupported.\nIn the spatial (4-D) case, for input with shape\n\\((N, C, H_{in}, W_{in})\\) and grid with shape\n\\((N, H_{out}, W_{out}, 2)\\), the output will have shape\n\\((N, C, H_{out}, W_{out})\\).\nFor each output location output[n, :, h, w], the size-2 vector\ngrid[n, h, w] specifies input pixel locations x and y,\nwhich are used to interpolate the output value output[n, :, h, w].\nIn the case of 5D inputs, grid[n, d, h, w] specifies the\nx, y, z pixel locations for interpolating\noutput[n, :, d, h, w]. mode argument specifies nearest or\nbilinear interpolation method to sample the input pixels.\ngrid specifies the sampling pixel locations normalized by the\ninput spatial dimensions. Therefore, it should have most values in\nthe range of [-1, 1]. For example, values x = -1, y = -1 is the\nleft-top pixel of input, and values  x = 1, y = 1 is the\nright-bottom pixel of input.\nIf grid has values outside the range of [-1, 1], the corresponding\noutputs are handled as defined by padding_mode. Options are\npadding_mode=\"zeros\": use 0 for out-of-bound grid locations,\npadding_mode=\"border\": use border values for out-of-bound grid locations,\npadding_mode=\"reflection\": use values at locations reflected by\nthe border for out-of-bound grid locations. For location far away\nfrom the border, it will keep being reflected until becoming in bound,\ne.g., (normalized) pixel location x = -3.5 reflects by border -1\nand becomes x' = 1.5, then reflects by border 1 and becomes\nx'' = -0.5.",
        "return_value": "output Tensor\n\n",
        "parameters": "\ninput (Tensor) – input of shape \\((N, C, H_{in}, W_{in})\\) (4-D case)\nor \\((N, C, D_{in}, H_{in}, W_{in})\\) (5-D case)\ngrid (Tensor) – flow-field of shape \\((N, H_{out}, W_{out}, 2)\\) (4-D case)\nor \\((N, D_{out}, H_{out}, W_{out}, 3)\\) (5-D case)\nmode (str) – interpolation mode to calculate output values\n'bilinear' | 'nearest' | 'bicubic'. Default: 'bilinear'\n",
        "input_shape": "",
        "notes": "This function is often used in conjunction with affine_grid()\nto build Spatial Transformer Networks .\nNaN values in grid would be interpreted as -1.\nWhen mode='bilinear' and the input is 5-D, the interpolation mode\nused internally will actually be trilinear. However, when the input is 4-D,\nthe interpolation mode will legitimately be bilinear.\npadding_mode (str) – padding mode for outside grid values\n'zeros' | 'border' | 'reflection'. Default: 'zeros'\nalign_corners (bool) – Geometrically, we consider the pixels of the\ninput  as squares rather than points.\nIf set to True, the extrema (-1 and 1) are considered as referring\nto the center points of the input’s corner pixels. If set to False, they\nare instead considered as referring to the corner points of the input’s corner\npixels, making the sampling more resolution agnostic.\nThis option parallels the align_corners option in\ninterpolate(), and so whichever option is used here\nshould also be used there to resize the input image before grid sampling.\nDefault: False\nmode='bicubic' is implemented using the cubic convolution algorithm with \\(\\alpha=-0.75\\).\nThe constant \\(\\alpha\\) might be different from packages to packages.",
        "code_example": "This algorithm may “overshoot” the range of values it’s interpolating.\nClamp the results with :func: flow.clamp to ensure they are within the valid range.\n\n"
    },
    {
        "api_name": "oneflow.nn.functional.affine_grid",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.functional.affine_grid.html",
        "api_signature": "oneflow.nn.functional.affine_grid(theta, size: List[int], align_corners: bool = False)",
        "api_description": "The documentation is referenced from:\nGenerates a 2D or 3D flow field (sampling grid), given a batch of\naffine matrices theta.",
        "return_value": "output Tensor of size (\\(N, H, W, 2\\))\n\n",
        "parameters": "\ntheta (Tensor) – input batch of affine matrices with shape\n(\\(N, 2, 3\\)) for 2D or\n(\\(N, 3, 4\\)) for 3D\nsize (oneflow.Size) – the target output image size.\n(\\(N, C, H, W\\) for 2D or\n\\(N, C, D, H, W\\) for 3D)\nExample: oneflow.Size((32, 3, 24, 24))\nalign_corners (bool) – if True, consider -1 and 1\nto refer to the centers of the corner pixels rather than the image corners.\nRefer to grid_sample() for a more complete description.\nA grid generated by affine_grid() should be passed to grid_sample()\nwith the same setting for this option.\nDefault: False\n\n\n",
        "input_shape": "",
        "notes": "This function is often used in conjunction with grid_sample()\nto build Spatial Transformer Networks .",
        "code_example": ""
    },
    {
        "api_name": "oneflow.nn.functional.ctc_greedy_decoder",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.functional.ctc_greedy_decoder.html",
        "api_signature": "oneflow.nn.functional.ctc_greedy_decoder(log_probs: Tensor, input_lengths: Tensor, merge_repeated: bool = True)",
        "api_description": "Performs greedy decoding on the logits given in input (best path).",
        "return_value": "A Tensor of shape [batch_size, input_length], The decoded outputs.\nneg_sum_logits(oneflow.Tensor): A float matrix (batch_size x 1) containing, for the sequence found, the negative of the sum of the greatest logit at each timeframe.\n\n",
        "parameters": "\nlog_probs (oneflow.Tensor) – A Tensor of shape [input_length, batch_size, num_labels]. The logarithmized probabilities of the outputs (e.g. obtained with flow.nn.logsoftmax()).\ninput_lengths (oneflow.Tensor) – A Tensor of shape [batch_size]. It represent the lengths of the inputs. And the lengths are specified for each sequence to achieve masking under the assumption that sequences are padded to equal lengths.\nmerge_repeated (bool, optional) – If merge_repeated is True, merge repeated classes in output. This means that if consecutive logits’ maximum indices are the same, only the first of these is emitted. Defaults to True.\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> import numpy as np\n>>> log_probs = flow.tensor(\n...     [\n...         [[-1.54, -1.20, -1.95, -1.65, -1.81], [-1.84, -1.74, -1.58, -1.55, -1.12]],\n...         [[-1.68, -1.48, -1.89, -1.30, -2.07], [-1.13, -1.45, -1.24, -1.61, -1.66]],\n...         [[-1.56, -1.40, -2.83, -1.67, -1.48], [-1.20, -2.01, -2.05, -1.95, -1.24]],\n...         [[-2.09, -1.76, -1.36, -1.67, -1.45], [-1.85, -1.48, -1.34, -2.16, -1.55]],\n...     ]\n... )\n>>> input_lengths = flow.tensor([4, 4])\n>>> decoded, neg_sum_logits = flow.nn.functional.ctc_greedy_decoder(log_probs, input_lengths)\n>>> decoded\ntensor([[1, 3, 1, 2],\n        [0, 2, 0, 0]], dtype=oneflow.int64)\n>>> neg_sum_logits\ntensor([[5.2600],\n        [4.7900]], dtype=oneflow.float32)\n\n\n\n"
    },
    {
        "api_name": "oneflow.Tensor.new_empty",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.new_empty.html",
        "api_signature": "Tensor.new_empty(*size, dtype=None, device=None, placement=None, sbp=None, requires_grad=False)",
        "api_description": "",
        "return_value": "\n",
        "parameters": "\nsize (int...) – a list, tuple, or flow.Size of integers defining the shape of the output tensor.\ndtype (flow.dtype, optional) – the desired type of returned tensor. Default: if None, same flow.dtype as this tensor.\ndevice (flow.device, optional) – the desired device of returned tensor. Default: if None, same flow.device as this tensor.\nplacement (flow.placement, optional) – the desired placement of returned global tensor. Default: if None, the returned tensor is local one using the argument device.\nsbp (flow.sbp.sbp or tuple of flow.sbp.sbp, optional) – the desired sbp descriptor of returned global tensor. Default: if None, the returned tensor is local one using the argument device.\nrequires_grad (bool, optional) – If autograd should record operations on the returned tensor. Default: False.\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n\n>>> x = flow.ones(())\n>>> y = x.new_empty((2, 2))\n>>> y.shape\noneflow.Size([2, 2])\n\n\n\n"
    },
    {
        "api_name": "oneflow.Tensor.new_ones",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.new_ones.html",
        "api_signature": "Tensor.new_ones()",
        "api_description": "See oneflow.new_ones()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.new_zeros",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.new_zeros.html",
        "api_signature": "Tensor.new_zeros(size=None, dtype=None, device=None, placement=None, sbp=None, requires_grad=False)",
        "api_description": "",
        "return_value": "\n",
        "parameters": "\nsize (int...) – a list, tuple, or flow.Size of integers defining the shape of the output tensor.\ndtype (flow.dtype, optional) – the desired type of returned tensor. Default: if None, same flow.dtype as this tensor.\ndevice (flow.device, optional) – the desired device of returned tensor. Default: if None, same flow.device as this tensor.\nplacement (flow.placement, optional) – the desired placement of returned global tensor. Default: if None, the returned tensor is local one using the argument device.\nsbp (flow.sbp.sbp or tuple of flow.sbp.sbp, optional) – the desired sbp descriptor of returned global tensor. Default: if None, the returned tensor is local one using the argument device.\nrequires_grad (bool, optional) – If autograd should record operations on the returned tensor. Default: False.\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import numpy as np\n>>> import oneflow as flow\n\n>>> x = flow.Tensor(np.ones((1, 2, 3)))\n>>> y = x.new_zeros((2, 2))\n>>> y\ntensor([[0., 0.],\n        [0., 0.]], dtype=oneflow.float32)\n\n\n\n"
    },
    {
        "api_name": "oneflow.Tensor.new_full",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.new_full.html",
        "api_signature": "Tensor.new_full(size, fill_value, dtype=None, device=None, placement=None, sbp=None, requires_grad=False)",
        "api_description": "",
        "return_value": "\n",
        "parameters": "\nfill_value (scalar) – the number to fill the output tensor with.\nsize (int...) – a list, tuple, or flow.Size of integers defining the shape of the output tensor.\ndtype (flow.dtype, optional) – the desired type of returned tensor. Default: if None, same flow.dtype as this tensor.\ndevice (flow.device, optional) – the desired device of returned tensor. Default: if None, same flow.device as this tensor.\nplacement (flow.placement, optional) – the desired placement of returned global tensor. Default: if None, the returned tensor is local one using the argument device.\nsbp (flow.sbp.sbp or tuple of flow.sbp.sbp, optional) – the desired sbp descriptor of returned global tensor. Default: if None, the returned tensor is local one using the argument device.\nrequires_grad (bool, optional) – If autograd should record operations on the returned tensor. Default: False.\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import numpy as np\n>>> import oneflow as flow\n\n>>> tensor = flow.ones((2,), dtype=flow.float64)\n>>> tensor.new_full((3, 4), 3.141592)\ntensor([[3.1416, 3.1416, 3.1416, 3.1416],\n        [3.1416, 3.1416, 3.1416, 3.1416],\n        [3.1416, 3.1416, 3.1416, 3.1416]], dtype=oneflow.float64)\n\n\n\n"
    },
    {
        "api_name": "oneflow.Tensor.new_tensor",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.new_tensor.html",
        "api_signature": "Tensor.new_tensor(data, dtype=None, device=None, requires_grad=False, placement=None, sbp=None)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.is_cuda",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.is_cuda.html",
        "api_signature": null,
        "api_description": "Is True if the Tensor is stored on the GPU, False otherwise.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.is_global",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.is_global.html",
        "api_signature": null,
        "api_description": "Return whether this Tensor is a global tensor.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.device",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.device.html",
        "api_signature": null,
        "api_description": "Is the oneflow.device where this Tensor is, which is invalid for global tensor.\nThe documentation is referenced from:",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.grad",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.grad.html",
        "api_signature": null,
        "api_description": "Return the gradient calculated by autograd functions. This property is None by default.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.ndim",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.ndim.html",
        "api_signature": null,
        "api_description": "See oneflow.Tensor.dim()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.abs",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.abs.html",
        "api_signature": "Tensor.abs()",
        "api_description": "See oneflow.abs()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.acos",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.acos.html",
        "api_signature": "Tensor.acos()",
        "api_description": "See oneflow.acos()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.acosh",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.acosh.html",
        "api_signature": "Tensor.acosh()",
        "api_description": "See oneflow.acosh()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.add",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.add.html",
        "api_signature": "Tensor.add(other, *, alpha=1)",
        "api_description": "See oneflow.add()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.add_",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.add_.html",
        "api_signature": "Tensor.add_(other, *, alpha=1)",
        "api_description": "In-place version of oneflow.Tensor.add().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.addcdiv",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.addcdiv.html",
        "api_signature": "Tensor.addcdiv()",
        "api_description": "See oneflow.addcdiv()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.addcdiv_",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.addcdiv_.html",
        "api_signature": "Tensor.addcdiv_()",
        "api_description": "In-place version of oneflow.Tensor.addcdiv()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.addcmul",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.addcmul.html",
        "api_signature": "Tensor.addcmul()",
        "api_description": "See oneflow.addcmul()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.addcmul_",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.addcmul_.html",
        "api_signature": "Tensor.addcmul_()",
        "api_description": "In-place version of oneflow.Tensor.addcmul().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.addmm",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.addmm.html",
        "api_signature": "Tensor.addmm(mat1, mat2, alpha=1, beta=1)",
        "api_description": "See oneflow.addmm()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.all",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.all.html",
        "api_signature": "Tensor.all(dim=None, keepdim=False)",
        "api_description": "See oneflow.all()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.amin",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.amin.html",
        "api_signature": "Tensor.amin()",
        "api_description": "See oneflow.amin()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.amax",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.amax.html",
        "api_signature": "Tensor.amax()",
        "api_description": "See oneflow.amax()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.any",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.any.html",
        "api_signature": "Tensor.any(dim=None, keepdim=False)",
        "api_description": "See oneflow.any()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.arccos",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.arccos.html",
        "api_signature": "Tensor.arccos()",
        "api_description": "See oneflow.arccos()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.arccosh",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.arccosh.html",
        "api_signature": "Tensor.arccosh()",
        "api_description": "See oneflow.arccosh()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.arcsin",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.arcsin.html",
        "api_signature": "Tensor.arcsin()",
        "api_description": "See oneflow.arcsin()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.arcsinh",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.arcsinh.html",
        "api_signature": "Tensor.arcsinh()",
        "api_description": "See oneflow.arcsinh()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.arctan",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.arctan.html",
        "api_signature": "Tensor.arctan()",
        "api_description": "See oneflow.arctan()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.arctanh",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.arctanh.html",
        "api_signature": "Tensor.arctanh()",
        "api_description": "See oneflow.arctanh()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.argmax",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.argmax.html",
        "api_signature": "Tensor.argmax()",
        "api_description": "See oneflow.argmax()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.argmin",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.argmin.html",
        "api_signature": "Tensor.argmin()",
        "api_description": "See oneflow.argmin()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.argsort",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.argsort.html",
        "api_signature": "Tensor.argsort(dim=- 1, descending=None)",
        "api_description": "See oneflow.argsort()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.argwhere",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.argwhere.html",
        "api_signature": "Tensor.argwhere()",
        "api_description": "See oneflow.argwhere()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.asin",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.asin.html",
        "api_signature": "Tensor.asin()",
        "api_description": "See oneflow.asin()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.asinh",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.asinh.html",
        "api_signature": "Tensor.asinh()",
        "api_description": "See oneflow.asinh()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.atan",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.atan.html",
        "api_signature": "Tensor.atan()",
        "api_description": "See oneflow.atan()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.atan2",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.atan2.html",
        "api_signature": "Tensor.atan2()",
        "api_description": "See oneflow.atan2()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.atanh",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.atanh.html",
        "api_signature": "Tensor.atanh()",
        "api_description": "See oneflow.atanh()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.backward",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.backward.html",
        "api_signature": "Tensor.backward(gradient=None, retain_graph=False, create_graph=False)",
        "api_description": "Computes the gradient of current tensor w.r.t. graph leaves.\nThe graph is differentiated using the chain rule. If the tensor is non-scalar (i.e. its data has more than one element) and requires gradient, the function additionally requires specifying gradient. It should be a tensor of matching type and location, that contains the gradient of the differentiated function w.r.t. self.\nThis function accumulates gradients in the leaves - you might need to zero .grad attributes or set them to None before calling it. See Default gradient layouts for details on the memory layout of accumulated gradients.",
        "return_value": "",
        "parameters": "\ngradient (Tensor or None) – Gradient w.r.t. the tensor. If it is a tensor, it will be automatically converted to a Tensor that does not require grad unless create_graph is True. None values can be specified for scalar Tensors or ones that don’t require grad. If a None value would be acceptable then this argument is optional.\nretain_graph (bool, optional) – If False, the graph used to compute the grads will be freed. Note that in nearly all cases setting this option to True is not needed and often can be worked around in a much more efficient way. Defaults to the value of create_graph.\ncreate_graph (bool, optional) – If True, graph of the derivative will be constructed, allowing to compute higher order derivative products. Defaults to False.\n\n\n\n\n",
        "input_shape": "",
        "notes": "If you run any forward ops, create gradient, and/or call backward in a user-specified CUDA stream context, see Stream semantics of backward passes.",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.bmm",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.bmm.html",
        "api_signature": "Tensor.bmm()",
        "api_description": "See oneflow.bmm()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.bool",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.bool.html",
        "api_signature": "Tensor.bool()",
        "api_description": "Tensor.bool() is equivalent to Tensor.to(oneflow.bool). See oneflow.Tensor.to().",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> import numpy as np\n\n>>> input = flow.tensor(np.random.randn(1, 2, 3), dtype=flow.float32)\n>>> input = input.bool()\n>>> input.dtype\noneflow.bool\n\n\n\n"
    },
    {
        "api_name": "oneflow.Tensor.byte",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.byte.html",
        "api_signature": "Tensor.byte()",
        "api_description": "self.byte() is equivalent to self.to(oneflow.uint8).\nSee oneflow.Tensor.to()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.cast",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.cast.html",
        "api_signature": "Tensor.cast()",
        "api_description": "See oneflow.cast()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.ceil",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.ceil.html",
        "api_signature": "Tensor.ceil()",
        "api_description": "See oneflow.ceil()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.ceil_",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.ceil_.html",
        "api_signature": "Tensor.ceil_()",
        "api_description": "See oneflow.ceil_()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.chunk",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.chunk.html",
        "api_signature": "Tensor.chunk()",
        "api_description": "See oneflow.chunk()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.clamp",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.clamp.html",
        "api_signature": "Tensor.clamp()",
        "api_description": "See oneflow.clamp().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.clamp_",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.clamp_.html",
        "api_signature": "Tensor.clamp_()",
        "api_description": "Inplace version of oneflow.Tensor.clamp().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.clip",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.clip.html",
        "api_signature": "Tensor.clip()",
        "api_description": "Alias for oneflow.Tensor.clamp().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.clip_",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.clip_.html",
        "api_signature": "Tensor.clip_()",
        "api_description": "Alias for oneflow.Tensor.clamp_().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.clone",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.clone.html",
        "api_signature": "Tensor.clone()",
        "api_description": "See oneflow.clone()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> x = flow.tensor([1, 2, 3])\n>>> x.clone()\ntensor([1, 2, 3], dtype=oneflow.int64)\n\n\n\n"
    },
    {
        "api_name": "oneflow.Tensor.contiguous",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.contiguous.html",
        "api_signature": "Tensor.contiguous()",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.copy_",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.copy_.html",
        "api_signature": "Tensor.copy_(other: Union[oneflow.Tensor, numpy.ndarray])",
        "api_description": "Copies the elements from src into self tensor and returns self.\nThe src tensor must be broadcastable with the self tensor. It may be of a different data type or reside on a different device.",
        "return_value": "",
        "parameters": "\nsrc (Tensor) – the source tensor to copy from\nnon_blocking (bool) – if True and this copy is between CPU and GPU, the copy may occur asynchronously with respect to the host. For other cases, this argument has no effect.\n\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.cos",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.cos.html",
        "api_signature": "Tensor.cos()",
        "api_description": "See oneflow.cos()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.cosh",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.cosh.html",
        "api_signature": "Tensor.cosh()",
        "api_description": "See oneflow.cosh()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.cpu",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.cpu.html",
        "api_signature": "Tensor.cpu()",
        "api_description": "",
        "return_value": "If this object is already in CPU memory and on the correct device, then no copy is performed and the original object is returned.\n",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n\n>>> input = flow.tensor([1, 2, 3, 4, 5], device=flow.device(\"cuda\"))\n>>> output = input.cpu()\n>>> output.device\ndevice(type='cpu', index=0)\n\n\n\n"
    },
    {
        "api_name": "oneflow.Tensor.cuda",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.cuda.html",
        "api_signature": "Tensor.cuda()",
        "api_description": "",
        "return_value": "If this object is already in CUDA memory and on the correct device, then no copy is performed and the original object is returned.\n\n",
        "parameters": "device (flow.device) – The destination GPU device. Defaults to the current CUDA device.\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n\n>>> input = flow.Tensor([1, 2, 3, 4, 5])\n>>> output = input.cuda()\n>>> output.device\ndevice(type='cuda', index=0)\n\n\n\n"
    },
    {
        "api_name": "oneflow.Tensor.cumprod",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.cumprod.html",
        "api_signature": "Tensor.cumprod(dim, dtype=None)",
        "api_description": "See oneflow.cumprod()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.cumsum",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.cumsum.html",
        "api_signature": "Tensor.cumsum(dim, dtype=None)",
        "api_description": "See oneflow.cumsum()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.data",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.data.html",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.dot",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.dot.html",
        "api_signature": "Tensor.dot()",
        "api_description": "See oneflow.dot()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.detach",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.detach.html",
        "api_signature": "Tensor.detach()",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.placement",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.placement.html",
        "api_signature": null,
        "api_description": "Is the oneflow.placement where this Tensor is, which is invalid for local tensor.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.sbp",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.sbp.html",
        "api_signature": null,
        "api_description": "Is the oneflow.sbp representing that how the data of the global tensor is distributed, which is invalid for local tensor.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.diag",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.diag.html",
        "api_signature": "Tensor.diag()",
        "api_description": "See oneflow.diag()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.diagonal",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.diagonal.html",
        "api_signature": "Tensor.diagonal()",
        "api_description": "See oneflow.diagonal()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.dim",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.dim.html",
        "api_signature": "Tensor.dim()",
        "api_description": "Tensor.dim() → int",
        "return_value": "\n",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.div",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.div.html",
        "api_signature": "Tensor.div()",
        "api_description": "See oneflow.div()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.div_",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.div_.html",
        "api_signature": "Tensor.div_(value)",
        "api_description": "In-place version of oneflow.Tensor.div().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.double",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.double.html",
        "api_signature": "Tensor.double()",
        "api_description": "Tensor.double() is equivalent to Tensor.to(flow.float64). See oneflow.Tensor.to().",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> import numpy as np\n\n>>> input = flow.tensor(np.random.randn(1, 2, 3), dtype=flow.int)\n>>> input = input.double()\n>>> input.dtype\noneflow.float64\n\n\n\n"
    },
    {
        "api_name": "oneflow.Tensor.dtype",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.dtype.html",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.digamma",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.digamma.html",
        "api_signature": "Tensor.digamma()",
        "api_description": "See oneflow.digamma()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.element_size",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.element_size.html",
        "api_signature": "Tensor.element_size()",
        "api_description": "Tensor.element_size() → int",
        "return_value": "\n",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.eq",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.eq.html",
        "api_signature": "Tensor.eq(other)",
        "api_description": "See oneflow.eq()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.equal",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.equal.html",
        "api_signature": "Tensor.equal()",
        "api_description": "See oneflow.equal()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.erf",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.erf.html",
        "api_signature": "Tensor.erf()",
        "api_description": "See oneflow.erf()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.erfc",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.erfc.html",
        "api_signature": "Tensor.erfc()",
        "api_description": "See oneflow.erfc()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.erfinv",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.erfinv.html",
        "api_signature": "Tensor.erfinv()",
        "api_description": "See oneflow.erfinv()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.erfinv_",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.erfinv_.html",
        "api_signature": "Tensor.erfinv_()",
        "api_description": "Inplace version of oneflow.erfinv()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.exp",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.exp.html",
        "api_signature": "Tensor.exp()",
        "api_description": "See oneflow.exp()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.exp2",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.exp2.html",
        "api_signature": "Tensor.exp2()",
        "api_description": "See oneflow.exp2()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.expand",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.expand.html",
        "api_signature": "Tensor.expand()",
        "api_description": "See oneflow.expand()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.expand_as",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.expand_as.html",
        "api_signature": "Tensor.expand_as(other)",
        "api_description": "Expand this tensor to the same size as other.\nself.expand_as(other) is equivalent to self.expand(other.size()).\nPlease see expand() for more information about expand.",
        "return_value": "",
        "parameters": "other (oneflow.Tensor) – The result tensor has the same size\nas other.\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.expm1",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.expm1.html",
        "api_signature": "Tensor.expm1()",
        "api_description": "See oneflow.expm1()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.fill_",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.fill_.html",
        "api_signature": "Tensor.fill_()",
        "api_description": "Tensor.fill_(value) → Tensor\nFills self tensor with the specified value.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.flatten",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.flatten.html",
        "api_signature": "Tensor.flatten()",
        "api_description": "See oneflow.flatten()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.flip",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.flip.html",
        "api_signature": "Tensor.flip()",
        "api_description": "See oneflow.flip()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.float",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.float.html",
        "api_signature": "Tensor.float()",
        "api_description": "Tensor.float() is equivalent to Tensor.to(flow.float32). See oneflow.Tensor.to().",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> import numpy as np\n\n>>> input = flow.tensor(np.random.randn(1, 2, 3), dtype=flow.int)\n>>> input = input.float()\n>>> input.dtype\noneflow.float32\n\n\n\n"
    },
    {
        "api_name": "oneflow.Tensor.floor",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.floor.html",
        "api_signature": "Tensor.floor()",
        "api_description": "See oneflow.floor()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.floor_",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.floor_.html",
        "api_signature": "Tensor.floor_()",
        "api_description": "See oneflow.floor_()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.floor_divide",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.floor_divide.html",
        "api_signature": "Tensor.floor_divide()",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.fmod",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.fmod.html",
        "api_signature": "Tensor.fmod(other)",
        "api_description": "See oneflow.fmod()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.gather",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.gather.html",
        "api_signature": "Tensor.gather(dim, index)",
        "api_description": "See oneflow.gather()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.ge",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.ge.html",
        "api_signature": "Tensor.ge()",
        "api_description": "See oneflow.ge()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.get_device",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.get_device.html",
        "api_signature": "Tensor.get_device()",
        "api_description": "For CUDA tensors, this function returns the device ordinal of the GPU on which the tensor resides. For CPU tensors, an error is thrown.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.grad_fn",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.grad_fn.html",
        "api_signature": null,
        "api_description": "Return the function that created this tensor if it’s requires_grad is True.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.gt",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.gt.html",
        "api_signature": "Tensor.gt()",
        "api_description": "See oneflow.gt()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.gt_",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.gt_.html",
        "api_signature": "Tensor.gt_(value)",
        "api_description": "In-place version of oneflow.Tensor.gt().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.half",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.half.html",
        "api_signature": "Tensor.half()",
        "api_description": "self.half() is equivalent to self.to(dtype=oneflow.float16).\nSee oneflow.Tensor.to()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.in_top_k",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.in_top_k.html",
        "api_signature": "Tensor.in_top_k(targets, predictions, k)",
        "api_description": "See oneflow.in_top_k()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.index_select",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.index_select.html",
        "api_signature": "Tensor.index_select(dim, index)",
        "api_description": "See oneflow.index_select()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.index_add",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.index_add.html",
        "api_signature": "Tensor.index_add(dim, index, source, alpha=1)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.index_add_",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.index_add_.html",
        "api_signature": "Tensor.index_add_(dim, index, source, *, alpha=1)",
        "api_description": "Accumulate the elements of alpha times source into the self\ntensor by adding to the indices in the order given in index. For example,\nif dim == 0, index[i] == j, and alpha=-1, then the ith row of\nsource is subtracted from the jth row of self.\nThe dimth dimension of source must have the same size as the\nlength of index (which must be a vector), and all other dimensions must\nmatch self, or an error will be raised.\nFor a 3-D tensor the output is given as:\nself[index[i], :, :] += alpha * src[i, :, :]  # if dim == 0\nself[:, index[i], :] += alpha * src[:, i, :]  # if dim == 1\nself[:, :, index[i]] += alpha * src[:, :, i]  # if dim == 2",
        "return_value": "",
        "parameters": "\ndim (int) – dimension along which to index\nindex (Tensor) – indices of source to select from,\nshould have dtype either oneflow.int64 or oneflow.int32\nsource (Tensor) – the tensor containing values to add\n\n\nKeyword Arguments\nalpha (Number) – the scalar multiplier for source\n\n\n>>> import oneflow as flow\n>>> x = flow.ones(5, 3)\n>>> t = flow.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=flow.float)\n>>> index = flow.tensor([0, 4, 2])\n>>> x.index_add_(0, index, t)\ntensor([[ 2.,  3.,  4.],\n        [ 1.,  1.,  1.],\n        [ 8.,  9., 10.],\n        [ 1.,  1.,  1.],\n        [ 5.,  6.,  7.]], dtype=oneflow.float32)\n>>> x.index_add_(0, index, t, alpha=-1)\ntensor([[1., 1., 1.],\n        [1., 1., 1.],\n        [1., 1., 1.],\n        [1., 1., 1.],\n        [1., 1., 1.]], dtype=oneflow.float32)\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.int",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.int.html",
        "api_signature": "Tensor.int()",
        "api_description": "Tensor.int() is equivalent to Tensor.to(flow.int32). See oneflow.Tensor.to().",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> import numpy as np\n\n>>> input = flow.tensor(np.random.randn(1, 2, 3), dtype=flow.float32)\n>>> input = input.int()\n>>> input.dtype\noneflow.int32\n\n\n\n"
    },
    {
        "api_name": "oneflow.Tensor.is_contiguous",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.is_contiguous.html",
        "api_signature": "Tensor.is_contiguous()",
        "api_description": "",
        "return_value": "\n",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.is_floating_point",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.is_floating_point.html",
        "api_signature": "Tensor.is_floating_point()",
        "api_description": "See oneflow.is_floating_point()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.is_lazy",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.is_lazy.html",
        "api_signature": null,
        "api_description": "Return whether this Tensor is a lazy tensor.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.is_leaf",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.is_leaf.html",
        "api_signature": null,
        "api_description": "All Tensors that have requires_grad which is False will be leaf Tensors by convention.\nFor Tensor that have requires_grad which is True, they will be leaf Tensors if they\nwere created by source operations.\nOnly leaf Tensors will have their grad populated during a call to backward(). To get\ngrad populated for non-leaf Tensors, you can use retain_grad().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> a = flow.rand(10, requires_grad=False)\n>>> a.is_leaf\nTrue\n>>> a = flow.rand(10, requires_grad=True)\n>>> a.is_leaf\nTrue\n>>> b = a.cuda()\n>>> b.is_leaf\nFalse\n>>> c = a + 2\n>>> c.is_leaf\nFalse\n\n\n\n"
    },
    {
        "api_name": "oneflow.Tensor.isinf",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.isinf.html",
        "api_signature": "Tensor.isinf()",
        "api_description": "See oneflow.isinf()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.isnan",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.isnan.html",
        "api_signature": "Tensor.isnan()",
        "api_description": "See oneflow.isnan()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.item",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.item.html",
        "api_signature": "Tensor.item()",
        "api_description": "",
        "return_value": "For other cases, see tolist().\nThis operation is not differentiable.\n\n",
        "parameters": "input (Tensor) – the input tensor.\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> x = flow.tensor([1.0])\n>>> x.item()\n1.0\n\n\n\n"
    },
    {
        "api_name": "oneflow.Tensor.le",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.le.html",
        "api_signature": "Tensor.le()",
        "api_description": "See oneflow.le()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.lerp",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.lerp.html",
        "api_signature": "Tensor.lerp()",
        "api_description": "See oneflow.lerp()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.lerp_",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.lerp_.html",
        "api_signature": "Tensor.lerp_()",
        "api_description": "See oneflow.lerp_()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.log",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.log.html",
        "api_signature": "Tensor.log()",
        "api_description": "See oneflow.log()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.log1p",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.log1p.html",
        "api_signature": "Tensor.log1p()",
        "api_description": "See oneflow.log1p()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.log2",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.log2.html",
        "api_signature": "Tensor.log2()",
        "api_description": "See oneflow.log2()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.log10",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.log10.html",
        "api_signature": "Tensor.log10()",
        "api_description": "See oneflow.log10()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.logical_and",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.logical_and.html",
        "api_signature": "Tensor.logical_and()",
        "api_description": "See oneflow.logical_and()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.logical_or",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.logical_or.html",
        "api_signature": "Tensor.logical_or()",
        "api_description": "See oneflow.logical_or()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.logical_not",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.logical_not.html",
        "api_signature": "Tensor.logical_not()",
        "api_description": "See oneflow.logical_not()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.logical_xor",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.logical_xor.html",
        "api_signature": "Tensor.logical_xor()",
        "api_description": "See oneflow.logical_xor()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.long",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.long.html",
        "api_signature": "Tensor.long()",
        "api_description": "Tensor.long() is equivalent to Tensor.to(flow.int64). See oneflow.Tensor.to().",
        "return_value": "",
        "parameters": "input (Tensor) – the input tensor.\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> import numpy as np\n\n>>> input = flow.tensor(np.random.randn(1, 2, 3), dtype=flow.float32)\n>>> input = input.long()\n>>> input.dtype\noneflow.int64\n\n\n\n"
    },
    {
        "api_name": "oneflow.Tensor.lt",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.lt.html",
        "api_signature": "Tensor.lt()",
        "api_description": "See oneflow.lt()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.masked_fill",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.masked_fill.html",
        "api_signature": "Tensor.masked_fill()",
        "api_description": "See oneflow.masked_fill()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.masked_fill_",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.masked_fill_.html",
        "api_signature": "Tensor.masked_fill_()",
        "api_description": "In-place version of oneflow.Tensor.masked_fill().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.masked_select",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.masked_select.html",
        "api_signature": "Tensor.masked_select(mask)",
        "api_description": "See oneflow.masked_select()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.matmul",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.matmul.html",
        "api_signature": "Tensor.matmul()",
        "api_description": "See oneflow.matmul()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.mm",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.mm.html",
        "api_signature": "Tensor.mm()",
        "api_description": "See oneflow.mm()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.mv",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.mv.html",
        "api_signature": "Tensor.mv()",
        "api_description": "See oneflow.mv()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.max",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.max.html",
        "api_signature": "Tensor.max(dim, index)",
        "api_description": "See oneflow.max()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.maximum",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.maximum.html",
        "api_signature": "Tensor.maximum()",
        "api_description": "See oneflow.maximum()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.median",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.median.html",
        "api_signature": "Tensor.median()",
        "api_description": "See oneflow.median()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.mean",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.mean.html",
        "api_signature": "Tensor.mean(dim=None, keepdim=False)",
        "api_description": "See oneflow.mean()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.min",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.min.html",
        "api_signature": "Tensor.min(dim, index)",
        "api_description": "See oneflow.min()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.minimum",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.minimum.html",
        "api_signature": "Tensor.minimum()",
        "api_description": "See oneflow.minimum()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.mish",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.mish.html",
        "api_signature": "Tensor.mish()",
        "api_description": "See oneflow.mish()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.mode",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.mode.html",
        "api_signature": "Tensor.mode()",
        "api_description": "See oneflow.mode()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.mul",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.mul.html",
        "api_signature": "Tensor.mul(value)",
        "api_description": "See oneflow.mul()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.mul_",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.mul_.html",
        "api_signature": "Tensor.mul_(value)",
        "api_description": "In-place version of oneflow.Tensor.mul().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.frac",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.frac.html",
        "api_signature": "Tensor.frac()",
        "api_description": "See oneflow.frac().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.frac_",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.frac_.html",
        "api_signature": "Tensor.frac_()",
        "api_description": "In-place version of oneflow.Tensor.frac().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.nansum",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.nansum.html",
        "api_signature": "Tensor.nansum()",
        "api_description": "See oneflow.nansum()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> x = flow.tensor([1., 2., float(\"nan\")])\n>>> x.nansum()\ntensor(3., dtype=oneflow.float32)\n>>> x = flow.tensor([[1., float(\"nan\")], [float(\"nan\"), 2]])\n>>> x.nansum(dim=1, keepdim=True)\ntensor([[1.],\n        [2.]], dtype=oneflow.float32)\n\n\n\n"
    },
    {
        "api_name": "oneflow.Tensor.narrow",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.narrow.html",
        "api_signature": "Tensor.narrow()",
        "api_description": "See oneflow.narrow()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.ndimension",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.ndimension.html",
        "api_signature": "Tensor.ndimension()",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.ne",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.ne.html",
        "api_signature": "Tensor.ne()",
        "api_description": "See oneflow.ne()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.neg",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.neg.html",
        "api_signature": "Tensor.neg()",
        "api_description": "See oneflow.neg()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.negative",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.negative.html",
        "api_signature": "Tensor.negative()",
        "api_description": "See oneflow.negative()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.nelement",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.nelement.html",
        "api_signature": "Tensor.nelement()",
        "api_description": "Tensor.nelement() → int\nAlias for numel()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.nonzero",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.nonzero.html",
        "api_signature": "Tensor.nonzero(input, as_tuple=False)",
        "api_description": "See oneflow.nonzero()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.norm",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.norm.html",
        "api_signature": "Tensor.norm(p=None, dim=None, keepdim=False, dtype=None)",
        "api_description": "See oneflow.norm()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.normal_",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.normal_.html",
        "api_signature": "Tensor.normal_(mean=0, std=1, *, generator=None)",
        "api_description": "Fills self tensor with elements samples from the normal distribution parameterized by mean and std.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.numel",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.numel.html",
        "api_signature": "Tensor.numel()",
        "api_description": "See oneflow.numel()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.numpy",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.numpy.html",
        "api_signature": "Tensor.numpy(dtype=None)",
        "api_description": "Tensor.numpy() → numpy.ndarray",
        "return_value": "\n\n\n",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.offload",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.offload.html",
        "api_signature": "Tensor.offload()",
        "api_description": "Transfer tensor data from GPU memory back to host (CPU) memory. If the tensor is already in host (CPU) memory, the operation does nothing and gives a warning.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "Both global tensor and local tensor of oneflow are applicable to this operation.\nUse with oneflow.Tensor.load() and oneflow.Tensor.is_offloaded().\nThe behavior of load() is the opposite of offload(), is_offloaded() returns a boolean indicating whether the tensor has been moved to CPU memory.\nIn addition, support for offloading elements of oneflow.nn.Module.parameters() is provided.",
        "code_example": ">>> import oneflow as flow\n>>> import numpy as np\n\n>>> # local tensor\n>>> x = flow.tensor(np.random.randn(1024, 1024, 100), dtype=flow.float32, device=flow.device(\"cuda\"), )\n>>> before_id = id(x)\n>>> x.offload() # Move the Tensor from the GPU to the CPU\n>>> after_id = id(x)\n>>> after_id == before_id\nTrue\n>>> x.is_offloaded()\nTrue\n>>> x.load() # Move the Tensor from the cpu to the gpu\n>>> x.is_offloaded()\nFalse\n\n\n>>> import oneflow as flow\n\n>>> # global tensor\n>>> # Run on 2 ranks respectively\n>>> placement = flow.placement(\"cuda\", ranks=[0, 1])\n>>> sbp = flow.sbp.broadcast\n>>> x = flow.randn(1024, 1024, 100, dtype=flow.float32, placement=placement, sbp=sbp) \n>>> before_id = id(x) \n>>> x.offload() \n>>> after_id = id(x) \n>>> print(after_id == before_id) \n>>> print(x.is_offloaded()) \n>>> x.load() \n>>> print(x.is_offloaded()) \n\n\n\n"
    },
    {
        "api_name": "oneflow.Tensor.load",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.load.html",
        "api_signature": "Tensor.load()",
        "api_description": "Load tensor data stored on the host (CPU) back to GPU memory. If the tensor is already in GPU memory, the operation does nothing and gives a warning.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.is_offloaded",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.is_offloaded.html",
        "api_signature": "Tensor.is_offloaded()",
        "api_description": "Determine whether the tensor has been moved to CPU memory and the CUDA device memory has been released.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.permute",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.permute.html",
        "api_signature": "Tensor.permute()",
        "api_description": "See oneflow.permute()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.pow",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.pow.html",
        "api_signature": "Tensor.pow()",
        "api_description": "See oneflow.pow()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.prod",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.prod.html",
        "api_signature": "Tensor.prod(dim=None, keepdim=False)",
        "api_description": "See oneflow.prod()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.quantile",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.quantile.html",
        "api_signature": "Tensor.quantile()",
        "api_description": "See oneflow.quantile()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.reciprocal",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.reciprocal.html",
        "api_signature": "Tensor.reciprocal()",
        "api_description": "See oneflow.reciprocal()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.register_hook",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.register_hook.html",
        "api_signature": "Tensor.register_hook(hook)",
        "api_description": "Registers a backward hook.\nThe hook will be called every time a gradient with respect to the Tensor is computed.\nThe hook should have the following signature:\nhook(grad) -> Tensor or None\nThe hook should not modify its argument, but it can optionally return a new gradient which\nwill be used in place of grad.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> x = flow.ones(5, requires_grad=True)\n>>> def hook(grad):\n...     return grad * 2\n>>> x.register_hook(hook)\n>>> y = x * 2\n>>> y.sum().backward()\n>>> x.grad\ntensor([4., 4., 4., 4., 4.], dtype=oneflow.float32)\n\n\n\n"
    },
    {
        "api_name": "oneflow.Tensor.relu",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.relu.html",
        "api_signature": "Tensor.relu()",
        "api_description": "See oneflow.relu()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.repeat",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.repeat.html",
        "api_signature": "Tensor.repeat(*size)",
        "api_description": "See oneflow.repeat()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.repeat_interleave",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.repeat_interleave.html",
        "api_signature": "Tensor.repeat_interleave(repeats, dim=None, *, output_size=None)",
        "api_description": "See oneflow.repeat_interleave()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.requires_grad",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.requires_grad.html",
        "api_signature": null,
        "api_description": "Is True if gradient need to be computed for this Tensor, False otherwise.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.requires_grad_",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.requires_grad_.html",
        "api_signature": "Tensor.requires_grad_(requires_grad=True)",
        "api_description": "Sets this tensor’s requires_grad attribute in-place. Returns this tensor.",
        "return_value": "",
        "parameters": "requires_grad (bool) – Change the requires_grad flag for this Tensor. Default is True.\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> a = flow.rand(10, requires_grad=False)\n>>> a.requires_grad\nFalse\n>>> a = a.requires_grad_(requires_grad=True)\n>>> a.requires_grad\nTrue\n\n\n\n"
    },
    {
        "api_name": "oneflow.Tensor.reshape",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.reshape.html",
        "api_signature": "Tensor.reshape()",
        "api_description": "See oneflow.reshape()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.reshape_as",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.reshape_as.html",
        "api_signature": "Tensor.reshape_as(other)",
        "api_description": "",
        "return_value": "self.reshape_as(other) is equivalent to self.reshape(other.sizes()).\nThis method returns a view if other.sizes() is compatible with the current shape.\nSee oneflow.Tensor.view() on when it is possible to return a view.\nPlease see reshape() for more information about reshape. See oneflow.reshape()\n",
        "parameters": "other (oneflow.Tensor) – The result tensor has the same shape as other.\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.retain_grad",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.retain_grad.html",
        "api_signature": "Tensor.retain_grad()",
        "api_description": "Enables this Tensor to have their grad populated during backward(). This is a no-op\nfor leaf tensors.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.roll",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.roll.html",
        "api_signature": "Tensor.roll()",
        "api_description": "See oneflow.roll()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.round",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.round.html",
        "api_signature": "Tensor.round()",
        "api_description": "See oneflow.round()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.round_",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.round_.html",
        "api_signature": "Tensor.round_()",
        "api_description": "See oneflow.round_()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.rsqrt",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.rsqrt.html",
        "api_signature": "Tensor.rsqrt()",
        "api_description": "See oneflow.rsqrt()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.selu",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.selu.html",
        "api_signature": "Tensor.selu()",
        "api_description": "See oneflow.selu()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.shape",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.shape.html",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.sigmoid",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.sigmoid.html",
        "api_signature": "Tensor.sigmoid()",
        "api_description": "See oneflow.sigmoid()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.sign",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.sign.html",
        "api_signature": "Tensor.sign()",
        "api_description": "See oneflow.sign()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.silu",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.silu.html",
        "api_signature": "Tensor.silu()",
        "api_description": "See oneflow.silu()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.sin",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.sin.html",
        "api_signature": "Tensor.sin()",
        "api_description": "See oneflow.sin()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.sin_",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.sin_.html",
        "api_signature": "Tensor.sin_()",
        "api_description": "See oneflow.sin_()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.sinh",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.sinh.html",
        "api_signature": "Tensor.sinh()",
        "api_description": "See oneflow.sinh()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.size",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.size.html",
        "api_signature": "Tensor.size()",
        "api_description": "",
        "return_value": "The interface is consistent with PyTorch.\n\n",
        "parameters": "idx (int, optional) – The dimension for which to retrieve the size.\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.softmax",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.softmax.html",
        "api_signature": "Tensor.softmax()",
        "api_description": "See oneflow.softmax()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.softplus",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.softplus.html",
        "api_signature": "Tensor.softplus()",
        "api_description": "See oneflow.softplus()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.softsign",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.softsign.html",
        "api_signature": "Tensor.softsign()",
        "api_description": "See oneflow.softsign()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.sort",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.sort.html",
        "api_signature": "Tensor.sort(dim: int = - 1, descending: bool = False)",
        "api_description": "See oneflow.sort()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.split",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.split.html",
        "api_signature": "Tensor.split()",
        "api_description": "See oneflow.split()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.sqrt",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.sqrt.html",
        "api_signature": "Tensor.sqrt()",
        "api_description": "See oneflow.sqrt()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.square",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.square.html",
        "api_signature": "Tensor.square()",
        "api_description": "See oneflow.square()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.squeeze",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.squeeze.html",
        "api_signature": "Tensor.squeeze(dim=None)",
        "api_description": "See oneflow.squeeze()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.squeeze_",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.squeeze_.html",
        "api_signature": "Tensor.squeeze_(dim=None)",
        "api_description": "In-place version of oneflow.Tensor.squeeze()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.std",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.std.html",
        "api_signature": "Tensor.std()",
        "api_description": "See oneflow.std()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.storage_offset",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.storage_offset.html",
        "api_signature": "Tensor.storage_offset()",
        "api_description": "",
        "return_value": "Example:\n>>> import oneflow as flow\n>>> x = flow.tensor([1, 2, 3, 4, 5])\n>>> x.storage_offset()\n0\n\n\n\n",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.stride",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.stride.html",
        "api_signature": "Tensor.stride()",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.logsumexp",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.logsumexp.html",
        "api_signature": "Tensor.logsumexp()",
        "api_description": "See oneflow.logsumexp()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.sum",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.sum.html",
        "api_signature": "Tensor.sum(dim=None, keepdim=False)",
        "api_description": "See oneflow.sum()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.swapaxes",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.swapaxes.html",
        "api_signature": "Tensor.swapaxes()",
        "api_description": "See oneflow.swapaxes()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.swapdims",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.swapdims.html",
        "api_signature": "Tensor.swapdims()",
        "api_description": "See oneflow.swapdims()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.sub",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.sub.html",
        "api_signature": "Tensor.sub()",
        "api_description": "See oneflow.sub()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.sub_",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.sub_.html",
        "api_signature": "Tensor.sub_(value)",
        "api_description": "In-place version of oneflow.Tensor.sub().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.tan",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.tan.html",
        "api_signature": "Tensor.tan()",
        "api_description": "See oneflow.tan()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.tanh",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.tanh.html",
        "api_signature": "Tensor.tanh()",
        "api_description": "See oneflow.tanh()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.tile",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.tile.html",
        "api_signature": "Tensor.tile(*dims)",
        "api_description": "See oneflow.tile()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.to",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.to.html",
        "api_signature": "Tensor.to(*args, **kwargs)",
        "api_description": "Performs Tensor dtype and/or device conversion.A flow.dtype and flow.device are inferred from the arguments of input.to(*args, **kwargs).",
        "return_value": "A Tensor.\n\n",
        "parameters": "\ninput (oneflow.Tensor) – An input tensor.\n*args (oneflow.Tensor or oneflow.device or oneflow.dtype) – Positional arguments\n**kwargs (oneflow.device or oneflow.dtype) – Key-value arguments\n\n\n",
        "input_shape": "",
        "notes": "If the input Tensor already\nhas the correct flow.dtype and flow.device, then input is returned.\nOtherwise, the returned tensor is a copy of input with the desired.",
        "code_example": ">>> import numpy as np\n>>> import oneflow as flow\n\n>>> arr = np.random.randint(1, 9, size=(1, 2, 3, 4))\n>>> input = flow.Tensor(arr)\n>>> output = input.to(dtype=flow.float32)\n>>> np.array_equal(arr.astype(np.float32), output.numpy())\nTrue\n\n\n\n"
    },
    {
        "api_name": "oneflow.Tensor.local_to_global",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.local_to_global.html",
        "api_signature": "Tensor.local_to_global(placement=None, sbp=None, *, check_meta=True, copy=False)",
        "api_description": "Creates a global tensor from a local tensor.",
        "return_value": "",
        "parameters": "\nplacement (flow.placement, optional) – the desired placement of returned global tensor. Default: None\nsbp (flow.sbp.sbp or tuple of flow.sbp.sbp, optional) – the desired sbp of returned global tensor. Default: None\n\n\nKeyword Arguments\n\ncheck_meta (bool, optional) – indicates whether to check meta information when createing global tensor from local\ntensor. Only can be set to False when the shape and dtype of the input local tensor on each rank are the same. If set to False, the\nexecution of local_to_global can be accelerated. Default: True\ncopy (bool, optional) – When copy is set, the returned global tensor takes the replication of this tensor as its local component in the current rank. Default: False\n\n\n\n>>> # Run on 2 ranks respectively\n>>> import oneflow as flow\n>>> input = flow.tensor([0., 1.], dtype=flow.float32) \n>>> output = input.local_to_global(placement=flow.placement(\"cpu\", ranks=[0, 1]), sbp=[flow.sbp.split(0)], check_meta=False) \n>>> print(output.size()) \n>>> print(output) \n\n\n>>> # results on rank 0\noneflow.Size([4])\ntensor([0., 1., 0., 1.], placement=oneflow.placement(type=\"cpu\", ranks=[0, 1]), sbp=(oneflow.sbp.split(dim=0),), dtype=oneflow.float32)\n\n\n>>> # results on rank 1\noneflow.Size([4])\ntensor([0., 1., 0., 1.], placement=oneflow.placement(type=\"cpu\", ranks=[0, 1]), sbp=(oneflow.sbp.split(dim=0),), dtype=oneflow.float32)\n\n\n\n",
        "input_shape": "",
        "notes": "This tensor must be local tensor.\nBoth placement and sbp are required.\nThe returned global tensor takes this tensor as its local component in the current rank.\nThere is no data communication usually, but when sbp is oneflow.sbp.broadcast, the data on rank 0 will be broadcast to other ranks.\nWarning\nWhen the sbp is oneflow.sbp.broadcast, the data on the non-0 rank will be modified. If you want to keep the input local tensor unchanged,\nplease set the arg copy to True.",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.global_to_global",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.global_to_global.html",
        "api_signature": "Tensor.global_to_global(placement=None, sbp=None, *, grad_sbp=None, check_meta=False, copy=False)",
        "api_description": "Performs Tensor placement and/or sbp conversion.",
        "return_value": "",
        "parameters": "\nplacement (flow.placement, optional) – the desired placement of returned global tensor. Default: None\nsbp (flow.sbp.sbp or tuple of flow.sbp.sbp, optional) – the desired sbp of returned global tensor. Default: None\n\n\nKeyword Arguments\n\ngrad_sbp (flow.sbp.sbp or tuple of flow.sbp.sbp, optional) – manually specify the sbp of this tensor’s grad\ntensor in the backward pass. If None, the grad tensor sbp will be infered automatically. Default: None\ncheck_meta (bool, optional) – indicates whether to check meta information. If set to True, check the consistency\nof the input meta information (placement and sbp) on each rank. Default: False\ncopy (bool, optional) – When copy is set, a new Tensor is created even when the Tensor already matches the desired conversion. Default: False\n\n\n\n>>> # Run on 2 ranks respectively\n>>> import oneflow as flow\n>>> input = flow.tensor([0., 1.], dtype=flow.float32, placement=flow.placement(\"cpu\", ranks=[0, 1]), sbp=[flow.sbp.broadcast]) \n>>> output = input.global_to_global(placement=flow.placement(\"cpu\", ranks=[0, 1]), sbp=[flow.sbp.split(0)]) \n>>> print(output.size()) \n>>> print(output) \n\n\n>>> # results on rank 0\noneflow.Size([2])\ntensor([0., 1.], placement=oneflow.placement(type=\"cpu\", ranks=[0, 1]), sbp=(oneflow.sbp.split(dim=0),), dtype=oneflow.float32)\n\n\n>>> # results on rank 1\noneflow.Size([2])\ntensor([0., 1.], placement=oneflow.placement(type=\"cpu\", ranks=[0, 1]), sbp=(oneflow.sbp.split(dim=0),), dtype=oneflow.float32)\n\n\n\n",
        "input_shape": "",
        "notes": "This tensor must be global tensor.\nAt least one of placement and sbp is required.\nIf placement and sbp are all the same as this tensor’s own placement and sbp, then returns this tensor own.",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.to_global",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.to_global.html",
        "api_signature": "Tensor.to_global(placement=None, sbp=None, **kwargs)",
        "api_description": "Creates a global tensor if this tensor is a local tensor, otherwise performs Tensor placement and/or sbp conversion.",
        "return_value": "",
        "parameters": "\nplacement (flow.placement, optional) – the desired placement of returned global tensor. Default: None\nsbp (flow.sbp.sbp or tuple of flow.sbp.sbp, optional) – the desired sbp of returned global tensor. Default: None\n\n\nKeyword Arguments\n\ngrad_sbp (flow.sbp.sbp or tuple of flow.sbp.sbp, optional) – manually specify the sbp of this tensor’s grad\ntensor in the backward pass. If None, the grad tensor sbp will be infered automatically. It is only used if this tensor is a\nglobal tensor. Default: None\ncheck_meta (bool, optional) – indicates whether to check meta information. If set to True, check the input meta\ninformation on each rank. Default: True if this tensor is a local tensor, False if this tensor is a global tensor\ncopy (bool, optional) – When copy is set, copy occurres in this operation. For local tensor, the returned global tensor takes the\nreplication of this tensor as its local component in the current rank. For global tensor, a new Tensor is created even when\nthe Tensor already matches the desired conversion. Default: False\n\n\n\nFor local tensor:\n>>> # Run on 2 ranks respectively\n>>> import oneflow as flow\n>>> input = flow.tensor([0., 1.], dtype=flow.float32) \n>>> output = input.to_global(placement=flow.placement(\"cpu\", ranks=[0, 1]), sbp=[flow.sbp.split(0)], check_meta=False) \n>>> print(output.size()) \n>>> print(output) \n\n\n>>> # results on rank 0\noneflow.Size([4])\ntensor([0., 1., 0., 1.], placement=oneflow.placement(type=\"cpu\", ranks=[0, 1]), sbp=(oneflow.sbp.split(dim=0),), dtype=oneflow.float32)\n\n\n>>> # results on rank 1\noneflow.Size([4])\ntensor([0., 1., 0., 1.], placement=oneflow.placement(type=\"cpu\", ranks=[0, 1]), sbp=(oneflow.sbp.split(dim=0),), dtype=oneflow.float32)\n\n\nFor global tensor:\n>>> # Run on 2 ranks respectively\n>>> import oneflow as flow\n>>> input = flow.tensor([0., 1.], dtype=flow.float32, placement=flow.placement(\"cpu\", ranks=[0, 1]), sbp=[flow.sbp.broadcast]) \n>>> output = input.to_global(placement=flow.placement(\"cpu\", ranks=[0, 1]), sbp=[flow.sbp.split(0)]) \n>>> print(output.size()) \n>>> print(output) \n\n\n>>> # results on rank 0\noneflow.Size([2])\ntensor([0., 1.], placement=oneflow.placement(type=\"cpu\", ranks=[0, 1]), sbp=(oneflow.sbp.split(dim=0),), dtype=oneflow.float32)\n\n\n>>> # results on rank 1\noneflow.Size([2])\ntensor([0., 1.], placement=oneflow.placement(type=\"cpu\", ranks=[0, 1]), sbp=(oneflow.sbp.split(dim=0),), dtype=oneflow.float32)\n\n\n\n",
        "input_shape": "",
        "notes": "This tensor can be local tensor or global tensor.\nFor local tensor\nBoth placement and sbp are required.\nThe returned global tensor takes this tensor as its local component in the current rank.\nThere is no data communication usually, but when sbp is oneflow.sbp.broadcast, the data on rank 0 will be broadcast to other ranks.\nFor global tensor\nAt least one of placement and sbp is required.\nIf placement and sbp are all the same as this tensor’s own placement and sbp, then returns this tensor own.\nWarning\nWhen the input tensor is a local tensor and sbp is oneflow.sbp.broadcast, the data on the non-0 rank will be modified.\nIf you want to keep the input local tensor unchanged, please set the arg copy to True.",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.to_local",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.to_local.html",
        "api_signature": "Tensor.to_local(**kwargs)",
        "api_description": "",
        "return_value": "\nKeyword Arguments\ncopy (bool, optional) – When copy is set, a new replicated tensor of the local component of this global tensor in the current rank is returned. Default: False\n\n\n\n",
        "parameters": "",
        "input_shape": "",
        "notes": "This tensor should be a global tensor, and it returns a empty tensor if there is no local component in the current rank.\nNo copy occurred in this operation if copy is not set.",
        "code_example": ">>> # Run on 2 ranks respectively\n>>> import oneflow as flow\n>>> x = flow.tensor([0., 1.], dtype=flow.float32, placement=flow.placement(\"cpu\", ranks=[0, 1]), sbp=[flow.sbp.split(0)]) \n>>> y = x.to_local() \n>>> print(y.size()) \n>>> print(y) \n\n\n>>> # results on rank 0\noneflow.Size([1])\ntensor([0.], dtype=oneflow.float32)\n\n\n>>> # results on rank 1\noneflow.Size([1])\ntensor([1.], dtype=oneflow.float32)\n\n\n\n"
    },
    {
        "api_name": "oneflow.Tensor.to_consistent",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.to_consistent.html",
        "api_signature": "Tensor.to_consistent(*args, **kwargs)",
        "api_description": "This interface is no longer available, please use oneflow.Tensor.to_global() instead.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.tolist",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.tolist.html",
        "api_signature": "Tensor.tolist()",
        "api_description": "",
        "return_value": "just like with item(). Tensors are automatically moved to the CPU first if necessary.\nThis operation is not differentiable.\n\n",
        "parameters": "input (Tensor) – the input tensor.\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> input = flow.tensor([[1,2,3], [4,5,6]])\n>>> input.tolist()\n[[1, 2, 3], [4, 5, 6]]\n\n\n\n"
    },
    {
        "api_name": "oneflow.Tensor.topk",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.topk.html",
        "api_signature": "Tensor.topk()",
        "api_description": "See oneflow.topk()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.transpose",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.transpose.html",
        "api_signature": "Tensor.transpose()",
        "api_description": "See oneflow.transpose()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.tril",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.tril.html",
        "api_signature": "Tensor.tril()",
        "api_description": "See oneflow.tril()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.triu",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.triu.html",
        "api_signature": "Tensor.triu()",
        "api_description": "See oneflow.triu()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.trunc",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.trunc.html",
        "api_signature": "Tensor.trunc()",
        "api_description": "See oneflow.trunc()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.type_as",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.type_as.html",
        "api_signature": "Tensor.type_as()",
        "api_description": "",
        "return_value": "\n\n\n",
        "parameters": "\ninput (Tensor) – the input tensor.\ntarget (Tensor) – the tensor which has the desired type.\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> import numpy as np\n\n>>> input = flow.tensor(np.random.randn(1, 2, 3), dtype=flow.float32)\n>>> target = flow.tensor(np.random.randn(4, 5, 6), dtype = flow.int32)\n>>> input = input.type_as(target)\n>>> input.dtype\noneflow.int32\n\n\n\n"
    },
    {
        "api_name": "oneflow.Tensor.type",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.type.html",
        "api_signature": "Tensor.type(dtype=None, non_blocking=False, **kwargs)",
        "api_description": "",
        "return_value": "If this is already of the correct type, no copy is performed and the original object is returned.\n\n",
        "parameters": "\ndtype (oneflow.dtype or oneflow.tensortype or string, optional) – The desired type.\nnon_blocking (bool) – (Not Implemented yet) If True, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed asynchronously with respect to the host.\nOtherwise, the argument has no effect.\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> a = flow.tensor([1, 2], dtype=flow.float32)\n>>> a.type()\n'oneflow.FloatTensor'\n>>> a.type(flow.int8)  # dtype input\ntensor([1, 2], dtype=oneflow.int8)\n>>> a.type(flow.cuda.DoubleTensor)  # tensortype input\ntensor([1., 2.], device='cuda:0', dtype=oneflow.float64)\n>>> a.type(\"oneflow.HalfTensor\")  # string input\ntensor([1., 2.], dtype=oneflow.float16)\n\n\n\n"
    },
    {
        "api_name": "oneflow.Tensor.t",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.t.html",
        "api_signature": "Tensor.t()",
        "api_description": "See oneflow.t()\nTensor.t() → Tensor",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.T",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.T.html",
        "api_signature": null,
        "api_description": "Is this Tensor with its dimensions reversed.\nIf n is the number of dimensions in x, x.T is equivalent to x.permute(n-1, n-2, …, 0).",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.unbind",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.unbind.html",
        "api_signature": "Tensor.unbind()",
        "api_description": "See oneflow.unbind()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.unfold",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.unfold.html",
        "api_signature": "Tensor.unfold()",
        "api_description": "",
        "return_value": "tensor in the dimension dimension.\nStep between two slices is given by step.\nIf sizedim is the size of dimension dimension for self, the size of dimension dimension in the\nreturned tensor will be (sizedim - size) / step + 1.\nAn additional dimension of size size is appended in the returned tensor.\nThe interface is consistent with PyTorch.\nThe documentation is referenced from: https://pytorch.org/docs/1.10/generated/torch.Tensor.unfold.html.\n\n",
        "parameters": "\ndimension (int) – dimension in which unfolding happens\nsize (int) – the size of each slice that is unfolded\nstep (int) – the step between each slice\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import numpy as np\n>>> import oneflow as flow\n\n>>> x = flow.arange(1, 8)\n>>> x\ntensor([1, 2, 3, 4, 5, 6, 7], dtype=oneflow.int64)\n>>> x.unfold(0, 2, 1)\ntensor([[1, 2],\n        [2, 3],\n        [3, 4],\n        [4, 5],\n        [5, 6],\n        [6, 7]], dtype=oneflow.int64)\n>>> x.unfold(0, 2, 2)\ntensor([[1, 2],\n        [3, 4],\n        [5, 6]], dtype=oneflow.int64)\n\n\n\n"
    },
    {
        "api_name": "oneflow.Tensor.uniform_",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.uniform_.html",
        "api_signature": "Tensor.uniform_(a=0, b=1)",
        "api_description": "Tensor.uniform_(from=0, to=1) → Tensor\nFills self tensor with numbers sampled from the continuous uniform distribution:\n\\[P(x)=1/(to-from)\\]",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.unsqueeze",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.unsqueeze.html",
        "api_signature": "Tensor.unsqueeze(dim)",
        "api_description": "See oneflow.unsqueeze()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.unsqueeze_",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.unsqueeze_.html",
        "api_signature": "Tensor.unsqueeze_(dim)",
        "api_description": "In-place version of oneflow.Tensor.unsqueeze()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.as_strided",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.as_strided.html",
        "api_signature": "Tensor.as_strided(size, stride, storage_offset=None)",
        "api_description": "See oneflow.as_strided()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.as_strided_",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.as_strided_.html",
        "api_signature": "Tensor.as_strided_(size, stride, storage_offset=None)",
        "api_description": "In-place version of oneflow.Tensor.as_strided()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.var",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.var.html",
        "api_signature": "Tensor.var()",
        "api_description": "See oneflow.var()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.view",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.view.html",
        "api_signature": "Tensor.view()",
        "api_description": "",
        "return_value": "different shape.\nThe returned tensor shares the same data and must have the same number\nof elements, but may have a different size. For a tensor to be viewed, the new\nview size must be compatible with its original size and stride, i.e., each new\nview dimension must either be a subspace of an original dimension, or only span\nacross original dimensions \\(d, d+1, \\dots, d+k\\) that satisfy the following\ncontiguity-like condition that \\(\\forall i = d, \\dots, d+k-1\\),\n\n\\[\\text{stride}[i] = \\text{stride}[i+1] \\times \\text{size}[i+1]\\]\nOtherwise, it will not be possible to view self tensor as shape\nwithout copying it (e.g., via contiguous()). When it is unclear whether a\nview() can be performed, it is advisable to use reshape(), which\nreturns a view if the shapes are compatible, and copies (equivalent to calling\ncontiguous()) otherwise.\nThe interface is consistent with PyTorch.\nThe documentation is referenced from: https://pytorch.org/docs/1.10/generated/torch.Tensor.view.html.\n\nA Tensor has the same type as input.\n\n\n",
        "parameters": "\ninput – A Tensor.\n*shape – flow.Size or int…\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import numpy as np\n>>> import oneflow as flow\n\n>>> x = np.array(\n...    [[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12], [13, 14, 15, 16]]\n... ).astype(np.float32)\n>>> input = flow.Tensor(x)\n\n>>> y = input.view(2, 2, 2, -1).numpy().shape\n>>> y\n(2, 2, 2, 2)\n\n\n\n"
    },
    {
        "api_name": "oneflow.Tensor.view_as",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.view_as.html",
        "api_signature": "Tensor.view_as(other)",
        "api_description": "Expand this tensor to the same size as other.\nself.view_as(other) is equivalent to self.view(other.size()).\nPlease see view() for more information about view.",
        "return_value": "",
        "parameters": "other (oneflow.Tensor) – The result tensor has the same size\nas other.\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.where",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.where.html",
        "api_signature": "Tensor.where(x=None, y=None)",
        "api_description": "See oneflow.where()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.zero_",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.zero_.html",
        "api_signature": "Tensor.zero_()",
        "api_description": "Fills self tensor with zeros.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.nms",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.nms.html",
        "api_signature": "Tensor.nms(scores, iou_threshold: float)",
        "api_description": "See oneflow.nms()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.pin_memory",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.pin_memory.html",
        "api_signature": "Tensor.pin_memory()",
        "api_description": "Copies the tensor to pinned memory, if it’s not already pinned.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.is_pinned",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.is_pinned.html",
        "api_signature": "Tensor.is_pinned()",
        "api_description": "",
        "return_value": "\n",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.inverse",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.inverse.html",
        "api_signature": "Tensor.inverse()",
        "api_description": "See oneflow.linalg.inv()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.cross",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.cross.html",
        "api_signature": "Tensor.cross(other, dim=None)",
        "api_description": "See oneflow.cross()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.scatter",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.scatter.html",
        "api_signature": "Tensor.scatter(dim, index, src, *, reduce=None)",
        "api_description": "See oneflow.scatter()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.scatter_",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.scatter_.html",
        "api_signature": "Tensor.scatter_(dim, index, src, *, reduce=None)",
        "api_description": "Inplace version of oneflow.Tensor.scatter()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.scatter_add",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.scatter_add.html",
        "api_signature": "Tensor.scatter_add()",
        "api_description": "See oneflow.scatter_add()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.scatter_add_",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.scatter_add_.html",
        "api_signature": "Tensor.scatter_add_(dim, index, src)",
        "api_description": "Inplace version of oneflow.Tensor.scatter_add()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.bernoulli",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.bernoulli.html",
        "api_signature": "Tensor.bernoulli()",
        "api_description": "See oneflow.bernoulli()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> x = flow.Tensor([1, 1, 1])\n>>> x.bernoulli()\ntensor([1., 1., 1.], dtype=oneflow.float32)\n>>> x.bernoulli(p=0.0)\ntensor([0., 0., 0.], dtype=oneflow.float32)\n\n\n\n"
    },
    {
        "api_name": "oneflow.Tensor.bernoulli_",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.bernoulli_.html",
        "api_signature": "Tensor.bernoulli_()",
        "api_description": "The inplace version of oneflow.Tensor.bernoulli_().\nSee oneflow.Tensor.bernoulli()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> x = flow.Tensor([1, 1, 1])\n>>> x.bernoulli_(p=0.0)\ntensor([0., 0., 0.], dtype=oneflow.float32)\n>>> x\ntensor([0., 0., 0.], dtype=oneflow.float32)\n\n\n\n"
    },
    {
        "api_name": "oneflow.Tensor.bincount",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.bincount.html",
        "api_signature": "Tensor.bincount()",
        "api_description": "See oneflow.bincount()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> x = flow.Tensor([0, 2, 3]).int()\n>>> x.bincount()\ntensor([1, 0, 1, 1], dtype=oneflow.int64)\n>>> weight = flow.Tensor([0.1, 0.2, 0.3])\n>>> x.bincount(weight)\ntensor([0.1000, 0.0000, 0.2000, 0.3000], dtype=oneflow.float32)\n>>> x.bincount(weight, minlength=5)\ntensor([0.1000, 0.0000, 0.2000, 0.3000, 0.0000], dtype=oneflow.float32)\n\n\n\n"
    },
    {
        "api_name": "oneflow.Tensor.isclose",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.isclose.html",
        "api_signature": "Tensor.isclose()",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.allclose",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.allclose.html",
        "api_signature": "Tensor.allclose(other, atol=1e-08, rtol=1e-05, equal_nan=False)",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.broadcast_to",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.broadcast_to.html",
        "api_signature": "Tensor.broadcast_to()",
        "api_description": "See oneflow.broadcast_to()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.Tensor.unique",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.unique.html",
        "api_signature": "Tensor.unique()",
        "api_description": "See oneflow.unique()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> x = flow.tensor([3, 1, 2, 0 ,2])\n>>> x.unique()\ntensor([0, 1, 2, 3], dtype=oneflow.int64)\n>>> x, indices = x.unique(return_inverse=True)\n>>> indices\ntensor([3, 1, 2, 0, 2], dtype=oneflow.int32)\n>>> x, counts = x.unique(return_counts=True)\n>>> counts\ntensor([1, 1, 1, 1], dtype=oneflow.int32)\n\n\n\n"
    },
    {
        "api_name": "oneflow.Tensor.bitwise_and",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.bitwise_and.html",
        "api_signature": "Tensor.bitwise_and()",
        "api_description": "See oneflow.bitwise_and()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> x = flow.tensor([1, 2, 3])\n>>> x.bitwise_and(4)\ntensor([0, 0, 0], dtype=oneflow.int64)\n>>> y = flow.tensor([2, 1, 0])\n>>> x.bitwise_and(y)\ntensor([0, 0, 0], dtype=oneflow.int64)\n\n\n\n"
    },
    {
        "api_name": "oneflow.Tensor.bitwise_or",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.bitwise_or.html",
        "api_signature": "Tensor.bitwise_or()",
        "api_description": "See oneflow.bitwise_or()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> x = flow.tensor([1, 2, 3])\n>>> x.bitwise_or(4)\ntensor([5, 6, 7], dtype=oneflow.int64)\n>>> y = flow.tensor([2, 1, 0])\n>>> x.bitwise_or(y)\ntensor([3, 3, 3], dtype=oneflow.int64)\n\n\n\n"
    },
    {
        "api_name": "oneflow.Tensor.bitwise_xor",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.bitwise_xor.html",
        "api_signature": "Tensor.bitwise_xor()",
        "api_description": "See oneflow.bitwise_xor()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> x = flow.tensor([1, 2, 3])\n>>> x.bitwise_xor(4)\ntensor([5, 6, 7], dtype=oneflow.int64)\n>>> y = flow.tensor([2, 1, 0])\n>>> x.bitwise_xor(y)\ntensor([3, 3, 3], dtype=oneflow.int64)\n\n\n\n"
    },
    {
        "api_name": "oneflow.Tensor.baddbmm",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.Tensor.baddbmm.html",
        "api_signature": "Tensor.baddbmm()",
        "api_description": "See oneflow.baddbmm()",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> x = flow.randn(2, 3, 4)\n>>> batch1 = flow.randn(2, 3, 5)\n>>> batch2 = flow.randn(2, 5, 4)\n>>> x.baddbmm(batch1, batch2, alpha=2, beta=2) \n\n\n\n"
    },
    {
        "api_name": "oneflow.autograd.backward",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.autograd.backward.html",
        "api_signature": "oneflow.autograd.backward(tensors: Union[oneflow.Tensor, Sequence[oneflow.Tensor]], grad_tensors: Optional[Union[oneflow.Tensor, Sequence[oneflow.Tensor]]], retain_graph: bool = False, create_graph: bool = False)",
        "api_description": "Computes the sum of gradients of given tensors with respect to graph leaves.\nThe documentation is referenced from:\nThe graph is differentiated using the chain rule. If any of tensors are non-scalar (i.e.\ntheir data has more than one element) and require gradient, then the Jacobian-vector product\nwould be computed, in this case the function additionally requires specifying grad_tensors.\nIt should be a sequence of matching length, that contains the “vector” in the Jacobian-vector\nproduct, usually the gradient of the differentiated function w.r.t. corresponding tensors.\n(None is an acceptable value for all tensors that don’t need gradient.)\nThis function accumulates gradients in the leaves - you might need to zero .grad attributes\nor set them to None before calling it.",
        "return_value": "",
        "parameters": "\ntensors (Tensor or Sequence[Tensor]) – Tensors of which the derivative will be computed.\ngrad_tensors (Tensor or Sequence[Tensor], optional) – The “vector” in the Jacobian-vector\nproduct, usually gradients each element of corresponding tensors. (None values can be\nspecified for scalar Tensors or ones that don’t require grad.)\nretain_graph (bool, optional) – If False, the graph used to compute the grads will be\nreset after backward is complete. Defaults to False. Note that in nearly all cases\nsetting this option to True is not needed and often can be worked around in a much\nmore efficient way. Defaults to the value of create_graph.\ncreate_graph (bool, optional) – If True, graph of the derivative will be constructed,\nallowing to compute higher order derivative products. Defaults to False.\n\n\n\n\n",
        "input_shape": "",
        "notes": "Using this method with create_graph=True will create a reference cycle between the\nparameter and its gradient which can cause a memory leak. We recommend using\nautograd.grad when creating the graph to avoid this. If you have to use this function,\nmake sure to reset the .grad fields of your parameters to None after use to break\nthe cycle and avoid the leak.",
        "code_example": ""
    },
    {
        "api_name": "oneflow.autograd.grad",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.autograd.grad.html",
        "api_signature": "oneflow.autograd.grad(outputs: Union[oneflow.Tensor, Sequence[oneflow.Tensor]], inputs: Union[oneflow.Tensor, Sequence[oneflow.Tensor]], grad_outputs: Optional[Union[oneflow.Tensor, Sequence[oneflow.Tensor]]] = None, retain_graph: bool = False, create_graph: bool = False, allow_unused: bool = False)",
        "api_description": "Computes and returns the sum of gradients of outputs with respect to the inputs.\nThe documentation is referenced from:\nThe graph is differentiated using the chain rule. grad_outputs should be a sequence of\nlength matching outputs, containing the “vector” in the Jacobian-vector product.\n(None is an acceptable value for that tensor don’t require gradient.)",
        "return_value": "A tuple of tensors containing the gradients for each inputs.\n\n",
        "parameters": "\noutputs (Sequence[Tensor]) – Tensors of which the derivative will be computed.\ninputs (Sequence[Tensor]) – Inputs w.r.t. which the derivative will be returned(and not\naccumulated into .grad).\ngrad_outputs (Sequence[Tensor], optional) – The “vector” in the Jacobian-vector product.\nUsually gradients w.r.t. each output. None values can be specified for scalar Tensors\nor ones that don’t require grad. Defaults to None.\nretain_graph (bool, optional) – If False, the graph used to compute the grads will be\nreset after backward is complete. Defaults to False. Note that in nearly all cases\nsetting this option to True is not needed and often can be worked around in a much\nmore efficient way. Defaults to the value of create_graph.\ncreate_graph (bool, optional) – If True, graph of the derivative will be constructed,\nallowing to compute higher order derivative products. Defaults to False.\nallow_unused (bool, optional) – If False, specifying inputs that were not\nused when computing outputs (and therefore their grad is always zero)\nis an error. Defaults to False.\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.autograd.no_grad",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.autograd.no_grad.html",
        "api_signature": null,
        "api_description": "Context-manager that disabled gradient calculation.\nDisabling gradient calculation is useful for inference, when you are sure that\nyou will not call Tensor.backward(). It will reduce memory consumption for computations\nthat would otherwise have requires_grad=True.\nIn this mode, the result of every computation will have requires_grad=False, even when\nthe inputs have requires_grad=True.\nThis context manager is thread local; it will not affect computation in other threads.\nAlso functions as a decorator. (Make sure to instantiate with parenthesis.)\n>>> import oneflow as flow\n>>> x = flow.ones(2, 3, requires_grad=True)\n>>> with flow.no_grad():\n...     y = x * x\n>>> y.requires_grad\nFalse\n>>> @flow.no_grad()\n... def no_grad_func(x):\n...     return x * x\n>>> y = no_grad_func(x)\n>>> y.requires_grad\nFalse\n__init__()¶\nInitialize self.  See help(type(self)) for accurate signature.\nMethods\n__call__(func)\nCall self as a function.\n__delattr__(name, /)\nImplement delattr(self, name).\n__dir__()\nDefault dir() implementation.\n__enter__()\n__eq__(value, /)\nReturn self==value.\n__exit__(exc_type, exc_val, exc_tb)\n__format__(format_spec, /)\nDefault object formatter.\n__ge__(value, /)\nReturn self>=value.\n__getattribute__(name, /)\nReturn getattr(self, name).\n__gt__(value, /)\nReturn self>value.\n__hash__()\nReturn hash(self).\n__init__()\nInitialize self.\n__init_subclass__\nThis method is called when a class is subclassed.\n__le__(value, /)\nReturn self<=value.\n__lt__(value, /)\nReturn self<value.\n__ne__(value, /)\nReturn self!=value.\n__new__(**kwargs)\nCreate and return a new object.\n__reduce__()\nHelper for pickle.\n__reduce_ex__(protocol, /)\nHelper for pickle.\n__repr__()\nReturn repr(self).\n__setattr__(name, value, /)\nImplement setattr(self, name, value).\n__sizeof__()\nSize of object in memory, in bytes.\n__str__()\nReturn str(self).\n__subclasshook__\nAbstract classes can override this to customize issubclass().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.autograd.enable_grad",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.autograd.enable_grad.html",
        "api_signature": null,
        "api_description": "Context-manager that enabled gradient calculation.\nEnables gradient calculation, if it has been disabled via no_grad.\nThis context manager is thread local; it will not affect computation in other threads.\nAlso functions as a decorator. (Make sure to instantiate with parenthesis.)\n>>> import oneflow as flow\n>>> x = flow.ones(2, 3, requires_grad=True)\n>>> with flow.no_grad():\n...     with flow.enable_grad():\n...         y = x * x\n>>> y.requires_grad\nTrue\n>>> @flow.enable_grad()\n... def no_grad_func(x):\n...     return x * x\n>>> with flow.no_grad():\n...     y = no_grad_func(x)\n>>> y.requires_grad\nTrue\n__init__()¶\nInitialize self.  See help(type(self)) for accurate signature.\nMethods\n__call__(func)\nCall self as a function.\n__delattr__(name, /)\nImplement delattr(self, name).\n__dir__()\nDefault dir() implementation.\n__enter__()\n__eq__(value, /)\nReturn self==value.\n__exit__(exc_type, exc_val, exc_tb)\n__format__(format_spec, /)\nDefault object formatter.\n__ge__(value, /)\nReturn self>=value.\n__getattribute__(name, /)\nReturn getattr(self, name).\n__gt__(value, /)\nReturn self>value.\n__hash__()\nReturn hash(self).\n__init__()\nInitialize self.\n__init_subclass__\nThis method is called when a class is subclassed.\n__le__(value, /)\nReturn self<=value.\n__lt__(value, /)\nReturn self<value.\n__ne__(value, /)\nReturn self!=value.\n__new__(**kwargs)\nCreate and return a new object.\n__reduce__()\nHelper for pickle.\n__reduce_ex__(protocol, /)\nHelper for pickle.\n__repr__()\nReturn repr(self).\n__setattr__(name, value, /)\nImplement setattr(self, name, value).\n__sizeof__()\nSize of object in memory, in bytes.\n__str__()\nReturn str(self).\n__subclasshook__\nAbstract classes can override this to customize issubclass().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.autograd.set_grad_enabled",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.autograd.set_grad_enabled.html",
        "api_signature": "oneflow.autograd.set_grad_enabled(is_train=True)",
        "api_description": "Context-manager that enabled gradient calculation.\nEnables gradient calculation, if it has been disabled via no_grad.\nThis context manager is thread local; it will not affect computation in other threads.\nAlso functions as a decorator. (Make sure to instantiate with parenthesis.)",
        "return_value": "",
        "parameters": "mode (bool) – Flag whether to enable or disable gradient calculation. (default: True)\n\n\n>>> import oneflow as flow\n>>> x = flow.ones(2, 3, requires_grad=True)\n>>> with flow.set_grad_enabled(True):\n...     y = x * x\n>>> y.requires_grad\nTrue\n>>> @flow.set_grad_enabled(False)\n... def no_grad_func(x):\n...     return x * x\n>>> y = no_grad_func(x)\n>>> y.requires_grad\nFalse\n\n\n\n\n__init__(is_train=True)¶\nInitialize self.  See help(type(self)) for accurate signature.\n\nMethods\n\n\n\n\n\n\n__call__(func)\nCall self as a function.\n\n__delattr__(name, /)\nImplement delattr(self, name).\n\n__dir__()\nDefault dir() implementation.\n\n__enter__()\n\n\n__eq__(value, /)\nReturn self==value.\n\n__exit__(exc_type, exc_val, exc_tb)\n\n\n__format__(format_spec, /)\nDefault object formatter.\n\n__ge__(value, /)\nReturn self>=value.\n\n__getattribute__(name, /)\nReturn getattr(self, name).\n\n__gt__(value, /)\nReturn self>value.\n\n__hash__()\nReturn hash(self).\n\n__init__([is_train])\nInitialize self.\n\n__init_subclass__\nThis method is called when a class is subclassed.\n\n__le__(value, /)\nReturn self<=value.\n\n__lt__(value, /)\nReturn self<value.\n\n__ne__(value, /)\nReturn self!=value.\n\n__new__(**kwargs)\nCreate and return a new object.\n\n__reduce__()\nHelper for pickle.\n\n__reduce_ex__(protocol, /)\nHelper for pickle.\n\n__repr__()\nReturn repr(self).\n\n__setattr__(name, value, /)\nImplement setattr(self, name, value).\n\n__sizeof__()\nSize of object in memory, in bytes.\n\n__str__()\nReturn str(self).\n\n__subclasshook__\nAbstract classes can override this to customize issubclass().\n\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.autograd.inference_mode",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.autograd.inference_mode.html",
        "api_signature": "oneflow.autograd.inference_mode(mode=True)",
        "api_description": "Context-manager that enables or disables inference mode\nInferenceMode is a new context manager analogous to no_grad to be used when you arecertain\nyour operations will have no interactions with autograd (e.g., model training). Code run\nunder this mode gets better performance by disabling view tracking and version counter bumps.\nThis context manager is thread local; it will not affect computation in other threads.\nAlso functions as a decorator. (Make sure to instantiate with parenthesis.)",
        "return_value": "",
        "parameters": "mode (bool) – Flag whether to enable or disable inference mode. (default: True)\n\n\n>>> import oneflow as flow\n>>> x = flow.ones(2, 3, requires_grad=True)\n>>> with flow.inference_mode():\n...     y = x * x\n>>> y.requires_grad\nFalse\n>>> @flow.inference_mode()\n... def no_grad_func(x):\n...     return x * x\n>>> y = no_grad_func(x)\n>>> y.requires_grad\nFalse\n\n\n\n\n__init__(mode=True)¶\nInitialize self.  See help(type(self)) for accurate signature.\n\nMethods\n\n\n\n\n\n\n__call__(func)\nCall self as a function.\n\n__delattr__(name, /)\nImplement delattr(self, name).\n\n__dir__()\nDefault dir() implementation.\n\n__enter__()\n\n\n__eq__(value, /)\nReturn self==value.\n\n__exit__(exc_type, exc_val, exc_tb)\n\n\n__format__(format_spec, /)\nDefault object formatter.\n\n__ge__(value, /)\nReturn self>=value.\n\n__getattribute__(name, /)\nReturn getattr(self, name).\n\n__gt__(value, /)\nReturn self>value.\n\n__hash__()\nReturn hash(self).\n\n__init__([mode])\nInitialize self.\n\n__init_subclass__\nThis method is called when a class is subclassed.\n\n__le__(value, /)\nReturn self<=value.\n\n__lt__(value, /)\nReturn self<value.\n\n__ne__(value, /)\nReturn self!=value.\n\n__new__(**kwargs)\nCreate and return a new object.\n\n__reduce__()\nHelper for pickle.\n\n__reduce_ex__(protocol, /)\nHelper for pickle.\n\n__repr__()\nReturn repr(self).\n\n__setattr__(name, value, /)\nImplement setattr(self, name, value).\n\n__sizeof__()\nSize of object in memory, in bytes.\n\n__str__()\nReturn str(self).\n\n__subclasshook__\nAbstract classes can override this to customize issubclass().\n\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.autograd.Function.forward",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.autograd.Function.forward.html",
        "api_signature": "Function.forward(ctx, *inputs)",
        "api_description": "Override this function for custom forward calculation.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.autograd.Function.backward",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.autograd.Function.backward.html",
        "api_signature": "Function.backward(ctx, *out_grads)",
        "api_description": "Override this function for custom backward calculation.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.autograd.Function.apply",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.autograd.Function.apply.html",
        "api_signature": "Function.apply(*inputs)",
        "api_description": "Calculate output tensors and build backward graph.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow._oneflow_internal.autograd.Function.FunctionCtx.mark_non_differentiable",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow._oneflow_internal.autograd.Function.FunctionCtx.mark_non_differentiable.html",
        "api_signature": "FunctionCtx.mark_non_differentiable()",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow._oneflow_internal.autograd.Function.FunctionCtx.save_for_backward",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow._oneflow_internal.autograd.Function.FunctionCtx.save_for_backward.html",
        "api_signature": "FunctionCtx.save_for_backward()",
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow._oneflow_internal.autograd.Function.FunctionCtx.saved_tensors",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow._oneflow_internal.autograd.Function.FunctionCtx.saved_tensors.html",
        "api_signature": null,
        "api_description": "",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.cuda.is_available",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.cuda.is_available.html",
        "api_signature": "oneflow.cuda.is_available()",
        "api_description": "",
        "return_value": "\n",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.cuda.device_count",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.cuda.device_count.html",
        "api_signature": "oneflow.cuda.device_count()",
        "api_description": "",
        "return_value": "\n",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.cuda.current_device",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.cuda.current_device.html",
        "api_signature": "oneflow.cuda.current_device()",
        "api_description": "",
        "return_value": "\n",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.cuda.set_device",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.cuda.set_device.html",
        "api_signature": "oneflow.cuda.set_device(device: Union[oneflow._oneflow_internal.device, str, int])",
        "api_description": "Sets the current device.\nThe documentation is referenced from:\nUsage of this function is discouraged in favor of device. In most\ncases it’s better to use CUDA_VISIBLE_DEVICES environmental variable.",
        "return_value": "",
        "parameters": "device (flow.device or int) – selected device. This function is a no-op\nif this argument is negative.\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.cuda.synchronize",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.cuda.synchronize.html",
        "api_signature": "oneflow.cuda.synchronize(device: Optional[Union[oneflow._oneflow_internal.device, str, int]] = None)",
        "api_description": "Waits for all kernels in all streams on a CUDA device to complete.",
        "return_value": "",
        "parameters": "device (flow.device or int, optional) – device for which to synchronize.\nIt uses the current device, given by current_device(),\nif device is None (default).\n\n\n\n",
        "input_shape": "",
        "notes": "In the eager mode of oneflow, all operations will be converted\ninto instructions executed in the virtual machine,\nso in order to comply with the semantics of synchronization,\nthis function will call the eager.Sync() function before the device is synchronized,\nwhich may affect the operations executed in other devices.",
        "code_example": ""
    },
    {
        "api_name": "oneflow.cuda.get_device_properties",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.cuda.get_device_properties.html",
        "api_signature": "oneflow.cuda.get_device_properties(device: Optional[Union[oneflow._oneflow_internal.device, str, int]] = None)",
        "api_description": "Gets the properties of a device.\nThe documentation is referenced from:",
        "return_value": "the properties of the device.\n\n\n\n",
        "parameters": "device (oneflow.device or str or int) – device for which to return the properties of the device.\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.cuda.get_device_capability",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.cuda.get_device_capability.html",
        "api_signature": "oneflow.cuda.get_device_capability(device: Optional[Union[oneflow._oneflow_internal.device, str, int]] = None)",
        "api_description": "Gets the cuda capability of a device.\nThe documentation is referenced from:",
        "return_value": "the major and minor cuda capability of the device\n\n",
        "parameters": "device (oneflow.device or int or str, optional) – device for which to return the\ndevice capability. It uses the current device, given by\ncurrent_device(), if device is None\n(default).\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.cuda.get_device_name",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.cuda.get_device_name.html",
        "api_signature": "oneflow.cuda.get_device_name(device: Optional[Union[oneflow._oneflow_internal.device, str, int]] = None)",
        "api_description": "Gets the name of a device.\nThe documentation is referenced from:",
        "return_value": "the name of the device\n\n",
        "parameters": "device (oneflow.device or int or str, optional) – device for which to return the\nname. It uses the current device, given by current_device(),\nif device is None (default).\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.cuda.manual_seed_all",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.cuda.manual_seed_all.html",
        "api_signature": "oneflow.cuda.manual_seed_all(seed)",
        "api_description": "Sets the seed for generating random numbers on all GPUs.\nThe documentation is referenced from:\nIt’s safe to call this function if CUDA is not available; in that\ncase, it is silently ignored.",
        "return_value": "",
        "parameters": "seed (int) – The desired seed.\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.cuda.manual_seed",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.cuda.manual_seed.html",
        "api_signature": "oneflow.cuda.manual_seed(seed: int)",
        "api_description": "Sets the seed for generating random numbers for the current GPU.\nThe documentation is referenced from:\nIt’s safe to call this function if CUDA is not available; in that\ncase, it is silently ignored.",
        "return_value": "",
        "parameters": "seed (int) – The desired seed.\n\n\n\nWarning\nIf you are working with a multi-GPU model, this function is insufficient\nto get determinism.  To seed all GPUs, use manual_seed_all().\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.cuda.get_rng_state",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.cuda.get_rng_state.html",
        "api_signature": "oneflow.cuda.get_rng_state(device: Union[int, str, oneflow._oneflow_internal.device] = 'cuda')",
        "api_description": "",
        "return_value": "\n",
        "parameters": "device (flow.device or int, optional) – The device to return the RNG state of.\nDefault: 'cuda' (i.e., flow.device('cuda'), the current CUDA device).\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.cuda.get_rng_state_all",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.cuda.get_rng_state_all.html",
        "api_signature": "oneflow.cuda.get_rng_state_all()",
        "api_description": "",
        "return_value": "\n",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.cuda.set_rng_state",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.cuda.set_rng_state.html",
        "api_signature": "oneflow.cuda.set_rng_state(new_state: oneflow.Tensor, device: Union[int, str, oneflow._oneflow_internal.device] = 'cuda')",
        "api_description": "Sets the random number generator state of the specified GPU.",
        "return_value": "",
        "parameters": "\nnew_state (flow.ByteTensor) – The desired state\ndevice (flow.device or int, optional) – The device to set the RNG state.\nDefault: 'cuda' (i.e., flow.device('cuda'), the current CUDA device).\n\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.cuda.set_rng_state_all",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.cuda.set_rng_state_all.html",
        "api_signature": "oneflow.cuda.set_rng_state_all(new_states: Iterable[oneflow.Tensor])",
        "api_description": "Sets the random number generator state of all devices.",
        "return_value": "",
        "parameters": "new_states (Iterable of flow.ByteTensor) – The desired state for each device\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.cuda.HalfTensor",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.cuda.HalfTensor.html",
        "api_signature": null,
        "api_description": "The tensortype oneflow.cuda.HalfTensor is not available.\n__init__()¶\nInitialize self.  See help(type(self)) for accurate signature.\nMethods\n__delattr__(name, /)\nImplement delattr(self, name).\n__dir__()\nDefault dir() implementation.\n__eq__(value, /)\nReturn self==value.\n__format__(format_spec, /)\nDefault object formatter.\n__ge__(value, /)\nReturn self>=value.\n__getattribute__(name, /)\nReturn getattr(self, name).\n__gt__(value, /)\nReturn self>value.\n__hash__()\nReturn hash(self).\n__init__()\nInitialize self.\n__init_subclass__\nThis method is called when a class is subclassed.\n__le__(value, /)\nReturn self<=value.\n__lt__(value, /)\nReturn self<value.\n__ne__(value, /)\nReturn self!=value.\n__new__(**kwargs)\nCreate and return a new object.\n__reduce__()\nHelper for pickle.\n__reduce_ex__(protocol, /)\nHelper for pickle.\n__repr__()\nReturn repr(self).\n__setattr__(name, value, /)\nImplement setattr(self, name, value).\n__sizeof__()\nSize of object in memory, in bytes.\n__str__()\nReturn str(self).\n__subclasshook__\nAbstract classes can override this to customize issubclass().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.cuda.FloatTensor",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.cuda.FloatTensor.html",
        "api_signature": null,
        "api_description": "The tensortype oneflow.cuda.FloatTensor is not available.\n__init__()¶\nInitialize self.  See help(type(self)) for accurate signature.\nMethods\n__delattr__(name, /)\nImplement delattr(self, name).\n__dir__()\nDefault dir() implementation.\n__eq__(value, /)\nReturn self==value.\n__format__(format_spec, /)\nDefault object formatter.\n__ge__(value, /)\nReturn self>=value.\n__getattribute__(name, /)\nReturn getattr(self, name).\n__gt__(value, /)\nReturn self>value.\n__hash__()\nReturn hash(self).\n__init__()\nInitialize self.\n__init_subclass__\nThis method is called when a class is subclassed.\n__le__(value, /)\nReturn self<=value.\n__lt__(value, /)\nReturn self<value.\n__ne__(value, /)\nReturn self!=value.\n__new__(**kwargs)\nCreate and return a new object.\n__reduce__()\nHelper for pickle.\n__reduce_ex__(protocol, /)\nHelper for pickle.\n__repr__()\nReturn repr(self).\n__setattr__(name, value, /)\nImplement setattr(self, name, value).\n__sizeof__()\nSize of object in memory, in bytes.\n__str__()\nReturn str(self).\n__subclasshook__\nAbstract classes can override this to customize issubclass().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.cuda.DoubleTensor",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.cuda.DoubleTensor.html",
        "api_signature": null,
        "api_description": "The tensortype oneflow.cuda.DoubleTensor is not available.\n__init__()¶\nInitialize self.  See help(type(self)) for accurate signature.\nMethods\n__delattr__(name, /)\nImplement delattr(self, name).\n__dir__()\nDefault dir() implementation.\n__eq__(value, /)\nReturn self==value.\n__format__(format_spec, /)\nDefault object formatter.\n__ge__(value, /)\nReturn self>=value.\n__getattribute__(name, /)\nReturn getattr(self, name).\n__gt__(value, /)\nReturn self>value.\n__hash__()\nReturn hash(self).\n__init__()\nInitialize self.\n__init_subclass__\nThis method is called when a class is subclassed.\n__le__(value, /)\nReturn self<=value.\n__lt__(value, /)\nReturn self<value.\n__ne__(value, /)\nReturn self!=value.\n__new__(**kwargs)\nCreate and return a new object.\n__reduce__()\nHelper for pickle.\n__reduce_ex__(protocol, /)\nHelper for pickle.\n__repr__()\nReturn repr(self).\n__setattr__(name, value, /)\nImplement setattr(self, name, value).\n__sizeof__()\nSize of object in memory, in bytes.\n__str__()\nReturn str(self).\n__subclasshook__\nAbstract classes can override this to customize issubclass().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.cuda.BoolTensor",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.cuda.BoolTensor.html",
        "api_signature": null,
        "api_description": "The tensortype oneflow.cuda.BoolTensor is not available.\n__init__()¶\nInitialize self.  See help(type(self)) for accurate signature.\nMethods\n__delattr__(name, /)\nImplement delattr(self, name).\n__dir__()\nDefault dir() implementation.\n__eq__(value, /)\nReturn self==value.\n__format__(format_spec, /)\nDefault object formatter.\n__ge__(value, /)\nReturn self>=value.\n__getattribute__(name, /)\nReturn getattr(self, name).\n__gt__(value, /)\nReturn self>value.\n__hash__()\nReturn hash(self).\n__init__()\nInitialize self.\n__init_subclass__\nThis method is called when a class is subclassed.\n__le__(value, /)\nReturn self<=value.\n__lt__(value, /)\nReturn self<value.\n__ne__(value, /)\nReturn self!=value.\n__new__(**kwargs)\nCreate and return a new object.\n__reduce__()\nHelper for pickle.\n__reduce_ex__(protocol, /)\nHelper for pickle.\n__repr__()\nReturn repr(self).\n__setattr__(name, value, /)\nImplement setattr(self, name, value).\n__sizeof__()\nSize of object in memory, in bytes.\n__str__()\nReturn str(self).\n__subclasshook__\nAbstract classes can override this to customize issubclass().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.cuda.ByteTensor",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.cuda.ByteTensor.html",
        "api_signature": null,
        "api_description": "The tensortype oneflow.cuda.ByteTensor is not available.\n__init__()¶\nInitialize self.  See help(type(self)) for accurate signature.\nMethods\n__delattr__(name, /)\nImplement delattr(self, name).\n__dir__()\nDefault dir() implementation.\n__eq__(value, /)\nReturn self==value.\n__format__(format_spec, /)\nDefault object formatter.\n__ge__(value, /)\nReturn self>=value.\n__getattribute__(name, /)\nReturn getattr(self, name).\n__gt__(value, /)\nReturn self>value.\n__hash__()\nReturn hash(self).\n__init__()\nInitialize self.\n__init_subclass__\nThis method is called when a class is subclassed.\n__le__(value, /)\nReturn self<=value.\n__lt__(value, /)\nReturn self<value.\n__ne__(value, /)\nReturn self!=value.\n__new__(**kwargs)\nCreate and return a new object.\n__reduce__()\nHelper for pickle.\n__reduce_ex__(protocol, /)\nHelper for pickle.\n__repr__()\nReturn repr(self).\n__setattr__(name, value, /)\nImplement setattr(self, name, value).\n__sizeof__()\nSize of object in memory, in bytes.\n__str__()\nReturn str(self).\n__subclasshook__\nAbstract classes can override this to customize issubclass().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.cuda.CharTensor",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.cuda.CharTensor.html",
        "api_signature": null,
        "api_description": "The tensortype oneflow.cuda.CharTensor is not available.\n__init__()¶\nInitialize self.  See help(type(self)) for accurate signature.\nMethods\n__delattr__(name, /)\nImplement delattr(self, name).\n__dir__()\nDefault dir() implementation.\n__eq__(value, /)\nReturn self==value.\n__format__(format_spec, /)\nDefault object formatter.\n__ge__(value, /)\nReturn self>=value.\n__getattribute__(name, /)\nReturn getattr(self, name).\n__gt__(value, /)\nReturn self>value.\n__hash__()\nReturn hash(self).\n__init__()\nInitialize self.\n__init_subclass__\nThis method is called when a class is subclassed.\n__le__(value, /)\nReturn self<=value.\n__lt__(value, /)\nReturn self<value.\n__ne__(value, /)\nReturn self!=value.\n__new__(**kwargs)\nCreate and return a new object.\n__reduce__()\nHelper for pickle.\n__reduce_ex__(protocol, /)\nHelper for pickle.\n__repr__()\nReturn repr(self).\n__setattr__(name, value, /)\nImplement setattr(self, name, value).\n__sizeof__()\nSize of object in memory, in bytes.\n__str__()\nReturn str(self).\n__subclasshook__\nAbstract classes can override this to customize issubclass().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.cuda.IntTensor",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.cuda.IntTensor.html",
        "api_signature": null,
        "api_description": "The tensortype oneflow.cuda.IntTensor is not available.\n__init__()¶\nInitialize self.  See help(type(self)) for accurate signature.\nMethods\n__delattr__(name, /)\nImplement delattr(self, name).\n__dir__()\nDefault dir() implementation.\n__eq__(value, /)\nReturn self==value.\n__format__(format_spec, /)\nDefault object formatter.\n__ge__(value, /)\nReturn self>=value.\n__getattribute__(name, /)\nReturn getattr(self, name).\n__gt__(value, /)\nReturn self>value.\n__hash__()\nReturn hash(self).\n__init__()\nInitialize self.\n__init_subclass__\nThis method is called when a class is subclassed.\n__le__(value, /)\nReturn self<=value.\n__lt__(value, /)\nReturn self<value.\n__ne__(value, /)\nReturn self!=value.\n__new__(**kwargs)\nCreate and return a new object.\n__reduce__()\nHelper for pickle.\n__reduce_ex__(protocol, /)\nHelper for pickle.\n__repr__()\nReturn repr(self).\n__setattr__(name, value, /)\nImplement setattr(self, name, value).\n__sizeof__()\nSize of object in memory, in bytes.\n__str__()\nReturn str(self).\n__subclasshook__\nAbstract classes can override this to customize issubclass().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.cuda.LongTensor",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.cuda.LongTensor.html",
        "api_signature": null,
        "api_description": "The tensortype oneflow.cuda.LongTensor is not available.\n__init__()¶\nInitialize self.  See help(type(self)) for accurate signature.\nMethods\n__delattr__(name, /)\nImplement delattr(self, name).\n__dir__()\nDefault dir() implementation.\n__eq__(value, /)\nReturn self==value.\n__format__(format_spec, /)\nDefault object formatter.\n__ge__(value, /)\nReturn self>=value.\n__getattribute__(name, /)\nReturn getattr(self, name).\n__gt__(value, /)\nReturn self>value.\n__hash__()\nReturn hash(self).\n__init__()\nInitialize self.\n__init_subclass__\nThis method is called when a class is subclassed.\n__le__(value, /)\nReturn self<=value.\n__lt__(value, /)\nReturn self<value.\n__ne__(value, /)\nReturn self!=value.\n__new__(**kwargs)\nCreate and return a new object.\n__reduce__()\nHelper for pickle.\n__reduce_ex__(protocol, /)\nHelper for pickle.\n__repr__()\nReturn repr(self).\n__setattr__(name, value, /)\nImplement setattr(self, name, value).\n__sizeof__()\nSize of object in memory, in bytes.\n__str__()\nReturn str(self).\n__subclasshook__\nAbstract classes can override this to customize issubclass().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.cuda.empty_cache",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.cuda.empty_cache.html",
        "api_signature": "oneflow.cuda.empty_cache()",
        "api_description": "Releases all unoccupied cached memory currently held by the caching\nallocators of all OneFlow streams so those can be re-allocated in OneFlow streams\nor other GPU application and visible in nvidia-smi.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "empty_cache() may enable one stream to release memory\nand then freed memory can be used by another stream. It may also help reduce\nfragmentation of GPU memory in certain cases.",
        "code_example": ""
    },
    {
        "api_name": "oneflow.env.get_world_size",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.env.get_world_size.html",
        "api_signature": "oneflow.env.get_world_size()",
        "api_description": "",
        "return_value": "\nThe world size of the process group.\n\n\n\n",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.env.get_rank",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.env.get_rank.html",
        "api_signature": "oneflow.env.get_rank()",
        "api_description": "",
        "return_value": "Rank is globally unique, range of which is [0, world_size).\n\nThe rank of the process group.\n\n\n\n",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.env.get_local_rank",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.env.get_local_rank.html",
        "api_signature": "oneflow.env.get_local_rank()",
        "api_description": "",
        "return_value": "Local rank is not globally unique. It is only unique per process on a machine.\n\nThe the local rank of process on current machine.\n\n\n\n",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.env.get_node_size",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.env.get_node_size.html",
        "api_signature": "oneflow.env.get_node_size()",
        "api_description": "",
        "return_value": "\nThe the number of machines in the process group.\n\n\n\n",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.env.init_rdma",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.env.init_rdma.html",
        "api_signature": "oneflow.env.init_rdma()",
        "api_description": "Init RDMA in the current envirment. If the current envirment support\nRDMA, turning on RDMA by calling oneflow.env.init_rdma() can speed up\ndata transfer.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "Make sure to avoid using fork() after oneflow.env.init_rdma() is invoked.\nOtherwise, data corruption or segmentation fault  may result!\nRequires all devices to execute oneflow.env.init_rdma() simultaneously.\nOtherwise, deadlock may result!",
        "code_example": ""
    },
    {
        "api_name": "oneflow.env.rdma_is_initialized",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.env.rdma_is_initialized.html",
        "api_signature": "oneflow.env.rdma_is_initialized()",
        "api_description": "",
        "return_value": "\nWhether RDMA is initialized or not.\n\n\n\n",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.comm.all_reduce",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.comm.all_reduce.html",
        "api_signature": "oneflow.comm.all_reduce(tensor)",
        "api_description": "Reduces the tensor data across all machines in such a way that all get\nthe final result.\nAfter the call tensor is going to be bitwise identical in all processes.",
        "return_value": "",
        "parameters": "tensor (Tensor) – the input tensor\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> # We have 1 process groups, 2 ranks.\n>>> import oneflow as flow\n\n>>> tensor = flow.tensor([[1, 2], [3, 4]], device=\"cuda\") + flow.env.get_local_rank()\n>>> # tensor on rank0\n>>> tensor \ntensor([[1, 2],\n        [3, 4]], device='cuda:0', dtype=oneflow.int64)\n>>> # tensor on rank1\n>>> tensor \ntensor([[2, 3],\n        [4, 5]], device='cuda:1', dtype=oneflow.int64)\n>>> flow.comm.all_reduce(tensor)\n>>> tensor.numpy()\narray([[3, 5],\n       [7, 9]], dtype=int64)\n\n\n\n"
    },
    {
        "api_name": "oneflow.comm.all_gather",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.comm.all_gather.html",
        "api_signature": "oneflow.comm.all_gather(tensor_list, tensor)",
        "api_description": "Gathers tensors from the whole group in a list.",
        "return_value": "",
        "parameters": "\ntensor_list (list[Tensor]) – Output list. It should contain\ncorrectly-sized tensors to be used for output of the collective.\ntensor (Tensor) – Tensor to be broadcast from current process.\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> # We have 1 process groups, 2 ranks.\n>>> import oneflow as flow\n\n>>> input = flow.tensor([[1, 2], [3, 4]], device=\"cuda\") + flow.env.get_local_rank()\n>>> # input on rank0\n>>> input \ntensor([[1, 2],\n        [3, 4]], device='cuda:0', dtype=oneflow.int64)\n>>> # input on rank1\n>>> input \ntensor([[2, 3],\n        [4, 5]], device='cuda:1', dtype=oneflow.int64)\n>>> tensor_list = [flow.zeros(2, 2, dtype=flow.int64) for _ in range(2)]\n>>> flow.comm.all_gather(tensor_list, input)\n>>> # result on rank0\n>>> tensor_list \n[tensor([[1, 2],\n        [3, 4]], device='cuda:0', dtype=oneflow.int64), tensor([[2, 3],\n        [4, 5]], device='cuda:0', dtype=oneflow.int64)]\n>>> # result on rank1\n>>> tensor_list \n[tensor([[1, 2],\n        [3, 4]], device='cuda:1', dtype=oneflow.int64), tensor([[2, 3],\n        [4, 5]], device='cuda:1', dtype=oneflow.int64)]\n\n\n\n"
    },
    {
        "api_name": "oneflow.comm.all_gather_into_tensor",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.comm.all_gather_into_tensor.html",
        "api_signature": "oneflow.comm.all_gather_into_tensor(output_tensor, input_tensor)",
        "api_description": "Gather tensors from all ranks and put them in a single output tensor.",
        "return_value": "",
        "parameters": "\noutput_tensor (Tensor) – Output tensor to accommodate tensor elements\nfrom all ranks. It must be correctly sized to have one of the\nfollowing forms:\n(i) a concatenation of all the input tensors along the primary\ndimension; for definition of “concatenation”, see oneflow.cat();\n(ii) a stack of all the input tensors along the primary dimension;\nfor definition of “stack”, see oneflow.stack().\nExamples below may better explain the supported output forms.\ninput_tensor (Tensor) – Tensor to be gathered from current rank.\nThe input tensors in this API must have the same size across all ranks.\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> # We have 1 process groups, 2 ranks.\n>>> # All tensors below are of flow.int64 dtype and on CUDA devices.\n>>> import oneflow as flow\n>>> tensor_in = flow.tensor([[1, 2, 3], [4, 5, 6]], dtype=flow.int64, device=\"cuda\") + flow.env.get_rank() * 6\n>>> tensor_in \ntensor([[1, 2, 3],\n        [4, 5, 6]], device='cuda:0', dtype=oneflow.int64)\n>>> # Output in concatenation form\n>>> tensor_out = flow.zeros(4, 3, dtype=flow.int64, device=\"cuda\")\n>>> flow.comm.all_gather_into_tensor(tensor_out, tensor_in)\n>>> # result on rank0\n>>> tensor_out \ntensor([[ 1,  2,  3],\n        [ 4,  5,  6],\n        [ 7,  8,  9],\n        [10, 11, 12]], device='cuda:0', dtype=oneflow.int64)\n>>> # result on rank1\n>>> tensor_out \ntensor([[ 1,  2,  3],\n        [ 4,  5,  6],\n        [ 7,  8,  9],\n        [10, 11, 12]], device='cuda:1', dtype=oneflow.int64)\n>>> # Output in stack form\n>>> tensor_out2 = flow.zeros(2, 3, 2, dtype=flow.int64, device=\"cuda\")\n>>> flow.comm.all_gather_into_tensor(tensor_out2, tensor_in)\n>>> # result on rank0\n>>> tensor_out2 \ntensor([[[ 1,  2],\n         [ 3,  4],\n         [ 5,  6]],\n\n        [[ 7,  8],\n         [ 9, 10],\n         [11, 12]]], device='cuda:0', dtype=oneflow.int64)\n>>> # result on rank1\n>>> tensor_out2 \ntensor([[[ 1,  2],\n         [ 3,  4],\n         [ 5,  6]],\n\n        [[ 7,  8],\n         [ 9, 10],\n         [11, 12]]], device='cuda:1', dtype=oneflow.int64)\n\n\n\n"
    },
    {
        "api_name": "oneflow.comm.all_to_all",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.comm.all_to_all.html",
        "api_signature": "oneflow.comm.all_to_all(output_tensor_list, input_tensor_list)",
        "api_description": "Each process scatters list of input tensors to all processes in a group and\nreturn gathered list of tensors in output list.",
        "return_value": "",
        "parameters": "\noutput_tensor_list (list[Tensor]) – List of tensors to be gathered one\nper rank.\ninput_tensor_list (list[Tensor]) – List of tensors to scatter one per rank.\n\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.comm.broadcast",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.comm.broadcast.html",
        "api_signature": "oneflow.comm.broadcast(tensor, src)",
        "api_description": "Broadcasts the tensor to the whole group.\ntensor must have the same number of elements in all processes\nparticipating in the collective.",
        "return_value": "",
        "parameters": "\ntensor (Tensor) – Data to be sent if src is the rank of current\nprocess, and tensor to be used to save received data otherwise.\nsrc (int) – Source rank.\n\n\n\n>>> # We have 1 process groups, 2 ranks.\n>>> import oneflow as flow\n>>> tensor = flow.tensor([[1, 2], [3, 4]], device=\"cuda\") + flow.env.get_local_rank()\n>>> # input on rank0\n>>> tensor \ntensor([[1, 2],\n        [3, 4]], device='cuda:0', dtype=oneflow.int64)\n>>> # input on rank1\n>>> tensor \ntensor([[2, 3],\n        [4, 5]], device='cuda:1', dtype=oneflow.int64)\n>>> flow.comm.broadcast(tensor, 0)\n>>> # result on rank0\n>>> tensor \ntensor([[1, 2],\n        [3, 4]], device='cuda:0', dtype=oneflow.int64)\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.comm.barrier",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.comm.barrier.html",
        "api_signature": "oneflow.comm.barrier()",
        "api_description": "Synchronizes all processes.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.comm.gather",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.comm.gather.html",
        "api_signature": "oneflow.comm.gather(tensor, gather_list=None, dst=0)",
        "api_description": "Gathers a list of tensors in a single process.",
        "return_value": "",
        "parameters": "\ntensor (Tensor) – Input tensor.\ngather_list (list[Tensor], optional) – List of appropriately-sized\ntensors to use for gathered data (default is None, must be specified\non the destination rank)\ndst (int, optional) – Destination rank (default is 0)\n\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.comm.reduce",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.comm.reduce.html",
        "api_signature": "oneflow.comm.reduce(tensor, dst)",
        "api_description": "Reduces the tensor data across all machines.\nOnly the process with rank dst is going to receive the final result.",
        "return_value": "",
        "parameters": "\ntensor (Tensor) – Input and output of the collective. The function\noperates in-place.\ndst (int) – Destination rank\n\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.comm.reduce_scatter",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.comm.reduce_scatter.html",
        "api_signature": "oneflow.comm.reduce_scatter(output, input_list)",
        "api_description": "Reduces, then scatters a list of tensors to all processes in a group.",
        "return_value": "",
        "parameters": "\noutput (Tensor) – Output tensor.\ninput_list (list[Tensor]) – List of tensors to reduce and scatter.\n\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.comm.reduce_scatter_tensor",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.comm.reduce_scatter_tensor.html",
        "api_signature": "oneflow.comm.reduce_scatter_tensor(output_tensor, input_tensor)",
        "api_description": "Reduces, then scatters a tensor to all ranks.",
        "return_value": "",
        "parameters": "\noutput (Tensor) – Output tensor. It should have the same size across all\nranks.\ninput (Tensor) – Input tensor to be reduced and scattered. Its size\nshould be output tensor size times the world size. The input tensor\ncan have one of the following shapes:\n(i) a concatenation of the output tensors along the primary\ndimension, or\n(ii) a stack of the output tensors along the primary dimension.\nFor definition of “concatenation”, see oneflow.cat().\nFor definition of “stack”, see oneflow.stack().\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> # We have 1 process groups, 2 ranks.\n>>> # All tensors below are of flow.int64 dtype and on CUDA devices.\n>>> import oneflow as flow\n>>> tensor_in = flow.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]], dtype=flow.int64, device=\"cuda\")\n>>> tensor_in \ntensor([[ 1,  2,  3],\n        [ 4,  5,  6],\n        [ 7,  8,  9],\n        [10, 11, 12]], device='cuda:0', dtype=oneflow.int64)\n>>> # Output in concatenation form\n>>> tensor_out = flow.zeros(2, 3, dtype=flow.int64, device=\"cuda\")\n>>> flow.comm.reduce_scatter_tensor(tensor_out, tensor_in)\n>>> # result on rank0\n>>> tensor_out \ntensor([[ 2,  4,  6],\n        [ 8, 10, 12]], device='cuda:0', dtype=oneflow.int64)\n>>> # result on rank1\n>>> tensor_out \ntensor([[14, 16, 18],\n        [20, 22, 24]], device='cuda:1', dtype=oneflow.int64)\n>>> # Output in stack form\n>>> tensor_in2 = tensor_in.reshape(2, 3, 2)\n>>> tensor_out2 = flow.zeros(2, 3, dtype=flow.int64, device=\"cuda\")\n>>> flow.comm.reduce_scatter_tensor(tensor_out2, tensor_in2)\n>>> # result on rank0\n>>> tensor_out2 \ntensor([[ 2,  4,  6],\n        [ 8, 10, 12]], device='cuda:0', dtype=oneflow.int64)\n>>> # result on rank1\n>>> tensor_out2 \ntensor([[14, 16, 18],\n        [20, 22, 24]], device='cuda:1', dtype=oneflow.int64)\n\n\n\n"
    },
    {
        "api_name": "oneflow.comm.recv",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.comm.recv.html",
        "api_signature": "oneflow.comm.recv()",
        "api_description": "Receives a tensor synchronously.\nAll(send_meta is False) or none of shape, dtype and device should have value.",
        "return_value": "if out is None, return received tensor. otherwise got data from out self without return.\n\n\n\n",
        "parameters": "\nsrc (int, optional) – Source rank. Will receive from any\nprocess if unspecified.\nshape (optional) – output tensor shape.\ndataType (optional) – output tensor data type.\ndevice (optional) – output tensor device.\nout (Tensor, optional) – Tensor to fill with received data.\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.comm.scatter",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.comm.scatter.html",
        "api_signature": "oneflow.comm.scatter(tensor, scatter_list=None, src=0)",
        "api_description": "Scatters a list of tensors to all processes in a group.\nEach process will receive exactly one tensor and store its data in the\ntensor argument.",
        "return_value": "",
        "parameters": "\ntensor (Tensor) – Output tensor.\nscatter_list (list[Tensor]) – List of tensors to scatter (default is\nNone, must be specified on the source rank)\nsrc (int) – Source rank (default is 0)\n\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.comm.send",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.comm.send.html",
        "api_signature": "oneflow.comm.send()",
        "api_description": "Sends a tensor synchronously.",
        "return_value": "",
        "parameters": "\ntensor (Tensor) – Tensor to send.\ndst (int) – Destination rank.\nsend_meta (Bool) – Whether to send meta information (default is True)\n\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.distributions.Distribution",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.distributions.Distribution.html",
        "api_signature": "oneflow.distributions.Distribution(batch_shape=oneflow.Size([])",
        "api_description": "Distribution is the abstract base class for probability distributions.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.distributions.Categorical",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.distributions.Categorical.html",
        "api_signature": "oneflow.distributions.Categorical(probs=None, logits=None, validate_args=None)",
        "api_description": "Creates a categorical distribution parameterized by either probs or\nlogits (but not both).",
        "return_value": "",
        "parameters": "\nprobs (Tensor) – event probabilities\nlogits (Tensor) – event log probabilities (unnormalized)\n\n\n\nSee also: oneflow.multinomial()\n",
        "input_shape": "",
        "notes": "It is equivalent to the distribution that oneflow.multinomial()\nsamples from.\nSamples are integers from \\(\\{0, \\ldots, K-1\\}\\) where K is probs.size(-1).\nIf probs is 1-dimensional with length-K, each element is the relative probability\nof sampling the class at that index.\nIf probs is N-dimensional, the first N-1 dimensions are treated as a batch of\nrelative probability vectors.\nThe probs argument must be non-negative, finite and have a non-zero sum,\nand it will be normalized to sum to 1 along the last dimension. probs\nwill return this normalized value.\nThe logits argument will be interpreted as unnormalized log probabilities\nand can therefore be any real number. It will likewise be normalized so that\nthe resulting probabilities sum to 1 along the last dimension. logits\nwill return this normalized value.",
        "code_example": ">>> import oneflow as flow\n>>> gen = flow.manual_seed(0)\n>>> m = flow.distributions.categorical.Categorical(flow.tensor([ 0.25, 0.25, 0.25, 0.25 ]))\n>>> m.sample()  # equal probability of 0, 1, 2, 3\ntensor(3, dtype=oneflow.int64)\n\n\n\n"
    },
    {
        "api_name": "oneflow.linalg.norm",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.linalg.norm.html",
        "api_signature": "oneflow.linalg.norm(input, ord=None, dim=None, keepdim=False, *, dtype=None, out=None)",
        "api_description": "",
        "return_value": "This function can calculate one of eight different types of matrix norms, or one\nof an infinite number of vector norms, depending on both the number of reduction\ndimensions and the value of the ord parameter.\n\n",
        "parameters": "\ninput (Tensor) – The input tensor. If dim is None, input must be 1-D or 2-D, unless ord\nis None. If both dim and ord are None, the 2-norm of the input flattened to 1-D\nwill be returned. Its data type must be either a floating point or complex type. For complex\ninputs, the norm is calculated on of the absolute values of each element. If the input is\ncomplex and neither dtype nor out is specified, the result’s data type will\nbe the corresponding floating point type (e.g. float if input is complexfloat).\nord (int, inf, -inf, 'fro', 'nuc', optional) – order of norm. Default: ‘None’\nThe following norms can be calculated:\n\n\n\n\n\n\n\nord\nnorm for matrices\nnorm for vectors\n\n\n\nNone\nFrobenius norm\n2-norm\n\n’fro’\nFrobenius norm\n– not supported –\n\n‘nuc’\n– not supported yet –\n– not supported –\n\ninf\nmax(sum(abs(x), dim=1))\nmax(abs(x))\n\n-inf\nmin(sum(abs(x), dim=1))\nmin(abs(x))\n\n0\n– not supported –\nsum(x != 0)\n\n1\nmax(sum(abs(x), dim=0))\nas below\n\n-1\nmin(sum(abs(x), dim=0))\nas below\n\n2\n– not supported yet –\nas below\n\n-2\n– not supported yet –\nas below\n\nother\n– not supported –\nsum(abs(x)^{ord})^{(1 / ord)}\n\n\n\nwhere inf refers to float(‘inf’), NumPy’s inf object, or any equivalent object.\n\ndim (int, 2-tuple of ints, 2-list of ints, optional) – If dim is an int,\nvector norm will be calculated over the specified dimension. If dim\nis a 2-tuple of ints, matrix norm will be calculated over the specified\ndimensions. If dim is None, matrix norm will be calculated\nwhen the input tensor has two dimensions, and vector norm will be\ncalculated when the input tensor has one dimension. Default: None\nkeepdim (bool, optional) – If set to True, the reduced dimensions are retained\nin the result as dimensions with size one. Default: False\nout (Tensor, optional) – The output tensor.\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> from oneflow import linalg as LA\n>>> import numpy as np\n>>> a = flow.tensor(np.arange(9, dtype=np.float32) - 4)\n>>> a\ntensor([-4., -3., -2., -1.,  0.,  1.,  2.,  3.,  4.], dtype=oneflow.float32)\n>>> b = a.reshape(3, 3)\n>>> b\ntensor([[-4., -3., -2.],\n        [-1.,  0.,  1.],\n        [ 2.,  3.,  4.]], dtype=oneflow.float32)\n>>> LA.norm(a)\ntensor(7.7460, dtype=oneflow.float32)\n>>> LA.norm(b)\ntensor(7.7460, dtype=oneflow.float32)\n>>> LA.norm(b, 'fro')\ntensor(7.7460, dtype=oneflow.float32)\n>>> LA.norm(a, float('inf'))\ntensor(4., dtype=oneflow.float32)\n>>> LA.norm(b, float('inf'))\ntensor(9., dtype=oneflow.float32)\n>>> LA.norm(a, -float('inf'))\ntensor(0., dtype=oneflow.float32)\n>>> LA.norm(b, -float('inf'))\ntensor(2., dtype=oneflow.float32)\n>>> LA.norm(a, 1)\ntensor(20., dtype=oneflow.float32)\n>>> LA.norm(b, 1)\ntensor(7., dtype=oneflow.float32)\n>>> LA.norm(a, -1)\ntensor(0., dtype=oneflow.float32)\n>>> LA.norm(b, -1)\ntensor(6., dtype=oneflow.float32)\n>>> LA.norm(a, 2)\ntensor(7.7460, dtype=oneflow.float32)\n>>> LA.norm(a, -2)\ntensor(0., dtype=oneflow.float32)\n>>> LA.norm(a, 3)\ntensor(5.8480, dtype=oneflow.float32)\n>>> LA.norm(a, -3)\ntensor(0., dtype=oneflow.float32)\n>>> c = flow.tensor([[1., 2., 3.],\n...                   [-1, 1, 4]])\n>>> LA.norm(c, dim=0)\ntensor([1.4142, 2.2361, 5.0000], dtype=oneflow.float32)\n>>> LA.norm(c, dim=1, keepdim = True)\ntensor([[3.7417],\n        [4.2426]], dtype=oneflow.float32)\n>>> LA.norm(c, ord=1, dim=1)\ntensor([6., 6.], dtype=oneflow.float32)\n\n\n\n"
    },
    {
        "api_name": "oneflow.linalg.vector_norm",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.linalg.vector_norm.html",
        "api_signature": "oneflow.linalg.vector_norm(input, ord=2, dim=None, keepdim=False, *, dtype=None, out=None)",
        "api_description": "Computes a vector norm.\nSupports input of float, double dtypes.\nThis function does not necessarily treat multidimensonal attr:input as a batch of\nvectors, instead:\nIf dim= None, input will be flattened before the norm is computed.\nIf dim is an int or a tuple, the norm will be computed over these dimensions and the other dimensions will be treated as batch dimensions.\nThis behavior is for consistency with flow.linalg.norm().\nord defines the vector norm that is computed. The following norms are supported:\nord\nvector norm\n2 (default)\n2-norm (see below)\ninf\nmax(abs(x))\n-inf\nmin(abs(x))\n0\nsum(x != 0)\nother int or float\nsum(abs(x)^{ord})^{(1 / ord)}\nwhere inf refers to float(‘inf’), NumPy’s inf object, or any equivalent object.",
        "return_value": "A real-valued tensor.\n\n\n",
        "parameters": "\ninput (Tensor) – tensor, flattened by default, but this behavior can be\ncontrolled using dim.\nord (int, float, inf, -inf, 'fro', 'nuc', optional) – order of norm. Default: 2\ndim (int, Tuple[int], optional) – dimensions over which to compute\nthe norm. See above for the behavior when dim= None.\nDefault: None\nkeepdim (bool, optional) – If set to True, the reduced dimensions are retained\nin the result as dimensions with size one. Default: False\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.linalg.matrix_norm",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.linalg.matrix_norm.html",
        "api_signature": "oneflow.linalg.matrix_norm(input, ord='fro', dim=(- 2, - 1)",
        "api_description": "Computes a matrix norm.\nSupport input of float, double, cfloat and cdouble dtypes.\nAlso supports batches of matrices: the norm will be computed over the\ndimensions specified by the 2-tuple dim and the other dimensions will\nbe treated as batch dimensions. The output will have the same batch dimensions.\nord defines the matrix norm that is computed. The following norms are supported:\nord\nmatrix norm\n‘fro’ (default)\nFrobenius norm\n‘nuc’\n– not supported yet –\ninf\nmax(sum(abs(x), dim=1))\n-inf\nmin(sum(abs(x), dim=1))\n1\nmax(sum(abs(x), dim=0))\n-1\nmin(sum(abs(x), dim=0))\n2\n– not supported yet –\n-2\n– not supported yet –\nwhere inf refers to float(‘inf’), NumPy’s inf object, or any equivalent object.",
        "return_value": "A real-valued tensor.\n\n\n",
        "parameters": "\ninput (Tensor) – tensor with two or more dimensions. By default its\nshape is interpreted as (*, m, n) where * is zero or more\nbatch dimensions, but this behavior can be controlled using dim.\nord (int, inf, -inf, 'fro', 'nuc', optional) – order of norm. Default: ‘fro’\ndim (Tuple[int, int], optional) – dimensions over which to compute the norm. Default: (-2, -1)\nkeepdim (bool, optional) – If set to True, the reduced dimensions are retained\nin the result as dimensions with size one. Default: False\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.linalg.diagonal",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.linalg.diagonal.html",
        "api_signature": "oneflow.linalg.diagonal(self, input, offset=0, dim1=- 2, dim2=- 1)",
        "api_description": "Alias for oneflow.diagonal() with defaults dim1= -2, dim2= -1.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.linalg.inv",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.linalg.inv.html",
        "api_signature": "oneflow.linalg.inv(A)",
        "api_description": "Computes the inverse of a square matrix if it exists.\nThrows a RuntimeError if the matrix is not invertible.\nLetting \\(\\mathbb{K}\\) be \\(\\mathbb{R}\\) or \\(\\mathbb{C}\\),\nfor a matrix \\(A \\in \\mathbb{K}^{n     imes n}\\),\nits inverse matrix \\(A^{-1} \\in \\mathbb{K}^{n      imes n}\\) (if it exists) is defined as\n\\[A^{-1}A = AA^{-1} = \\mathrm{I}_n\\]\nwhere \\(\\mathrm{I}_n\\) is the n-dimensional identity matrix.\nThe inverse matrix exists if and only if \\(A\\) is invertible. In this case,\nthe inverse is unique.\nSupports input of float, double, cfloat and cdouble dtypes.\nAlso supports batches of matrices, and if A is a batch of matrices\nthen the output has the same batch dimensions.",
        "return_value": "",
        "parameters": "A (Tensor) – tensor of shape (*, n, n) where * is zero or more batch dimensions\nconsisting of invertible matrices.\n\nRaises\nRuntimeError – if the matrix A or any matrix in the batch of matrices A is not invertible.\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.linalg.cross",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.linalg.cross.html",
        "api_signature": "oneflow.linalg.cross(input, other, dim=- 1)",
        "api_description": "Computes the cross product of two 3-dimensional vectors.\nSupports input of float and double dtypes.\nAlso supports batches of vectors, for which it computes the product along the dimension dim.\nIn this case, the output has the same batch dimensions as the inputs broadcast to a common shape.",
        "return_value": "",
        "parameters": "\ninput (Tensor) – the first input tensor.\nother (Tensor) – the second input tensor.\ndim (int, optional) – the dimension along which to take the cross-product. Default: -1\n\n\nRaises\nRuntimeError – If after broadcasting input.size(dim) != 3 or other.size(dim) != 3.\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.linalg.det",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.linalg.det.html",
        "api_signature": "oneflow.linalg.det(A)",
        "api_description": "Computes the determinant of a square matrix.\nSupports input of float, double dtypes. Also supports batches of matrices,\nand if A is a batch of matrices then the output has the same batch dimensions.",
        "return_value": "the output Tensor.\n\n",
        "parameters": "A (Tensor) – tensor of shape (*, n, n) where * is zero or more batch dimensions.\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.optim.Optimizer.add_param_group",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.optim.Optimizer.add_param_group.html",
        "api_signature": "Optimizer.add_param_group(param_group)",
        "api_description": "Add a param group to the Optimizer s param_groups.\nThis can be useful when fine tuning a pre-trained network as frozen layers can be made\ntrainable and added to the Optimizer as training progresses.",
        "return_value": "",
        "parameters": "param_group (dict) – Specifies what Tensors should be optimized along with group\nspecific optimization options.\n\n\nExample:\n>>> import oneflow\n>>> import oneflow.optim as optim\n>>> w1 = oneflow.ones(3, 3)\n>>> w1.requires_grad = True\n>>> w2 = oneflow.ones(3, 3)\n>>> w2.requires_grad = True\n>>> o = optim.SGD([w1])\n>>> o.param_groups[0]\n{'lr': 0.001, 'momentum': 0.0, 'dampening': 0.0, 'weight_decay': 0.0, 'nesterov': False, 'maximize': False, 'params': [tensor([[1., 1., 1.],\n        [1., 1., 1.],\n        [1., 1., 1.]], dtype=oneflow.float32, requires_grad=True)]}\n>>> o.add_param_group({'params': w2})\n>>> o.param_groups[1]\n{'lr': 0.001, 'momentum': 0.0, 'dampening': 0.0, 'weight_decay': 0.0, 'nesterov': False, 'maximize': False, 'params': [tensor([[1., 1., 1.],\n        [1., 1., 1.],\n        [1., 1., 1.]], dtype=oneflow.float32, requires_grad=True)]}\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.optim.Optimizer.load_state_dict",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.optim.Optimizer.load_state_dict.html",
        "api_signature": "Optimizer.load_state_dict(state_dict)",
        "api_description": "Load the state of the optimizer which is created by state_dict function.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.optim.Optimizer.state_dict",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.optim.Optimizer.state_dict.html",
        "api_signature": "Optimizer.state_dict()",
        "api_description": "",
        "return_value": "It contains two entries:\n\nstate - a dict holding current optimization state. Its content\ndiffers between optimizer classes.\nparam_group - a dict containing all parameter groups.\n\nIt almost copied from: https://pytorch.org/docs/1.10/_modules/torch/optim/optimizer.html#Optimizer.state_dict.\n\n",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.optim.Optimizer.step",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.optim.Optimizer.step.html",
        "api_signature": "Optimizer.step(closure: Optional[Callable] = None)",
        "api_description": "Performs a single optimization step (parameter update).",
        "return_value": "The loss.\n\n",
        "parameters": "closure (Union[Callable, None], optional) – A closure that reevaluates the model and returns the loss. Optional for most optimizers.\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.optim.Optimizer.zero_grad",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.optim.Optimizer.zero_grad.html",
        "api_signature": "Optimizer.zero_grad(set_to_none: bool = False)",
        "api_description": "Sets the gradients of all optimized oneflow.Tensor s to zero.",
        "return_value": "",
        "parameters": "set_to_none (bool) – instead of setting to zero, set the grads to None.\nThis will in general have lower memory footprint, and can modestly\nimprove performance. However, it changes certain behaviors.\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": "it, a None attribute or a Tensor full of 0s will behave differently.\n2. If the user requests zero_grad(set_to_none=True) followed by a\nbackward pass, grads are guaranteed to be None for params that did not\nreceive a gradient.\n3. Optimizers have a different behavior if the gradient is 0 or None\n(in one case it does the step with a gradient of 0 and in the other\nit skips the step altogether).\n\n\n\n"
    },
    {
        "api_name": "oneflow.optim.Adagrad",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.optim.Adagrad.html",
        "api_signature": "oneflow.optim.Adagrad(params: Union[Iterator[oneflow.nn.Parameter], List[Dict]], lr: float = 0.001, lr_decay: float = 0.0, weight_decay: float = 0, initial_accumulator_value: float = 0.0, eps: float = 1e-10, contiguous_params: bool = False)",
        "api_description": "Implements Adagrad Optimizer.\nThe formula is:\n\\[ \\begin{align}\\begin{aligned}& S_{t} = S_{t-1} + grad \\odot grad\\\\& decay\\_lr = \\frac{learning\\_rate}{(1 + (train\\_step - 1) * lr\\_decay)}\\\\& X_{t} = X_{t-1} - \\frac{decay\\_lr}{\\sqrt{S_{t} + \\epsilon}} \\odot grad\\end{aligned}\\end{align} \\]",
        "return_value": "\nstep([closure])\nPerforms a single optimization step.\n\nzero_grad([set_to_none])\nSets the gradients of all optimized oneflow.Tensor s to zero.\n\n\n\nAttributes\n\n\n\n\n\n\nsupport_sparse\nWhether the Optimizer support sparse update.\n\n\n\n\n",
        "parameters": "\nparams (Union[Iterator[Parameter], List[Dict]]) – iterable of parameters to optimize or dicts defining\ngroups (parameter) – \nlr (float, optional) – The learning rate. Defaults to 0.001.\nlr_decay (float, optional) – The decay factor of learning rate. Defaults to 0.0.\nweight_decay (float, optional) – The weight decay. Defaults to 0.\ninitial_accumulator_value (float, optional) – The initial value of S. Defaults to 0.0.\neps (float, optional) – A small constant terms added to the denominator to improve numerical stability. Defaults to 1e-10.\ncontiguous_params (bool, optional) – whether to use contiguous ParamGroup\nwhich puts all parameters of the same type, device and group into the\nsame tensor and update them together. (default: False)\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": "Example 1:\n# Assume net is a custom model.\nadagrad = flow.optim.Adagrad(net.parameters(), lr=1e-3)\n\nfor epoch in range(epochs):\n    # Read data, Compute the loss and so on.\n    # ...\n    loss.backward()\n    adagrad.step()\n    adagrad.zero_grad()\n\n\nExample 2:\n# Assume net is a custom model.\nadagrad = flow.optim.Adagrad(\n    [\n        {\n            \"params\": net.parameters(),\n            \"lr\": learning_rate,\n            \"clip_grad_max_norm\": 0.5,\n            \"clip_grad_norm_type\": 2.0,\n        }\n    ],\n)\n\nfor epoch in range(epochs):\n    # Read data, Compute the loss and so on.\n    # ...\n    loss.backward()\n    adagrad.clip_grad()\n    adagrad.step()\n    adagrad.zero_grad()\n\n\nIf you want to use clip_grad, you can refer this example.\nFor more details of clip_grad_max_norm and clip_grad_norm_type, you can refer to oneflow.nn.utils.clip_grad_norm_().\n\n\n__init__(params: Union[Iterator[oneflow.nn.Parameter], List[Dict]], lr: float = 0.001, lr_decay: float = 0.0, weight_decay: float = 0, initial_accumulator_value: float = 0.0, eps: float = 1e-10, contiguous_params: bool = False)¶\nInitialize self.  See help(type(self)) for accurate signature.\n\nMethods\n\n\n\n\n\n\n__delattr__(name, /)\nImplement delattr(self, name).\n\n__dir__()\nDefault dir() implementation.\n\n__eq__(value, /)\nReturn self==value.\n\n__format__(format_spec, /)\nDefault object formatter.\n\n__ge__(value, /)\nReturn self>=value.\n\n__getattribute__(name, /)\nReturn getattr(self, name).\n\n__gt__(value, /)\nReturn self>value.\n\n__hash__()\nReturn hash(self).\n\n__init__(params[, lr, lr_decay, …])\nInitialize self.\n\n__init_subclass__\nThis method is called when a class is subclassed.\n\n__le__(value, /)\nReturn self<=value.\n\n__lt__(value, /)\nReturn self<value.\n\n__ne__(value, /)\nReturn self!=value.\n\n__new__(**kwargs)\nCreate and return a new object.\n\n__reduce__()\nHelper for pickle.\n\n__reduce_ex__(protocol, /)\nHelper for pickle.\n\n__repr__()\nReturn repr(self).\n\n__setattr__(name, value, /)\nImplement setattr(self, name, value).\n\n__sizeof__()\nSize of object in memory, in bytes.\n\n__str__()\nReturn str(self).\n\n__subclasshook__\nAbstract classes can override this to customize issubclass().\n\n_check_variables_in_graph(vars_conf)\n\n\n_check_variables_optimizer_bound(vars_conf)\n\n\n_generate_conf_for_graph(train_conf, vars_conf)\n\n\n_generate_grad_clip_conf_for_optim_conf(…)\n\n\n_generate_indexed_slices_optimizer_conf(…)\n\n\n_generate_lr_scale_for_optim_conf(…)\n\n\n_parse_input_parameters(parameters)\nSupports such parameters:\n\nadd_param_group(param_group)\nAdd a param group to the Optimizer s param_groups.\n\nclip_grad([error_if_nonfinite])\nClips gradient norm of an iterable of parameters.\n\nload_state_dict(state_dict)\nLoad the state of the optimizer which is created by state_dict function.\n\nstate_dict()\n"
    },
    {
        "api_name": "oneflow.optim.Adam",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.optim.Adam.html",
        "api_signature": "oneflow.optim.Adam(params: Union[Iterator[oneflow.nn.Parameter], List[Dict]], lr: float = 0.001, betas: Tuple[float, float] = (0.9, 0.999)",
        "api_description": "Implements Adam algorithm.\nIt has been proposed in Adam: A Method for Stochastic Optimization.\nThe implementation of the L2 penalty follows changes proposed in\nDecoupled Weight Decay Regularization.\nThis algorithm can adjust the learning rate of each parameter dynamically according to the 1st-moment estimates and the 2nd-moment estimates of gradient.\nthe equation of parameters updating is:\n\\[ \\begin{align}\\begin{aligned}& V_t = \\beta_1*V_{t-1} + (1-\\beta_1)*grad\\\\& S_t = \\beta_2*S_{t-1} + (1-\\beta_2)*{grad} \\odot {grad}\\\\& \\hat{g} = learning\\_rate*\\frac{{V_t}}{\\sqrt{{S_t}}+\\epsilon}\\\\& param_{new} = param_{old} - \\hat{g}\\end{aligned}\\end{align} \\]",
        "return_value": "\nstep([closure])\nPerforms a single optimization step.\n\nzero_grad([set_to_none])\nSets the gradients of all optimized oneflow.Tensor s to zero.\n\n\n\nAttributes\n\n\n\n\n\n\nsupport_sparse\nWhether the Optimizer support sparse update.\n\n\n\n\n",
        "parameters": "\nparams (iterable) – iterable of parameters to optimize or dicts defining\nparameter groups\nlr (float, optional) – learning rate (default: 1e-3)\nbetas (Tuple[float, float], optional) – coefficients used for computing\nrunning averages of gradient and its square (default: (0.9, 0.999))\neps (float, optional) – term added to the denominator to improve\nnumerical stability (default: 1e-8)\nweight_decay (float, optional) – weight decay (L2 penalty) (default: 0)\namsgrad (bool, optional) – whether to use the AMSGrad variant of this algorithm. (default: False)\ndo_bias_correction (bool, optional) – whether to do bias correction (default: True)\ncontiguous_params (bool, optional) – whether to use contiguous ParamGroup\nwhich puts all parameters of the same type, device and group into the\nsame tensor and update them together. (default: False)\nfused (bool, optional) – whether to divide all the parameters into several groups, then\nupdate each group of parameters with the fused kernel. (default: False)\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": "Example 1:\n# Assume net is a custom model.\nadam = flow.optim.Adam(net.parameters(), lr=1e-3)\n\nfor epoch in range(epochs):\n    # Read data, Compute the loss and so on.\n    # ...\n    loss.backward()\n    adam.step()\n    adam.zero_grad()\n\n\nExample 2:\n# Assume net is a custom model.\nadam = flow.optim.Adam(\n    [\n        {\n            \"params\": net.parameters(),\n            \"lr\": learning_rate,\n            \"clip_grad_max_norm\": 0.5,\n            \"clip_grad_norm_type\": 2.0,\n        }\n    ],\n)\n\nfor epoch in range(epochs):\n    # Read data, Compute the loss and so on.\n    # ...\n    loss.backward()\n    adam.clip_grad()\n    adam.step()\n    adam.zero_grad()\n\n\nIf you want to use clip_grad, you can refer this example.\nFor more details of clip_grad_max_norm and clip_grad_norm_type, you can refer to oneflow.nn.utils.clip_grad_norm_().\n\n\n__init__(params: Union[Iterator[oneflow.nn.Parameter], List[Dict]], lr: float = 0.001, betas: Tuple[float, float] = (0.9, 0.999), eps: float = 1e-08, weight_decay: float = 0, amsgrad: bool = False, do_bias_correction: bool = True, contiguous_params: bool = False, fused: bool = False)¶\nInitialize self.  See help(type(self)) for accurate signature.\n\nMethods\n\n\n\n\n\n\n__delattr__(name, /)\nImplement delattr(self, name).\n\n__dir__()\nDefault dir() implementation.\n\n__eq__(value, /)\nReturn self==value.\n\n__format__(format_spec, /)\nDefault object formatter.\n\n__ge__(value, /)\nReturn self>=value.\n\n__getattribute__(name, /)\nReturn getattr(self, name).\n\n__gt__(value, /)\nReturn self>value.\n\n__hash__()\nReturn hash(self).\n\n__init__(params[, lr, betas, eps, …])\nInitialize self.\n\n__init_subclass__\nThis method is called when a class is subclassed.\n\n__le__(value, /)\nReturn self<=value.\n\n__lt__(value, /)\nReturn self<value.\n\n__ne__(value, /)\nReturn self!=value.\n\n__new__(**kwargs)\nCreate and return a new object.\n\n__reduce__()\nHelper for pickle.\n\n__reduce_ex__(protocol, /)\nHelper for pickle.\n\n__repr__()\nReturn repr(self).\n\n__setattr__(name, value, /)\nImplement setattr(self, name, value).\n\n__sizeof__()\nSize of object in memory, in bytes.\n\n__str__()\nReturn str(self).\n\n__subclasshook__\nAbstract classes can override this to customize issubclass().\n\n_check_variables_in_graph(vars_conf)\n\n\n_check_variables_optimizer_bound(vars_conf)\n\n\n_fused_update(param_group)\n\n\n_generate_conf_for_graph(train_conf, vars_conf)\n\n\n_generate_grad_clip_conf_for_optim_conf(…)\n\n\n_generate_indexed_slices_optimizer_conf(…)\n\n\n_generate_lr_scale_for_optim_conf(…)\n\n\n_parse_input_parameters(parameters)\nSupports such parameters:\n\n_single_tensor_update(param_group)\n\n\nadd_param_group(param_group)\nAdd a param group to the Optimizer s param_groups.\n\nclip_grad([error_if_nonfinite])\nClips gradient norm of an iterable of parameters.\n\nload_state_dict(state_dict)\nLoad the state of the optimizer which is created by state_dict function.\n\nstate_dict()\n"
    },
    {
        "api_name": "oneflow.optim.AdamW",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.optim.AdamW.html",
        "api_signature": "oneflow.optim.AdamW(params: Union[Iterator[oneflow.nn.Parameter], List[Dict]], lr: float = 0.001, betas: Tuple[float, float] = (0.9, 0.999)",
        "api_description": "Implements AdamW algorithm.\nThe original Adam algorithm was proposed in Adam: A Method for Stochastic Optimization.\nThe AdamW variant was proposed in Decoupled Weight Decay Regularization.\nThe optimizer of the Adam-weight-decay algorithm.\n(More details please refer to Adam-weight-decay).\nSo we use Adam-weight-decay algorithm to solve this problem.\nthe equation of parameters updating is:\n\\[ \\begin{align}\\begin{aligned}& V_t = \\beta_1*V_{t-1} + (1-\\beta_1)*grad\\\\& S_t = \\beta_2*S_{t-1} + (1-\\beta_2)*{grad} \\odot {grad}\\\\& \\hat{g} = learning\\_rate*(\\frac{{V_t}}{\\sqrt{{S_t}}+\\epsilon}+\\lambda*param_{old})\\\\& param_{new} = param_{old} - \\hat{g}\\end{aligned}\\end{align} \\]",
        "return_value": "\nstep([closure])\nPerforms a single optimization step.\n\nzero_grad([set_to_none])\nSets the gradients of all optimized oneflow.Tensor s to zero.\n\n\n\nAttributes\n\n\n\n\n\n\nsupport_sparse\nWhether AdamW Optimizer support sparse update.\n\n\n\n\n",
        "parameters": "\nparams (iterable) – iterable of parameters to optimize or dicts defining\nparameter groups\nlr (float, optional) – learning rate (default: 1e-3)\nbetas (Tuple[float, float], optional) – coefficients used for computing\nrunning averages of gradient and its square (default: (0.9, 0.999))\neps (float, optional) – term added to the denominator to improve\nnumerical stability (default: 1e-8)\nweight_decay (float, optional) – weight decay (L2 penalty) (In the equation is λ, default: 0)\namsgrad (bool, optional) – whether to use the AMSGrad variant of this algorithm. (default: False)\ndo_bias_correction (bool, optional) – whether to do bias correction (default: True)\ncontiguous_params (bool, optional) – whether to use contiguous ParamGroup\nwhich puts all parameters of the same type, device and group into the\nsame tensor and update them together. (default: False)\nfused (bool, optional) – whether to divide all the parameters into several groups, then\nupdate each group of parameters with the fused kernel. (default: False)\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": "Example 1:\n# Assume net is a custom model.\nadamw = flow.optim.AdamW(net.parameters(), lr=1e-3)\n\nfor epoch in range(epochs):\n    # Read data, Compute the loss and so on.\n    # ...\n    loss.backward()\n    adamw.step()\n    adamw.zero_grad()\n\n\nExample 2:\n# Assume net is a custom model.\nadamw = flow.optim.AdamW(\n    [\n        {\n            \"params\": net.parameters(),\n            \"lr\": learning_rate,\n            \"clip_grad_max_norm\": 0.5,\n            \"clip_grad_norm_type\": 2.0,\n        }\n    ],\n)\n\nfor epoch in range(epochs):\n    # Read data, Compute the loss and so on.\n    # ...\n    loss.backward()\n    adamw.clip_grad()\n    adamw.step()\n    adamw.zero_grad()\n\n\nIf you want to use clip_grad, you can refer this example.\nFor more details of clip_grad_max_norm and clip_grad_norm_type, you can refer to oneflow.nn.utils.clip_grad_norm_().\n\n\n__init__(params: Union[Iterator[oneflow.nn.Parameter], List[Dict]], lr: float = 0.001, betas: Tuple[float, float] = (0.9, 0.999), eps: float = 1e-08, weight_decay: float = 0, amsgrad: bool = False, do_bias_correction: bool = True, contiguous_params: bool = False, fused: bool = False)¶\nInitialize self.  See help(type(self)) for accurate signature.\n\nMethods\n\n\n\n\n\n\n__delattr__(name, /)\nImplement delattr(self, name).\n\n__dir__()\nDefault dir() implementation.\n\n__eq__(value, /)\nReturn self==value.\n\n__format__(format_spec, /)\nDefault object formatter.\n\n__ge__(value, /)\nReturn self>=value.\n\n__getattribute__(name, /)\nReturn getattr(self, name).\n\n__gt__(value, /)\nReturn self>value.\n\n__hash__()\nReturn hash(self).\n\n__init__(params[, lr, betas, eps, …])\nInitialize self.\n\n__init_subclass__\nThis method is called when a class is subclassed.\n\n__le__(value, /)\nReturn self<=value.\n\n__lt__(value, /)\nReturn self<value.\n\n__ne__(value, /)\nReturn self!=value.\n\n__new__(**kwargs)\nCreate and return a new object.\n\n__reduce__()\nHelper for pickle.\n\n__reduce_ex__(protocol, /)\nHelper for pickle.\n\n__repr__()\nReturn repr(self).\n\n__setattr__(name, value, /)\nImplement setattr(self, name, value).\n\n__sizeof__()\nSize of object in memory, in bytes.\n\n__str__()\nReturn str(self).\n\n__subclasshook__\nAbstract classes can override this to customize issubclass().\n\n_check_variables_in_graph(vars_conf)\n\n\n_check_variables_optimizer_bound(vars_conf)\n\n\n_fused_update(param_group)\n\n\n_generate_conf_for_graph(train_conf, vars_conf)\n\n\n_generate_grad_clip_conf_for_optim_conf(…)\n\n\n_generate_indexed_slices_optimizer_conf(…)\n\n\n_generate_lr_scale_for_optim_conf(…)\n\n\n_parse_input_parameters(parameters)\nSupports such parameters:\n\n_single_tensor_update(param_group)\n\n\nadd_param_group(param_group)\nAdd a param group to the Optimizer s param_groups.\n\nclip_grad([error_if_nonfinite])\nClips gradient norm of an iterable of parameters.\n\nload_state_dict(state_dict)\nLoad the state of the optimizer which is created by state_dict function.\n\nstate_dict()\n"
    },
    {
        "api_name": "oneflow.optim.LAMB",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.optim.LAMB.html",
        "api_signature": "oneflow.optim.LAMB(params: Union[Iterator[oneflow.nn.Parameter], List[Dict]], lr: float = 0.001, betas: Tuple[float, float] = (0.9, 0.999)",
        "api_description": "Implements LAMB algorithm.\nLAMB was proposed in Large Batch Optimization for Deep Learning: Training BERT in 76 minutes.\nThe equation of parameters updating is:\n\\[ \\begin{align}\\begin{aligned}& V_t = \\beta_1*V_{t-1} + (1-\\beta_1)*grad\\\\& S_t = \\beta_2*S_{t-1} + (1-\\beta_2)*{grad} \\odot {grad}\\\\& \\hat{u} = \\frac{{V_t}}{\\sqrt{{S_t}}+\\epsilon}\\\\& \\hat{r} = learning\\_rate * \\frac{||param_{old}||_2}{||\\hat{u}||_2}\\\\& param_{new} = param_{old} - \\hat{r} * \\hat{u}\\end{aligned}\\end{align} \\]",
        "return_value": "\nstep([closure])\nPerforms a single optimization step.\n\nzero_grad([set_to_none])\nSets the gradients of all optimized oneflow.Tensor s to zero.\n\n\n\nAttributes\n\n\n\n\n\n\nsupport_sparse\nWhether the Optimizer support sparse update.\n\n\n\n\n",
        "parameters": "\nparameters (iterable) – iterable of parameters to optimize or dicts defining\nparameter groups\nlr (float, optional) – learning rate (default: 1e-3)\nbetas (Tuple[float, float], optional) – coefficients used for computing\nrunning averages of gradient and its square (default: (0.9, 0.999))\neps (float, optional) – term added to the denominator to improve\nnumerical stability (default: 1e-8)\nweight_decay (float, optional) – weight decay (L2 penalty) (default: 0)\nadam_w_mode (bool, optional) – apply L2 regularization or weight decay True for\ndecoupled weight decay (also known as AdamW) (default: True)\ndo_bias_correction (bool, optional) – whether to do bias correction (default: True)\namsgrad (bool, optional) – whether to use the AMSGrad variant of this algorithm.\nNOT SUPPORTED now! (default: False)\ncontiguous_params (bool, optional) – whether to use contiguous ParamGroup\nwhich puts all parameters of the same type, device and group into the\nsame tensor and update them together. (default: False)\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": "Example 1:\n# Assume net is a custom model.\nlamb = flow.optim.LAMB(net.parameters(), lr=1e-3)\n\nfor epoch in range(epochs):\n    # Read data, Compute the loss and so on.\n    # ...\n    loss.backward()\n    lamb.step()\n    lamb.zero_grad()\n\n\nExample 2:\n# Assume net is a custom model.\nlamb = flow.optim.LAMB(\n    [\n        {\n            \"params\": net.parameters(),\n            \"lr\": learning_rate,\n            \"clip_grad_max_norm\": 0.5,\n            \"clip_grad_norm_type\": 2.0,\n        }\n    ],\n)\n\nfor epoch in range(epochs):\n    # Read data, Compute the loss and so on.\n    # ...\n    loss.backward()\n    lamb.clip_grad()\n    lamb.step()\n    lamb.zero_grad()\n\n\nIf you want to use clip_grad, you can refer this example.\nFor more details of clip_grad_max_norm and clip_grad_norm_type, you can refer to oneflow.nn.utils.clip_grad_norm_().\n\n\n__init__(params: Union[Iterator[oneflow.nn.Parameter], List[Dict]], lr: float = 0.001, betas: Tuple[float, float] = (0.9, 0.999), eps: float = 1e-08, weight_decay: float = 0, adam_w_mode: bool = True, do_bias_correction: bool = True, amsgrad: bool = False, contiguous_params: bool = False)¶\nInitialize self.  See help(type(self)) for accurate signature.\n\nMethods\n\n\n\n\n\n\n__delattr__(name, /)\nImplement delattr(self, name).\n\n__dir__()\nDefault dir() implementation.\n\n__eq__(value, /)\nReturn self==value.\n\n__format__(format_spec, /)\nDefault object formatter.\n\n__ge__(value, /)\nReturn self>=value.\n\n__getattribute__(name, /)\nReturn getattr(self, name).\n\n__gt__(value, /)\nReturn self>value.\n\n__hash__()\nReturn hash(self).\n\n__init__(params[, lr, betas, eps, …])\nInitialize self.\n\n__init_subclass__\nThis method is called when a class is subclassed.\n\n__le__(value, /)\nReturn self<=value.\n\n__lt__(value, /)\nReturn self<value.\n\n__ne__(value, /)\nReturn self!=value.\n\n__new__(**kwargs)\nCreate and return a new object.\n\n__reduce__()\nHelper for pickle.\n\n__reduce_ex__(protocol, /)\nHelper for pickle.\n\n__repr__()\nReturn repr(self).\n\n__setattr__(name, value, /)\nImplement setattr(self, name, value).\n\n__sizeof__()\nSize of object in memory, in bytes.\n\n__str__()\nReturn str(self).\n\n__subclasshook__\nAbstract classes can override this to customize issubclass().\n\n_check_variables_in_graph(vars_conf)\n\n\n_check_variables_optimizer_bound(vars_conf)\n\n\n_generate_conf_for_graph(train_conf, vars_conf)\n\n\n_generate_grad_clip_conf_for_optim_conf(…)\n\n\n_generate_indexed_slices_optimizer_conf(…)\n\n\n_generate_lr_scale_for_optim_conf(…)\n\n\n_parse_input_parameters(parameters)\nSupports such parameters:\n\nadd_param_group(param_group)\nAdd a param group to the Optimizer s param_groups.\n\nclip_grad([error_if_nonfinite])\nClips gradient norm of an iterable of parameters.\n\nload_state_dict(state_dict)\nLoad the state of the optimizer which is created by state_dict function.\n\nstate_dict()\n"
    },
    {
        "api_name": "oneflow.optim.RMSprop",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.optim.RMSprop.html",
        "api_signature": "oneflow.optim.RMSprop(params: Union[Iterator[oneflow.nn.Parameter], List[Dict]], lr: float = 0.001, alpha: float = 0.99, eps: float = 1e-08, weight_decay: float = 0, momentum: float = 0.0, centered: bool = False, contiguous_params: bool = False)",
        "api_description": "Implements RMSprop algorithm.\noot Mean Squared Propagation (RMSProp) is an unpublished, adaptive learning\nrate method. The original slides proposed RMSProp: Slide 29 of\nhttp://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf .\nThe original equation is as follows:\n\\[ \\begin{align}\\begin{aligned}r(w, t) = \\alpha r(w, t-1) + (1 - \\alpha)(\\nabla Q_{i}(w))^2\\\\\\begin{split}W = w - \\frac{\\eta} {\\\\sqrt{r(w,t) + \\epsilon}} \\nabla Q_{i}(w)\\end{split}\\end{aligned}\\end{align} \\]\nThe first equation calculates moving average of the squared gradient for\neach weight. Then dividing the gradient by \\(sqrt{v(w,t)}\\).\nIn some cases, adding a momentum term :math: beta is beneficial.\nIn our implementation, Nesterov momentum is used:\n\\[ \\begin{align}\\begin{aligned}r(w, t) = \\alpha r(w, t-1) + (1 - \\alpha)(\\nabla Q_{i}(w))^2\\\\\\begin{split}v(w, t) = \\beta v(w, t-1) + \\frac{\\eta} {\\\\sqrt{r(w,t) +\n\\epsilon}} \\nabla Q_{i}(w)\\end{split}\\\\w = w - v(w, t)\\end{aligned}\\end{align} \\]\nif centered is True:\n\\[ \\begin{align}\\begin{aligned}r(w, t) = \\alpha r(w, t-1) + (1 - \\alpha)(\\nabla Q_{i}(w))^2\\\\g(w, t) = \\alpha g(w, t-1) + (1 - \\alpha)\\nabla Q_{i}(w)\\\\\\begin{split}v(w, t) = \\beta v(w, t-1) + \\frac{\\eta} {\\\\sqrt{r(w,t) - (g(w, t))^2 +\n\\epsilon}} \\nabla Q_{i}(w)\\end{split}\\\\w = w - v(w, t)\\end{aligned}\\end{align} \\]\nwhere, \\(\\alpha\\) is a hyperparameter and typical values are 0.99, 0.95\nand so on. \\(\\beta\\) is the momentum term. \\(\\epsilon\\) is a\nsmoothing term to avoid division by zero, usually set somewhere in range\nfrom 1e-4 to 1e-8.",
        "return_value": "\nstep([closure])\nPerforms a single optimization step.\n\nzero_grad([set_to_none])\nSets the gradients of all optimized oneflow.Tensor s to zero.\n\n\n\nAttributes\n\n\n\n\n\n\nsupport_sparse\nWhether the Optimizer support sparse update.\n\n\n\n\n",
        "parameters": "\nparams (iterable) – iterable of parameters to optimize or dicts defining\nparameter groups\nlr (float, optional) – learning rate (default: 1e-2)\nmomentum (float, optional) – momentum factor (default: 0, oneflow not support momenmtum > 0 now!)\nalpha (float, optional) – smoothing constant (default: 0.99)\neps (float, optional) – term added to the denominator to improve\nnumerical stability (default: 1e-8)\ncentered (bool, optional) – if True, compute the centered RMSProp,\nthe gradient is normalized by an estimation of its variance\nweight_decay (float, optional) – weight decay (L2 penalty) (default: 0)\ncontiguous_params (bool, optional) – whether to use contiguous ParamGroup\nwhich puts all parameters of the same type, device and group into the\nsame tensor and update them together. (default: False)\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": "Example 1:\n# Assume net is a custom model.\nrmsprop = flow.optim.RMSprop(net.parameters(), lr=1e-3)\n\nfor epoch in range(epochs):\n    # Read data, Compute the loss and so on.\n    # ...\n    loss.backward()\n    rmsprop.step()\n    rmsprop.zero_grad()\n\n\nExample 2:\n# Assume net is a custom model.\nrmsprop = flow.optim.RMSprop(\n    [\n        {\n            \"params\": net.parameters(),\n            \"lr\": learning_rate,\n            \"clip_grad_max_norm\": 0.5,\n            \"clip_grad_norm_type\": 2.0,\n        }\n    ],\n)\n\nfor epoch in range(epochs):\n    # Read data, Compute the loss and so on.\n    # ...\n    loss.backward()\n    rmsprop.clip_grad()\n    rmsprop.step()\n    rmsprop.zero_grad()\n\n\nIf you want to use clip_grad, you can refer this example.\nFor more details of clip_grad_max_norm and clip_grad_norm_type, you can refer to oneflow.nn.utils.clip_grad_norm_().\n\n\n__init__(params: Union[Iterator[oneflow.nn.Parameter], List[Dict]], lr: float = 0.001, alpha: float = 0.99, eps: float = 1e-08, weight_decay: float = 0, momentum: float = 0.0, centered: bool = False, contiguous_params: bool = False)¶\nInitialize self.  See help(type(self)) for accurate signature.\n\nMethods\n\n\n\n\n\n\n__delattr__(name, /)\nImplement delattr(self, name).\n\n__dir__()\nDefault dir() implementation.\n\n__eq__(value, /)\nReturn self==value.\n\n__format__(format_spec, /)\nDefault object formatter.\n\n__ge__(value, /)\nReturn self>=value.\n\n__getattribute__(name, /)\nReturn getattr(self, name).\n\n__gt__(value, /)\nReturn self>value.\n\n__hash__()\nReturn hash(self).\n\n__init__(params[, lr, alpha, eps, …])\nInitialize self.\n\n__init_subclass__\nThis method is called when a class is subclassed.\n\n__le__(value, /)\nReturn self<=value.\n\n__lt__(value, /)\nReturn self<value.\n\n__ne__(value, /)\nReturn self!=value.\n\n__new__(**kwargs)\nCreate and return a new object.\n\n__reduce__()\nHelper for pickle.\n\n__reduce_ex__(protocol, /)\nHelper for pickle.\n\n__repr__()\nReturn repr(self).\n\n__setattr__(name, value, /)\nImplement setattr(self, name, value).\n\n__sizeof__()\nSize of object in memory, in bytes.\n\n__str__()\nReturn str(self).\n\n__subclasshook__\nAbstract classes can override this to customize issubclass().\n\n_check_variables_in_graph(vars_conf)\n\n\n_check_variables_optimizer_bound(vars_conf)\n\n\n_generate_conf_for_graph(train_conf, vars_conf)\n\n\n_generate_grad_clip_conf_for_optim_conf(…)\n\n\n_generate_indexed_slices_optimizer_conf(…)\n\n\n_generate_lr_scale_for_optim_conf(…)\n\n\n_parse_input_parameters(parameters)\nSupports such parameters:\n\nadd_param_group(param_group)\nAdd a param group to the Optimizer s param_groups.\n\nclip_grad([error_if_nonfinite])\nClips gradient norm of an iterable of parameters.\n\nload_state_dict(state_dict)\nLoad the state of the optimizer which is created by state_dict function.\n\nstate_dict()\n"
    },
    {
        "api_name": "oneflow.optim.SGD",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.optim.SGD.html",
        "api_signature": "oneflow.optim.SGD(params: Union[Iterator[oneflow.nn.Parameter], List[Dict]], lr: float = 0.001, momentum: float = 0.0, dampening: float = 0.0, weight_decay: float = 0.0, nesterov: bool = False, maximize: bool = False, contiguous_params: bool = False, fused: bool = False)",
        "api_description": "Implements SGD algorithm.\nThis algorithm takes a random sample’s gradient as an approximate estimate of\nthe overall gradient in small batch gradient descent.\nWhen the momentum = 0, the equation of parameters updating is:\n\\[param_{new} = param_{old} - learning\\_rate * grad\\]\nWith momentum, the equation of parameters updating is:\n\\[ \\begin{align}\\begin{aligned}& V_t = \\beta * V_{t-1} - learning\\_rate * (g_t + param_{old} * weight\\_decay)\\\\& param_{new} = param_{old} + V_t\\end{aligned}\\end{align} \\]",
        "return_value": "\nstep([closure])\nPerforms a single optimization step. :param closure: A closure that reevaluates the model                 and returns the loss. :type closure: callable, optional.\n\nzero_grad([set_to_none])\nSets the gradients of all optimized oneflow.Tensor s to zero.\n\n\n\nAttributes\n\n\n\n\n\n\nsupport_sparse\nWhether SGD Optimizer support sparse update.\n\n\n\n\n",
        "parameters": "\nparams (iterable) – iterable of parameters to optimize or dicts defining\nparameter groups\nlr (float, optional) – learning rate (default: 1e-3)\nmomentum (float, optional) – Momentum factor (default: 0.0)\nweight_decay (float, optional) – weight decay (L2 penalty) (default: 0.0)\ncontiguous_params (bool, optional) – whether to use contiguous ParamGroup\nwhich puts all parameters of the same type, device and group into the\nsame tensor and update them together. (default: False)\nfused (bool, optional) – whether to divide all the parameters into several groups, then\nupdate each group of parameters with the fused kernel. (default: False)\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": "Example 1:\n# Assume net is a custom model.\nsgd = flow.optim.SGD(net.parameters(), lr=1e-3)\n\nfor epoch in range(epochs):\n    # Read data, Compute the loss and so on.\n    # ...\n    loss.backward()\n    sgd.step()\n    sgd.zero_grad()\n\n\nExample 2:\n# Assume net is a custom model.\nsgd = flow.optim.SGD(\n    [\n        {\n            \"params\": net.parameters(),\n            \"lr\": learning_rate,\n            \"clip_grad_max_norm\": 0.5,\n            \"clip_grad_norm_type\": 2.0,\n        }\n    ],\n)\n\nfor epoch in range(epochs):\n    # Read data, Compute the loss and so on.\n    # ...\n    loss.backward()\n    sgd.clip_grad()\n    sgd.step()\n    sgd.zero_grad()\n\n\nIf you want to use clip_grad, you can refer this example.\nFor more details of clip_grad_max_norm and clip_grad_norm_type, you can refer to oneflow.nn.utils.clip_grad_norm_().\n\n\n__init__(params: Union[Iterator[oneflow.nn.Parameter], List[Dict]], lr: float = 0.001, momentum: float = 0.0, dampening: float = 0.0, weight_decay: float = 0.0, nesterov: bool = False, maximize: bool = False, contiguous_params: bool = False, fused: bool = False)¶\nInitialize self.  See help(type(self)) for accurate signature.\n\nMethods\n\n\n\n\n\n\n__delattr__(name, /)\nImplement delattr(self, name).\n\n__dir__()\nDefault dir() implementation.\n\n__eq__(value, /)\nReturn self==value.\n\n__format__(format_spec, /)\nDefault object formatter.\n\n__ge__(value, /)\nReturn self>=value.\n\n__getattribute__(name, /)\nReturn getattr(self, name).\n\n__gt__(value, /)\nReturn self>value.\n\n__hash__()\nReturn hash(self).\n\n__init__(params[, lr, momentum, dampening, …])\nInitialize self.\n\n__init_subclass__\nThis method is called when a class is subclassed.\n\n__le__(value, /)\nReturn self<=value.\n\n__lt__(value, /)\nReturn self<value.\n\n__ne__(value, /)\nReturn self!=value.\n\n__new__(**kwargs)\nCreate and return a new object.\n\n__reduce__()\nHelper for pickle.\n\n__reduce_ex__(protocol, /)\nHelper for pickle.\n\n__repr__()\nReturn repr(self).\n\n__setattr__(name, value, /)\nImplement setattr(self, name, value).\n\n__sizeof__()\nSize of object in memory, in bytes.\n\n__str__()\nReturn str(self).\n\n__subclasshook__\nAbstract classes can override this to customize issubclass().\n\n_check_variables_in_graph(vars_conf)\n\n\n_check_variables_optimizer_bound(vars_conf)\n\n\n_fused_update(param_group)\n\n\n_generate_conf_for_graph(train_conf, vars_conf)\n\n\n_generate_grad_clip_conf_for_optim_conf(…)\n\n\n_generate_indexed_slices_optimizer_conf(…)\n\n\n_generate_lr_scale_for_optim_conf(…)\n\n\n_parse_input_parameters(parameters)\nSupports such parameters:\n\n_single_tensor_update(param_group)\n\n\nadd_param_group(param_group)\nAdd a param group to the Optimizer s param_groups.\n\nclip_grad([error_if_nonfinite])\nClips gradient norm of an iterable of parameters.\n\nload_state_dict(state_dict)\nLoad the state of the optimizer which is created by state_dict function.\n\nstate_dict()\n"
    },
    {
        "api_name": "oneflow.optim.LBFGS",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.optim.LBFGS.html",
        "api_signature": "oneflow.optim.LBFGS(params: Union[Iterator[oneflow.nn.Parameter], List[Dict]], lr: float = 0.001, max_iter: int = 20, max_eval: Optional[int] = None, tolerance_grad: float = 1e-07, tolerance_change: float = 1e-09, history_size: int = 100, line_search_fn=None, contiguous_params: bool = False)",
        "api_description": "Implements LBFGS algorithm\nIt has been propose in On the limited memory BFGS method for large scale optimization.\nThe implementation of the two-loop recursion proposed in Updating Quasi-Newton Matrices with Limited Storage.\nThe implementation of the strong_wolfe line search  proposed in Numerical_Optimization_v2\nThis algorithm uses an estimated inverse Hessian matrix to steer its search through variable space and determine the optimal direction.\nThe line search algorithm terminates with a step length that satisfies the strong Wolfe conditions.\nThis optimizer only support one parameter group.",
        "return_value": "\nstep([closure])\nPerforms a single optimization step.\n\nzero_grad([set_to_none])\nSets the gradients of all optimized oneflow.Tensor s to zero.\n\n\n\nAttributes\n\n\n\n\n\n\nsupport_sparse\nWhether the Optimizer support sparse update.\n\n\n\n\n",
        "parameters": "\nparams (iterable) – iterable of parameters to optimize or dicts defining\nparameter groups\nlr (float, optional) – learning rate (default: 1e-3)\nmax_iter (int,optional) – max iteration per step (default: 20)\nmax_eval (int,optional) – max func evals per step (default: max_iter * 1.25)\ntolerance_grad (float, optional) – termination tolerance on first order optimality (default 1e-7)\ntolerance_change (float, optional) – termination tolerance on paramter changes (default: 1e-9)\nhistory_size (int,optional) – paramter update history size (default: 100)\nline_search_fn (str,optional) – line search function strong_wolfe or None (default: None)\ncontiguous_params (bool, optional) – whether to use contiguous ParamGroup\nwhich puts all parameters of the same type, device and group into the\nsame tensor and update them together. (default: False)\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": "# Assume net is a custom model.\nlbfgs = flow.optim.LBFGS(net.parameters())\n\nfor epoch in range (epochs):\n    def closure():\n        lbfgs.zero_grad()\n        # Read data, Compute the loss and so on.\n        loss.backward()\n        return loss\n    lbfgs.step(closure)\n\n\n\n\n__init__(params: Union[Iterator[oneflow.nn.Parameter], List[Dict]], lr: float = 0.001, max_iter: int = 20, max_eval: Optional[int] = None, tolerance_grad: float = 1e-07, tolerance_change: float = 1e-09, history_size: int = 100, line_search_fn=None, contiguous_params: bool = False)¶\nInitialize self.  See help(type(self)) for accurate signature.\n\nMethods\n\n\n\n\n\n\n__delattr__(name, /)\nImplement delattr(self, name).\n\n__dir__()\nDefault dir() implementation.\n\n__eq__(value, /)\nReturn self==value.\n\n__format__(format_spec, /)\nDefault object formatter.\n\n__ge__(value, /)\nReturn self>=value.\n\n__getattribute__(name, /)\nReturn getattr(self, name).\n\n__gt__(value, /)\nReturn self>value.\n\n__hash__()\nReturn hash(self).\n\n__init__(params[, lr, max_iter, max_eval, …])\nInitialize self.\n\n__init_subclass__\nThis method is called when a class is subclassed.\n\n__le__(value, /)\nReturn self<=value.\n\n__lt__(value, /)\nReturn self<value.\n\n__ne__(value, /)\nReturn self!=value.\n\n__new__(**kwargs)\nCreate and return a new object.\n\n__reduce__()\nHelper for pickle.\n\n__reduce_ex__(protocol, /)\nHelper for pickle.\n\n__repr__()\nReturn repr(self).\n\n__setattr__(name, value, /)\nImplement setattr(self, name, value).\n\n__sizeof__()\nSize of object in memory, in bytes.\n\n__str__()\nReturn str(self).\n\n__subclasshook__\nAbstract classes can override this to customize issubclass().\n\n_check_variables_in_graph(vars_conf)\n\n\n_check_variables_optimizer_bound(vars_conf)\n\n\n_gather_flat_grad()\n\n\n_generate_grad_clip_conf_for_optim_conf(…)\n\n\n_generate_indexed_slices_optimizer_conf(…)\n\n\n_generate_lr_scale_for_optim_conf(…)\n\n\n_numel()\n\n\n_parse_input_parameters(parameters)\nSupports such parameters:\n\n_try_direction(closure, x, t, d)\n\n\n_update(step_size, direction)\n\n\nadd_param_group(param_group)\nAdd a param group to the Optimizer s param_groups.\n\nclip_grad([error_if_nonfinite])\nClips gradient norm of an iterable of parameters.\n\nload_state_dict(state_dict)\nLoad the state of the optimizer which is created by state_dict function.\n\nstate_dict()\n"
    },
    {
        "api_name": "oneflow.optim.lr_scheduler.CosineAnnealingLR",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.optim.lr_scheduler.CosineAnnealingLR.html",
        "api_signature": "oneflow.optim.lr_scheduler.CosineAnnealingLR(optimizer: oneflow.optim.optimizer.Optimizer, T_max: int, eta_min: float = 0.0, last_step: int = - 1, verbose: bool = False)",
        "api_description": "Set the learning rate of each parameter group using a cosine annealing\nschedule, where \\(\\eta_{max}\\) is set to the initial lr and\n\\(T_{cur}\\) is the number of epochs since the last restart in SGDR:\n\\[\\begin{split}\\begin{aligned}\n\\eta_t & = \\eta_{min} + \\frac{1}{2}(\\eta_{max} - \\eta_{min})\\left(1\n+ \\cos\\left(\\frac{T_{cur}}{T_{max}}\\pi\\right)\\right),\n& T_{cur} \\neq (2k+1)T_{max}; \\\\\n\\eta_{t+1} & = \\eta_{t} + \\frac{1}{2}(\\eta_{max} - \\eta_{min})\n\\left(1 - \\cos\\left(\\frac{1}{T_{max}}\\pi\\right)\\right),\n& T_{cur} = (2k+1)T_{max}.\n\\end{aligned}\\end{split}\\]\nWhen last_step=-1, sets initial lr as lr. Notice that because the schedule\nis defined recursively, the learning rate can be simultaneously modified\noutside this scheduler by other operators. If the learning rate is set\nsolely by this scheduler, the learning rate at each step becomes:\n\\[\\eta_t = \\eta_{min} + \\frac{1}{2}(\\eta_{max} - \\eta_{min})\\left(1 +\n\\cos\\left(\\frac{T_{cur}}{T_{max}}\\pi\\right)\\right)\\]\nIt has been proposed in\nSGDR: Stochastic Gradient Descent with Warm Restarts. Note that this only\nimplements the cosine annealing part of SGDR, and not the restarts.",
        "return_value": "",
        "parameters": "\noptimizer (Optimizer) – Wrapped optimizer.\nT_max (int) – Maximum number of iterations.\neta_min (float) – Minimum learning rate. Default: 0.\nlast_step (int) – The index of last epoch. Default: -1.\nverbose (bool) – If True, prints a message to stdout for\neach update. Default: False.\n\n\n\n\n\n__init__(optimizer: oneflow.optim.optimizer.Optimizer, T_max: int, eta_min: float = 0.0, last_step: int = - 1, verbose: bool = False)¶\nInitialize self.  See help(type(self)) for accurate signature.\n\nMethods\n\n\n\n\n\n\n__delattr__(name, /)\nImplement delattr(self, name).\n\n__dir__()\nDefault dir() implementation.\n\n__eq__(value, /)\nReturn self==value.\n\n__format__(format_spec, /)\nDefault object formatter.\n\n__ge__(value, /)\nReturn self>=value.\n\n__getattribute__(name, /)\nReturn getattr(self, name).\n\n__gt__(value, /)\nReturn self>value.\n\n__hash__()\nReturn hash(self).\n\n__init__(optimizer, T_max[, eta_min, …])\nInitialize self.\n\n__init_subclass__\nThis method is called when a class is subclassed.\n\n__le__(value, /)\nReturn self<=value.\n\n__lt__(value, /)\nReturn self<value.\n\n__ne__(value, /)\nReturn self!=value.\n\n__new__(**kwargs)\nCreate and return a new object.\n\n__reduce__()\nHelper for pickle.\n\n__reduce_ex__(protocol, /)\nHelper for pickle.\n\n__repr__()\nReturn repr(self).\n\n__setattr__(name, value, /)\nImplement setattr(self, name, value).\n\n__sizeof__()\nSize of object in memory, in bytes.\n\n__str__()\nReturn str(self).\n\n__subclasshook__\nAbstract classes can override this to customize issubclass().\n\n_generate_conf_for_graph(lr_conf)\n\n\n_init_base_lrs()\n\n\nget_last_lr()\nReturn last computed learning rate by current scheduler.\n\nget_lr(base_lr, step)\nCompute learning rate using chainable form of the scheduler\n\nload_state_dict(state_dict)\nLoad the schedulers state.\n\nprint_lr(group, lr)\nDisplay the current learning rate.\n\nstate_dict()\nReturn the state of the scheduler as a dict.\n\nstep()\n\n\nupdate_lrs(lrs)\n\n\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.optim.lr_scheduler.CosineDecayLR",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.optim.lr_scheduler.CosineDecayLR.html",
        "api_signature": "oneflow.optim.lr_scheduler.CosineDecayLR(optimizer: oneflow.optim.optimizer.Optimizer, decay_steps: int, alpha: float = 0.0, last_step: int = - 1, verbose: bool = False)",
        "api_description": "This operator creates a Cosine decayed learning rate scheduler.\nBefore the decay_steps are specified by user, the learning rate will be updated as:\n\\[ \\begin{align}\\begin{aligned}& cos\\_decay = 0.5*(1+cos(\\pi*\\frac{current\\_step}{decay\\_steps}))\\\\& decay\\_factor = (1-\\alpha)*cos\\_decay+\\alpha\\\\& learning\\_rate = base\\_learning\\_rate*decay\\_factor\\end{aligned}\\end{align} \\]\nAfter the decay_steps specified by user, the learning rate will be :\n\\[learning\\_rate = {base\\_learning\\_rate}*{\\alpha}\\]\nIt has been proposed in\nSGDR: Stochastic Gradient Descent with Warm Restarts. Note that this only\nimplements the cosine annealing part of SGDR, and not the restarts.",
        "return_value": "",
        "parameters": "\noptimizer (Optimizer) – Wrapped optimizer.\ndecay_steps (int) – The decay steps in the scheduler.\nalpha (float, optional) – The learning rate scale factor (\\(\\alpha\\)). (default: 0.0)\nlast_step (int, optional) – The index of last step. (default: -1)\nverbose (bool, optional) – If True, prints a message to stdout for each update. (default: False)\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": "import oneflow as flow\n\n...\ncosine_decay_lr = flow.optim.lr_scheduler.CosineDecayLR(optimizer, decay_steps=100, alpha=0.0)\nfor epoch in range(num_epoch):\n    train(...)\n    cosine_decay_lr.step()\n\n\n\n\n__init__(optimizer: oneflow.optim.optimizer.Optimizer, decay_steps: int, alpha: float = 0.0, last_step: int = - 1, verbose: bool = False)¶\nInitialize self.  See help(type(self)) for accurate signature.\n\nMethods\n\n\n\n\n\n\n__delattr__(name, /)\nImplement delattr(self, name).\n\n__dir__()\nDefault dir() implementation.\n\n__eq__(value, /)\nReturn self==value.\n\n__format__(format_spec, /)\nDefault object formatter.\n\n__ge__(value, /)\nReturn self>=value.\n\n__getattribute__(name, /)\nReturn getattr(self, name).\n\n__gt__(value, /)\nReturn self>value.\n\n__hash__()\nReturn hash(self).\n\n__init__(optimizer, decay_steps[, alpha, …])\nInitialize self.\n\n__init_subclass__\nThis method is called when a class is subclassed.\n\n__le__(value, /)\nReturn self<=value.\n\n__lt__(value, /)\nReturn self<value.\n\n__ne__(value, /)\nReturn self!=value.\n\n__new__(**kwargs)\nCreate and return a new object.\n\n__reduce__()\nHelper for pickle.\n\n__reduce_ex__(protocol, /)\nHelper for pickle.\n\n__repr__()\nReturn repr(self).\n\n__setattr__(name, value, /)\nImplement setattr(self, name, value).\n\n__sizeof__()\nSize of object in memory, in bytes.\n\n__str__()\nReturn str(self).\n\n__subclasshook__\nAbstract classes can override this to customize issubclass().\n\n_generate_conf_for_graph(lr_conf)\n\n\n_init_base_lrs()\n\n\nget_last_lr()\nReturn last computed learning rate by current scheduler.\n\nget_lr(base_lr, step)\nCompute learning rate using chainable form of the scheduler\n\nload_state_dict(state_dict)\nLoad the schedulers state.\n\nprint_lr(group, lr)\nDisplay the current learning rate.\n\nstate_dict()\nReturn the state of the scheduler as a dict.\n\nstep()\n\n\nupdate_lrs(lrs)\n\n\n\n\n\n"
    },
    {
        "api_name": "oneflow.optim.lr_scheduler.ExponentialLR",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.optim.lr_scheduler.ExponentialLR.html",
        "api_signature": "oneflow.optim.lr_scheduler.ExponentialLR(optimizer: oneflow.optim.optimizer.Optimizer, gamma: float, last_step: int = - 1, verbose: bool = False)",
        "api_description": "Decays the learning rate of each parameter group by gamma every epoch.\nWhen last_epoch=-1, sets initial lr as lr.",
        "return_value": "",
        "parameters": "\noptimizer (Optimizer) – Wrapped optimizer.\ngamma (float) – Multiplicative factor of learning rate decay.\nlast_step (int) – The index of last step. Default: -1.\nverbose (bool) – If True, prints a message to stdout for\neach update. Default: False.\n\n\n\n\n\n__init__(optimizer: oneflow.optim.optimizer.Optimizer, gamma: float, last_step: int = - 1, verbose: bool = False)¶\nInitialize self.  See help(type(self)) for accurate signature.\n\nMethods\n\n\n\n\n\n\n__delattr__(name, /)\nImplement delattr(self, name).\n\n__dir__()\nDefault dir() implementation.\n\n__eq__(value, /)\nReturn self==value.\n\n__format__(format_spec, /)\nDefault object formatter.\n\n__ge__(value, /)\nReturn self>=value.\n\n__getattribute__(name, /)\nReturn getattr(self, name).\n\n__gt__(value, /)\nReturn self>value.\n\n__hash__()\nReturn hash(self).\n\n__init__(optimizer, gamma[, last_step, verbose])\nInitialize self.\n\n__init_subclass__\nThis method is called when a class is subclassed.\n\n__le__(value, /)\nReturn self<=value.\n\n__lt__(value, /)\nReturn self<value.\n\n__ne__(value, /)\nReturn self!=value.\n\n__new__(**kwargs)\nCreate and return a new object.\n\n__reduce__()\nHelper for pickle.\n\n__reduce_ex__(protocol, /)\nHelper for pickle.\n\n__repr__()\nReturn repr(self).\n\n__setattr__(name, value, /)\nImplement setattr(self, name, value).\n\n__sizeof__()\nSize of object in memory, in bytes.\n\n__str__()\nReturn str(self).\n\n__subclasshook__\nAbstract classes can override this to customize issubclass().\n\n_generate_conf_for_graph(lr_conf)\n\n\n_init_base_lrs()\n\n\nget_last_lr()\nReturn last computed learning rate by current scheduler.\n\nget_lr(base_lr, step)\nCompute learning rate using chainable form of the scheduler\n\nload_state_dict(state_dict)\nLoad the schedulers state.\n\nprint_lr(group, lr)\nDisplay the current learning rate.\n\nstate_dict()\nReturn the state of the scheduler as a dict.\n\nstep()\n\n\nupdate_lrs(lrs)\n\n\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.optim.lr_scheduler.LambdaLR",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.optim.lr_scheduler.LambdaLR.html",
        "api_signature": "oneflow.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda, last_step=- 1, verbose=False)",
        "api_description": "Sets the learning rate of each parameter group to the initial lr times a given function.\nWhen last_step=-1, sets initial lr as lr.\n\\[learning\\_rate = base\\_learning\\_rate*lambda(last\\_step)\\]",
        "return_value": "\nstep()\nPerforms a single learning rate schedule step.\n\nupdate_lrs(lrs)\n\n\n\n\n\n",
        "parameters": "\noptimizer (Optimizer) – Wrapped optimizer.\nlr_lambda (function or list) – A function which computes a multiplicative factor given an integer\nparameter epoch, or a list of such functions, one for each group in optimizer.param_groups.\nlast_step (int, optional) – The index of last step. (default: -1)\nverbose (bool, optional) – If True, prints a message to stdout for each update. (default: False)\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": "import oneflow as flow\n\n...\nlambda1 = lambda step: step // 30\nlambda2 = lambda step: 0.95 * step\nlambda_lr = flow.optim.lr_scheduler.LambdaLR(optimizer, [lambda1, lambda2])\nfor epoch in range(num_epoch):\n    train(...)\n    lambda_lr.step()\n\n\n\n\n__init__(optimizer, lr_lambda, last_step=- 1, verbose=False)¶\nInitialize self.  See help(type(self)) for accurate signature.\n\nMethods\n\n\n\n\n\n\n__delattr__(name, /)\nImplement delattr(self, name).\n\n__dir__()\nDefault dir() implementation.\n\n__eq__(value, /)\nReturn self==value.\n\n__format__(format_spec, /)\nDefault object formatter.\n\n__ge__(value, /)\nReturn self>=value.\n\n__getattribute__(name, /)\nReturn getattr(self, name).\n\n__gt__(value, /)\nReturn self>value.\n\n__hash__()\nReturn hash(self).\n\n__init__(optimizer, lr_lambda[, last_step, …])\nInitialize self.\n\n__init_subclass__\nThis method is called when a class is subclassed.\n\n__le__(value, /)\nReturn self<=value.\n\n__lt__(value, /)\nReturn self<value.\n\n__ne__(value, /)\nReturn self!=value.\n\n__new__(**kwargs)\nCreate and return a new object.\n\n__reduce__()\nHelper for pickle.\n\n__reduce_ex__(protocol, /)\nHelper for pickle.\n\n__repr__()\nReturn repr(self).\n\n__setattr__(name, value, /)\nImplement setattr(self, name, value).\n\n__sizeof__()\nSize of object in memory, in bytes.\n\n__str__()\nReturn str(self).\n\n__subclasshook__\nAbstract classes can override this to customize issubclass().\n\n_init_base_lrs()\n\n\nget_last_lr()\nReturn last computed learning rate by current scheduler.\n\nget_lr(base_lr, step)\nCompute learning rate using chainable form of the scheduler\n\nload_state_dict(state_dict)\nLoads the schedulers state.\n\nprint_lr(group, lr)\nDisplay the current learning rate.\n\nstate_dict()\n"
    },
    {
        "api_name": "oneflow.optim.lr_scheduler.MultiStepLR",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.optim.lr_scheduler.MultiStepLR.html",
        "api_signature": "oneflow.optim.lr_scheduler.MultiStepLR(optimizer: oneflow.optim.optimizer.Optimizer, milestones: list, gamma: float = 0.1, last_step: int = - 1, verbose: bool = False)",
        "api_description": "Decays the learning rate of each parameter group by gamma once the number of step\nreaches one of the milestones. Notice that such decay can happen simultaneously with\nother changes to the learning rate from outside this scheduler.When last_step=-1, sets initial lr as lr.",
        "return_value": "",
        "parameters": "\noptimizer (Optimizer) – Wrapped optimizer.\nmilestones (list) – List of step indices. Must be increasing\ngamma (float, optional) – Multiplicative factor of learning rate decay. (default: 0.1)\nlast_step (int, optional) – The index of last step. (default: -1)\nverbose (bool, optional) – If True, prints a message to stdout for each update. (default: False)\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": "import oneflow as flow\n\n...\nmultistep_lr = flow.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[30,80], gamma=0.1)\nfor epoch in range(num_epoch):\n    train(...)\n    multistep_lr.step()\n\n\n\n\n__init__(optimizer: oneflow.optim.optimizer.Optimizer, milestones: list, gamma: float = 0.1, last_step: int = - 1, verbose: bool = False)¶\nInitialize self.  See help(type(self)) for accurate signature.\n\nMethods\n\n\n\n\n\n\n__delattr__(name, /)\nImplement delattr(self, name).\n\n__dir__()\nDefault dir() implementation.\n\n__eq__(value, /)\nReturn self==value.\n\n__format__(format_spec, /)\nDefault object formatter.\n\n__ge__(value, /)\nReturn self>=value.\n\n__getattribute__(name, /)\nReturn getattr(self, name).\n\n__gt__(value, /)\nReturn self>value.\n\n__hash__()\nReturn hash(self).\n\n__init__(optimizer, milestones[, gamma, …])\nInitialize self.\n\n__init_subclass__\nThis method is called when a class is subclassed.\n\n__le__(value, /)\nReturn self<=value.\n\n__lt__(value, /)\nReturn self<value.\n\n__ne__(value, /)\nReturn self!=value.\n\n__new__(**kwargs)\nCreate and return a new object.\n\n__reduce__()\nHelper for pickle.\n\n__reduce_ex__(protocol, /)\nHelper for pickle.\n\n__repr__()\nReturn repr(self).\n\n__setattr__(name, value, /)\nImplement setattr(self, name, value).\n\n__sizeof__()\nSize of object in memory, in bytes.\n\n__str__()\nReturn str(self).\n\n__subclasshook__\nAbstract classes can override this to customize issubclass().\n\n_generate_conf_for_graph(lr_conf)\n\n\n_init_base_lrs()\n\n\nget_last_lr()\nReturn last computed learning rate by current scheduler.\n\nget_lr(base_lr, step)\nCompute learning rate using chainable form of the scheduler\n\nload_state_dict(state_dict)\nLoad the schedulers state.\n\nprint_lr(group, lr)\nDisplay the current learning rate.\n\nstate_dict()\nReturn the state of the scheduler as a dict.\n\nstep()\n\n\nupdate_lrs(lrs)\n\n\n\n\n\n"
    },
    {
        "api_name": "oneflow.optim.lr_scheduler.PolynomialLR",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.optim.lr_scheduler.PolynomialLR.html",
        "api_signature": "oneflow.optim.lr_scheduler.PolynomialLR(optimizer, decay_batch: int, end_learning_rate: float = 0.0001, power: float = 1.0, cycle: bool = False, last_step: int = - 1, verbose: bool = False)",
        "api_description": "This operator creates a polynomial decayed learning rate scheduler.\nThe learning rate will be updated as follows:\nIf cycle is True, the equation is:\n\\[\\begin{split}\\begin{aligned}\n& decay\\_batch = decay\\_batch*ceil(\\frac{current\\_batch}{decay\\_batch}) \\\\\n& learning\\_rate = (base\\_lr-end\\_lr)*(1-\\frac{current\\_batch}{decay\\_batch})^{power}+end\\_lr\n\\end{aligned}\\end{split}\\]\nIf cycle is False, the equation is:\n\\[\\begin{split}\\begin{aligned}\n& current\\_batch = min(decay\\_batch, current\\_batch) \\\\\n& learning\\_rate = (base\\_lr-end\\_lr)*(1-\\frac{current\\_batch}{decay\\_batch})^{power}+end\\_lr\n\\end{aligned}\\end{split}\\]",
        "return_value": "",
        "parameters": "\noptimizer (Optimizer) – Wrapper optimizer.\ndecay_batch (int) – The decayed steps.\nend_learning_rate (float, optional) – The final learning rate. Defaults to 0.0001.\npower (float, optional) – The power of polynomial. Defaults to 1.0.\ncycle (bool, optional) – If cycle is True, the scheduler will decay the learning rate every decay steps. Defaults to False.\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": "import oneflow as flow\n\n...\npolynomial_scheduler = flow.optim.lr_scheduler.PolynomialLR(\n    optimizer, decay_batch=5, end_learning_rate=0.00001, power=2\n    )\n\nfor epoch in range(num_epoch):\n    train(...)\n    polynomial_scheduler.step()\n\n\n\n\n__init__(optimizer, decay_batch: int, end_learning_rate: float = 0.0001, power: float = 1.0, cycle: bool = False, last_step: int = - 1, verbose: bool = False)¶\nInitialize self.  See help(type(self)) for accurate signature.\n\nMethods\n\n\n\n\n\n\n__delattr__(name, /)\nImplement delattr(self, name).\n\n__dir__()\nDefault dir() implementation.\n\n__eq__(value, /)\nReturn self==value.\n\n__format__(format_spec, /)\nDefault object formatter.\n\n__ge__(value, /)\nReturn self>=value.\n\n__getattribute__(name, /)\nReturn getattr(self, name).\n\n__gt__(value, /)\nReturn self>value.\n\n__hash__()\nReturn hash(self).\n\n__init__(optimizer, decay_batch[, …])\nInitialize self.\n\n__init_subclass__\nThis method is called when a class is subclassed.\n\n__le__(value, /)\nReturn self<=value.\n\n__lt__(value, /)\nReturn self<value.\n\n__ne__(value, /)\nReturn self!=value.\n\n__new__(**kwargs)\nCreate and return a new object.\n\n__reduce__()\nHelper for pickle.\n\n__reduce_ex__(protocol, /)\nHelper for pickle.\n\n__repr__()\nReturn repr(self).\n\n__setattr__(name, value, /)\nImplement setattr(self, name, value).\n\n__sizeof__()\nSize of object in memory, in bytes.\n\n__str__()\nReturn str(self).\n\n__subclasshook__\nAbstract classes can override this to customize issubclass().\n\n_generate_conf_for_graph(lr_conf)\n\n\n_init_base_lrs()\n\n\nget_last_lr()\nReturn last computed learning rate by current scheduler.\n\nget_lr(base_lr, step)\nCompute learning rate using chainable form of the scheduler\n\nload_state_dict(state_dict)\nLoad the schedulers state.\n\nprint_lr(group, lr)\nDisplay the current learning rate.\n\nstate_dict()\nReturn the state of the scheduler as a dict.\n\nstep()\n\n\nupdate_lrs(lrs)\n\n\n\n\n\n"
    },
    {
        "api_name": "oneflow.optim.lr_scheduler.ReduceLROnPlateau",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.optim.lr_scheduler.ReduceLROnPlateau.html",
        "api_signature": "oneflow.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, threshold=0.0001, threshold_mode='rel', cooldown=0, min_lr=0, eps=1e-08, verbose=False)",
        "api_description": "Reduce learning rate when a metric has stopped improving.\nModels often benefit from reducing the learning rate by a factor\nof 2-10 once learning stagnates. This scheduler reads a metrics\nquantity and if no improvement is seen for a ‘patience’ number\nof epochs, the learning rate is reduced.",
        "return_value": "\nstep(metrics)\nPerforms a single learning rate schedule step.\n\n\n\nAttributes\n\n\n\n\n\n\nin_cooldown\nWhether the learning rate scheduler in cooldown phase.\n\n\n\n\n",
        "parameters": "\noptimizer (Optimizer) – Wrapped optimizer.\nmode (str) – One of min, max. In min mode, lr will\nbe reduced when the quantity monitored has stopped\ndecreasing; in max mode it will be reduced when the\nquantity monitored has stopped increasing. Default: ‘min’.\nfactor (float) – Factor by which the learning rate will be\nreduced. new_lr = lr * factor. Default: 0.1.\npatience (int) – Number of epochs with no improvement after\nwhich learning rate will be reduced. For example, if\npatience = 2, then we will ignore the first 2 epochs\nwith no improvement, and will only decrease the LR after the\n3rd epoch if the loss still hasn’t improved then.\nDefault: 10.\nthreshold (float) – Threshold for measuring the new optimum,\nto only focus on significant changes. Default: 1e-4.\nthreshold_mode (str) – One of rel, abs. In rel mode,\ndynamic_threshold = best * ( 1 + threshold ) in ‘max’\nmode or best * ( 1 - threshold ) in min mode.\nIn abs mode, dynamic_threshold = best + threshold in\nmax mode or best - threshold in min mode. Default: ‘rel’.\ncooldown (int) – Number of epochs to wait before resuming\nnormal operation after lr has been reduced. Default: 0.\nmin_lr (float or list) – A scalar or a list of scalars. A\nlower bound on the learning rate of all param groups\nor each group respectively. Default: 0.\neps (float) – Minimal decay applied to lr. If the difference\nbetween new and old lr is smaller than eps, the update is\nignored. Default: 1e-8.\nverbose (bool) – If True, prints a message to stdout for\neach update. Default: False.\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": "optimizer = flow.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\nscheduler = flow.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min')\nfor epoch in range(10):\n    train(...)\n    val_loss = validate(...)\n    # Note that step should be called after validate()\n    scheduler.step(val_loss)\n\n\n\n\n__init__(optimizer, mode='min', factor=0.1, patience=10, threshold=0.0001, threshold_mode='rel', cooldown=0, min_lr=0, eps=1e-08, verbose=False)¶\nInitialize self.  See help(type(self)) for accurate signature.\n\nMethods\n\n\n\n\n\n\n__delattr__(name, /)\nImplement delattr(self, name).\n\n__dir__()\nDefault dir() implementation.\n\n__eq__(value, /)\nReturn self==value.\n\n__format__(format_spec, /)\nDefault object formatter.\n\n__ge__(value, /)\nReturn self>=value.\n\n__getattribute__(name, /)\nReturn getattr(self, name).\n\n__gt__(value, /)\nReturn self>value.\n\n__hash__()\nReturn hash(self).\n\n__init__(optimizer[, mode, factor, …])\nInitialize self.\n\n__init_subclass__\nThis method is called when a class is subclassed.\n\n__le__(value, /)\nReturn self<=value.\n\n__lt__(value, /)\nReturn self<value.\n\n__ne__(value, /)\nReturn self!=value.\n\n__new__(**kwargs)\nCreate and return a new object.\n\n__reduce__()\nHelper for pickle.\n\n__reduce_ex__(protocol, /)\nHelper for pickle.\n\n__repr__()\nReturn repr(self).\n\n__setattr__(name, value, /)\nImplement setattr(self, name, value).\n\n__sizeof__()\nSize of object in memory, in bytes.\n\n__str__()\nReturn str(self).\n\n__subclasshook__\nAbstract classes can override this to customize issubclass().\n\n_init_is_better(mode, threshold, threshold_mode)\n\n\n_reduce_lr(epoch)\n\n\n_reset()\nResets num_bad_steps counter and cooldown counter.\n\nis_better(a, best)\nWhether the metric has improvement.\n\nload_state_dict(state_dict)\nLoads the schedulers state.\n\nstate_dict()\n"
    },
    {
        "api_name": "oneflow.optim.lr_scheduler.StepLR",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.optim.lr_scheduler.StepLR.html",
        "api_signature": "oneflow.optim.lr_scheduler.StepLR(optimizer: oneflow.optim.optimizer.Optimizer, step_size: int, gamma: float = 0.1, last_step: int = - 1, verbose: bool = False)",
        "api_description": "Decays the learning rate of each parameter group by gamma every step_size steps.\nNotice that such decay can happen simultaneously with other changes to the learning\nrate fromoutside this scheduler. When last_step=-1, sets initial lr as lr.",
        "return_value": "",
        "parameters": "\noptimizer (Optimizer) – Wrapped optimizer.\nstep_size (int) – Period of learning rate decay.\ngamma (float, optional) – Multiplicative factor of learning rate decay. (default: 0.1)\nlast_step (int, optional) – The index of last step. (default: -1)\nverbose (bool, optional) – If True, prints a message to stdout for each update. (default: False)\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": "import oneflow as flow\n\n...\nstep_lr = flow.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\nfor epoch in range(num_epoch):\n    train(...)\n    step_lr.step()\n\n\n\n\n__init__(optimizer: oneflow.optim.optimizer.Optimizer, step_size: int, gamma: float = 0.1, last_step: int = - 1, verbose: bool = False)¶\nInitialize self.  See help(type(self)) for accurate signature.\n\nMethods\n\n\n\n\n\n\n__delattr__(name, /)\nImplement delattr(self, name).\n\n__dir__()\nDefault dir() implementation.\n\n__eq__(value, /)\nReturn self==value.\n\n__format__(format_spec, /)\nDefault object formatter.\n\n__ge__(value, /)\nReturn self>=value.\n\n__getattribute__(name, /)\nReturn getattr(self, name).\n\n__gt__(value, /)\nReturn self>value.\n\n__hash__()\nReturn hash(self).\n\n__init__(optimizer, step_size[, gamma, …])\nInitialize self.\n\n__init_subclass__\nThis method is called when a class is subclassed.\n\n__le__(value, /)\nReturn self<=value.\n\n__lt__(value, /)\nReturn self<value.\n\n__ne__(value, /)\nReturn self!=value.\n\n__new__(**kwargs)\nCreate and return a new object.\n\n__reduce__()\nHelper for pickle.\n\n__reduce_ex__(protocol, /)\nHelper for pickle.\n\n__repr__()\nReturn repr(self).\n\n__setattr__(name, value, /)\nImplement setattr(self, name, value).\n\n__sizeof__()\nSize of object in memory, in bytes.\n\n__str__()\nReturn str(self).\n\n__subclasshook__\nAbstract classes can override this to customize issubclass().\n\n_generate_conf_for_graph(lr_conf)\n\n\n_init_base_lrs()\n\n\nget_last_lr()\nReturn last computed learning rate by current scheduler.\n\nget_lr(base_lr, step)\nCompute learning rate using chainable form of the scheduler\n\nload_state_dict(state_dict)\nLoad the schedulers state.\n\nprint_lr(group, lr)\nDisplay the current learning rate.\n\nstate_dict()\nReturn the state of the scheduler as a dict.\n\nstep()\n\n\nupdate_lrs(lrs)\n\n\n\n\n\n"
    },
    {
        "api_name": "oneflow.optim.lr_scheduler.ConstantLR",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.optim.lr_scheduler.ConstantLR.html",
        "api_signature": "oneflow.optim.lr_scheduler.ConstantLR(optimizer: oneflow.optim.optimizer.Optimizer, factor: float = 0.3333333333333333, total_iters: int = 5, last_step: int = - 1, verbose: bool = False)",
        "api_description": "Decays the learning rate of each parameter group by a small constant factor until the\nnumber of step reaches a pre-defined milestone: total_iters.",
        "return_value": "",
        "parameters": "\noptimizer (Optimizer) – Wrapped optimizer.\nfactor (float) – The number we multiply learning rate until the milestone. Default: 1./3.\ntotal_iters (int) – The number of steps that the scheduler decays the learning rate.\nDefault: 5.\nlast_step (int) – The last step. Default: -1.\nverbose (bool) – If True, prints a message to stdout for\neach step. Default: False.\n\n\n\nExample\n>>> # Assuming optimizer uses lr = 0.05 for all groups\n>>> # lr = 0.025   if step == 0\n>>> # lr = 0.025   if step == 1\n>>> # lr = 0.025   if step == 2\n>>> # lr = 0.025   if step == 3\n>>> # lr = 0.05    if step >= 4\n>>> scheduler = ConstantLR(self.opt, factor=0.5, total_iters=4)\n>>> for step in range(100):\n>>>     train(...)\n>>>     validate(...)\n>>>     scheduler.step()\n\n\n\n\n__init__(optimizer: oneflow.optim.optimizer.Optimizer, factor: float = 0.3333333333333333, total_iters: int = 5, last_step: int = - 1, verbose: bool = False)¶\nInitialize self.  See help(type(self)) for accurate signature.\n\nMethods\n\n\n\n\n\n\n__delattr__(name, /)\nImplement delattr(self, name).\n\n__dir__()\nDefault dir() implementation.\n\n__eq__(value, /)\nReturn self==value.\n\n__format__(format_spec, /)\nDefault object formatter.\n\n__ge__(value, /)\nReturn self>=value.\n\n__getattribute__(name, /)\nReturn getattr(self, name).\n\n__gt__(value, /)\nReturn self>value.\n\n__hash__()\nReturn hash(self).\n\n__init__(optimizer[, factor, total_iters, …])\nInitialize self.\n\n__init_subclass__\nThis method is called when a class is subclassed.\n\n__le__(value, /)\nReturn self<=value.\n\n__lt__(value, /)\nReturn self<value.\n\n__ne__(value, /)\nReturn self!=value.\n\n__new__(**kwargs)\nCreate and return a new object.\n\n__reduce__()\nHelper for pickle.\n\n__reduce_ex__(protocol, /)\nHelper for pickle.\n\n__repr__()\nReturn repr(self).\n\n__setattr__(name, value, /)\nImplement setattr(self, name, value).\n\n__sizeof__()\nSize of object in memory, in bytes.\n\n__str__()\nReturn str(self).\n\n__subclasshook__\nAbstract classes can override this to customize issubclass().\n\n_generate_conf_for_graph(lr_conf)\n\n\n_init_base_lrs()\n\n\nget_last_lr()\nReturn last computed learning rate by current scheduler.\n\nget_lr(base_lr, step)\nCompute learning rate using chainable form of the scheduler\n\nload_state_dict(state_dict)\nLoad the schedulers state.\n\nprint_lr(group, lr)\nDisplay the current learning rate.\n\nstate_dict()\nReturn the state of the scheduler as a dict.\n\nstep()\n\n\nupdate_lrs(lrs)\n\n\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.optim.lr_scheduler.LinearLR",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.optim.lr_scheduler.LinearLR.html",
        "api_signature": "oneflow.optim.lr_scheduler.LinearLR(optimizer: oneflow.optim.optimizer.Optimizer, start_factor: float = 0.3333333333333333, end_factor: float = 1.0, total_iters: int = 5, last_step: int = - 1, verbose: bool = False)",
        "api_description": "Decays the learning rate of each parameter group by linearly changing small\nmultiplicative factor until the number of step reaches a pre-defined milestone: total_iters.",
        "return_value": "",
        "parameters": "\noptimizer (Optimizer) – Wrapped optimizer.\nstart_factor (float) – The number we multiply learning rate in the first step.\nThe multiplication factor changes towards end_factor in the following steps.\nDefault: 1./3.\nend_factor (float) – The number we multiply learning rate at the end of linear changing\nprocess. Default: 1.0.\ntotal_iters (int) – The number of iterations that multiplicative factor reaches to 1.\nDefault: 5.\nlast_step (int) – The index of the last step. Default: -1.\nverbose (bool) – If True, prints a message to stdout for\neach update. Default: False.\n\n\n\nExample\n>>> # Assuming optimizer uses lr = 0.05 for all groups\n>>> # lr = 0.025    if step == 0\n>>> # lr = 0.03125  if step == 1\n>>> # lr = 0.0375   if step == 2\n>>> # lr = 0.04375  if step == 3\n>>> # lr = 0.05    if step >= 4\n>>> scheduler = LinearLR(self.opt, start_factor=0.5, total_iters=4)\n>>> for step in range(100):\n>>>     train(...)\n>>>     validate(...)\n>>>     scheduler.step()\n\n\n\n\n__init__(optimizer: oneflow.optim.optimizer.Optimizer, start_factor: float = 0.3333333333333333, end_factor: float = 1.0, total_iters: int = 5, last_step: int = - 1, verbose: bool = False)¶\nInitialize self.  See help(type(self)) for accurate signature.\n\nMethods\n\n\n\n\n\n\n__delattr__(name, /)\nImplement delattr(self, name).\n\n__dir__()\nDefault dir() implementation.\n\n__eq__(value, /)\nReturn self==value.\n\n__format__(format_spec, /)\nDefault object formatter.\n\n__ge__(value, /)\nReturn self>=value.\n\n__getattribute__(name, /)\nReturn getattr(self, name).\n\n__gt__(value, /)\nReturn self>value.\n\n__hash__()\nReturn hash(self).\n\n__init__(optimizer[, start_factor, …])\nInitialize self.\n\n__init_subclass__\nThis method is called when a class is subclassed.\n\n__le__(value, /)\nReturn self<=value.\n\n__lt__(value, /)\nReturn self<value.\n\n__ne__(value, /)\nReturn self!=value.\n\n__new__(**kwargs)\nCreate and return a new object.\n\n__reduce__()\nHelper for pickle.\n\n__reduce_ex__(protocol, /)\nHelper for pickle.\n\n__repr__()\nReturn repr(self).\n\n__setattr__(name, value, /)\nImplement setattr(self, name, value).\n\n__sizeof__()\nSize of object in memory, in bytes.\n\n__str__()\nReturn str(self).\n\n__subclasshook__\nAbstract classes can override this to customize issubclass().\n\n_generate_conf_for_graph(lr_conf)\n\n\n_init_base_lrs()\n\n\nget_last_lr()\nReturn last computed learning rate by current scheduler.\n\nget_lr(base_lr, step)\nCompute learning rate using chainable form of the scheduler\n\nload_state_dict(state_dict)\nLoad the schedulers state.\n\nprint_lr(group, lr)\nDisplay the current learning rate.\n\nstate_dict()\nReturn the state of the scheduler as a dict.\n\nstep()\n\n\nupdate_lrs(lrs)\n\n\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.optim.lr_scheduler.ChainedScheduler",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.optim.lr_scheduler.ChainedScheduler.html",
        "api_signature": "oneflow.optim.lr_scheduler.ChainedScheduler(schedulers)",
        "api_description": "Chains list of learning rate schedulers. It takes a list of chainable learning\nrate schedulers and performs consecutive step() functions belong to them by just\none call.",
        "return_value": "\nstep()\n\n\nupdate_lrs(lrs)\n\n\n\n\n\n",
        "parameters": "schedulers (list) – List of chained schedulers.\n\n\nExample\n>>> # Assuming optimizer uses lr = 1. for all groups\n>>> # lr = 0.09     if step == 0\n>>> # lr = 0.081    if step == 1\n>>> # lr = 0.729    if step == 2\n>>> # lr = 0.6561   if step == 3\n>>> # lr = 0.59049  if step >= 4\n>>> scheduler1 = ConstantLR(self.opt, factor=0.1, total_iters=2)\n>>> scheduler2 = ExponentialLR(self.opt, gamma=0.9)\n>>> scheduler = ChainedScheduler([scheduler1, scheduler2])\n>>> for _ in range(100):\n>>>     train(...)\n>>>     validate(...)\n>>>     scheduler.step()\n\n\n\n\n__init__(schedulers)¶\nInitialize self.  See help(type(self)) for accurate signature.\n\nMethods\n\n\n\n\n\n\n__delattr__(name, /)\nImplement delattr(self, name).\n\n__dir__()\nDefault dir() implementation.\n\n__eq__(value, /)\nReturn self==value.\n\n__format__(format_spec, /)\nDefault object formatter.\n\n__ge__(value, /)\nReturn self>=value.\n\n__getattribute__(name, /)\nReturn getattr(self, name).\n\n__gt__(value, /)\nReturn self>value.\n\n__hash__()\nReturn hash(self).\n\n__init__(schedulers)\nInitialize self.\n\n__init_subclass__\nThis method is called when a class is subclassed.\n\n__le__(value, /)\nReturn self<=value.\n\n__lt__(value, /)\nReturn self<value.\n\n__ne__(value, /)\nReturn self!=value.\n\n__new__(**kwargs)\nCreate and return a new object.\n\n__reduce__()\nHelper for pickle.\n\n__reduce_ex__(protocol, /)\nHelper for pickle.\n\n__repr__()\nReturn repr(self).\n\n__setattr__(name, value, /)\nImplement setattr(self, name, value).\n\n__sizeof__()\nSize of object in memory, in bytes.\n\n__str__()\nReturn str(self).\n\n__subclasshook__\nAbstract classes can override this to customize issubclass().\n\n_generate_conf_for_graph(lr_conf)\n\n\n_init_base_lrs()\n\n\nget_last_lr()\nReturn last computed learning rate by current scheduler.\n\nget_lr(base_lr, step)\nCompute learning rate using chainable form of the scheduler\n\nload_state_dict(state_dict)\nLoads the schedulers state.\n\nprint_lr(group, lr)\nDisplay the current learning rate.\n\nstate_dict()\n",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.optim.lr_scheduler.SequentialLR",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.optim.lr_scheduler.SequentialLR.html",
        "api_signature": "oneflow.optim.lr_scheduler.SequentialLR(optimizer: oneflow.optim.optimizer.Optimizer, schedulers: Sequence[oneflow.nn.optimizer.lr_scheduler.LRScheduler], milestones: Sequence[int], interval_rescaling: Union[Sequence[bool], bool] = False, last_step: int = - 1, verbose: bool = False)",
        "api_description": "Receives the list of schedulers that is expected to be called sequentially during\noptimization process and milestone points that provides exact intervals to reflect\nwhich scheduler is supposed to be called at a given step.",
        "return_value": "",
        "parameters": "\noptimizer (Optimizer) – Wrapped optimizer.\nschedulers (list) – List of chained schedulers.\nmilestones (list) – List of integers that reflects milestone points.\ninterval_rescaling (bool or list) – Each scheduler has a corresponding ‘interval_rescaling’.\nIf it is set to True, scheduler will start and end at the same values as it would\nif it were the only scheduler, otherwise all schedulers share the same step.\nDefault is False for all schedulers.\nlast_step (int) – The index of last step. Default: -1.\nverbose (bool) – Default: False. Print lr if is set to True.\n\n\n\nExample\n>>> # Assuming optimizer uses lr = 1. for all groups\n>>> # lr = 0.1     if step == 0\n>>> # lr = 0.1     if step == 1\n>>> # lr = 0.9     if step == 2\n>>> # lr = 0.81    if step == 3\n>>> # lr = 0.729   if step == 4\n>>> scheduler1 = ConstantLR(self.opt, factor=0.1, total_iters=2)\n>>> scheduler2 = ExponentialLR(self.opt, gamma=0.9)\n>>> scheduler = SequentialLR(self.opt, schedulers=[scheduler1, scheduler2], milestones=[2])\n>>> for step in range(100):\n>>>     train(...)\n>>>     validate(...)\n>>>     scheduler.step()\n\n\n\n\n__init__(optimizer: oneflow.optim.optimizer.Optimizer, schedulers: Sequence[oneflow.nn.optimizer.lr_scheduler.LRScheduler], milestones: Sequence[int], interval_rescaling: Union[Sequence[bool], bool] = False, last_step: int = - 1, verbose: bool = False)¶\nInitialize self.  See help(type(self)) for accurate signature.\n\nMethods\n\n\n\n\n\n\n__delattr__(name, /)\nImplement delattr(self, name).\n\n__dir__()\nDefault dir() implementation.\n\n__eq__(value, /)\nReturn self==value.\n\n__format__(format_spec, /)\nDefault object formatter.\n\n__ge__(value, /)\nReturn self>=value.\n\n__getattribute__(name, /)\nReturn getattr(self, name).\n\n__gt__(value, /)\nReturn self>value.\n\n__hash__()\nReturn hash(self).\n\n__init__(optimizer, schedulers, milestones)\nInitialize self.\n\n__init_subclass__\nThis method is called when a class is subclassed.\n\n__le__(value, /)\nReturn self<=value.\n\n__lt__(value, /)\nReturn self<value.\n\n__ne__(value, /)\nReturn self!=value.\n\n__new__(**kwargs)\nCreate and return a new object.\n\n__reduce__()\nHelper for pickle.\n\n__reduce_ex__(protocol, /)\nHelper for pickle.\n\n__repr__()\nReturn repr(self).\n\n__setattr__(name, value, /)\nImplement setattr(self, name, value).\n\n__sizeof__()\nSize of object in memory, in bytes.\n\n__str__()\nReturn str(self).\n\n__subclasshook__\nAbstract classes can override this to customize issubclass().\n\n_generate_conf_for_graph(lr_conf)\n\n\n_init_base_lrs()\n\n\nget_last_lr()\nReturn last computed learning rate by current scheduler.\n\nget_lr(base_lr, step)\nCompute learning rate using chainable form of the scheduler\n\nload_state_dict(state_dict)\nLoad the schedulers state.\n\nprint_lr(group, lr)\nDisplay the current learning rate.\n\nstate_dict()\nReturn the state of the scheduler as a dict.\n\nstep()\n\n\nupdate_lrs(lrs)\n\n\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.optim.lr_scheduler.CosineAnnealingWarmRestarts",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.optim.lr_scheduler.CosineAnnealingWarmRestarts.html",
        "api_signature": "oneflow.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer: oneflow.optim.optimizer.Optimizer, T_0: int, T_mult: int = 1, eta_min: float = 0.0, decay_rate: float = 1.0, restart_limit: int = 0, last_step: int = - 1, verbose: bool = False)",
        "api_description": "Set the learning rate of each parameter group using a cosine annealing\nschedule, where \\(\\eta_{max}\\) is set to the initial lr, \\(T_{cur}\\)\nis the number of steps since the last restart and \\(T_{i}\\) is the number\nof steps between two warm restarts in SGDR:\n\\[\\eta_t = \\eta_{min} + \\frac{1}{2}(\\eta_{max} - \\eta_{min})\\left(1 +\n\\cos\\left(\\frac{T_{cur}}{T_{i}}\\pi\\right)\\right)\\]\nWhen \\(T_{cur}=T_{i}\\), set \\(\\eta_t = \\eta_{min}\\).\nWhen \\(T_{cur}=0\\) after restart, set \\(\\eta_t=\\eta_{max}\\).\nIt has been proposed in\nSGDR: Stochastic Gradient Descent with Warm Restarts.",
        "return_value": "",
        "parameters": "\noptimizer (Optimizer) – Wrapped optimizer.\nT_0 (int) – Number of iterations for the first restart.\nT_mult (int, optional) – A factor increases \\(T_{i}\\) after a restart. Default: 1.\neta_min (float, optional) – Minimum learning rate. Default: 0.\ndecay_rate (float, optional) – Decay rate every restarts.\nrestart_limit (int, optional) – The limit of restarts. 0 indicate unlimited restarts. Default: 0.\nlast_step (int, optional) – The index of last step. Default: -1.\nverbose (bool) – If True, prints a message to stdout for\neach update. Default: False.\n\n\n\n\n\n__init__(optimizer: oneflow.optim.optimizer.Optimizer, T_0: int, T_mult: int = 1, eta_min: float = 0.0, decay_rate: float = 1.0, restart_limit: int = 0, last_step: int = - 1, verbose: bool = False)¶\nInitialize self.  See help(type(self)) for accurate signature.\n\nMethods\n\n\n\n\n\n\n__delattr__(name, /)\nImplement delattr(self, name).\n\n__dir__()\nDefault dir() implementation.\n\n__eq__(value, /)\nReturn self==value.\n\n__format__(format_spec, /)\nDefault object formatter.\n\n__ge__(value, /)\nReturn self>=value.\n\n__getattribute__(name, /)\nReturn getattr(self, name).\n\n__gt__(value, /)\nReturn self>value.\n\n__hash__()\nReturn hash(self).\n\n__init__(optimizer, T_0[, T_mult, eta_min, …])\nInitialize self.\n\n__init_subclass__\nThis method is called when a class is subclassed.\n\n__le__(value, /)\nReturn self<=value.\n\n__lt__(value, /)\nReturn self<value.\n\n__ne__(value, /)\nReturn self!=value.\n\n__new__(**kwargs)\nCreate and return a new object.\n\n__reduce__()\nHelper for pickle.\n\n__reduce_ex__(protocol, /)\nHelper for pickle.\n\n__repr__()\nReturn repr(self).\n\n__setattr__(name, value, /)\nImplement setattr(self, name, value).\n\n__sizeof__()\nSize of object in memory, in bytes.\n\n__str__()\nReturn str(self).\n\n__subclasshook__\nAbstract classes can override this to customize issubclass().\n\n_generate_conf_for_graph(lr_conf)\n\n\n_init_base_lrs()\n\n\nget_last_lr()\nReturn last computed learning rate by current scheduler.\n\nget_lr(base_lr, step)\nCompute learning rate using chainable form of the scheduler\n\nload_state_dict(state_dict)\nLoad the schedulers state.\n\nprint_lr(group, lr)\nDisplay the current learning rate.\n\nstate_dict()\nReturn the state of the scheduler as a dict.\n\nstep()\n\n\nupdate_lrs(lrs)\n\n\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.nn.Graph.__init__",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.Graph.__init__.html",
        "api_signature": "Graph.__init__(*, enable_get_runtime_state_dict: bool = False, debug_v_level: int = - 1, debug_ranks: Optional[Union[int, List[int]]] = None, debug_max_py_stack_depth: int = 2, debug_only_user_py_stack=True, debug_op_repr_with_py_stack=False)",
        "api_description": "Initializes internal Graph states. It MUST be called in __init__ method of subclass.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> class CustomGraph(flow.nn.Graph):\n...     def __init__(self):\n...         super().__init__() # MUST be called\n...         # Then define the graph attributes\n...     def build(self):\n...         pass\n\n\n\n"
    },
    {
        "api_name": "oneflow.nn.Graph.build",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.Graph.build.html",
        "api_signature": "Graph.build(*args, **kwargs)",
        "api_description": "The build() method must be overridden to define neural network\ncomputaion logic.\nThe build() method of nn.Graph is very similar to the forward()\nmethod of nn.Module. It is used to describe the computatioin logical of\na neural network.\nWhen a graph object being called for the first time, the build()\nmethod will be called implicitly to build the computatioin graph.\nMake sure to call modules’s train() or eval() method before the\nfirst call of your graph to make the module executing the right\ntraining or evaluation logic if needed.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "build() method’s inputs and outputs support list/tuple/dict,\nbut the item in them must be one of these types:\nTensor\nNone",
        "code_example": ">>> import oneflow as flow\n>>> linear = flow.nn.Linear(3, 8, False)\n>>> class MyGraph(flow.nn.Graph):\n...     def __init__(self):\n...         super().__init__()\n...         self.model = linear\n...     def build(self, x):\n...         return self.model(x)\n\n>>> linear_graph = MyGraph()\n>>> x = flow.randn(4, 3)\n>>> linear.eval() # make linear module executing in evaluation mode\nLinear(in_features=3, out_features=8, bias=False)\n>>> y = linear_graph(x) # The build() method is called implicitly\n\n\n\n"
    },
    {
        "api_name": "oneflow.nn.Graph.add_optimizer",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.Graph.add_optimizer.html",
        "api_signature": "Graph.add_optimizer(optim: oneflow.optim.optimizer.Optimizer, *, lr_sch: Optional[oneflow.nn.optimizer.lr_scheduler.LRScheduler] = None, is_sparse: bool = False)",
        "api_description": "Add an optimizer, an learning rate scheduler to the graph.\nTo do training with nn.Graph, you should do 2 more things:\nAdd at least one optimizer(learning rate schedulers are optional) with add_optimizer() method.\nCall loss tensor’s backward() method in build() method.",
        "return_value": "",
        "parameters": "\noptim (oneflow.optim.Optimizer) – The optimizer.\nlr_sch – The learning rate scheduler, see oneflow.optim.lr_scheduler.\nis_sparse – When set to be True, treat optim as a sparse optimizer. Default is False.\n\n\n\n\n",
        "input_shape": "",
        "notes": "optimizer’s clip_grad() if a optimizer is set to do grad cliping.\noptimizer’s step().\noptimizer’s zero_grad().\nlearn rate scheduler’s step().\nAlso note that only scalar tensor are allowed to call backward()\nin nn.Graph.build() for the moment. So you may call methods such as Tensor.mean()\nto make the loss tensor a scalar tensor.\nIf you want to output the learning rate information for each step,\nset the verbose parameter of the lr_scheduler to True, and you will see the result at rank 0.\nThis feature is the same as eager mode.",
        "code_example": ">>> import oneflow as flow\n>>> loss_fn = flow.nn.MSELoss(reduction=\"sum\")\n>>> model = flow.nn.Sequential(flow.nn.Linear(3, 1), flow.nn.Flatten(0, 1))\n>>> optimizer = flow.optim.SGD(model.parameters(), lr=1e-6)\n>>> class LinearTrainGraph(flow.nn.Graph):\n...     def __init__(self):\n...         super().__init__()\n...         self.model = model\n...         self.loss_fn = loss_fn\n...         # Add an optimizer\n...         self.add_optimizer(optimizer)\n...     def build(self, x, y):\n...         y_pred = self.model(x)\n...         loss = self.loss_fn(y_pred, y)\n...         # Call loss tensor's backward(), loss tensor must be a scalar tensor\n...         loss.backward()\n...         return loss\n\n>>> linear_graph = LinearTrainGraph()\n>>> x = flow.randn(10, 3)\n>>> y = flow.randn(10)\n>>> model.train() # make model executing in training mode\nSequential(\n  (0): Linear(in_features=3, out_features=1, bias=True)\n  (1): Flatten(start_dim=0, end_dim=1)\n)\n>>> for t in range(3):\n...     loss = linear_graph(x, y)\n\n\n\n"
    },
    {
        "api_name": "oneflow.nn.Graph.set_grad_scaler",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.Graph.set_grad_scaler.html",
        "api_signature": "Graph.set_grad_scaler(grad_scaler: Optional[oneflow.amp.grad_scaler.GradScaler] = None)",
        "api_description": "Set the GradScaler for gradient and loss scaling.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.nn.Graph.__call__",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.Graph.__call__.html",
        "api_signature": "Graph.__call__(*args, **kwargs)",
        "api_description": "Call nn.Graph subclass instance to run your customized graph.\nCall your customized graph after the instantiation:",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "The first call takes longer than later calls, because nn.Graph\nwill do the computaion graph generation and optimization at the first call.\nDonot override this function.",
        "code_example": "g = CustomGraph()\nout_tensors = g(input_tensors)\n\n\nThe inputs of __call__ method must match the inputs of build()\nmethod. And the __call__ method will return outputs matching the\noutputs of build() method.\n\n"
    },
    {
        "api_name": "oneflow.nn.graph.graph_config.GraphConfig.enable_amp",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.graph.graph_config.GraphConfig.enable_amp.html",
        "api_signature": "GraphConfig.enable_amp(mode: bool = True, *, dtype: oneflow._oneflow_internal.dtype = oneflow.float16)",
        "api_description": "If set to true, then graph will use mixed precision mode, it means use both float16 and float32 during model training.",
        "return_value": "",
        "parameters": "mode (bool, optional) – The default value is True.\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": "import oneflow as flow\n\nclass Graph(flow.nn.Graph):\n    def __init__(self):\n        super().__init__()\n        self.linear = flow.nn.Linear(3, 8, False)\n        self.config.enable_amp(True) # Use mixed precision mode.\n    def build(self, x):\n        return self.linear(x)\n\ngraph = Graph()\n\n\n\n"
    },
    {
        "api_name": "oneflow.nn.graph.graph_config.GraphConfig.enable_zero",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.graph.graph_config.GraphConfig.enable_zero.html",
        "api_signature": "GraphConfig.enable_zero(mode: bool = True, *, stage: int = 2, shard_min_size: int = 1024, shard_restore_level: int = 1)",
        "api_description": "Enable ZeRO redundancy optimizer.\nThis optimization will reduce optimizer states memory consumption as described\nby ZeRO https://arxiv.org/abs/1910.02054 .\nThe default zero stage is 2.",
        "return_value": "",
        "parameters": "\nmode (bool) – if set to true, optimizer states of Data Parallel will be sharded across devices.\nstage (int) – optimization stage, range from 1 to 3.\nshard_min_size (int) – min size (element count) of a shard of an optimizer state.\nshard_restore_level (int) – level to restore sharded parameter to whole parameter for consumer operators, level 0 is no restore, level 1 is soft restore, level 2 is hard restore. Note that this parameter is at pre-alpha stage.\n\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": "import oneflow as flow\n\nclass Graph(flow.nn.Graph):\n    def __init__(self):\n        super().__init__()\n        self.linear = flow.nn.Linear(3, 8, False)\n        self.config.enable_zero()\n    def build(self, x):\n        return self.linear(x)\n\ngraph = Graph()\n\n\n\n"
    },
    {
        "api_name": "oneflow.nn.graph.graph_config.GraphConfig.allow_fuse_model_update_ops",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.graph.graph_config.GraphConfig.allow_fuse_model_update_ops.html",
        "api_signature": "GraphConfig.allow_fuse_model_update_ops(mode: bool = True)",
        "api_description": "If set to true, try to fuse cast + scale + l1_l2_regularize_gradient + model_update to one op to improve performance.",
        "return_value": "",
        "parameters": "mode (bool, optional) – The default value is True.\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": "import oneflow as flow\n\nclass Graph(flow.nn.Graph):\n    def __init__(self):\n        super().__init__()\n        self.linear = flow.nn.Linear(3, 8, False)\n        self.config.allow_fuse_model_update_ops(True)\n    def build(self, x):\n        return self.linear(x)\n\ngraph = Graph()\n\n\n\n"
    },
    {
        "api_name": "oneflow.nn.graph.graph_config.GraphConfig.allow_fuse_add_to_output",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.graph.graph_config.GraphConfig.allow_fuse_add_to_output.html",
        "api_signature": "GraphConfig.allow_fuse_add_to_output(mode: bool = True)",
        "api_description": "If set to true, try to fuse a binary element-wise add operator to one of the predecessors to improve performance.",
        "return_value": "",
        "parameters": "mode (bool, optional) – The default value is True.\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": "import oneflow as flow\n\nclass Graph(flow.nn.Graph):\n    def __init__(self):\n        super().__init__()\n        self.bn1 = flow.nn.BatchNorm1d(100)\n        self.config.allow_fuse_add_to_output(True)\n    def build(self, x):\n        bn = self.bn1(x)\n        out = bn + x\n        return out\n\ngraph = Graph()\n\n\n\n"
    },
    {
        "api_name": "oneflow.nn.graph.graph_config.GraphConfig.allow_fuse_cast_scale",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.graph.graph_config.GraphConfig.allow_fuse_cast_scale.html",
        "api_signature": "GraphConfig.allow_fuse_cast_scale(mode: bool = True)",
        "api_description": "If set to true, try to fuse cast and scalar_mul_by_tensor to improve performance.",
        "return_value": "",
        "parameters": "mode (bool, optional) – The default value is True.\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": "import oneflow as flow\n\ndef model(x):\n    return flow.mul(1,flow.cast(x,flow.int8))\n\nclass Graph(flow.nn.Graph):\n    def __init__(self):\n        super().__init__()\n        self.m=model\n        self.config.allow_fuse_cast_scale(True)\n    def build(self, x):\n        return self.m(x)\n\ngraph = Graph()\n\n\n\n"
    },
    {
        "api_name": "oneflow.nn.graph.graph_config.GraphConfig.set_gradient_accumulation_steps",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.graph.graph_config.GraphConfig.set_gradient_accumulation_steps.html",
        "api_signature": "GraphConfig.set_gradient_accumulation_steps(value)",
        "api_description": "Set num of steps to accumulate gradient.",
        "return_value": "",
        "parameters": "value (int) – num of steps.\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": "import oneflow as flow\n\nclass Graph(flow.nn.Graph):\n    def __init__(self):\n        super().__init__()\n        self.linear = flow.nn.Linear(3, 8, False)\n        # Let graph do gradient accumulation, such as pipelining parallelism depends on gradient accumulation.\n        self.config.set_gradient_accumulation_steps(4)\n    def build(self, x):\n        return self.linear(x)\n\ngraph = Graph()\n\n\n\n"
    },
    {
        "api_name": "oneflow.nn.graph.graph_config.GraphConfig.enable_cudnn_conv_heuristic_search_algo",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.graph.graph_config.GraphConfig.enable_cudnn_conv_heuristic_search_algo.html",
        "api_signature": "GraphConfig.enable_cudnn_conv_heuristic_search_algo(mode: bool = True)",
        "api_description": "Whether enable cudnn conv operation to use heuristic search algorithm.",
        "return_value": "",
        "parameters": "mode (bool, optional) – The default value is True.\n\n\n\n",
        "input_shape": "",
        "notes": "It is recommended to use flow.backends.cudnn.enable_conv_heuristic_search_algo(False) instead of this function.",
        "code_example": "import oneflow as flow\n\nclass Graph(flow.nn.Graph):\n    def __init__(self):\n        super().__init__()\n        self.m = flow.nn.Conv2d(16, 32, (3, 5), stride=(2, 1), padding=(4, 2), dilation=(3, 1))\n        # Do not enable the cudnn conv operation to use the heuristic search algorithm.\n        self.config.enable_cudnn_conv_heuristic_search_algo(False)\n    def build(self, x):\n        return self.m(x)\n\ngraph = Graph()\n\n\n\n"
    },
    {
        "api_name": "oneflow.nn.graph.graph_config.GraphConfig.enable_straighten_algorithm",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.graph.graph_config.GraphConfig.enable_straighten_algorithm.html",
        "api_signature": "GraphConfig.enable_straighten_algorithm(mode: str = 'MemoryFirst')",
        "api_description": "Whether enable the straighten algorithm.\nstraighten_algorithm_tag 1: Disable\nDisable the straighten algorithm in the task graph.\nWould use the original topography order for executing task nodes.\nstraighten_algorithm_tag 2: SpeedFirst\nUnder the second configuration, the straighten algorithm would try to speed up the training as much as possible.\nIf using nccl compute stream, setting the tag to 2 might not speed up the training.\nIf not using nccl compute stream, setting the tag to 2 might speed up data parallelism by 0.6% and model parallelism by 6%.\nConsidering memory, enabling the straighten algorithm is forbidden with one machine/device only, and not recommended under pipeline parallelism.\nstraighten_algorithm_tag 3: MemoryFirst\nUnder the third configuration, the straighten algorithm would try to compress memory as much as possible.\nIt might save up to 13% of the memory for some models.\nAnd might save nothing for some models.\nstraighten_algorithm_tag 4: OverlapCpuGpu\nUnder the forth configuration, the straighten algorithm would try to run the cpu nodes and gpu nodes alternately.\nSuch procedure would reduce the gaps of the execution on gpus.\nIt might speed up the training by 2%.\nIf no cpu nodes exist, the straighten_algorithm_tag would be switch to 3 automatically.\nstraighten_algorithm_tag 5: DelayShortGpu\nUnder the fifth configuration, the straighten algorithm would try to delay the cpu nodes.\nSuch procedure would reduce the gaps of the execution on gpus.\nIt might speed up the validation (or training).\nIf no cpu nodes exist, the straighten_algorithm_tag would be switch to 3 automatically.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.nn.graph.graph_config.GraphConfig.enable_compress_memory",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.graph.graph_config.GraphConfig.enable_compress_memory.html",
        "api_signature": "GraphConfig.enable_compress_memory(mode: bool = True)",
        "api_description": "If true, then the graph will try its best to find the minimum memory allocation strategy.\nThis process might take several minutes for a small graph and half an hour for a large one.\nThe compressed memory would be closed to the lower bound of the peak memory.\nIt benefits a lot if you need to train a lot of batches.",
        "return_value": "",
        "parameters": "mode (bool, optional) – [description]. Default is True.\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.nn.graph.graph_block.GraphModule.set_stage",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.graph.graph_block.GraphModule.set_stage.html",
        "api_signature": "GraphModule.set_stage(stage_id: Optional[int] = None, placement=None)",
        "api_description": "Set stage id and placement of nn.Module in pipeline parallelism.",
        "return_value": "",
        "parameters": "\nstage_id (int) – stage id of this module.\nplacement (flow.placement) – the placement of all tensor in this module.\n\n\n\n\n",
        "input_shape": "",
        "notes": "There will be automatically do tensor.to_global(placement) for all input tensor of\nthis module. So there is no need to write to_global() in the module forward when using\nPipeline Parallelism which is not recommended.",
        "code_example": "# module0 and module1 are two nn.Module in a nn.Graph.\n# When a nn.Module is added into a nn.Graph, it is wrapped into a ProxyModule.\n# We can set Stage ID and Placement by using ProxyModule.to(GraphModule).set_stage()\n# The Stage ID is numbered starting from 0 and increasing by 1.\n# The Placement is all tensors placement of this module.\nimport oneflow as flow\nfrom oneflow.nn.graph import GraphModule\nP_0 = flow.placement(type = \"cuda\", ranks = [0, 1])\nP_1 = flow.placement(type = \"cuda\", ranks = [2, 3])\nself.module0.to(GraphModule).set_stage(stage_id = 0, placement = P0)\nself.module1.to(GraphModule).set_stage(stage_id = 1, placement = P1)\n\n\n\n"
    },
    {
        "api_name": "oneflow.nn.graph.graph_block.GraphModule.activation_checkpointing",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.graph.graph_block.GraphModule.activation_checkpointing.html",
        "api_signature": null,
        "api_description": "Set/Get whether do activation checkpointing in this nn.Module.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": "import oneflow as flow\nfrom oneflow.nn.graph import GraphModule\n\nclass Graph(flow.nn.Graph):\n    def __init__(self):\n        super().__init__()\n        self.linear1 = flow.nn.Linear(3, 5, False)\n        self.linear2 = flow.nn.Linear(5, 8, False)\n        self.linear1.to(GraphModule).activation_checkpointing = True\n        self.linear2.to(GraphModule).activation_checkpointing = True\n\n    def build(self, x):\n        y_pred = self.linear1(x)\n        y_pred = self.linear2(y_pred)\n        return y_pred\n\ngraph = Graph()\n\n\n\n"
    },
    {
        "api_name": "oneflow.nn.Graph.state_dict",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.Graph.state_dict.html",
        "api_signature": "Graph.state_dict(destination=None)",
        "api_description": "",
        "return_value": "States of modules/optimizers/lr schedulers in a graph are included.\nKeys of modules’ state dict are corresponding to their name in the graph.\nValues of modules’ state dict are corresponding to their nn.Module’s\nstate dict.\nOther keys and tensors are states of optimizers/lr schedulers/etc.\n\na dictionary containing the whole state of the graph.\n\n",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.nn.Graph.load_state_dict",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.Graph.load_state_dict.html",
        "api_signature": "Graph.load_state_dict(state_dict: Dict[str, Union[Dict[str, oneflow.Tensor], oneflow.Tensor]], strict: bool = True)",
        "api_description": "Copies module’s states and other graph states from state_dict\ninto this graph. If strict is True, then\nthe keys of state_dict must exactly match the keys returned\nby this module’s nn.Graph.state_dict() function.",
        "return_value": "",
        "parameters": "\nstate_dict (dict) – a dict containing module’s states and other graph states.\nstrict (bool, optional) – whether to strictly enforce that the keys\nin state_dict match the keys returned by this graph’s\nnn.Graph.state_dict() function. Default: True.\n\n\n\n\n",
        "input_shape": "",
        "notes": "nn.Graph’s state dict can only be loaded before the first call of a graph.",
        "code_example": ""
    },
    {
        "api_name": "oneflow.nn.Graph.__repr__",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.Graph.__repr__.html",
        "api_signature": "Graph.__repr__()",
        "api_description": "For printing the graph structure.\nThe graph structure can be printed after graph instantiation.\nAfter the first call of graph, inputs and outputs will be added to\nthe graph structure.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": "g = CustomGraph()\nprint(g)\n\nout_tensors = g(input_tensors)\nprint(g) # Inputs and Outputs infos are added\n\n\n\n"
    },
    {
        "api_name": "oneflow.nn.Graph.debug",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.Graph.debug.html",
        "api_signature": "Graph.debug(v_level: int = - 1, *, ranks: Optional[Union[int, List[int]]] = None, max_py_stack_depth: int = 2, only_user_py_stack=True, op_repr_with_py_stack=False)",
        "api_description": "Open or close debug mode of the graph.\nIf in debug mode, logs of computation graph building infos or warnings will be\nprinted. Otherwise, only errors will be printed.\nEach nn.Module inside a nn.Graph also has a debug() method to enable debug mode.\nUse v_level to choose verbose debug info level, default level is 0, max level is 3.\nv_level -1 will disable the debug mode of the graph (i.e. no info will be printed).\nv_level 0 will print warning and graph building stages. v_level 1 will additionally\nprint graph build info of each nn.Module. v_level 2 will additionally print graph build\ninfo of each operation. v_level 3 will additionally print more detailed info of each\noperation.\nUse ranks to choose which rank to print the debug information.\nUse max_py_stack_depth to specify the max Python stack depth for the debug information.\nUse only_user_py_stack to only print the operators’ locations which are from users’ code or models.\nUse op_repr_with_py_stack to print operators’ locations when printing nn.Graph’s repr.",
        "return_value": "",
        "parameters": "\nv_level (int) – choose verbose debug info level, default v_level is 0, max v_level is 3. v_level can be set to -1 to close the debug mode.\nranks (int or list(int)) – choose ranks to print the debug information. Default rank 0.\nYou can choose any valid rank. Ranks equals -1 means debug on all ranks.\nmax_py_stack_depth (int) – the maximum depth for the Python stack debug information. Default: 2.\nonly_user_py_stack (bool) – only to print the operators’ locations from users’ code. Default: True.\nop_repr_with_py_stack (bool) – print operators’ locations when printing nn.Graph’s repr. Default: False.\n\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": "g = CustomGraph()\ng.debug()  # Open debug mode\nout_tensors = g(input_tensors)  # Will print log for debug at the first call\n\n\n\n"
    },
    {
        "api_name": "oneflow.nn.Graph.name",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.Graph.name.html",
        "api_signature": null,
        "api_description": "Name auto-generated for this graph.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.nn.graph.graph_config.GraphConfig.enable_auto_parallel",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.graph.graph_config.GraphConfig.enable_auto_parallel.html",
        "api_signature": "GraphConfig.enable_auto_parallel(mode: bool = True)",
        "api_description": "If true, then graph will use the auto parallel algorithm to select a parallelism strategy.",
        "return_value": "",
        "parameters": "mode (bool, optional) – [description]. Default is True.\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.nn.graph.graph_config.GraphConfig.enable_auto_parallel_ignore_user_sbp_config",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.graph.graph_config.GraphConfig.enable_auto_parallel_ignore_user_sbp_config.html",
        "api_signature": "GraphConfig.enable_auto_parallel_ignore_user_sbp_config(mode: bool = True)",
        "api_description": "If true, it will ignore all user configurations of SBP.",
        "return_value": "",
        "parameters": "mode (bool, optional) – [description]. Default is True.\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.nn.graph.graph_config.GraphConfig.set_auto_parallel_computation_cost_ratio",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.graph.graph_config.GraphConfig.set_auto_parallel_computation_cost_ratio.html",
        "api_signature": "GraphConfig.set_auto_parallel_computation_cost_ratio(ratio)",
        "api_description": "Set coefficient of computation cost in auto-parallel algorithm.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.nn.graph.graph_config.GraphConfig.set_auto_parallel_wait_time",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.graph.graph_config.GraphConfig.set_auto_parallel_wait_time.html",
        "api_signature": "GraphConfig.set_auto_parallel_wait_time(cost)",
        "api_description": "Set wait time for auto-parallel algorithm.\nwait time: An auto-parallel parameter. Describe the mutable extra time it will take when\ncommunication between devices occurs. It will be added to the copy cost and may get reduced\nwhen cover by computation cost.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.nn.graph.graph_config.GraphConfig.enable_auto_parallel_trunk_algo",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.graph.graph_config.GraphConfig.enable_auto_parallel_trunk_algo.html",
        "api_signature": "GraphConfig.enable_auto_parallel_trunk_algo(mode: bool = True)",
        "api_description": "Find the trunk of the SBP graph, then reduce the wait time for tributaries.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.nn.graph.graph_config.GraphConfig.enable_auto_parallel_sbp_collector",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.graph.graph_config.GraphConfig.enable_auto_parallel_sbp_collector.html",
        "api_signature": "GraphConfig.enable_auto_parallel_sbp_collector(mode: bool = True)",
        "api_description": "Use “sbp collector” to create “sbp proxy” for nodes with multiple downstream operators.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.nn.graph.graph_config.GraphConfig.enable_auto_memory",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.graph.graph_config.GraphConfig.enable_auto_memory.html",
        "api_signature": "GraphConfig.enable_auto_memory(mode: str = 'AdaptiveMemory')",
        "api_description": "Whether we use a parallelism strategy with less memory\nAuto memory strategy 1: Disable\nDisable auto memory in auto parallel.\nIgnore the memory and try our best to speed up the training.\nAuto memory strategy 2: SlightMemoryDown\nTry to decrease the memory while maintaining the throughput.\nAuto memory strategy 3: ModerateMemoryDown\nDecrease the memory, throughput might or might not be affected.\nSimilar to data parallelism + ZeRO.\nAuto memory strategy 4: HeavyMemoryDown\nTry our best to decrease the memory, ignoring the throughput.\nAuto memory strategy 5: AdaptiveMemory\nUse normal auto parallelism without consideration of memory while we have enough memory.\nGradually decrease the memory to avoid out of memory while we have inadequate memory.\nAlways try to find the highest throughput under the current limitation of memory.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.nn.image.Resize",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.image.Resize.html",
        "api_signature": null,
        "api_description": "alias of oneflow.nn.modules.dataset.ImageResize",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.nn.image.batch_align",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.image.batch_align.html",
        "api_signature": null,
        "api_description": "alias of oneflow.nn.modules.dataset.ImageBatchAlign",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.nn.image.decode",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.image.decode.html",
        "api_signature": null,
        "api_description": "alias of oneflow.nn.modules.dataset.ImageDecode",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.nn.image.flip",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.image.flip.html",
        "api_signature": null,
        "api_description": "alias of oneflow.nn.modules.dataset.ImageFlip",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.nn.image.normalize",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.nn.image.normalize.html",
        "api_signature": null,
        "api_description": "alias of oneflow.nn.modules.dataset.ImageNormalize",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.utils.global_view.to_global",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.utils.global_view.to_global.html",
        "api_signature": "oneflow.utils.global_view.to_global(input, placement=None, sbp=None, warn_on_non_tensor_leaf=True, **kwargs)",
        "api_description": "Converts the input tensor or input tensor(s) in list/tuple/dict to global tensor(s).",
        "return_value": "The converted input.\n\n\nFor a tensor input: please refer to the examples in oneflow.Tensor.to_global().\nFor an input of other type (take a state dict as an example):\n>>> # Run on 2 ranks respectively\n>>> import oneflow as flow\n>>> from oneflow import nn\n>>> placement = flow.placement(\"cpu\", ranks=[0, 1]) \n>>> sbp = (flow.sbp.broadcast,) \n>>> model = nn.Sequential(nn.Linear(8, 4), nn.ReLU(), nn.Linear(4, 2)) \n>>> global_state_dict = flow.utils.global_view.to_global(model.state_dict(), placement, sbp) \n>>> for val in state_dict.values(): \n>>>     print(val.is_global) \n\n\n>>> # results on rank 0\nTrue\nTrue\nTrue\nTrue\n\n\n>>> # results on rank 1\nTrue\nTrue\nTrue\nTrue\n\n\n\n",
        "parameters": "\ninput (oneflow.Tensor/None/list/tuple/dict) – the input that needs to be converted.\nplacement (oneflow.placement, optional) – the desired placement of the input. Default: None\nsbp (oneflow.sbp.sbp, list/tuple of oneflow.sbp.sbp or Callable[[Tensor], oneflow.sbp.sbp], optional) – the desired sbp of the input or self-defined functions in order to specify SBP. Default: None\nwarn_on_non_tensor_leaf (bool, optional) – whether to warn when the leaf is not a tensor. Default: True\n\n\n",
        "input_shape": "",
        "notes": "Both placement and sbp are required if the input is local, otherwise at least one of placement and sbp is required.\nFor the input of dict type, such as the state dict of the model, the unified sbp cannot be used when calling the to_global method, and the sbp needs to be specialized.\nUsually used for making graph models’s state dict global.\nIf you want to do the split(0) operation, but there are tensors that cannot be split by dim 0, then these tensors can specify sbp.\nIt is worth noting that, for a tensor of shape (1, n), you can specify SBP is oneflow.sbp.split(1).",
        "code_example": "flow.utils.global_view.to_global(state_dict, placement=placement, sbp=get_sbp)\n# Defines a function to return the specified SBP.\ndef get_sbp(state_dict, tensor):\n    if tensor is state_dict[\"System-Train-TrainStep\"]:\n        return oneflow.sbp.broadcast\n    if tensor is state_dict[\"module_pipeline\"][\"m_stage3.linear.weight\"]:\n        return oneflow.sbp.split(1)\n    if tensor is state_dict[\"module_pipeline\"][\"m_stage3.linear.bias\"]:\n        return oneflow.sbp.broadcast\n    return oneflow.sbp.split(0)\n\n\n\n"
    },
    {
        "api_name": "oneflow.utils.global_view.to_local",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.utils.global_view.to_local.html",
        "api_signature": "oneflow.utils.global_view.to_local(input, *, copy=False)",
        "api_description": "",
        "return_value": "\nThe converted input.\n\n\nFor a tensor input: please refer to the examples in oneflow.Tensor.to_local().\nFor an input of other type (take a state dict as an example):\n>>> # Run on 2 ranks respectively\n>>> import oneflow as flow\n>>> from oneflow import nn\n>>> placement = flow.placement(\"cpu\", ranks=[0, 1]) \n>>> sbp = (flow.sbp.broadcast,) \n>>> model = nn.Sequential(nn.Linear(8, 4), nn.ReLU(), nn.Linear(4, 2)) \n>>> model = model.to_global(placement=placement, sbp=sbp) \n>>> local_state_dict = flow.utils.global_view.to_local(model.state_dict()) \n>>> for val in local_state_dict.values(): \n>>>     print(val.is_global) \n\n\n>>> # results on rank 0\nFalse\nFalse\nFalse\nFalse\n\n\n>>> # results on rank 1\nFalse\nFalse\nFalse\nFalse\n\n\n\n",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.utils.global_view.global_mode",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.utils.global_view.global_mode.html",
        "api_signature": "oneflow.utils.global_view.global_mode(enabled, placement=None, sbp=None)",
        "api_description": "Create a scope to provide global information for the computation process within it.\nIt provides convinience for converting from local execution to global execution, especially for converting to ddp global execution.\nMake the source op create the global tensor directly.\nMake it legal for the “to(device)” API  of the global tensor.\nMake it legal to use “.device” to get the device type of the global tensor.",
        "return_value": "",
        "parameters": "\nenabled (bool) – whether the global mode is enbaled.\nplacement (oneflow.placement, optional) – the desired placement of the input. Default: None\nsbp (oneflow.sbp.sbp, list/tuple of oneflow.sbp.sbp, optional) – the desired sbp of the input or self-defined functions in order to specify SBP. Default: None\n\n\n\n",
        "input_shape": "",
        "notes": "Both placement and sbp are required if the global mode is enabled.",
        "code_example": "class LinearEvalGraphWithDDP(flow.nn.Graph):\n    def __init__(self):\n        super().__init__()\n        self.linear_dp = linear_dp\n\n    def build(self, x):\n        with global_mode(True, placement=P, sbp=B):\n            device = self.linear_dp.weight.device\n\n            x = x.to(device)\n\n            out = self.linear_dp(x)\n\n            # The local tensor will be converted to global\n            sample = flow.randn(out.shape, device=\"cpu\").to(device)\n            out = out + sample * 100\n            out = out - sample * 100\n\n        return out\n\n\nwith global_mode(False):\n    # The tensor will be keeped as local.\n    sample = flow.randn(out.shape, device=\"cpu\").to(device)\n    out = out + sample * 100\n    out = out - sample * 100\n\n\n\n"
    },
    {
        "api_name": "oneflow.utils.global_view.current_global_mode",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.utils.global_view.current_global_mode.html",
        "api_signature": null,
        "api_description": "Get the current global mode information.\nUse the current_global_mode to get the information of global mode, including enabled, placement and sbp.",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "The sbp property is supposed to return a list/tuple of oneflow.sbp.sbp.",
        "code_example": "with global_mode(True, placement=P, sbp=B):\n    # Get the global mode info.\n    cur_global_mode = global_view.current_global_mode()\n    test_case.assertTrue(cur_global_mode.is_enabled)\n    test_case.assertEqual(cur_global_mode.placement, P)\n    test_case.assertEqual(cur_global_mode.sbp[0], B)\n\n\n\n"
    },
    {
        "api_name": "oneflow.utils.tensor.from_torch",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.utils.tensor.from_torch.html",
        "api_signature": "oneflow.utils.tensor.from_torch(torch_tensor)",
        "api_description": "Create a oneflow tensor from torch tensor.\nThe returned tensor and torch tensor share the same memory.",
        "return_value": "oneflow.Tensor\n\n\n",
        "parameters": "input (torch.Tensor) – Input Tensor\n\n",
        "input_shape": "",
        "notes": "This function can be used in special data processing stages, torch’s some cpu ops can be used.",
        "code_example": "import oneflow as flow\nimport torch\n\ntorch_t = torch.tensor([[1, 2, 3], [4, 5, 6]])\nflow_t = flow.utils.tensor.from_torch(torch_t)\n\n\nThis feature from_torch is at Alpha Stage.\n\n"
    },
    {
        "api_name": "oneflow.utils.tensor.to_torch",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.utils.tensor.to_torch.html",
        "api_signature": "oneflow.utils.tensor.to_torch(flow_tensor)",
        "api_description": "Create a torch tensor from oneflow tensor.\nThe returned tensor and oneflow tensor share the same memory.",
        "return_value": "torch.Tensor\n\n\n",
        "parameters": "input (oneflow.Tensor) – Input Tensor\n\n",
        "input_shape": "",
        "notes": "Currently only local tensor is supported.",
        "code_example": "import oneflow as flow\nimport torch\n\nflow_t = flow.tensor([[1, 2, 3], [4, 5, 6]])\ntorch_t = flow.utils.tensor.to_torch(flow_t)\n\n\nThis feature to_torch is at Alpha Stage.\n\n"
    },
    {
        "api_name": "oneflow.one_embedding.make_uniform_initializer",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.one_embedding.make_uniform_initializer.html",
        "api_signature": "oneflow.one_embedding.make_uniform_initializer(low=0.0, high=1.0)",
        "api_description": "make uniform initializer param of make_table_options",
        "return_value": "initializer param of make_table_options\n\n",
        "parameters": "\nlow (float) – A python scalar. Lower bound of the range of random values to generate.\nhigh (float) – A python scalar. Upper bound of the range of random values to generate.\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> initializer = flow.one_embedding.make_uniform_initializer(low=-scale, high=scale)\n>>> # pass the initializer to flow.one_embedding.make_table_options\n>>> # ...\n\n\n\n"
    },
    {
        "api_name": "oneflow.one_embedding.make_normal_initializer",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.one_embedding.make_normal_initializer.html",
        "api_signature": "oneflow.one_embedding.make_normal_initializer(mean=0.0, std=1.0)",
        "api_description": "make normal initializer param of make_table_options",
        "return_value": "initializer param of make_table_options\n\n",
        "parameters": "\nmean (float) – A python scalar. Mean of the random values to generate.\nstd (float) – A python scalar. Standard deviation of the random values to generate.\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> initializer = flow.one_embedding.make_normal_initializer(mean=0, std=0.01)\n>>> # pass the initializer to flow.one_embedding.make_table_options\n>>> # ...\n\n\n\n"
    },
    {
        "api_name": "oneflow.one_embedding.make_device_mem_store_options",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.one_embedding.make_device_mem_store_options.html",
        "api_signature": "oneflow.one_embedding.make_device_mem_store_options(persistent_path, capacity, size_factor=1, storage_dim=- 1, physical_block_size=4096)",
        "api_description": "make GPU only store_options param of MultiTableEmbedding",
        "return_value": "GPU only store_options param of MultiTableEmbedding\n\n",
        "parameters": "\npersistent_path (str, list) – persistent storage path of Embedding. If passed a str, current rank Embedding will be saved in path/rank_id-num_ranks path. If passed a list, the list length must equals num_ranks, each elem of list represent the path of rank_id Embedding.\ncapacity (int) – total capacity of Embedding\nsize_factor (int, optional) – store size factor of embedding_dim, if SGD update, and momentum = 0, should be 1, if momentum > 0, it should be 2. if Adam, should be 3. Defaults to 1.\nstorage_dim (int, optional) – number of elements in embedding storage, if set storage_dim, the size_factor param will be invalid. if SGD update, and momentum = 0, storage_dim should be embedding_size*1, if momentum > 0, storage_dim should be embedding_size*2. if Adam, storage_dim should be embedding_size*3. Defaults to -1.\nphysical_block_size (int, optional) – physical_block_size should be sector size. Defaults to 4096.\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.one_embedding.make_cached_ssd_store_options",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.one_embedding.make_cached_ssd_store_options.html",
        "api_signature": "oneflow.one_embedding.make_cached_ssd_store_options(cache_budget_mb, persistent_path, capacity=None, size_factor=1, storage_dim=- 1, physical_block_size=4096, host_cache_budget_mb=0)",
        "api_description": "make SSD use GPU and host as cache store_options param of MultiTableEmbedding. If cache_budget_mb > 0 and host_cache_budget_mb > 0, use GPU and host memory as multi-level cache.",
        "return_value": "SSD use GPU and host as cache store_options param of MultiTableEmbedding\n\n",
        "parameters": "\ncache_budget_mb (int) – the MB budget of per GPU as cache.\npersistent_path (str, list) – persistent storage path of Embedding, must use fast SSD because of frequently random disk access during training. If passed a str, current rank Embedding will be saved in path/rank_id-num_ranks path. If passed a list, the list length must equals num_ranks, each elem of list represent the path of rank_id Embedding.\ncapacity (int) – total capacity of Embedding\nsize_factor (int, optional) – store size factor of embedding_dim, if SGD update, and momentum = 0, should be 1, if momentum > 0, it should be 2. if Adam, should be 3. Defaults to 1.\nstorage_dim (int, optional) – number of elements in embedding storage, if set storage_dim, the size_factor param will be invalid. if SGD update, and momentum = 0, storage_dim should be embedding_size*1, if momentum > 0, storage_dim should be embedding_size*2. if Adam, storage_dim should be embedding_size*3. Defaults to -1.\nphysical_block_size (int, optional) – physical_block_size should be sector size. Defaults to 4096.\nhost_cache_budget_mb (int) – the MB budget of host memory as cache per rank. Defaults to 0.\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> store_options = flow.one_embedding.make_cached_ssd_store_options(\n>>>     cache_budget_mb=8192, persistent_path=\"/your_path_to_ssd\", capacity=vocab_size,\n>>> )\n>>> # pass the store_options to the \"store_options\" param of flow.one_embedding.MultiTableEmbedding\n>>> # ...\n\n\n\n"
    },
    {
        "api_name": "oneflow.one_embedding.make_cached_host_mem_store_options",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.one_embedding.make_cached_host_mem_store_options.html",
        "api_signature": "oneflow.one_embedding.make_cached_host_mem_store_options(cache_budget_mb, persistent_path, capacity, size_factor=1, storage_dim=- 1, physical_block_size=4096)",
        "api_description": "make host use GPU as cache store_options param of MultiTableEmbedding",
        "return_value": "host use GPU as cache store_options param of MultiTableEmbedding\n\n",
        "parameters": "\ncache_budget_mb (int) – the MB budget of per GPU as cache.\npersistent_path (str, list) – persistent storage path of Embedding. If passed a str, current rank Embedding will be saved in path/rank_id-num_ranks path. If passed a list, the list length must equals num_ranks, each elem of list represent the path of rank_id Embedding.\ncapacity (int) – total capacity of Embedding\nsize_factor (int, optional) – store size factor of embedding_dim, if SGD update, and momentum = 0, should be 1, if momentum > 0, it should be 2. if Adam, should be 3. Defaults to 1.\nstorage_dim (int, optional) – number of elements in embedding storage, if set storage_dim, the size_factor param will be invalid. if SGD update, and momentum = 0, storage_dim should be embedding_size*1, if momentum > 0, storage_dim should be embedding_size*2. if Adam, storage_dim should be embedding_size*3. Defaults to -1.\nphysical_block_size (int, optional) – physical_block_size should be sector size. Defaults to 4096.\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.one_embedding.MultiTableEmbedding.forward",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.one_embedding.MultiTableEmbedding.forward.html",
        "api_signature": "MultiTableEmbedding.forward(ids, table_ids=None)",
        "api_description": "Embedding lookup operation",
        "return_value": "the result of embedding lookup\n\n",
        "parameters": "\nids (flow.tensor) – the feature ids\ntable_ids (flow.tensor, optional) – the table_id of each id, must be same shape as ids. There is no need to pass table_ids, if has config only one table or the ids has shape (batch_size, num_tables), and each column’s id belongs to the column_id th table, otherwise, you should pass the tensor_ids.\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.one_embedding.MultiTableEmbedding.save_snapshot",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.one_embedding.MultiTableEmbedding.save_snapshot.html",
        "api_signature": "MultiTableEmbedding.save_snapshot(snapshot_name)",
        "api_description": "save snapshot",
        "return_value": "",
        "parameters": "snapshot_name (str) – the snapshot_name, snapshot will be saved in the snapshots dir under your_configed_persistent_path\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> # use embedding create by flow.one_embedding.MultiTableEmbedding\n>>> embedding.save_snapshot(\"my_snapshot1\")\n>>> # a snapshot named \"my_snapshot1\" have been saved in the \"snapshots\" dir under your_configed_persistent_path\n>>> # which can be reload by flow.one_embedding.load_snapshot\n\n\n\n"
    },
    {
        "api_name": "oneflow.one_embedding.MultiTableEmbedding.load_snapshot",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.one_embedding.MultiTableEmbedding.load_snapshot.html",
        "api_signature": "MultiTableEmbedding.load_snapshot(snapshot_name)",
        "api_description": "load snapshot",
        "return_value": "",
        "parameters": "snapshot_name (str) – the snapshot_name, snapshot will be load from your_configed_persistent_path\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> # use embedding create by flow.one_embedding.MultiTableEmbedding\n>>> embedding.load_snapshot(\"my_snapshot1\")\n>>> # load a snapshot named \"my_snapshot1\" from your_configed_persistent_path\n\n\n\n"
    },
    {
        "api_name": "oneflow.one_embedding.MultiTableMultiColumnEmbedding.forward",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.one_embedding.MultiTableMultiColumnEmbedding.forward.html",
        "api_signature": "MultiTableMultiColumnEmbedding.forward(ids, table_ids=None)",
        "api_description": "Embedding lookup operation",
        "return_value": "the result of embedding lookup\n\n",
        "parameters": "\nids (flow.tensor) – the feature ids\ntable_ids (flow.tensor, optional) – the table_id of each id, must be same shape as ids. There is no need to pass table_ids, if has config only one table or the ids has shape (batch_size, num_tables), and each column’s id belongs to the column_id th table, otherwise, you should pass the tensor_ids.\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.one_embedding.MultiTableMultiColumnEmbedding.save_snapshot",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.one_embedding.MultiTableMultiColumnEmbedding.save_snapshot.html",
        "api_signature": "MultiTableMultiColumnEmbedding.save_snapshot(snapshot_name)",
        "api_description": "save snapshot",
        "return_value": "",
        "parameters": "snapshot_name (str) – the snapshot_name, snapshot will be saved in the snapshots dir under your_configed_persistent_path\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> # use embedding create by flow.one_embedding.MultiTableEmbedding\n>>> embedding.save_snapshot(\"my_snapshot1\")\n>>> # a snapshot named \"my_snapshot1\" have been saved in the \"snapshots\" dir under your_configed_persistent_path\n>>> # which can be reload by flow.one_embedding.load_snapshot\n\n\n\n"
    },
    {
        "api_name": "oneflow.one_embedding.MultiTableMultiColumnEmbedding.load_snapshot",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.one_embedding.MultiTableMultiColumnEmbedding.load_snapshot.html",
        "api_signature": "MultiTableMultiColumnEmbedding.load_snapshot(snapshot_name)",
        "api_description": "load snapshot",
        "return_value": "",
        "parameters": "snapshot_name (str) – the snapshot_name, snapshot will be load from your_configed_persistent_path\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> # use embedding create by flow.one_embedding.MultiTableEmbedding\n>>> embedding.load_snapshot(\"my_snapshot1\")\n>>> # load a snapshot named \"my_snapshot1\" from your_configed_persistent_path\n\n\n\n"
    },
    {
        "api_name": "oneflow.one_embedding.make_persistent_table_reader",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.one_embedding.make_persistent_table_reader.html",
        "api_signature": "oneflow.one_embedding.make_persistent_table_reader(paths, snapshot_name, key_type, value_type, storage_dim, physical_block_size=4096)",
        "api_description": "Creates a reader for reading persistent table.",
        "return_value": "",
        "parameters": "\npaths (list) – paths of tables to read\nsnapshot_name (str) – name of the snapshot to read\nkey_type (flow.dtype) – the data type of key\nvalue_type (flow.dtype) – the data type of value\nstorage_dim (int) – number of elements in each value\nphysical_block_size (int, optional) – physical_block_size should be sector size. Defaults to 4096\n\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.one_embedding.make_persistent_table_writer",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.one_embedding.make_persistent_table_writer.html",
        "api_signature": "oneflow.one_embedding.make_persistent_table_writer(paths, snapshot_name, key_type, value_type, storage_dim, physical_block_size=4096)",
        "api_description": "Creates a writer for writing persistent table.",
        "return_value": "",
        "parameters": "\npaths (list) – paths of tables to write\nsnapshot_name (str) – name of the snapshot to write\nkey_type (flow.dtype) – the data type of key\nvalue_type (flow.dtype) – the data type of value\nstorage_dim (int) – number of elements in each value\nphysical_block_size (int, optional) – physical_block_size should be sector size. Defaults to 4096\n\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.special.digamma",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.special.digamma.html",
        "api_signature": "oneflow.special.digamma(x: oneflow.Tensor)",
        "api_description": "Alias for oneflow.digamma().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.special.erf",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.special.erf.html",
        "api_signature": "oneflow.special.erf(x: oneflow.Tensor)",
        "api_description": "Alias for oneflow.erf().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.special.erfc",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.special.erfc.html",
        "api_signature": "oneflow.special.erfc(x: oneflow.Tensor)",
        "api_description": "Alias for oneflow.erfc().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.special.erfinv",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.special.erfinv.html",
        "api_signature": "oneflow.special.erfinv(x: oneflow.Tensor)",
        "api_description": "Alias for oneflow.erfinv().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.special.exp2",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.special.exp2.html",
        "api_signature": "oneflow.special.exp2(x: oneflow.Tensor)",
        "api_description": "Alias for oneflow.exp2().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.special.expm1",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.special.expm1.html",
        "api_signature": "oneflow.special.expm1(x: oneflow.Tensor)",
        "api_description": "Alias for oneflow.expm1().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.special.log1p",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.special.log1p.html",
        "api_signature": "oneflow.special.log1p(x: oneflow.Tensor)",
        "api_description": "Alias for oneflow.log1p().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.special.log_softmax",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.special.log_softmax.html",
        "api_signature": "oneflow.special.log_softmax(x: oneflow.Tensor, dim: int)",
        "api_description": "Alias for oneflow.nn.functional.log_softmax().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.special.logsumexp",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.special.logsumexp.html",
        "api_signature": "oneflow.special.logsumexp(x: oneflow.Tensor, dim: int, keepdim=False)",
        "api_description": "Alias for oneflow.logsumexp().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.special.round",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.special.round.html",
        "api_signature": "oneflow.special.round(x: oneflow.Tensor)",
        "api_description": "Alias for oneflow.round().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.special.softmax",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.special.softmax.html",
        "api_signature": "oneflow.special.softmax(x: oneflow.Tensor, dim: int)",
        "api_description": "Alias for oneflow.softmax().",
        "return_value": "",
        "parameters": "",
        "input_shape": "",
        "notes": "",
        "code_example": ""
    },
    {
        "api_name": "oneflow.special.zeta",
        "api_url": "https://oneflow.readthedocs.io/en/master/generated/oneflow.special.zeta.html",
        "api_signature": "oneflow.special.zeta(input, other)",
        "api_description": "Computes the Hurwitz zeta function, elementwise.\n\\[\\zeta(x, q) = \\sum_{k=0}^{\\infty} \\frac{1}{(k + q)^x}\\]",
        "return_value": "",
        "parameters": "\ninput (Tensor) – the input tensor corresponding to x.\nother (Tensor) – the input tensor corresponding to q.\n\n\n\n",
        "input_shape": "",
        "notes": "",
        "code_example": ">>> import oneflow as flow\n>>> x = flow.tensor([2., 4.])\n>>> flow.special.zeta(x, 1)\ntensor([1.6449, 1.0823], dtype=oneflow.float32)\n>>> flow.special.zeta(x, flow.tensor([1., 2.]))\ntensor([1.6449, 0.0823], dtype=oneflow.float32)\n>>> flow.special.zeta(2,flow.tensor([1., 2.]))\ntensor([1.6449, 0.6449], dtype=oneflow.float32)\n\n\n\n"
    }
]